{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Ingest\n",
    "\n",
    "ingest is performed by the es.py script in the folder ./elasticsearch/  \n",
    "this script is the last preprocessing steps  \n",
    "\n",
    "several tasks are done here:\n",
    "- evualuate summarizers\n",
    "- calculate word and sentence count\n",
    "- summarize\n",
    "- evaluate Zero-shot-Categorizer\n",
    "- categorize\n",
    "- equalize all different sources into a uniform set of JSON-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook converts the CSV to ES mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some long text\n",
    "# source: https://www.kaggle.com/c/stanford-covid-vaccine\n",
    "text1 = '''\n",
    "Winning the fight against the COVID-19 pandemic will require an effective vaccine that can be equitably and widely distributed. Building upon decades of research has allowed scientists to accelerate the search for a vaccine against COVID-19, but every day that goes by without a vaccine has enormous costs for the world nonetheless. We need new, fresh ideas from all corners of the world. Could online gaming and crowdsourcing help solve a worldwide pandemic? Pairing scientific and crowdsourced intelligence could help computational biochemists make measurable progress.\n",
    "mRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19, but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA). Conventional vaccines (like your seasonal flu shots) are packaged in disposable syringes and shipped under refrigeration around the world, but that is not currently possible for mRNA vaccines.\n",
    "Researchers have observed that RNA molecules have the tendency to spontaneously degrade. This is a serious limitation--a single cut can render the mRNA vaccine useless. Currently, little is known on the details of where in the backbone of a given RNA is most prone to being affected. Without this knowledge, current mRNA vaccines against COVID-19 must be prepared and shipped under intense refrigeration, and are unlikely to reach more than a tiny fraction of human beings on the planet unless they can be stabilized.\n",
    "The Eterna community, led by Professor Rhiju Das, a computational biochemist at Stanford’s School of Medicine, brings together scientists and gamers to solve puzzles and invent medicine. Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles. The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules. The Eterna community has previously unlocked new scientific principles, made new diagnostics against deadly diseases, and engaged the world’s most potent intellectual resources for the betterment of the public. The Eterna community has advanced biotechnology through its contribution in over 20 publications, including advances in RNA biotechnology.\n",
    "In this competition, we are looking to leverage the data science expertise of the Kaggle community to develop models and design rules for RNA degradation. Your model will predict likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position. We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines. These final test sequences are currently being synthesized and experimentally characterized at Stanford University in parallel to your modeling efforts -- Nature will score your models!\n",
    "Improving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve. Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19. The problem we are trying to solve has eluded academic labs, industry R&D groups, and supercomputers, and so we are turning to you. To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic. \n",
    "'''\n",
    "\n",
    "# and a short one\n",
    "text2 = 'The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 564\n",
      "words: 9\n",
      "words: 0\n"
     ]
    }
   ],
   "source": [
    "# function to count words\n",
    "def word_count(text):\n",
    "    if isinstance(text, str):\n",
    "        s = text.split(' ')\n",
    "        return len(s)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print('words:', word_count(text1))\n",
    "print('words:', word_count(text2))\n",
    "print('words:', word_count(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: 20\n",
      "sentences: 1\n",
      "sentences: 0\n"
     ]
    }
   ],
   "source": [
    "# function to count sentences\n",
    "def sentence_count(text):\n",
    "    if isinstance(text, str):\n",
    "        s = text.split('. ')\n",
    "        return len(s)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print('sentences:', sentence_count(text1))\n",
    "print('sentences:', sentence_count(text2))\n",
    "print('sentences:', sentence_count(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 139\n",
      "Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles.\n",
      "The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules.\n",
      "We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines.\n",
      "Improving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve.\n",
      "Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19.\n",
      "To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.\n"
     ]
    }
   ],
   "source": [
    "# text summarization 100% -> n%\n",
    "def nltk_ratio(text, ratio=0.25):\n",
    "    return summarize(text, ratio=ratio)\n",
    "\n",
    "sum_nltk_ratio = nltk_ratio(text1, ratio=0.25)\n",
    "print('words:', word_count(sum_nltk_ratio))\n",
    "print(sum_nltk_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 98\n",
      "Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles.\n",
      "We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines.\n",
      "Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19.\n",
      "To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.\n"
     ]
    }
   ],
   "source": [
    "# text summarization 100% -> n words\n",
    "def nltk_count(text, word_count=100):\n",
    "    return summarize(text, word_count=word_count)\n",
    "\n",
    "sum_nltk_count = nltk_count(text1, word_count=100)\n",
    "print('words:', word_count(sum_nltk_count))\n",
    "print(sum_nltk_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adaptive summarization\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/text-summarization-approaches-nlp-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART\n",
    "# Importing the model\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Loading the model and tokenizer for bart-large-cnn\\ntokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\\nmodel=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\\n#\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Loading the model and tokenizer for bart-large-cnn\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Encoding the inputs and passing them to model.generate()\\ndef bart(text):\\n    inputs = tokenizer.batch_encode_plus([text],return_tensors='pt')\\n    summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\\n\\n    # Decoding and printing the summary\\n    bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\n    \\n    return bart_summary\\n\\n# long text\\nstart = time.time()\\nsum_bart_l = bart(text1)\\nend = time.time()\\n\\nprint('### long text ###')\\nprint('runtime:', end-start)\\nprint('words:', word_count(sum_bart_l))\\nprint('sentences:', sentence_count(sum_bart_l))\\nprint(sum_bart_l)\\nprint('')\\n\\n# short text\\nprint('### short text ###')\\nstart = time.time()\\nsum_bart_s = bart(text2)\\nend = time.time()\\n\\nprint('runtime:', end-start)\\nprint('words:', word_count(sum_bart_s))\\nprint('sentences:', sentence_count(sum_bart_s))\\nprint(sum_bart_s)\\n#\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Encoding the inputs and passing them to model.generate()\n",
    "def bart(text):\n",
    "    inputs = tokenizer.batch_encode_plus([text],return_tensors='pt')\n",
    "    summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
    "\n",
    "    # Decoding and printing the summary\n",
    "    bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return bart_summary\n",
    "\n",
    "# long text\n",
    "start = time.time()\n",
    "sum_bart_l = bart(text1)\n",
    "end = time.time()\n",
    "\n",
    "print('### long text ###')\n",
    "print('runtime:', end-start)\n",
    "print('words:', word_count(sum_bart_l))\n",
    "print('sentences:', sentence_count(sum_bart_l))\n",
    "print(sum_bart_l)\n",
    "print('')\n",
    "\n",
    "# short text\n",
    "print('### short text ###')\n",
    "start = time.time()\n",
    "sum_bart_s = bart(text2)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime:', end-start)\n",
    "print('words:', word_count(sum_bart_s))\n",
    "print('sentences:', sentence_count(sum_bart_s))\n",
    "print(sum_bart_s)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# T5\n",
    "# https://towardsdatascience.com/summarize-reddit-comments-using-t5-bart-gpt-2-xlnet-models-a3e78a5ab944\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# long text\\nstart = time.time()\\nsum_t5_l = t5(text1)\\nend = time.time()\\n\\nprint('### long text ###')\\nprint('runtime:', end-start)\\nprint('words:', word_count(sum_t5_l))\\nprint('sentences:', sentence_count(sum_t5_l))\\nprint(sum_t5_l)\\nprint('')\\n\\n# short text\\nprint('### short text ###')\\nstart = time.time()\\nsum_t5_s = t5(text2)\\nend = time.time()\\n\\nprint('runtime:', end-start)\\nprint('words:', word_count(sum_t5_s))\\nprint('sentences:', sentence_count(sum_t5_s))\\nprint(sum_t5_s)\\n#\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def t5(text):\n",
    "    Preprocessed_text = \"summarize: \" + text\n",
    "    tokens_input = tokenizer.encode(Preprocessed_text,return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(tokens_input, min_length=100, max_length=180, length_penalty=4.0)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "'''\n",
    "# long text\n",
    "start = time.time()\n",
    "sum_t5_l = t5(text1)\n",
    "end = time.time()\n",
    "\n",
    "print('### long text ###')\n",
    "print('runtime:', end-start)\n",
    "print('words:', word_count(sum_t5_l))\n",
    "print('sentences:', sentence_count(sum_t5_l))\n",
    "print(sum_t5_l)\n",
    "print('')\n",
    "\n",
    "# short text\n",
    "print('### short text ###')\n",
    "start = time.time()\n",
    "sum_t5_s = t5(text2)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime:', end-start)\n",
    "print('words:', word_count(sum_t5_s))\n",
    "print('sentences:', sentence_count(sum_t5_s))\n",
    "print(sum_t5_s)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# industry categories\n",
    "\n",
    "# https://www.census.gov/programs-surveys/aces/information/iccl.html\n",
    "cat_sic = ['Agriculture','Forestry','Fishing','Mining','Construction','Manufacturing','Transportation','Communications','Electric','Gas','Sanitary','Wholesale Trade','Retail Trade','Finance','Insurance','Real Estate','Services','Public Administration']\n",
    "# https://www.marketing91.com/19-types-of-business-industries/\n",
    "cat_19 = ['Aerospace','Transport','Computer','Telecommunication','Agriculture','Construction','Education','Pharmaceutical','Food','Health care','Hospitality','Entertainment','News Media','Energy','Manufacturing','Music','Mining','Worldwide web','Electronics']\n",
    "# https://simplicable.com/new/industries\n",
    "cat_simple = ['Advertising','Agriculture','Communication','Construction','Creative','Education','Entertainment','Fashion','Finance','Health care','Information Technology','Manufacturing','Media','Retail','Research','Robotics','Space']\n",
    "\n",
    "cat = ['Accommodation & Food','Accounting','Agriculture','Banking & Insurance','Biotechnological & Life Sciences','Construction & Engineering','Economics','Education & Research','Emergency & Relief','Finance','Government and Public Works','Healthcare','Justice, Law and Regulations','Manufacturing','Media & Publishing','Miscellaneous','Physics','Real Estate, Rental & Leasing','Utilities','Wholesale & Retail']\n",
    "subcat = ['Failure','Food','Fraud','General','Genomics','Insurance and Risk','Judicial Applied','Life-sciences','Machine Learning','Maintenance','Management and Operations','Marketing','Material Science','Physical','Policy and Regulatory','Politics','Preventative and Reactive','Quality','Real Estate','Rental & Leasing','Restaurant','Retail','School','Sequencing','Social Policies','Student','Textual Analysis','Tools','Tourism','Trading & Investment','Transportation','Valuation','Water & Pollution','Wholesale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# zero shot classification\n",
    "# https://towardsdatascience.com/zero-shot-text-classification-with-hugging-face-7f533ba83cd6\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# test classifictaion with nltk (200 words)\\n\\ns = nltk_count(text1, word_count=100)\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_sic = classifier(s, cat_sic, multi_class=True)\\nend = time.time()\\n\\nprint('runtime sic:', end-start)\\n#print(res_sic)\\nprint(res_sic['labels'][0:3])\\nprint(res_sic['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_c19 = classifier(s, cat_19, multi_class=True)\\nend = time.time()\\n\\nprint('runtime c19:', end-start)\\n#print(res_c19)\\nprint(res_c19['labels'][0:3])\\nprint(res_c19['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_simple = classifier(s, cat_simple, multi_class=True)\\nend = time.time()\\n\\nprint('runtime simple:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat)\\nres_simple = classifier(s, cat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime category:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), subcat)\\nres_simple = classifier(s, subcat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime subcategory:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n#\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test classifictaion with nltk (200 words)\n",
    "\n",
    "s = nltk_count(text1, word_count=100)\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_sic = classifier(s, cat_sic, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime sic:', end-start)\n",
    "#print(res_sic)\n",
    "print(res_sic['labels'][0:3])\n",
    "print(res_sic['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_c19 = classifier(s, cat_19, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime c19:', end-start)\n",
    "#print(res_c19)\n",
    "print(res_c19['labels'][0:3])\n",
    "print(res_c19['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_simple = classifier(s, cat_simple, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime simple:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat)\n",
    "res_simple = classifier(s, cat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime category:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), subcat)\n",
    "res_simple = classifier(s, subcat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime subcategory:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# test classifictaion with t5\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_sic = classifier(sum_t5_l, cat_sic, multi_class=True)\\nend = time.time()\\n\\nprint('runtime sic:', end-start)\\n#print(res_sic)\\nprint(res_sic['labels'][0:3])\\nprint(res_sic['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_c19 = classifier(sum_t5_l, cat_19, multi_class=True)\\nend = time.time()\\n\\nprint('runtime c19:', end-start)\\n#print(res_c19)\\nprint(res_c19['labels'][0:3])\\nprint(res_c19['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_simple = classifier(sum_t5_l, cat_simple, multi_class=True)\\nend = time.time()\\n\\nprint('runtime simple:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat)\\nres_simple = classifier(sum_t5_l, cat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime category:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), subcat)\\nres_simple = classifier(sum_t5_l, subcat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime subcategory:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n#\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test classifictaion with t5\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_sic = classifier(sum_t5_l, cat_sic, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime sic:', end-start)\n",
    "#print(res_sic)\n",
    "print(res_sic['labels'][0:3])\n",
    "print(res_sic['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_c19 = classifier(sum_t5_l, cat_19, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime c19:', end-start)\n",
    "#print(res_c19)\n",
    "print(res_c19['labels'][0:3])\n",
    "print(res_c19['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_simple = classifier(sum_t5_l, cat_simple, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime simple:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat)\n",
    "res_simple = classifier(sum_t5_l, cat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime category:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), subcat)\n",
    "res_simple = classifier(sum_t5_l, subcat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime subcategory:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# test classifictaion with bart\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_sic = classifier(sum_bart_l, cat_sic, multi_class=True)\\nend = time.time()\\n\\nprint('runtime sic:', end-start)\\n#print(res_sic)\\nprint(res_sic['labels'][0:3])\\nprint(res_sic['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_c19 = classifier(sum_bart_l, cat_19, multi_class=True)\\nend = time.time()\\n\\nprint('runtime c19:', end-start)\\n#print(res_c19)\\nprint(res_c19['labels'][0:3])\\nprint(res_c19['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_simple = classifier(sum_bart_l, cat_simple, multi_class=True)\\nend = time.time()\\n\\nprint('runtime simple:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat)\\nres_simple = classifier(sum_bart_l, cat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime category:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), subcat)\\nres_simple = classifier(sum_bart_l, subcat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime subcategory:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n#\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test classifictaion with bart\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_sic = classifier(sum_bart_l, cat_sic, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime sic:', end-start)\n",
    "#print(res_sic)\n",
    "print(res_sic['labels'][0:3])\n",
    "print(res_sic['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_c19 = classifier(sum_bart_l, cat_19, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime c19:', end-start)\n",
    "#print(res_c19)\n",
    "print(res_c19['labels'][0:3])\n",
    "print(res_c19['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_simple = classifier(sum_bart_l, cat_simple, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime simple:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat)\n",
    "res_simple = classifier(sum_bart_l, cat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime category:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), subcat)\n",
    "res_simple = classifier(sum_bart_l, subcat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime subcategory:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Biotechnological & Life Sciences': 0.8398016095161438, 'Healthcare': 0.7811025977134705, 'Education & Research': 0.7176370620727539, 'Utilities': 0.6823796629905701}\n",
      "{'category': None, 'score': None}\n",
      "{'Biotechnological & Life Sciences': 0.8398016095161438, 'Healthcare': 0.7811025977134705, 'Education & Research': 0.7176370620727539, 'Utilities': 0.6823796629905701, 'runtime': 16.264}\n",
      "{'category': None, 'score': None, 'runtime': 23.03}\n"
     ]
    }
   ],
   "source": [
    "# category function\n",
    "def categorize(text, categories, first=True, treshold=0, runtime=False):\n",
    "    start = time.time()\n",
    "    res = classifier(text, categories, multi_class=True)\n",
    "    #print(res)\n",
    "    end = time.time()\n",
    "    dur = round(end-start, 3)\n",
    "    if first == True:\n",
    "        ret = {\n",
    "            'category': res['labels'][0],\n",
    "            'score': res['scores'][0],\n",
    "        } if res['scores'][0] >= treshold else {\n",
    "            'category': None,\n",
    "            'score': None,\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        ret = dict(zip(res['labels'], res['scores']))\n",
    "        ret = {key: val for key, val in filter(lambda sub: sub[1] >= treshold, ret.items())}\n",
    "        \n",
    "    if runtime == True:\n",
    "        ret['runtime'] = dur\n",
    "    return ret\n",
    "        \n",
    "\n",
    "print(categorize(nltk_count(text1), cat, first=False, treshold=0.5))\n",
    "print(categorize(nltk_count(text1), cat, first=True, treshold=0.9))\n",
    "print(categorize(nltk_count(text1), cat, first=False, treshold=0.5, runtime=True))\n",
    "print(categorize(nltk_count(text1), cat, first=True, treshold=0.9, runtime=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# measure error treshold\\ncsv_in = '../data/database/db_04_analyzed_v02.csv'\\ncsv_out = '../data/database/categorizer.csv'\\ndf = pd.read_csv(csv_in, sep=';')\\nprint(df.shape)\\n\\ndf_out = []\\nquit = 0\\nmatch_c = sim_c = match_sc = sim_sc = 0\\n\\nstart = time.time()\\nfor index, row in df.iterrows():\\n    print('###')\\n    print(index, row['link'])\\n    c = row['industry']\\n    sc = row['type']\\n    d = row['description']\\n    item = {\\n        'link': row['link'],\\n        'category': c,\\n        'subcategory': sc,\\n    }\\n    \\n    print('category:', c)\\n    try:\\n        c_guess = categorize(d, cat, first=False, treshold=0.25)\\n    except:\\n        c_guess = {}\\n    print('guess:', c_guess)\\n    item['category_guess'] = json.dumps(c_guess)\\n    item['category_match'] = False\\n    item['category_similar'] = False\\n    if len(c_guess) > 0:\\n        keys = list(c_guess.keys())\\n        if c == keys[0]:\\n            print('MATCH')\\n            item['category_match'] = True\\n            match_c += 1\\n        elif c in keys:\\n            print('SIMILAR')\\n            item['category_similar'] = True\\n            sim_c += 1\\n    \\n    print('subcategory:', sc)\\n    try:\\n        sc_guess = categorize(d, subcat, first=False, treshold=0.25)\\n    except:\\n        sc_guess = {}\\n    print('guess:', sc_guess)\\n    item['subcategory_guess'] = json.dumps(sc_guess)\\n    item['subcategory_match'] = False\\n    item['subcategory_similar'] = False\\n    if len(sc_guess) > 0:\\n        keys = list(sc_guess.keys())\\n        if sc == keys[0]:\\n            print('MATCH')\\n            item['subcategory_match'] = True\\n            match_sc += 1\\n        elif sc in keys:\\n            print('SIMILAR')\\n            item['subcategory_similar'] = True\\n            sim_sc += 1\\n    \\n    df_out.append(item)\\n    \\n    if quit != 0 and index+1 >= quit:\\n        break\\nend = time.time()\\n\\ndf_out = pd.DataFrame(df_out)\\ndf_out.to_csv(csv_out, sep=';', index=False)\\nprint('done in', round(end-start, 3), 'sec')\\nprint(index+1, match_c, sim_c, match_sc, sim_sc)\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# measure error treshold\n",
    "csv_in = '../data/database/db_04_analyzed_v02.csv'\n",
    "csv_out = '../data/database/categorizer.csv'\n",
    "df = pd.read_csv(csv_in, sep=';')\n",
    "print(df.shape)\n",
    "\n",
    "df_out = []\n",
    "quit = 0\n",
    "match_c = sim_c = match_sc = sim_sc = 0\n",
    "\n",
    "start = time.time()\n",
    "for index, row in df.iterrows():\n",
    "    print('###')\n",
    "    print(index, row['link'])\n",
    "    c = row['industry']\n",
    "    sc = row['type']\n",
    "    d = row['description']\n",
    "    item = {\n",
    "        'link': row['link'],\n",
    "        'category': c,\n",
    "        'subcategory': sc,\n",
    "    }\n",
    "    \n",
    "    print('category:', c)\n",
    "    try:\n",
    "        c_guess = categorize(d, cat, first=False, treshold=0.25)\n",
    "    except:\n",
    "        c_guess = {}\n",
    "    print('guess:', c_guess)\n",
    "    item['category_guess'] = json.dumps(c_guess)\n",
    "    item['category_match'] = False\n",
    "    item['category_similar'] = False\n",
    "    if len(c_guess) > 0:\n",
    "        keys = list(c_guess.keys())\n",
    "        if c == keys[0]:\n",
    "            print('MATCH')\n",
    "            item['category_match'] = True\n",
    "            match_c += 1\n",
    "        elif c in keys:\n",
    "            print('SIMILAR')\n",
    "            item['category_similar'] = True\n",
    "            sim_c += 1\n",
    "    \n",
    "    print('subcategory:', sc)\n",
    "    try:\n",
    "        sc_guess = categorize(d, subcat, first=False, treshold=0.25)\n",
    "    except:\n",
    "        sc_guess = {}\n",
    "    print('guess:', sc_guess)\n",
    "    item['subcategory_guess'] = json.dumps(sc_guess)\n",
    "    item['subcategory_match'] = False\n",
    "    item['subcategory_similar'] = False\n",
    "    if len(sc_guess) > 0:\n",
    "        keys = list(sc_guess.keys())\n",
    "        if sc == keys[0]:\n",
    "            print('MATCH')\n",
    "            item['subcategory_match'] = True\n",
    "            match_sc += 1\n",
    "        elif sc in keys:\n",
    "            print('SIMILAR')\n",
    "            item['subcategory_similar'] = True\n",
    "            sim_sc += 1\n",
    "    \n",
    "    df_out.append(item)\n",
    "    \n",
    "    if quit != 0 and index+1 >= quit:\n",
    "        break\n",
    "end = time.time()\n",
    "\n",
    "df_out = pd.DataFrame(df_out)\n",
    "df_out.to_csv(csv_out, sep=';', index=False)\n",
    "print('done in', round(end-start, 3), 'sec')\n",
    "print(index+1, match_c, sim_c, match_sc, sim_sc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### language detection & helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 'sw', 'language': 'swahili', 'probability': '0.9999971210408874'}\n",
      "de\n",
      "en\n",
      "en\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# language detection\n",
    "# https://towardsdatascience.com/how-to-detect-and-translate-languages-for-nlp-project-dfd52af0c3b5\n",
    "from langdetect import detect, detect_langs, DetectorFactory\n",
    "\n",
    "language_codes = {'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu', 'fil': 'Filipino', 'he': 'Hebrew'}\n",
    "\n",
    "def lingo(text, simple=True):\n",
    "    DetectorFactory.seed = 0\n",
    "    try:\n",
    "        if simple == True:\n",
    "            return detect(text) #language_codes[detect(text)]\n",
    "        else:\n",
    "            l = str(detect_langs(text)[0]).split(':')\n",
    "            l = {\n",
    "                'code': l[0],\n",
    "                'language': language_codes[ l[0] ],\n",
    "                'probability': l[1],\n",
    "            }\n",
    "            return l\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "sentence = \"Tanzania ni nchi inayoongoza kwa utalii barani afrika\"\n",
    "sentence2 = \"Heute schneit es.\"\n",
    "\n",
    "print(lingo(sentence, simple=False))\n",
    "print(lingo(sentence2))\n",
    "print(lingo(text1))\n",
    "print(lingo(text2))\n",
    "print(lingo(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to rebuild list from string\n",
    "# that happens when it is stored in CSV without json-encode the data\n",
    "def str_to_list(s):\n",
    "    s = s.replace(\"'\", \"\").replace(' ,', ',').replace(\n",
    "        '[', '').replace(']', '').split(',')\n",
    "    s = [i.replace('\"','').strip() for i in s if i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create folder create_folder\n",
    "def create_folder(path):\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path))\n",
    "            print(path + ' created')\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic store data to file function\n",
    "def store_data(data, file, mode='w', toJson=False):\n",
    "    if toJson:\n",
    "        data = json.dumps(data)\n",
    "    with open(file, mode, encoding='utf-8') as fp:\n",
    "        result = fp.write(data)\n",
    "        return result\n",
    "    \n",
    "# generic load data from file function\n",
    "def load_data(file, fromJson=False):\n",
    "    if os.path.isfile(file):\n",
    "        with open(file, 'r', encoding='utf-8', errors=\"ignore\") as fp:\n",
    "            data = fp.read()\n",
    "            if fromJson:\n",
    "                data = json.loads(data)\n",
    "            return data\n",
    "    else:\n",
    "        return 'file not found'\n",
    "\n",
    "# test text\n",
    "#print(store_data('Hello', '../data/repositories/mlart/test.txt'))\n",
    "#print(load_data('../data/repositories/mlart/test.txt'))\n",
    "\n",
    "# test json\n",
    "#print(store_data({'msg':'Hello World'}, '../data/repositories/mlart/test.json', toJson=True))\n",
    "#print(load_data('../data/repositories/mlart/test.json', fromJson=True))\n",
    "\n",
    "#store_data(result[0]['html'], '../data/repositories/kaggle/notebook.html')\n",
    "#store_data(result[0]['iframe'], '../data/repositories/kaggle/kernel.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "def clean_text(text):\n",
    "    # Ref: https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1#gistcomment-3208085\n",
    "    # Ref: https://en.wikipedia.org/wiki/Unicode_block\n",
    "    EMOJI_PATTERN = re.compile(\n",
    "        \"([\"\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"])\"\n",
    "    )\n",
    "    text = re.sub(EMOJI_PATTERN, '', text)\n",
    "    \n",
    "    # additional cleanup\n",
    "    text = text.replace('•','').replace('\\n',' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RNN']\n"
     ]
    }
   ],
   "source": [
    "tag_filter_v01 = load_data('../data/patterns/tag_filter_v01.json', fromJson=True)\n",
    "tag_filter_v01 = {k:v for k, v in tag_filter_v01.items() if not tag_filter_v01[k] == ''}\n",
    "tag_filter_v01 = {k:v if v != 'null' else None for k, v in tag_filter_v01.items()}\n",
    "#print(json.dumps(tag_filter_v01, indent=2))\n",
    "\n",
    "tag_filter_v02 = load_data('../data/patterns/tag_filter_v02.json', fromJson=True)\n",
    "tag_filter_v02 = {k:v for k, v in tag_filter_v02.items() if not tag_filter_v02[k] == ''}\n",
    "tag_filter_v02 = {k:v if v != 'null' else None for k, v in tag_filter_v02.items()}\n",
    "#print(json.dumps(tag_filter_v01, indent=2))\n",
    "\n",
    "def tag_equalizer(tags, pattern=tag_filter_v01):\n",
    "    tags = [pattern.get(x, x) for x in tags]\n",
    "    tags = list(filter(None, tags))\n",
    "    return tags\n",
    "\n",
    "print(tag_equalizer(['tpu', 'rnn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer\n",
    "ADDITIONAL_STOPWORDS = []\n",
    "\n",
    "def lemmatizer(text):\n",
    "    \"\"\"\n",
    "    A simple function to clean up the data. All the words that\n",
    "    are not designated as a stop word is then lemmatized after\n",
    "    encoding and basic regex parsing are performed.\n",
    "    \"\"\"\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore')\n",
    "    .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper\n",
    "\n",
    "mapping to equalize the different datasets into a uniform shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper to convert CSV to the mapping of Elasticsearch index\n",
    "def mapper(row, style, extra={}):\n",
    "    '''\n",
    "    mapper to adopt csv to db-schema\n",
    "\n",
    "    \"title\"\n",
    "    \"summarization\"\n",
    "    \"words\"\n",
    "    \"sum_words\"\n",
    "    \"link\"\n",
    "    \"source\"\n",
    "    \"category\"\n",
    "    \"category_score\"\n",
    "    \"subcategory\"\n",
    "    \"subcategory_score\"\n",
    "    \"tags\"\n",
    "    \"kind\"\n",
    "    \"ml_libs\"\n",
    "    \"host\"\n",
    "    \"license\"\n",
    "    \"programming_language\"\n",
    "    \"ml_score\"\n",
    "    \"learn_score\"\n",
    "    \"explore_score\"\n",
    "    \"compete_score\"\n",
    "    \"engagement_score\"\n",
    "    \"date_project\"\n",
    "    \"date_scraped\"\n",
    "    '''\n",
    "\n",
    "    # kaggle competition mapping\n",
    "    if style == 'kaggle_competition':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            'description': row['subtitle'] + row['description'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': list(set(str_to_list(row['tags']) + str_to_list(row['type']))),\n",
    "            'kind': ['Project', '(Competition)', '(Dataset)'],\n",
    "            # 'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'kaggle.com',\n",
    "            # 'license': row['license'],\n",
    "            # 'programming_language': row['type'],\n",
    "            # 'ml_score': 0,\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "            'engagement_score': row['teams_score'],\n",
    "            'date_project': datetime.strptime(row['date_closed'], \"%Y-%m-%d %H:%M:%S\") if 'date_closed' in row else '',\n",
    "            # 'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            # 'ml_terms': row['ml_terms'],\n",
    "            # 'score_raw': json.dumps({'views': row['views'], 'votes': row['votes'], 'score_private': row['score_private'], 'score_public': row['score_public']}),\n",
    "        }\n",
    "    \n",
    "    # kaggle dataset mapping\n",
    "    if style == 'kaggle_dataset':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            'description': row['subtitle'] + row['description'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': list(set(str_to_list(row['tags']) + str_to_list(row['type']))),\n",
    "            'kind': ['Project', '(Dataset)'],\n",
    "            # 'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'kaggle.com',\n",
    "            # 'license': row['license'],\n",
    "            # 'programming_language': row['type'],\n",
    "            # 'ml_score': 0,\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "            'engagement_score': row['teams_score'],\n",
    "            'date_project': datetime.strptime(row['date_closed'], \"%Y-%m-%d %H:%M:%S\") if 'date_closed' in row else '',\n",
    "            # 'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            # 'ml_terms': row['ml_terms'],\n",
    "            # 'score_raw': json.dumps({'views': row['views'], 'votes': row['votes'], 'score_private': row['score_private'], 'score_public': row['score_public']}),\n",
    "        }\n",
    "    \n",
    "    # kaggle notebook mapping\n",
    "    if style == 'kaggle_notebook':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            'description': row['description'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': list(set(str_to_list(row['tags']) + str_to_list(row['tags']))),\n",
    "            'kind': ['Project', '(Notebook)'],\n",
    "            'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'kaggle.com',\n",
    "            'license': row['license'],\n",
    "            'programming_language': row['type'],\n",
    "            'ml_score': row['ml_detected'],\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "            'engagement_score': row['score_views'] if 'score_views' in row else None,\n",
    "            'date_project': datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S\") if row['date'] != '' else None,\n",
    "            'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\") if row['scraped_at'] != '' else None,\n",
    "            # 'ml_terms': row['ml_terms'],\n",
    "            # 'score_raw': json.dumps({'views': row['views'], 'votes': row['votes'], 'score_private': row['score_private'], 'score_public': row['score_public']}),\n",
    "        }\n",
    "\n",
    "    # github mapping\n",
    "    if style == 'github':\n",
    "        title = row['name'] if row['name'] != '' else row['title']\n",
    "        title = title.replace('-',' ').replace('_',' ').strip()\n",
    "        cat_score = 1 if row['industry'] != '' else 0\n",
    "        subcat_score = 1 if row['type'] != '' else 0\n",
    "        #tags = row['ml_tags'] if len(row['ml_tags']) > 0 else ''\n",
    "        ret = {\n",
    "            'title': title,\n",
    "            'description': row['description2'],\n",
    "            'link': row['link'],\n",
    "            'category': row['industry'],\n",
    "            'category_score': cat_score,\n",
    "            'subcategory': row['type'],\n",
    "            'subcategory_score': subcat_score,\n",
    "            'tags': str_to_list(row['ml_tags']),\n",
    "            'kind': 'Project',\n",
    "            'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'github.com',\n",
    "            'license': row['license'],\n",
    "            'programming_language': row['language_primary'],\n",
    "            'ml_score': row['ml_detected'],\n",
    "            'engagement_score': row['stars_score'],\n",
    "            'date_project': datetime.strptime(row['pushed_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            # 'ml_terms': row['keywords'],\n",
    "            # 'score_raw': json.dumps({'stars': row['stars'], 'contributors': row['contributors']}),\n",
    "        }\n",
    "\n",
    "    # mlart mapping\n",
    "    if style == 'mlart':\n",
    "        title = row['Title'] if row['Title'] != '' else row['title']\n",
    "        cat_score = 1 if row['Theme'] != '' else 0\n",
    "        subcat_score = 1 if row['Medium'] != '' else 0\n",
    "        ret = {\n",
    "            'title': title,\n",
    "            'description': row['subtitle'],\n",
    "            'link': row['url'],\n",
    "            'category': 'Miscellaneous',\n",
    "            'category_score': cat_score,\n",
    "            'subcategory': 'Art',\n",
    "            'subcategory_score': subcat_score,\n",
    "            'tags': str_to_list(row['Theme']) + str_to_list(row['Medium']) + str_to_list(row['Technology']),\n",
    "            'kind': 'Showcase',\n",
    "            # 'ml_libs': [],\n",
    "            'host': 'mlart.co',\n",
    "            # 'license': '',\n",
    "            # 'programming_language': '',\n",
    "            # 'ml_score': row['ml_detected'],\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 1,\n",
    "            'compete_score': 0,\n",
    "            # 'engagement_score': 0,\n",
    "            'date_project': datetime.strptime(row['Date'], \"%Y-%m-%d\"),\n",
    "            'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            # 'score_raw': json.dumps({'days_since_featured': row['Days Since Featured']}),\n",
    "        }\n",
    "\n",
    "    # thecleverprogrammer\n",
    "    if style == 'tcp':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            'description': row['description'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': str_to_list(row['ml_tags']),\n",
    "            'kind': 'Project',\n",
    "            'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'thecleverprogrammer.com',\n",
    "            # 'license': '',\n",
    "            'programming_language': 'Python',\n",
    "            'ml_score': row['ml_score'],\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "            # 'engagement_score': 0,\n",
    "            'date_project': datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            'date_scraped': datetime.strptime('2020-12-20', \"%Y-%m-%d\"),\n",
    "            # 'score_raw': json.dumps({'days_since_featured': row['Days Since Featured']}),\n",
    "        }\n",
    "        \n",
    "    # twominutepapers\n",
    "    if style == 'tmp':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            'description': row['description'],\n",
    "            'link': row['url'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': str_to_list(row['tags']),\n",
    "            'kind': 'Paper',\n",
    "            #'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'youtube.com',\n",
    "            # 'license': '',\n",
    "            #'programming_language': 'Python',\n",
    "            #'ml_score': row['ml_score'],\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 1,\n",
    "            'compete_score': 0,\n",
    "            'engagement_score': (float(row['score_likes']) + float(row['score_views'])) /2,\n",
    "            'date_project': datetime.strptime(row['date'], \"%Y-%m-%d\"),\n",
    "            'date_scraped': datetime.strptime(row['date_scraped'], \"%Y-%m-%d\"),\n",
    "            # 'score_raw': json.dumps({'days_since_featured': row['Days Since Featured']}),\n",
    "        }\n",
    "    \n",
    "    # zalando / bcgdv / medium\n",
    "    if style == 'manual':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            # 'description': row['description'] if row['description'] != '' else row['text'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': str_to_list(row['tags']),\n",
    "            # 'kind': 'Project',\n",
    "            # 'ml_libs': str_to_list(row['ml_libs']),\n",
    "            # 'host': 'thecleverprogrammer.com',\n",
    "            # 'license': '',\n",
    "            # 'programming_language': 'Python',\n",
    "            # 'ml_score': row['ml_score'],\n",
    "            # 'engagement_score': 0,\n",
    "            # 'date_project': datetime.strptime(row['date'], \"%d.%m.%Y\"),\n",
    "            # 'date_scraped': datetime.strptime(row['date_scraped'], \"%d.%m.%Y\"),\n",
    "            # 'score_raw': json.dumps({'days_since_featured': row['Days Since Featured']}),\n",
    "        }\n",
    "        if ret['title'] == '' and 'company' in row:\n",
    "            ret['title'] = row['company']\n",
    "            \n",
    "        if 'description' in row and row['description'] != '':\n",
    "            ret['description'] = row['description']\n",
    "        else:\n",
    "            ret['description'] = row['text']\n",
    "            \n",
    "        if 'source' in row:\n",
    "            ret['source'] = row['source']\n",
    "            \n",
    "        if 'category' in row:\n",
    "            ret['category'] = row['category']\n",
    "            if 'category_score' in row:\n",
    "                ret['category_score'] = row['category_score']\n",
    "            elif ret['category'] != '':\n",
    "                ret['category_score'] = 1\n",
    "        else:\n",
    "            ret['category'] = ''\n",
    "            \n",
    "        if 'subcategory' in row:\n",
    "            ret['subcategory']= row['subcategory']\n",
    "            if 'subcategory_score' in row:\n",
    "                ret['subcategory_score'] = row['subcategory_score']\n",
    "            elif ret['subcategory'] != '':\n",
    "                ret['subcategory_score'] = 1\n",
    "        else:\n",
    "            ret['subcategory'] = ''\n",
    "            \n",
    "        if 'date' in row and row['date'] != '':\n",
    "            try:\n",
    "                ret['date_project'] = datetime.strptime(row['date'], \"%d.%m.%Y\")\n",
    "            except:\n",
    "                try:\n",
    "                    ret['date_project'] = datetime.strptime(row['date'], \"%Y\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "        if 'date_scraped' in row:\n",
    "            row['date_scraped'] = datetime.strptime(row['date_scraped'], \"%d.%m.%Y\")\n",
    "        \n",
    "        if 'ml_score' in row:\n",
    "            ret['ml_score'] = row['ml_score']\n",
    "            \n",
    "        if 'learn_score' in row:\n",
    "            ret['learn_score'] = row['learn_score']\n",
    "            \n",
    "        if 'explore_score' in row:\n",
    "            ret['explore_score'] = row['explore_score']\n",
    "            \n",
    "        if 'compete_score' in row:\n",
    "            ret['compete_score'] = row['compete_score']\n",
    "        \n",
    "    attach = {**extra}\n",
    "    if 'tags' in attach:\n",
    "        ret['tags'].extend(attach['tags'])\n",
    "        attach.pop('tags')\n",
    "    ret.update(attach)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test gpu usage\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main transform loop\n",
    "\n",
    "this loop has different tasks:\n",
    "- utilize mapper to equalize data structure\n",
    "- perform all final preprocess steps\n",
    "- summarization\n",
    "- lemmatization\n",
    "- text cleaning\n",
    "- language detection\n",
    "- saving JSON representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to transform data row-wise\n",
    "def transform_loop(csv_in, csv_format, subfolder, quit=0, overwrite=False, inplace=True, printItem=False, extra={}):\n",
    "    \n",
    "    with open(csv_in, encoding='utf-8') as csvfile:\n",
    "        \n",
    "        # let's store converted csv to temp-folder for analysis\n",
    "        csv_out = '../data/database/csv/'\n",
    "        json_out = '../data/database/json/'\n",
    "        json_out_item = '../data/database/json/'+subfolder\n",
    "        create_folder(json_out_item)\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        # readCSV = csv.reader(csvfile, delimiter=';')\n",
    "        readCSV = csv.DictReader(csvfile, delimiter=';')\n",
    "        # next(readCSV, None)  # skip the headers\n",
    "        \n",
    "        i = j = 0\n",
    "        out = []\n",
    "        \n",
    "        for row in readCSV:\n",
    "            row = mapper(row, csv_format, extra=extra)\n",
    "            if printItem == True:\n",
    "                print(json.dumps(row, indent=3, sort_keys=True, default=str))\n",
    "            \n",
    "            # check if file already exists\n",
    "            link = row['link']\n",
    "            md5 = hashlib.md5(link.encode(\"utf-8\")).hexdigest()\n",
    "            \n",
    "            json_fp = json_out_item + md5 + '.json'\n",
    "            \n",
    "            old = {}\n",
    "            if os.path.isfile(json_fp) and overwrite == True or not os.path.isfile(json_fp):\n",
    "                if os.path.isfile(json_fp):\n",
    "                    old = load_data(json_fp, fromJson=True)\n",
    "                \n",
    "                print(i, row['link'])\n",
    "                item_start = time.time()\n",
    "\n",
    "                # clean title & description\n",
    "                row['title'] = clean_text(row['title'])\n",
    "                text = row['description'] = clean_text(row['description'])\n",
    "                words = row['words'] = word_count(text)\n",
    "                sentences = row['sentences'] = sentence_count(text)\n",
    "\n",
    "                # create summarization\n",
    "                if words > 200 and sentences > 1 and (not 'sum_nltk' in old or not 'sum_t5' in old):\n",
    "                    print('summarize')\n",
    "                    \n",
    "                    # nltk\n",
    "                    if not 'sum_nltk' in old:\n",
    "                        start = time.time()\n",
    "                        row['sum_nltk'] = nltk_count(text, word_count=200)\n",
    "                        end = time.time()\n",
    "                        dur = round(end-start,3)\n",
    "\n",
    "                        row['sum_nltk_words'] = word_count(row['sum_nltk'])\n",
    "                        row['sum_nltk_runtime'] = dur\n",
    "                        print('done (nltk)', dur, 'sec')\n",
    "                    \n",
    "                    # t5\n",
    "                    if not 'sum_t5' in old:\n",
    "                        start = time.time()\n",
    "                        row['sum_t5'] = t5(text)\n",
    "                        end = time.time()\n",
    "                        dur = round(end-start,3)\n",
    "\n",
    "                        row['sum_t5_words'] = word_count(row['sum_t5'])\n",
    "                        row['sum_t5_runtime'] = dur\n",
    "                        print('done (t5)', dur, 'sec')\n",
    "                \n",
    "                # detect language\n",
    "                if not 'language_code' in old:\n",
    "                    s = row['description'] if 'description' in row and row['description'] != '' else row['title']\n",
    "                    lang = lingo(s, simple=False)\n",
    "                    if lang != None:\n",
    "                        row['language_code'] = lang['code']\n",
    "                        row['language'] = lang['language']\n",
    "                        row['language_score'] = lang['probability']\n",
    "                    else:\n",
    "                        row['language_code'] = None\n",
    "                        row['language'] = None\n",
    "                        row['language_score'] = None\n",
    "\n",
    "                # equalizer\n",
    "                if 'programming_language' in row and row['programming_language'] == 'Python notebook':\n",
    "                    row['programming_language'] = 'Jupyter Notebook'\n",
    "                    \n",
    "                if 'license' in row:\n",
    "                    if row['license'] == 'Apache 2.0':\n",
    "                        row['license'] = 'Apache-2.0'\n",
    "                    if row['license'] == 'Learn more about GitHub Sponsors':\n",
    "                        row['license'] = None\n",
    "                    if row['license'] == 'Unlicense':\n",
    "                        row['license'] = None\n",
    "                        \n",
    "                row['tags'] = tag_equalizer(row['tags'], pattern=tag_filter_v01)\n",
    "                row['tags_descriptive'] = tag_equalizer(row['tags'], pattern=tag_filter_v02)\n",
    "                \n",
    "                # lemmatizer\n",
    "                row['description_lemmatized'] = ' '.join(lemmatizer(row['description']))\n",
    "                if 'sum_nltk' in row:\n",
    "                    row['sum_nltk_lemmatized'] = ' '.join(lemmatizer(row['sum_nltk']))\n",
    "                if 'summarization' in row:\n",
    "                    row['summarization_lemmatized'] = ' '.join(lemmatizer(row['summarization']))\n",
    "                \n",
    "\n",
    "                # convert datetime to string\n",
    "                if 'date_project' in row:\n",
    "                    row['date_project'] = str(row['date_project'])\n",
    "                if 'date_scraped' in row:\n",
    "                    row['date_scraped'] = str(row['date_scraped'])\n",
    "                    \n",
    "                # runtime\n",
    "                item_end = time.time()\n",
    "                item_dur = round(item_end-item_start, 3)\n",
    "                row['runtime'] = item_dur\n",
    "\n",
    "                #df = df.append(row, ignore_index=True)\n",
    "\n",
    "                # json encode\n",
    "                #out.append(row)\n",
    "                \n",
    "                if overwrite == True and inplace==True:\n",
    "                    row = {**old, **row}\n",
    "                    drop = ['score']\n",
    "                    for key in drop:\n",
    "                        if key in row:\n",
    "                            row.pop(key)\n",
    "                    # restore category, subcategory and runtime\n",
    "                    if 'category' in row and row['category'] == '' and 'category' in old:\n",
    "                        row['category'] = old['category']\n",
    "                    if 'category_score' in row and row['category_score'] == '' and 'category_score' in old:\n",
    "                        row['category_score'] = old['category_score']\n",
    "                    if 'subcategory' in row and row['subcategory'] == '' and 'subcategory' in old:\n",
    "                        row['subcategory'] = old['subcategory']\n",
    "                    if 'subcategory_score' in row and row['subcategory_score'] == '' and 'subcategory_score' in old:\n",
    "                        row['subcategory_score'] = old['subcategory_score']\n",
    "                    if 'runtime' in row and row['runtime'] == '' and 'runtime' in old:\n",
    "                        row['runtime'] = old['runtime']\n",
    "                            \n",
    "                #print(row)\n",
    "                #sys.exit()\n",
    "                \n",
    "                if row != old:\n",
    "                    store_data(row, json_fp, toJson=True)\n",
    "                    print('stored:', json_fp)\n",
    "                j += 1\n",
    "\n",
    "            #print(i, row['link'])\n",
    "            i += 1\n",
    "\n",
    "            # keep count of # rows processed\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "\n",
    "            if quit != 0 and i >= quit:\n",
    "                break\n",
    "\n",
    "        # store parsed csv\n",
    "        #fp = csv_in.split('/')[-1]\n",
    "        #df.to_csv(csv_out + fp, sep=';', index=False)\n",
    "        #path = json_out + fp\n",
    "        #path = path.replace('.csv', '.json')\n",
    "        #store_data(out, path, toJson=True)\n",
    "        \n",
    "        print('DONE parsed', i, 'items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the loop\n",
    "\n",
    "for different data sets and apply mapping rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp\n",
      "0 https://www.youtube.com/watch?v=-all65C-dh0\n",
      "stored: ../data/database/json/tmp/01a20cc93e10a44f59b663dccbb52591.json\n",
      "1 https://www.youtube.com/watch?v=-dbkE4FFPrI\n",
      "stored: ../data/database/json/tmp/a8314cdfdbb0b71debb6f9909bd589cb.json\n",
      "2 https://www.youtube.com/watch?v=-R9bJGNHltQ\n",
      "summarize\n",
      "done (nltk) 0.011 sec\n",
      "done (t5) 9.293 sec\n",
      "stored: ../data/database/json/tmp/e3368bd51757e525dbabbb1517209ee6.json\n",
      "3 https://www.youtube.com/watch?v=-rf_MDh-FiE\n",
      "stored: ../data/database/json/tmp/3e4dceb55538c999687e39ca43d2f9de.json\n",
      "4 https://www.youtube.com/watch?v=08V_F19HUfI\n",
      "summarize\n",
      "done (nltk) 0.009 sec\n",
      "done (t5) 8.792 sec\n",
      "stored: ../data/database/json/tmp/99cad7783108159accae21a068d6ee2a.json\n",
      "5 https://www.youtube.com/watch?v=14zkfDTN_qo\n",
      "stored: ../data/database/json/tmp/152c157f8744f110aa9cb0d675b5edc6.json\n",
      "6 https://www.youtube.com/watch?v=1aVSb-UbYWc\n",
      "stored: ../data/database/json/tmp/0a0adcd7ab573bc4b1c1d10634c15803.json\n",
      "7 https://www.youtube.com/watch?v=1PNhuHa7lS0\n",
      "stored: ../data/database/json/tmp/7e55362b2f611cf8a565fdf4f68179d2.json\n",
      "8 https://www.youtube.com/watch?v=1SHW1-qKKpY\n",
      "stored: ../data/database/json/tmp/df1331e7e111771182a29ebfccb169ab.json\n",
      "9 https://www.youtube.com/watch?v=1U3YKnuMS7g\n",
      "stored: ../data/database/json/tmp/e72af9b0f65a70f9c8ebdb8a6b513801.json\n",
      "10 https://www.youtube.com/watch?v=1zvohULpe_0\n",
      "stored: ../data/database/json/tmp/7239713d26ca07fb71ebebf32a2053cf.json\n",
      "11 https://www.youtube.com/watch?v=2ciR6rA85tg\n",
      "stored: ../data/database/json/tmp/3e69ee0dfa5a79a9c300bfb531107d2c.json\n",
      "12 https://www.youtube.com/watch?v=2FHHuRTkr_Y\n",
      "stored: ../data/database/json/tmp/8e95e071ebfe019f1c12d5e4dae9331a.json\n",
      "13 https://www.youtube.com/watch?v=2kOCTf8jIik\n",
      "stored: ../data/database/json/tmp/985d7e549abd8dfca2b02ba8e93083a7.json\n",
      "14 https://www.youtube.com/watch?v=2vnLBb18MuQ\n",
      "stored: ../data/database/json/tmp/43571164230bb77e00c8898c7ac30f6c.json\n",
      "15 https://www.youtube.com/watch?v=2VyhmbEjs9A\n",
      "stored: ../data/database/json/tmp/ca5bc19a2c432a0cd648535fe3a5289a.json\n",
      "16 https://www.youtube.com/watch?v=343n8xwozJI\n",
      "stored: ../data/database/json/tmp/dd463115b4cadc977d62e66223e7643a.json\n",
      "17 https://www.youtube.com/watch?v=3yOZxmlBG3Y\n",
      "stored: ../data/database/json/tmp/3ae47cc3100f8668ac4d20a199462568.json\n",
      "18 https://www.youtube.com/watch?v=4Df_BluxwkU\n",
      "stored: ../data/database/json/tmp/81d10bac5638bb4537931fa4363e0ca9.json\n",
      "19 https://www.youtube.com/watch?v=4h0uC9FPVMQ\n",
      "summarize\n",
      "done (nltk) 0.007 sec\n",
      "done (t5) 7.604 sec\n",
      "stored: ../data/database/json/tmp/38ab5aef902d7639ba9f6df8c46bde96.json\n",
      "20 https://www.youtube.com/watch?v=4MfG9CDufPA\n",
      "stored: ../data/database/json/tmp/793ac66da63e64a06a8e566295c5a185.json\n",
      "21 https://www.youtube.com/watch?v=4Y7RIAgOpn0\n",
      "stored: ../data/database/json/tmp/7445d42717d8c92e9a05b2de0ec5377e.json\n",
      "22 https://www.youtube.com/watch?v=5-xMV3sT3Tw\n",
      "stored: ../data/database/json/tmp/65f7a621da83bf208ecde2ad3ba7b37e.json\n",
      "23 https://www.youtube.com/watch?v=5PSWr2ovBvU\n",
      "stored: ../data/database/json/tmp/52c86cce1e3ee4859946bbfd183daaba.json\n",
      "24 https://www.youtube.com/watch?v=5vpklJw7uL0\n",
      "stored: ../data/database/json/tmp/66267750e6aa17da7d51d3f3bbf9da21.json\n",
      "25 https://www.youtube.com/watch?v=5xLSbj5SsSE\n",
      "stored: ../data/database/json/tmp/116e7a3abc1861a743de6a51c1c88581.json\n",
      "26 https://www.youtube.com/watch?v=674DL39dOOQ\n",
      "stored: ../data/database/json/tmp/56e2dfc7fc1d071bd62e1f96cd0cae7a.json\n",
      "27 https://www.youtube.com/watch?v=6aF9sJrzxaM\n",
      "stored: ../data/database/json/tmp/a313530fd0103048b3f38ede0381da8f.json\n",
      "28 https://www.youtube.com/watch?v=6c2T2cykE_A\n",
      "stored: ../data/database/json/tmp/b8e9bd90b3681c7f60603ce1082d34d7.json\n",
      "29 https://www.youtube.com/watch?v=6DVng5JVuhI\n",
      "stored: ../data/database/json/tmp/f1b7f35b49d5393a01f9aba191cbaad8.json\n",
      "30 https://www.youtube.com/watch?v=6JZNEb5uDu4\n",
      "stored: ../data/database/json/tmp/63d90d168339295d5c52988f4fbb4ccf.json\n",
      "31 https://www.youtube.com/watch?v=6rNcAVr-U4s\n",
      "stored: ../data/database/json/tmp/667dd7c221b8ab8a32abcce5ba2fd076.json\n",
      "32 https://www.youtube.com/watch?v=72_iAlYwl0c\n",
      "stored: ../data/database/json/tmp/9443f9beb3999059315cfa143c901182.json\n",
      "33 https://www.youtube.com/watch?v=7aLda2E0Yyg\n",
      "stored: ../data/database/json/tmp/ad73aa5381664552d16b02485b84c86e.json\n",
      "34 https://www.youtube.com/watch?v=7JbN9vXxGYE\n",
      "stored: ../data/database/json/tmp/d4bbda0a09d2c7f57078dd1f642ed084.json\n",
      "35 https://www.youtube.com/watch?v=7x2UvvD48Fw\n",
      "stored: ../data/database/json/tmp/5f3749420dddc0d46ae830e613e56b7b.json\n",
      "36 https://www.youtube.com/watch?v=8u3Hkbev2Gg\n",
      "stored: ../data/database/json/tmp/57b0473dcad5223ac54be0afc2e12983.json\n",
      "37 https://www.youtube.com/watch?v=8YWgar0uCF8\n",
      "stored: ../data/database/json/tmp/63932ff19e93612ea210d456b8436886.json\n",
      "38 https://www.youtube.com/watch?v=9bcbh2hC7Hw\n",
      "stored: ../data/database/json/tmp/7e6e3c8534e563a6eb0800d868e1c5e6.json\n",
      "39 https://www.youtube.com/watch?v=9BOdng9MpzU\n",
      "stored: ../data/database/json/tmp/de59d5c322b502363dac33b6ff58055f.json\n",
      "40 https://www.youtube.com/watch?v=9S2g7iixB9c\n",
      "stored: ../data/database/json/tmp/2a9efbd078dfc691d4c77aee4b927ff1.json\n",
      "41 https://www.youtube.com/watch?v=9xlSy9F5WtE\n",
      "stored: ../data/database/json/tmp/39e450764a53e682b21ec6b48c83ae61.json\n",
      "42 https://www.youtube.com/watch?v=a-ovvd_ZrmA\n",
      "summarize\n",
      "done (nltk) 0.004 sec\n",
      "done (t5) 9.054 sec\n",
      "stored: ../data/database/json/tmp/03bb343deb84037dc60f2170b18a5079.json\n",
      "43 https://www.youtube.com/watch?v=a1z6GXj8QK8\n",
      "stored: ../data/database/json/tmp/68ee28c8d1783021fa6e923f5191d3d4.json\n",
      "44 https://www.youtube.com/watch?v=a3sgFQjEfp4\n",
      "stored: ../data/database/json/tmp/ffc4792bf235e368c3f15843d559e50a.json\n",
      "45 https://www.youtube.com/watch?v=A7Gut679I-o\n",
      "summarize\n",
      "done (nltk) 0.004 sec\n",
      "done (t5) 6.415 sec\n",
      "stored: ../data/database/json/tmp/07bb3d578c7a05c1f3db36ae881035ca.json\n",
      "46 https://www.youtube.com/watch?v=aAsejHZC5EE\n",
      "stored: ../data/database/json/tmp/3d325afa53df5e9f3c5a8a1b1ec6d4fe.json\n",
      "47 https://www.youtube.com/watch?v=aKSILzbAqJs\n",
      "stored: ../data/database/json/tmp/7450e7ecbfa8f7f7bc4c9231bfe790ca.json\n",
      "48 https://www.youtube.com/watch?v=aMo7pkkaZ9o\n",
      "stored: ../data/database/json/tmp/6f645b5d40e6a4a8208ed40398df0c06.json\n",
      "49 https://www.youtube.com/watch?v=aR6M0MQBo2w\n",
      "stored: ../data/database/json/tmp/28c37a39a4f43ced111e6f855f26acd4.json\n",
      "50 https://www.youtube.com/watch?v=B70tT4WMyJk\n",
      "stored: ../data/database/json/tmp/1a1aefabd7c02c50515d15e14df763a7.json\n",
      "51 https://www.youtube.com/watch?v=bB54Wz4kq0E\n",
      "stored: ../data/database/json/tmp/5625df7ec6c2c8ccc43c294e91a2a6b0.json\n",
      "52 https://www.youtube.com/watch?v=bdM9c2OFYuw\n",
      "stored: ../data/database/json/tmp/70db181afad6da98266e9529f10d5658.json\n",
      "53 https://www.youtube.com/watch?v=BjwhMDhbqAs\n",
      "stored: ../data/database/json/tmp/ca09d1be88eebb416d223e779f2758f6.json\n",
      "54 https://www.youtube.com/watch?v=bLFISzfQCDQ\n",
      "stored: ../data/database/json/tmp/5a8796d23cbede01e72592d1e827fafd.json\n",
      "55 https://www.youtube.com/watch?v=brs1qCDzRdk\n",
      "stored: ../data/database/json/tmp/9694fac14b3fb766a324cb1387371c71.json\n",
      "56 https://www.youtube.com/watch?v=Bui3DWs02h4\n",
      "summarize\n",
      "done (nltk) 0.006 sec\n",
      "done (t5) 7.87 sec\n",
      "stored: ../data/database/json/tmp/cfe567d641d209ee114971c8f4d6deaa.json\n",
      "57 https://www.youtube.com/watch?v=bVGubOt_jLI\n",
      "stored: ../data/database/json/tmp/415e0a2cb2d52fc51c2068e3d58cb248.json\n",
      "58 https://www.youtube.com/watch?v=cLC_GHZCOVQ\n",
      "stored: ../data/database/json/tmp/41c2c02f421b0f1d22763ad4bc120914.json\n",
      "59 https://www.youtube.com/watch?v=CqFIVCD1WWo\n",
      "stored: ../data/database/json/tmp/65e11b44a9f7e0d3ed8fbf08bee2908d.json\n",
      "60 https://www.youtube.com/watch?v=cUWDeDRet4c\n",
      "stored: ../data/database/json/tmp/7816aa2a6334dd708fc3fddb108626ba.json\n",
      "61 https://www.youtube.com/watch?v=cVZzkSaxKmY\n",
      "stored: ../data/database/json/tmp/fd64603badcbe04e57afa2b23b7d9b1f.json\n",
      "62 https://www.youtube.com/watch?v=DglrYx9F3UU\n",
      "stored: ../data/database/json/tmp/c9943289ac58ab7991f049cdfe31de5c.json\n",
      "63 https://www.youtube.com/watch?v=dH1s49-lrBk\n",
      "stored: ../data/database/json/tmp/4c504877ca7169997bd3dddb9ed20db9.json\n",
      "64 https://www.youtube.com/watch?v=dQSzmngTbtw\n",
      "stored: ../data/database/json/tmp/bcb1b1591621a29c4b4c3a14c32193ac.json\n",
      "65 https://www.youtube.com/watch?v=dqxqbvyOnMY\n",
      "stored: ../data/database/json/tmp/dfd35d803350e45615d6da804b0f7869.json\n",
      "66 https://www.youtube.com/watch?v=Dvd1jQe3pq0\n",
      "stored: ../data/database/json/tmp/f2f0f04b84b8cc043a6d47ae4b5e0420.json\n",
      "67 https://www.youtube.com/watch?v=DW1AuOC9TQc\n",
      "stored: ../data/database/json/tmp/7e82a0ee0df24d2a8c2cd9b437287324.json\n",
      "68 https://www.youtube.com/watch?v=dxOHmvTaCN4\n",
      "stored: ../data/database/json/tmp/594f664db713187019129ca11e7ec84d.json\n",
      "69 https://www.youtube.com/watch?v=DzsZ2qMtEUE\n",
      "stored: ../data/database/json/tmp/72b515290bedeab450bf70ef15f328b9.json\n",
      "70 https://www.youtube.com/watch?v=e-WB4lfg30M\n",
      "stored: ../data/database/json/tmp/59ea00ee9671bf6c89ce516a4d66d871.json\n",
      "71 https://www.youtube.com/watch?v=EGnbAgbRIh4\n",
      "stored: ../data/database/json/tmp/640d16aab1b660f47439c4c34eb0b9fa.json\n",
      "72 https://www.youtube.com/watch?v=EQX1wsL2TSs\n",
      "stored: ../data/database/json/tmp/c17bf784cea53132d1038ab29448accb.json\n",
      "73 https://www.youtube.com/watch?v=f0Uzit_-h3M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stored: ../data/database/json/tmp/588ecd85035e14ff72598c7e1a9fd627.json\n",
      "74 https://www.youtube.com/watch?v=FeMSEaHR8aw\n",
      "stored: ../data/database/json/tmp/dc01106ab9fac4a8255576144f2af332.json\n",
      "75 https://www.youtube.com/watch?v=Fevg4aowNyc\n",
      "stored: ../data/database/json/tmp/a61448ad4a4cb96b11837038f8abe103.json\n",
      "76 https://www.youtube.com/watch?v=fklY2nH7AJo\n",
      "stored: ../data/database/json/tmp/1c6bcc177ec6a52fbc1858a7857f8c36.json\n",
      "77 https://www.youtube.com/watch?v=fl-7e8yBUic\n",
      "stored: ../data/database/json/tmp/84925a6e6f1791de6bd01d8b43191274.json\n",
      "78 https://www.youtube.com/watch?v=flOevlA9RyQ\n",
      "stored: ../data/database/json/tmp/c0ce8292e53cb21d3ac953aa30df4d9d.json\n",
      "79 https://www.youtube.com/watch?v=FMEk8cHF-OA\n",
      "stored: ../data/database/json/tmp/21c72f5da950ae20ac732db228777ed1.json\n",
      "80 https://www.youtube.com/watch?v=fTBeNAu18_s\n",
      "stored: ../data/database/json/tmp/5d2614bb1d3344a173330dde9ef43bb3.json\n",
      "81 https://www.youtube.com/watch?v=gHMY40kEXzs\n",
      "stored: ../data/database/json/tmp/935a9a57d0ee56469362c5a5d700382b.json\n",
      "82 https://www.youtube.com/watch?v=GNx8rgNcw5c\n",
      "stored: ../data/database/json/tmp/75eb47e9424373a5d4d1147d4ce791fe.json\n",
      "83 https://www.youtube.com/watch?v=gvjCu7zszbQ\n",
      "stored: ../data/database/json/tmp/69b6134b036762c42530b4249164dfd8.json\n",
      "84 https://www.youtube.com/watch?v=HANeLG0l2GA\n",
      "stored: ../data/database/json/tmp/511a48fb6cce90d604da9a384c4008ce.json\n",
      "85 https://www.youtube.com/watch?v=heB2tD0-r-c\n",
      "stored: ../data/database/json/tmp/fc1b4e3db8edcae0cf3edf098dceaa2f.json\n",
      "86 https://www.youtube.com/watch?v=heY2gfXSHBo\n",
      "stored: ../data/database/json/tmp/9f6e87652e9f6d3724a4b4823ac801ee.json\n",
      "87 https://www.youtube.com/watch?v=hnT-P3aALVE\n",
      "stored: ../data/database/json/tmp/db058a657ff7226cf747469bb6a93960.json\n",
      "88 https://www.youtube.com/watch?v=HO1LYJb818Q\n",
      "stored: ../data/database/json/tmp/f88ddaa303b8767d433ad4217f77e877.json\n",
      "89 https://www.youtube.com/watch?v=HOLoPgTzV6g\n",
      "stored: ../data/database/json/tmp/dac463cc94943f60419737ab0647c4ed.json\n",
      "90 https://www.youtube.com/watch?v=hPKJBXkyTKM\n",
      "stored: ../data/database/json/tmp/fd50f77dd33c33a7da83d15b1509c328.json\n",
      "91 https://www.youtube.com/watch?v=HSmm_vEVs10\n",
      "stored: ../data/database/json/tmp/637a89e57cc6c746994fd91178ffe543.json\n",
      "92 https://www.youtube.com/watch?v=HTUxsrO-P_8\n",
      "stored: ../data/database/json/tmp/2cd9bafa98f9baaa5c072e873b92f0fe.json\n",
      "93 https://www.youtube.com/watch?v=HUFh8cEDeII\n",
      "stored: ../data/database/json/tmp/c02d9b57a0dc9a7ae8d89b6cac64a7a6.json\n",
      "94 https://www.youtube.com/watch?v=HvHZXPd0Bjs\n",
      "stored: ../data/database/json/tmp/d1594991929a1f54fed73c5084a6f01d.json\n",
      "95 https://www.youtube.com/watch?v=hzpxXZJQNFg\n",
      "stored: ../data/database/json/tmp/1e67af70a9bdcfb4848b6c729d14c940.json\n",
      "96 https://www.youtube.com/watch?v=IFmj5M5Q5jg\n",
      "summarize\n",
      "done (nltk) 0.005 sec\n",
      "done (t5) 7.73 sec\n",
      "stored: ../data/database/json/tmp/23476ba67aea9714a8bf8683b1d07fee.json\n",
      "97 https://www.youtube.com/watch?v=Ih8EfvOzBOY\n",
      "stored: ../data/database/json/tmp/e0793a782f637eeabac0d6abb2089db9.json\n",
      "98 https://www.youtube.com/watch?v=ImIaoKsjgUE\n",
      "stored: ../data/database/json/tmp/df91c28084940604272a2a9fca3ed8fd.json\n",
      "99 https://www.youtube.com/watch?v=iOWamCtnwTc\n",
      "stored: ../data/database/json/tmp/bef35d5c3853adfc120c9a1970ef990a.json\n",
      "100\n",
      "100 https://www.youtube.com/watch?v=iTRnr6p7iYo\n",
      "stored: ../data/database/json/tmp/bc6f8aa399f3c187166ac075ea6c40cf.json\n",
      "101 https://www.youtube.com/watch?v=iuJwmM2-JWM\n",
      "stored: ../data/database/json/tmp/bd7620735d43031ea4f5a73a36279200.json\n",
      "102 https://www.youtube.com/watch?v=izZofvgaIig\n",
      "stored: ../data/database/json/tmp/cdbadcb93012ebc684a5ebfc7dfd7bc9.json\n",
      "103 https://www.youtube.com/watch?v=j7XWCCvBrwU\n",
      "stored: ../data/database/json/tmp/e12dc811e140c975f11bc2b2dc84e38d.json\n",
      "104 https://www.youtube.com/watch?v=j9FLOinaG94\n",
      "stored: ../data/database/json/tmp/e6128b6e4a3da67482060d05dd0750fb.json\n",
      "105 https://www.youtube.com/watch?v=jDxsGW5KUP0\n",
      "stored: ../data/database/json/tmp/3df29e1f2bc0d8b47fe3ee7b559feb64.json\n",
      "106 https://www.youtube.com/watch?v=Jkkjy7dVdaY\n",
      "stored: ../data/database/json/tmp/7eeb5b313048073c5dc7e42003c0f0dd.json\n",
      "107 https://www.youtube.com/watch?v=jMZqxfTls-0\n",
      "stored: ../data/database/json/tmp/1764d165bd5a39fd39d887b3f740d26d.json\n",
      "108 https://www.youtube.com/watch?v=JzOc_NNY_zY\n",
      "stored: ../data/database/json/tmp/a38f77ce6dadae0a7559ae7a79334637.json\n",
      "109 https://www.youtube.com/watch?v=K-0KJtk07YU\n",
      "stored: ../data/database/json/tmp/4d5f51007108e3e173c54319331a6a59.json\n",
      "110 https://www.youtube.com/watch?v=KEdrBMZx53w\n",
      "stored: ../data/database/json/tmp/199293009e2c0ef6db55846a47820d32.json\n",
      "111 https://www.youtube.com/watch?v=kf-KViOuktc\n",
      "stored: ../data/database/json/tmp/1a7f2f87d89dc14de0e0bd880ad44532.json\n",
      "112 https://www.youtube.com/watch?v=kfJMUeQO0S0\n",
      "stored: ../data/database/json/tmp/d78cf637fe0c04e807a8a0c5df288e4b.json\n",
      "113 https://www.youtube.com/watch?v=KgIrnR2O8KQ\n",
      "stored: ../data/database/json/tmp/c2f610ea09716b5cd714b150c56a317b.json\n",
      "114 https://www.youtube.com/watch?v=KL6U6iasUxs\n",
      "stored: ../data/database/json/tmp/536634617251400f9dc9d857e3261fea.json\n",
      "115 https://www.youtube.com/watch?v=kLnG073NYtw\n",
      "summarize\n",
      "done (nltk) 0.005 sec\n",
      "done (t5) 7.435 sec\n",
      "stored: ../data/database/json/tmp/51fe8e5b7a38131d063f40769817111b.json\n",
      "116 https://www.youtube.com/watch?v=kMa_B3wLxAM\n",
      "stored: ../data/database/json/tmp/9c827f903f00bf4464ede6a24a16f200.json\n",
      "117 https://www.youtube.com/watch?v=kQ2bqz3HPJE\n",
      "stored: ../data/database/json/tmp/2b51d2b1e75fd9f5db5d6d589e10d1e9.json\n",
      "118 https://www.youtube.com/watch?v=ksCSL6Ql0Yg\n",
      "stored: ../data/database/json/tmp/922114c07b11cf6d941cd05ce400c5df.json\n",
      "119 https://www.youtube.com/watch?v=kwqme8mEgz4\n",
      "stored: ../data/database/json/tmp/235c3aa6510de7c9cee62f224c96f04f.json\n",
      "120 https://www.youtube.com/watch?v=L7MOeQw47BM\n",
      "summarize\n",
      "done (nltk) 0.009 sec\n",
      "done (t5) 7.629 sec\n",
      "stored: ../data/database/json/tmp/9ff5104dc3089bd64224791031ef569d.json\n",
      "121 https://www.youtube.com/watch?v=lCoR-4OlIZI\n",
      "stored: ../data/database/json/tmp/fcc5e27b44c59dc3fd1dfcfcbfb37af2.json\n",
      "122 https://www.youtube.com/watch?v=Lcxz6dtYjI4\n",
      "stored: ../data/database/json/tmp/c009905559e23840d878b03dd8b2a789.json\n",
      "123 https://www.youtube.com/watch?v=ldO7RD3s4_s\n",
      "stored: ../data/database/json/tmp/5f9869f0b7a4b1a4543c41500b6ffef6.json\n",
      "124 https://www.youtube.com/watch?v=lf3ViWEeKqc\n",
      "stored: ../data/database/json/tmp/37ff94959429b30dbf3850c27885487a.json\n",
      "125 https://www.youtube.com/watch?v=LhhEv1dMpKE\n",
      "stored: ../data/database/json/tmp/fb34847ebfe54a2bdc8593cec2935f26.json\n",
      "126 https://www.youtube.com/watch?v=LmYKfU5O_NA\n",
      "stored: ../data/database/json/tmp/da28c991c64d350a23fa6e6a9af0bda8.json\n",
      "127 https://www.youtube.com/watch?v=LU3pdWTD4Rw\n",
      "stored: ../data/database/json/tmp/423bb56993c888ef1d685597c771e3a5.json\n",
      "128 https://www.youtube.com/watch?v=lxNEWuO6xQk\n",
      "stored: ../data/database/json/tmp/c13f30ebd30d932887d6feb9e486eb57.json\n",
      "129 https://www.youtube.com/watch?v=m9XyXiL6n8w\n",
      "stored: ../data/database/json/tmp/c556e97a058accfa3b3b6505ee194509.json\n",
      "130 https://www.youtube.com/watch?v=MCHw6fUyLMY\n",
      "stored: ../data/database/json/tmp/b0cee7e333190b7f6db473d045f44a9e.json\n",
      "131 https://www.youtube.com/watch?v=mECv52eSjBo\n",
      "stored: ../data/database/json/tmp/551a9273f4e5365b59275dae53cb59dc.json\n",
      "132 https://www.youtube.com/watch?v=MfaTOXxA8dM\n",
      "stored: ../data/database/json/tmp/757e5eb7510242fc0c519453747e6d1b.json\n",
      "133 https://www.youtube.com/watch?v=mkI6qfpEJmI\n",
      "stored: ../data/database/json/tmp/c6e1388062d86a84196e9636abb1e8d0.json\n",
      "134 https://www.youtube.com/watch?v=mL3CzZcBJZU\n",
      "stored: ../data/database/json/tmp/a7a933f31032c8449eda2a4b8010b821.json\n",
      "135 https://www.youtube.com/watch?v=mmeoUZ_wRm4\n",
      "stored: ../data/database/json/tmp/281719ef2309f95bf88838fe5c3eb5a1.json\n",
      "136 https://www.youtube.com/watch?v=MtWtY4DdiWs\n",
      "stored: ../data/database/json/tmp/52f97d9417756cae8921d1d6f78ecd7a.json\n",
      "137 https://www.youtube.com/watch?v=Mu0ew2F-SSA\n",
      "stored: ../data/database/json/tmp/39f9819256c1251631155bb92b287726.json\n",
      "138 https://www.youtube.com/watch?v=Mx8viOFKiIs\n",
      "stored: ../data/database/json/tmp/9055d939a3684a92efe45bb7d8805f0c.json\n",
      "139 https://www.youtube.com/watch?v=M_eaS7X-mIw\n",
      "stored: ../data/database/json/tmp/9fb82c3ec321eb2903fa3d0b2cda686f.json\n",
      "140 https://www.youtube.com/watch?v=n3aoc36V8LM\n",
      "stored: ../data/database/json/tmp/1f5baa3f950a33f3383b94de85811ffa.json\n",
      "141 https://www.youtube.com/watch?v=NEscK5RCtlo\n",
      "stored: ../data/database/json/tmp/97185f46edabc1c4ebe4a9f1af08d186.json\n",
      "142 https://www.youtube.com/watch?v=nfPBT71xYVQ\n",
      "stored: ../data/database/json/tmp/fd0869c5e3665b46ef9edc7379ea4118.json\n",
      "143 https://www.youtube.com/watch?v=ni6P5KU3SDU\n",
      "stored: ../data/database/json/tmp/0795cbc79a8d5bfb27b590522213c501.json\n",
      "144 https://www.youtube.com/watch?v=nK3giIsNAHg\n",
      "stored: ../data/database/json/tmp/d1354ee3ea6b43e35db08628c16e4b66.json\n",
      "145 https://www.youtube.com/watch?v=Nq2xvsVojVo\n",
      "stored: ../data/database/json/tmp/1b203245c4bdb9c5264866e7afb40299.json\n",
      "146 https://www.youtube.com/watch?v=nsuAQcvafCs\n",
      "stored: ../data/database/json/tmp/d709495bd30ab4fc89dd358a48d8c4f4.json\n",
      "147 https://www.youtube.com/watch?v=oitGRdHFNWw\n",
      "stored: ../data/database/json/tmp/df5167d4ccdabb0928d331a2f760d2f2.json\n",
      "148 https://www.youtube.com/watch?v=oleylS5XGpg\n",
      "stored: ../data/database/json/tmp/171b66829f1f946c8cb043b63e45a0b1.json\n",
      "149 https://www.youtube.com/watch?v=oltKUPTBz9Q\n",
      "stored: ../data/database/json/tmp/32c134daa30ae7912762507dda8e4d26.json\n",
      "150 https://www.youtube.com/watch?v=oWpp1YYcCsU\n",
      "stored: ../data/database/json/tmp/6d19a2d93f2e7c2922aa8898d1c1a7b2.json\n",
      "151 https://www.youtube.com/watch?v=p831XtyLA5M\n",
      "stored: ../data/database/json/tmp/ece642825dbfab1b724c9630e2a4302c.json\n",
      "152 https://www.youtube.com/watch?v=pAiiPNg0kDE\n",
      "stored: ../data/database/json/tmp/7cf919ac71d242db64b48126058dccc5.json\n",
      "153 https://www.youtube.com/watch?v=PMSV7CjBuZI\n",
      "stored: ../data/database/json/tmp/7761d81e4945ab9758770ab7ff14e323.json\n",
      "154 https://www.youtube.com/watch?v=psOPu3TldgY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stored: ../data/database/json/tmp/1a3b96db2cec03364fe20e648fee5bc7.json\n",
      "155 https://www.youtube.com/watch?v=pVgC-7QTr40\n",
      "stored: ../data/database/json/tmp/3bd0f4d87a1a2e3f9020d3e39392759d.json\n",
      "156 https://www.youtube.com/watch?v=qKhSZmS6aWw\n",
      "stored: ../data/database/json/tmp/38a09aaed3ad62acd93d6fc4dc3530d6.json\n",
      "157 https://www.youtube.com/watch?v=QkqNzrsaxYc\n",
      "stored: ../data/database/json/tmp/a0605b85a654b27cf1d126923c426abf.json\n",
      "158 https://www.youtube.com/watch?v=QmIM24JDE3A\n",
      "stored: ../data/database/json/tmp/4fa4391ecb8c4b683fd205d4e9487b7d.json\n",
      "159 https://www.youtube.com/watch?v=R5t74AC6I0A\n",
      "stored: ../data/database/json/tmp/196738ca62e253406256dbbfdc8c41b9.json\n",
      "160 https://www.youtube.com/watch?v=rAbhypxs1qQ\n",
      "stored: ../data/database/json/tmp/9a8a8b2e0db968048307015187b6c3b8.json\n",
      "161 https://www.youtube.com/watch?v=rCWTOOgVXyE\n",
      "summarize\n",
      "done (nltk) 0.005 sec\n",
      "done (t5) 11.614 sec\n",
      "stored: ../data/database/json/tmp/2fac97d8fe66a880945575af01cbad20.json\n",
      "162 https://www.youtube.com/watch?v=rskdLEl05KI\n",
      "stored: ../data/database/json/tmp/7436700d8220120f8752b4c78142fa2a.json\n",
      "163 https://www.youtube.com/watch?v=RygQnpQMdPI\n",
      "stored: ../data/database/json/tmp/0f55f6d3f3d0dfd1f4ae216b918468fb.json\n",
      "164 https://www.youtube.com/watch?v=SA4YEAWVpbk\n",
      "stored: ../data/database/json/tmp/c746e73abcfff68cd3261ab40dc53bf4.json\n",
      "165 https://www.youtube.com/watch?v=SauCsNkGr-E\n",
      "stored: ../data/database/json/tmp/5531ad15ad4a9200ced96b820c711f10.json\n",
      "166 https://www.youtube.com/watch?v=SC0D7aJOySY\n",
      "stored: ../data/database/json/tmp/3d7e590ddeb20f7ef71b459b79f17844.json\n",
      "167 https://www.youtube.com/watch?v=SmyiKmfnbhc\n",
      "stored: ../data/database/json/tmp/9a5a288284b8e428776ecdbe77884564.json\n",
      "168 https://www.youtube.com/watch?v=sSnDTPjfBYU\n",
      "stored: ../data/database/json/tmp/b6dcd772363717cd3909ce65fd358895.json\n",
      "169 https://www.youtube.com/watch?v=St5lxIxYGkI\n",
      "stored: ../data/database/json/tmp/9646f4c711575ad8b3cd809f29a11d60.json\n",
      "170 https://www.youtube.com/watch?v=SWW0nVQNm2w\n",
      "stored: ../data/database/json/tmp/7b3a870287320eafb439a13ac00558b3.json\n",
      "171 https://www.youtube.com/watch?v=sWZQxB2es88\n",
      "stored: ../data/database/json/tmp/ad2fa9ff6f7004760c6ad3288e48183e.json\n",
      "172 https://www.youtube.com/watch?v=tB0AVkPDDJU\n",
      "stored: ../data/database/json/tmp/ea927eab82455359f1a2ab8c20725fb0.json\n",
      "173 https://www.youtube.com/watch?v=TItYXBoJ1sc\n",
      "stored: ../data/database/json/tmp/8cc114f54f53f3026c00cad51fd29ba4.json\n",
      "174 https://www.youtube.com/watch?v=TRNUTN01SEg\n",
      "stored: ../data/database/json/tmp/77e070b53094e6863638fc5fec703bba.json\n",
      "175 https://www.youtube.com/watch?v=twWHwVaBfM8\n",
      "stored: ../data/database/json/tmp/9f8a4441831c3f482daed5f6f2b29dc9.json\n",
      "176 https://www.youtube.com/watch?v=T_g6S3f0Z5I\n",
      "stored: ../data/database/json/tmp/0ac2453954667bcfe243926a941d9dd6.json\n",
      "177 https://www.youtube.com/watch?v=u3C4zkxNtok\n",
      "stored: ../data/database/json/tmp/094d3519a2719c6750456fa00e6593cb.json\n",
      "178 https://www.youtube.com/watch?v=u7kQ5lNfUfg\n",
      "stored: ../data/database/json/tmp/37725e163f21954bc785ca12cc185ee2.json\n",
      "179 https://www.youtube.com/watch?v=u9UUWqVquXo\n",
      "stored: ../data/database/json/tmp/892732378d58769f70a1eac8746d8b12.json\n",
      "180 https://www.youtube.com/watch?v=UBORpapdAfU\n",
      "stored: ../data/database/json/tmp/600e315e6f859f5eb6ec89c91bd4c132.json\n",
      "181 https://www.youtube.com/watch?v=UEPbzj-ekAI\n",
      "stored: ../data/database/json/tmp/7f859cd4f345e264b44b4a1303071aa0.json\n",
      "182 https://www.youtube.com/watch?v=UePDRN94C8c\n",
      "stored: ../data/database/json/tmp/bedea14ab423e0574aebaa7943029c5d.json\n",
      "183 https://www.youtube.com/watch?v=UGAzi1QBVEg\n",
      "stored: ../data/database/json/tmp/c0f44267ec5960cdefcc3e0229c425af.json\n",
      "184 https://www.youtube.com/watch?v=uGhyOBSzdTs\n",
      "stored: ../data/database/json/tmp/8ec43572edead1257057ad49d52b1cce.json\n",
      "185 https://www.youtube.com/watch?v=UjuBLS15JqM\n",
      "stored: ../data/database/json/tmp/7db16867f42d38600db11752e2960fe0.json\n",
      "186 https://www.youtube.com/watch?v=UMSNBLAfC7o\n",
      "stored: ../data/database/json/tmp/9d2567523224683f954f0bc7d0e481ab.json\n",
      "187 https://www.youtube.com/watch?v=Uo6hFVRsjpA\n",
      "stored: ../data/database/json/tmp/1b15130fa1f0446850360eebae0abd32.json\n",
      "188 https://www.youtube.com/watch?v=uOiOhVgR3VA\n",
      "stored: ../data/database/json/tmp/79a0923582fe0e79cea6ad36620bb322.json\n",
      "189 https://www.youtube.com/watch?v=UPcR7S8ue1A\n",
      "stored: ../data/database/json/tmp/0f66e1631ceed1731b91510b116418a0.json\n",
      "190 https://www.youtube.com/watch?v=Uxax5EKg0zA\n",
      "summarize\n",
      "done (nltk) 0.003 sec\n",
      "done (t5) 10.681 sec\n",
      "stored: ../data/database/json/tmp/e4c777e2051bfb12a3b11321722ade89.json\n",
      "191 https://www.youtube.com/watch?v=v1oWke0Qf1E\n",
      "stored: ../data/database/json/tmp/587e9675277141079e5a844750481961.json\n",
      "192 https://www.youtube.com/watch?v=vaFhLAbPi8w\n",
      "stored: ../data/database/json/tmp/53389a67e41bd8eb18d368c3559ce40b.json\n",
      "193 https://www.youtube.com/watch?v=veWkBsK0nwU\n",
      "stored: ../data/database/json/tmp/f088f41f022534922f5182d231063446.json\n",
      "194 https://www.youtube.com/watch?v=vmkqFRyNUWo\n",
      "stored: ../data/database/json/tmp/244f9d39856e983f29e229af8b57084f.json\n",
      "195 https://www.youtube.com/watch?v=VrgYtFhVGmg\n",
      "stored: ../data/database/json/tmp/79ae6a20ef4e26184f0ee718d23f91be.json\n",
      "196 https://www.youtube.com/watch?v=vzg5Qe0pTKk\n",
      "stored: ../data/database/json/tmp/342a190c6b16c24d08489949b2497eeb.json\n",
      "197 https://www.youtube.com/watch?v=w2D5JR83pFI\n",
      "stored: ../data/database/json/tmp/6a5f4be7bb58f5d3daaa3c089e0734b7.json\n",
      "198 https://www.youtube.com/watch?v=wBrwN4dS-DA\n",
      "stored: ../data/database/json/tmp/43fb0d42901b4debf550f8b17486beb2.json\n",
      "199 https://www.youtube.com/watch?v=WhaRsrlaXLk\n",
      "stored: ../data/database/json/tmp/cfba20d53cc4a582d2bc01804881d8cb.json\n",
      "200\n",
      "200 https://www.youtube.com/watch?v=wlAgyf_e-hA\n",
      "stored: ../data/database/json/tmp/1f5712ecb842e174aa6cae66a90a636f.json\n",
      "201 https://www.youtube.com/watch?v=wlndIQHtiFw\n",
      "stored: ../data/database/json/tmp/241f1b44da8dffe4c85dcee2f39aa7e6.json\n",
      "202 https://www.youtube.com/watch?v=WovbLx8C0yA\n",
      "stored: ../data/database/json/tmp/3d65a98c9831fdda4760248e238e7195.json\n",
      "203 https://www.youtube.com/watch?v=WT0WtoYz2jE\n",
      "stored: ../data/database/json/tmp/fa98e7a5537169ff913dcd803ee68d9e.json\n",
      "204 https://www.youtube.com/watch?v=wz9cUncBdxw\n",
      "stored: ../data/database/json/tmp/328904ef543d09fe48f862da762a5e6b.json\n",
      "205 https://www.youtube.com/watch?v=XbuEYcFfl6s\n",
      "stored: ../data/database/json/tmp/e04ac49929c5358597e6154c4d880ac3.json\n",
      "206 https://www.youtube.com/watch?v=XcxzKLrCpyk\n",
      "stored: ../data/database/json/tmp/8133127be3935059d449a757d9c7f0cb.json\n",
      "207 https://www.youtube.com/watch?v=XgB3Xg5st2U\n",
      "stored: ../data/database/json/tmp/b831868e8cbeb5fb123b5a6eb2979ea4.json\n",
      "208 https://www.youtube.com/watch?v=XhH2Cc4thJw\n",
      "stored: ../data/database/json/tmp/d6da7104d4631eb1f811a8d38020847a.json\n",
      "209 https://www.youtube.com/watch?v=XmM1tF7AxdA\n",
      "stored: ../data/database/json/tmp/9e950ea8431c77e5ae58c5512cbd731b.json\n",
      "210 https://www.youtube.com/watch?v=xp-YOPcjkFw\n",
      "stored: ../data/database/json/tmp/4ec98cb6587c17dada4538e04c139246.json\n",
      "211 https://www.youtube.com/watch?v=XpwW3glj2T8\n",
      "stored: ../data/database/json/tmp/f63b07c4a3e77c9503f63686fca6e544.json\n",
      "212 https://www.youtube.com/watch?v=Yd4blFeRTEw\n",
      "stored: ../data/database/json/tmp/17b526d12b3c5275e68d00de29dc3bbf.json\n",
      "213 https://www.youtube.com/watch?v=YjjTPV2pXY0\n",
      "stored: ../data/database/json/tmp/0df3b634e92ff361e0e601aff998bda7.json\n",
      "214 https://www.youtube.com/watch?v=YTup-cvELK0\n",
      "stored: ../data/database/json/tmp/b8bbe77b0fd90e5845591e18bb1bbd83.json\n",
      "215 https://www.youtube.com/watch?v=YWK-bnyXvbg\n",
      "stored: ../data/database/json/tmp/e89d59026429f22a14475c5d638c8a58.json\n",
      "216 https://www.youtube.com/watch?v=ZBWTD2aNb_o\n",
      "stored: ../data/database/json/tmp/b9da1a66b0827c7269122e73c362a2fe.json\n",
      "217 https://www.youtube.com/watch?v=ZEjUqZU1hNQ\n",
      "stored: ../data/database/json/tmp/5f56cefbc75a2ff6d91bf72ca2421e27.json\n",
      "218 https://www.youtube.com/watch?v=ZHoNpxUHewQ\n",
      "stored: ../data/database/json/tmp/012e7479e2465174e48c2ac77cab1eb5.json\n",
      "219 https://www.youtube.com/watch?v=zjaz2mC1KhM\n",
      "stored: ../data/database/json/tmp/4d79b01ead031d07110e0a5a694886a9.json\n",
      "220 https://www.youtube.com/watch?v=zLzhsyeAie4\n",
      "stored: ../data/database/json/tmp/a3f4b29e4357368f3dd5dd0b2430dbc6.json\n",
      "221 https://www.youtube.com/watch?v=ZolWxY4f9wc\n",
      "stored: ../data/database/json/tmp/7e46fac53f93d641c2ca4515e361fa26.json\n",
      "222 https://www.youtube.com/watch?v=ZtP3gl_2kBM\n",
      "stored: ../data/database/json/tmp/d67100d5f79c7f27a2e31fbed969b6e3.json\n",
      "223 https://www.youtube.com/watch?v=ZUa5sNVSjGw\n",
      "stored: ../data/database/json/tmp/e332f5bd0fde79c0f7697240756e1f68.json\n",
      "224 https://www.youtube.com/watch?v=_BPJFFkxSbw\n",
      "stored: ../data/database/json/tmp/0a76792bb5b795b5ff11c493a248ccd7.json\n",
      "225 https://www.youtube.com/watch?v=_DN2rzHkpZE\n",
      "stored: ../data/database/json/tmp/a49c9063f779c7f74623336e97faeab5.json\n",
      "226 https://www.youtube.com/watch?v=_r-eIKkyAco\n",
      "stored: ../data/database/json/tmp/6f0348f70e46777031856ba4ceb7e768.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 https://www.youtube.com/watch?v=_yjHPu1aYCY\n",
      "stored: ../data/database/json/tmp/a6f2b4b1d85bec65b0e23335b81dc4e1.json\n",
      "228 https://www.youtube.com/watch?v=_ZLXKt4L-AA\n",
      "stored: ../data/database/json/tmp/decb786c608095965a1bc9b84cc303ef.json\n",
      "DONE parsed 229 items\n"
     ]
    }
   ],
   "source": [
    "# run the loop\n",
    "\n",
    "#transform = ['ka_c', 'ka_cn', 'ka_d', 'ka_dn', 'ma', 'gh', 'tcp', 'bc']\n",
    "transform = ['ka_c', 'ka_cn', 'ma', 'gh', 'tcp', 'bc', 'me_ft', 'bcg_fo', 'bcg_ha',\n",
    "             'me_ft', 'bcg_fo', 'bcg_ha', 'za_bl', 'za_jo', 'za_pr', 'za_pu', 'tmp']\n",
    "#transform = ['za_bl', 'za_jo', 'za_pr', 'za_pu']\n",
    "transform = ['tmp']\n",
    "\n",
    "datasets = {\n",
    "    # kaggle competitions\n",
    "    'ka_c': {\n",
    "        'csv_in': '../data/database/kaggle_competitions_correlated_01.csv',\n",
    "        'csv_format': 'kaggle_competition',\n",
    "    },\n",
    "    # kaggle competitions notebooks\n",
    "    'ka_cn': {\n",
    "        'csv_in': '../data/database/kaggle_competitions_01_original.csv',\n",
    "        'csv_format': 'kaggle_notebook',\n",
    "    },\n",
    "    # kaggle datasets\n",
    "    'ka_d': {\n",
    "        'csv_in': '../data/database/kaggle_datasets_correlated_01.csv',\n",
    "        'csv_format': 'kaggle_dataset',\n",
    "    },\n",
    "    # kaggle datasets notebooks\n",
    "    'ka_dn': {\n",
    "        'csv_in': '../data/database/kaggle_datasets_01_original.csv',\n",
    "        'csv_format': 'kaggle_notebook',\n",
    "    },\n",
    "    # mlart\n",
    "    'ma': {\n",
    "        'csv_in': '../data/database/mlart_01_original.csv',\n",
    "        'csv_format':'mlart',\n",
    "        'extra': {\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 1,\n",
    "            'compete_score': 0,\n",
    "        },\n",
    "    },\n",
    "    # github\n",
    "    'gh': {\n",
    "        'csv_in': '../data/database/db_04_analyzed_v02.csv',\n",
    "        'csv_format': 'github',\n",
    "        'extra': {\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0.25,\n",
    "        },\n",
    "    },\n",
    "    # thecleverprogrammer\n",
    "    'tcp': {\n",
    "        'csv_in': '../data/database/thecleverprogrammer_01_original.csv',\n",
    "        'csv_format': 'tcp',\n",
    "        'extra': {\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "        },\n",
    "    },\n",
    "    # blobcity\n",
    "    'bc': {\n",
    "        'csv_in': '../data/database/blobcity_02_analyzed.csv',\n",
    "        'csv_format': 'github',\n",
    "        'extra': {\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "        },\n",
    "    },\n",
    "    # medium_fintech\n",
    "    'me_ft': {\n",
    "        'csv_in': '../data/database/medium_fintech_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'medium.com',\n",
    "            'kind': 'Article',\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 1,\n",
    "        },\n",
    "        'out': 'me'\n",
    "    },\n",
    "    # bcgdv founded\n",
    "    'bcg_fo': {\n",
    "        'csv_in': '../data/database/bcgdv_founded_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'bcgdv.com',\n",
    "            'kind': 'Article',\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 1,\n",
    "        },\n",
    "        'out': 'bcg'\n",
    "    },\n",
    "    # bcgdv hackaton\n",
    "    'bcg_ha': {\n",
    "        'csv_in': '../data/database/bcgdv_hackaton_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'bcgdv.com',\n",
    "            'kind': ['Article', 'Project'],\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 1,\n",
    "        },\n",
    "        'out': 'bcg'\n",
    "    },\n",
    "    # zalando blog\n",
    "    'za_bl': {\n",
    "        'csv_in': '../data/database/zalando_blog_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'zalando.com',\n",
    "            'kind': 'Article'\n",
    "        },\n",
    "        'out': 'za'\n",
    "    },\n",
    "    # zalando jobs\n",
    "    'za_jo': {\n",
    "        'csv_in': '../data/database/zalando_jobs_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'zalando.com',\n",
    "            'kind': 'Article',\n",
    "            'tags': ['Fashion'],\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 1,\n",
    "        },\n",
    "        'out': 'za'\n",
    "    },\n",
    "    # zalando research projects\n",
    "    'za_pr': {\n",
    "        'csv_in': '../data/database/zalando_projects_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'zalando.com',\n",
    "            'kind': 'Article',\n",
    "        },\n",
    "        'out': 'za'\n",
    "    },\n",
    "    # zalando research publications\n",
    "    'za_pu': {\n",
    "        'csv_in': '../data/database/zalando_publications_04.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'zalando.com',\n",
    "            'kind': 'Article',\n",
    "            'date_scraped': datetime.strptime('17.01.2021', \"%d.%m.%Y\"),\n",
    "            'tags': ['Fashion'],\n",
    "            'learn_score': 0.5,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0.75,\n",
    "        },\n",
    "        'out': 'za'\n",
    "    },\n",
    "    # twominutepapers\n",
    "    'tmp': {\n",
    "        'csv_in': '../data/database/twominutepapers_01_original.csv',\n",
    "        'csv_format': 'tmp',\n",
    "    },\n",
    "}\n",
    "\n",
    "    \n",
    "for key in transform:\n",
    "    print(key)\n",
    "    item = datasets[key]\n",
    "    extra = item['extra'] if 'extra' in item else {}\n",
    "    out = key+'/' if not 'out' in item else item['out']+'/'\n",
    "    printItem=False\n",
    "    transform_loop(item['csv_in'], item['csv_format'], out, overwrite=True, extra=extra, printItem=printItem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform zero shot categorization\n",
    "\n",
    "zero shot categorization is computational intense  \n",
    "so let's keep it out from the loop and process it seperatly  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accommodation & Food', 'Accounting', 'Agriculture', 'Banking & Insurance', 'Biotechnological & Life Sciences', 'Construction & Engineering', 'Economics', 'Education & Research', 'Emergency & Relief', 'Finance', 'Government and Public Works', 'Healthcare', 'Justice, Law and Regulations', 'Manufacturing', 'Media & Publishing', 'Miscellaneous', 'Physics', 'Real Estate, Rental & Leasing', 'Utilities', 'Wholesale & Retail']\n",
      "['Failure', 'Food', 'Fraud', 'General', 'Genomics', 'Insurance and Risk', 'Judicial Applied', 'Life-sciences', 'Machine Learning', 'Maintenance', 'Management and Operations', 'Marketing', 'Material Science', 'Physical', 'Policy and Regulatory', 'Politics', 'Preventative and Reactive', 'Quality', 'Real Estate', 'Rental & Leasing', 'Restaurant', 'Retail', 'School', 'Sequencing', 'Social Policies', 'Student', 'Textual Analysis', 'Tools', 'Tourism', 'Trading & Investment', 'Transportation', 'Valuation', 'Water & Pollution', 'Wholesale']\n"
     ]
    }
   ],
   "source": [
    "print(cat)\n",
    "print(subcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder bc\n",
      "folder bcg\n",
      "folder gh\n",
      "folder ka_c\n",
      "folder ka_cn\n",
      "folder ma\n",
      "folder me\n",
      "folder tcp\n",
      "folder tmp\n",
      "###\n",
      "tmp\n",
      "files in folder: 229\n",
      "row: 0 item: 0 link: https://www.youtube.com/watch?v=ZHoNpxUHewQ file: 012e7479e2465174e48c2ac77cab1eb5.json\n",
      "categorize\n",
      "t5 skipped\n",
      "nltk skipped\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-65775eb67fed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m                         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description_category'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                         \u001b[0mc_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description_category_score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                         \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description_category_runtime'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'runtime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'description category'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'runtime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'runtime'"
     ]
    }
   ],
   "source": [
    "# classification\n",
    "\n",
    "folder = '../data/database/json/'\n",
    "subfolder = os.listdir(folder)\n",
    "#print(subfolder)\n",
    "\n",
    "#transform = ['ka_c', 'ka_cn', 'ka_d', 'ka_dn', 'ma', 'gh', 'tcp', 'bc']\n",
    "transform = ['ka_c', 'ka_cn', 'ma', 'gh', 'tcp', 'bc']\n",
    "transform = ['tmp']\n",
    "\n",
    "recreate_category = False\n",
    "save = True\n",
    "categorzie_t5 = False\n",
    "categorize_nltk = True\n",
    "categorize_fallback = True\n",
    "\n",
    "quit = 0\n",
    "i = j = 0\n",
    "for item in subfolder:\n",
    "    print('folder', item)\n",
    "    fp = os.path.join(folder, item)\n",
    "    if os.path.isdir(fp) and item in transform:\n",
    "        print('###')\n",
    "        print(item)\n",
    "        files = os.listdir(fp)\n",
    "        print('files in folder:', len(files))\n",
    "        for file in files:\n",
    "            row = load_data(os.path.join(folder, item, file), fromJson=True)\n",
    "            #print(row)\n",
    "            \n",
    "            print('row:', i, 'item:', j, 'link:', row['link'], 'file:', file)\n",
    "            \n",
    "            # zero shot categorization\n",
    "            if not 'category' in row or row.get('category') == '' or recreate_category == True:\n",
    "                print('categorize')\n",
    "                start = time.time()\n",
    "                j += 1\n",
    "\n",
    "                # create category and subcategory from t5\n",
    "                if 'sum_t5' in row and row['sum_t5'] != '' and categorzie_t5 == True:\n",
    "                    s = row['sum_t5']\n",
    "                    res = categorize(s, cat)\n",
    "                    #row['t5_category_raw'] = res\n",
    "                    c = row['t5_category'] = res['category']\n",
    "                    c_score = row['t5_category_score'] = res['score']\n",
    "                    row['t5_category_runtime'] = res['runtime']\n",
    "                    print('t5 category', res['runtime'], 'sec')\n",
    "\n",
    "                    res = categorize(s, subcat)\n",
    "                    #row['t5_subcategory_raw'] = res\n",
    "                    sc = row['t5_subcategory'] = res['category']\n",
    "                    sc_score = row['t5_subcategory_score'] = res['score']\n",
    "                    row['t5_subcategory_runtime'] = res['runtime']\n",
    "                    print('t5 subcategory', res['runtime'], 'sec')\n",
    "                else:\n",
    "                    print('t5 skipped')\n",
    "\n",
    "                # create category and subcategory from nltk\n",
    "                if 'sum_nltk' in row and row['sum_nltk'] != '' and categorize_nltk == True:\n",
    "                    s = row['sum_nltk']\n",
    "                    res = categorize(s, cat)\n",
    "                    #print(res)\n",
    "                    #row['nltk_category_raw'] = res\n",
    "                    c = row['nltk_category'] = res['category']\n",
    "                    c_score = row['nltk_category_score'] = res['score']\n",
    "                    row['nltk_category_runtime'] = res['runtime']\n",
    "                    print('nltk category', res['runtime'], 'sec')\n",
    "\n",
    "                    res = categorize(s, subcat)\n",
    "                    #print(res)\n",
    "                    #row['nltk_subcategory_raw'] = res\n",
    "                    sc = row['nltk_subcategory'] = res['category']\n",
    "                    sc_score = row['nltk_subcategory_score'] = res['score']\n",
    "                    row['nltk_subcategory_runtime'] = res['runtime']\n",
    "                    print('nltk subcategory', res['runtime'], 'sec')\n",
    "                else:\n",
    "                    print('nltk skipped')\n",
    "\n",
    "                # create category and subcategory from title or description if not already done\n",
    "                if categorize_fallback == True and not 't5_category' in row and not 'nltk_category' in row:\n",
    "                    if len(row['description']) > 0:\n",
    "                        s = row['description']\n",
    "                        res = categorize(s, cat)\n",
    "                        #row['description_category_raw'] = res\n",
    "                        c = row['description_category'] = res['category']\n",
    "                        c_score = row['description_category_score'] = res['score']\n",
    "                        row['description_category_runtime'] = res['runtime']\n",
    "                        print('description category', res['runtime'], 'sec')\n",
    "\n",
    "                        res = categorize(s, subcat)\n",
    "                        #row['description_subcategory_raw'] = res\n",
    "                        sc = row['description_subcategory'] = res['category']\n",
    "                        sc_score = row['description_subcategory_score'] = res['score']\n",
    "                        row['description_subcategory_runtime'] = res['runtime']\n",
    "                        print('description subcategory', res['runtime'], 'sec')\n",
    "                    else:\n",
    "                        s = row['title']\n",
    "                        if s != '':\n",
    "                            res = categorize(s, cat)\n",
    "                            #row['title_category_raw'] = res\n",
    "                            c = row['title_category'] = res['category']\n",
    "                            c_score = row['title_category_score'] = res['score']\n",
    "                            row['title_category_runtime'] = res['runtime']\n",
    "                            print('title category', res['runtime'], 'sec')\n",
    "\n",
    "                            res = categorize(s, subcat)\n",
    "                            #row['title_subcategory_raw'] = res\n",
    "                            sc = row['title_subcategory'] = res['category']\n",
    "                            sc_score = row['title_subcategory_score'] = res['score']\n",
    "                            row['title_subcategory_runtime'] = res['runtime']\n",
    "                            print('title subcategory', res['runtime'], 'sec')\n",
    "                        else:\n",
    "                            print('nothing found to categorize')\n",
    "                            c = sc = ''\n",
    "                            c_score = sc_score = 0\n",
    "                            j -= 1\n",
    "\n",
    "                row['category'] = c\n",
    "                row['category_score'] = c_score\n",
    "                row['subcategory'] = sc\n",
    "                row['subcategory_score'] = sc_score\n",
    "\n",
    "                end = time.time()\n",
    "                dur = round(end-start, 3)\n",
    "                row['runtime_cat'] = dur\n",
    "                \n",
    "                fp = os.path.join(folder, item, file)\n",
    "                if save == True:\n",
    "                    store_data(row, fp, toJson=True)\n",
    "                else:\n",
    "                    print('NOT SAVED')\n",
    "                    print(row)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            \n",
    "            if quit!= 0 and i >= quit:\n",
    "                break\n",
    "    if quit!= 0 and i >= quit:\n",
    "                break\n",
    "            \n",
    "print('DONE parsed', i, 'items')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
