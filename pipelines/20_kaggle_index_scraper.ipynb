{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle index scraper\n",
    "\n",
    "this script is a pipeline for Kaggle  \n",
    "\n",
    "firstoff, we try build an automation to scrape the index, to get an actual representation  \n",
    "in dedicated scripts we will scrape and preprocess the content  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle.com offers an api to download data\n",
    "# however we are not interested in the data itself\n",
    "# but the projects and notebooks associated with it\n",
    "# so we need to find a way to scrape the data from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from requests import Session\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"errors\":[\"Bad request\"],\"wasSuccessful\":false}\n"
     ]
    }
   ],
   "source": [
    "# unfortunatelly the page heavily relies on javascript and ajax calls\n",
    "# a simple approach to mimic the xmlhhtp-requests to harvest the data failed\n",
    "\n",
    "session = Session()\n",
    "\n",
    "# HEAD requests ask for *just* the headers, which is all you need to grab the\n",
    "# session cookie\n",
    "session.head('https://www.kaggle.com/')\n",
    "\n",
    "response = session.post(\n",
    "    url='https://www.kaggle.com/requests/SearchDatasetsRequest',\n",
    "    data={\"page\":290},\n",
    "    headers={\n",
    "        'Referer': 'https://www.kaggle.com/'\n",
    "    }\n",
    ")\n",
    "\n",
    "print (response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so i manually downloaded the sites, splitted in different subsections\n",
    "# this process is time consuming and does not provide a full list of all projects\n",
    "# let's scrape the data and parse them into csv-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reuse some generic functions\n",
    "\n",
    "# generic store data to file function\n",
    "def store_data(data, file, mode='w', toJson=False):\n",
    "    if toJson:\n",
    "        data = json.dumps(data)\n",
    "    with open(file, mode, encoding='utf-8') as fp:\n",
    "        result = fp.write(data)\n",
    "        return result\n",
    "    \n",
    "# generic load data from file function\n",
    "def load_data(file, fromJson=False):\n",
    "    if os.path.isfile(file):\n",
    "        with open(file, 'r', encoding='utf-8', errors=\"ignore\") as fp:\n",
    "            data = fp.read()\n",
    "            if fromJson:\n",
    "                data = json.loads(data)\n",
    "            return data\n",
    "    else:\n",
    "        return 'file not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets_all_incomplete.html', 'datasets_bigquery_cc_66.html', 'datasets_bigquery_cc_66_temp.html', 'datasets_bigquery_odb.html', 'datasets_bigquery_other.html', 'datasets_cc_newest.html', 'datasets_csv_cc_7678_incomplete.html', 'datasets_csv_cc_7678_temp.html', 'datasets_csv_gpl_385.html', 'datasets_csv_odb_945.html', 'datasets_json_cc_1106.html', 'datasets_json_gpl_39.html', 'datasets_json_odb_136.html', 'datasets_json_other_2488_temp.html', 'datasets_sqlite_cc_39.html', 'datasets_sqlite_odb.html', 'datasets_sqlite_other_35.html']\n"
     ]
    }
   ],
   "source": [
    "# load list of manually downloaded html-files\n",
    "\n",
    "path = '../data/repositories/kaggle/index/html/'\n",
    "files_all = os.listdir(path)\n",
    "files = []\n",
    "for file in files_all:\n",
    "    if os.path.isfile(path+file) and 'dataset' in file:\n",
    "        files.append(file)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape datasets from html-code\n",
    "\n",
    "html_path = '../data/repositories/kaggle/index/html/'\n",
    "csv_path = '../data/repositories/kaggle/index/csv_datasets/'\n",
    "html_files = [\n",
    "    'datasets_bigquery_cc_66-2.html',\n",
    "    #'datasets_csv_gpl_385.html',\n",
    "    #'datasets_bigquery_odb.html',\n",
    "]\n",
    "\n",
    "html_files = files\n",
    "\n",
    "def kaggle_scrape_datasets(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    result = []\n",
    "    \n",
    "    partial = soup.find('div', class_=\"km-list km-list--three-line\")\n",
    "    items = partial.find_all('li', {\"role\": \"listitem\"})\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        #print(item)\n",
    "        #print(i)\n",
    "        try:\n",
    "            link = item.select('a.sc-qcrOD')[0].get('href')\n",
    "            name = item.select('div.sc-Axmtr')[0].text.strip()\n",
    "            updated = item.select('span.sc-fzqBkg > span')[0].get('title')\n",
    "            updated = updated.split(' (')[0]\n",
    "            usability = item.select('div.sc-qQYBZ > span')\n",
    "            if len(usability) > 0:\n",
    "                usability = usability[0].text.strip()\n",
    "            else:\n",
    "                usability = ''\n",
    "            subline = item.select('span.sc-fzqBkg')[1].find(text=True, recursive=False)\n",
    "            subline = subline.split('·')\n",
    "            files = filesize = tasks = ''\n",
    "            for cell in subline:\n",
    "                cell = cell.strip()\n",
    "                if 'File' in cell:\n",
    "                    files = cell\n",
    "                if 'B' in cell:\n",
    "                    filesize = cell\n",
    "                if 'Task' in cell:\n",
    "                    tasks = cell\n",
    "            score = item.select('span.sc-fzoxKX')[0].text.strip()\n",
    "            badge = item.select('span.sc-qanSb')\n",
    "            if len(badge) > 0:\n",
    "                badge = badge[0].text.strip()\n",
    "            else:\n",
    "                badge = ''\n",
    "\n",
    "            result.append({\n",
    "                'link': link,\n",
    "                'name': name,\n",
    "                'updated': updated,\n",
    "                'usability': usability,\n",
    "                'files': files,\n",
    "                'filesize': filesize,\n",
    "                'tasks': tasks,\n",
    "                'score': score,\n",
    "                'badge': badge,\n",
    "            })\n",
    "        except:\n",
    "            print('an error occured')\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "for file in html_files:\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    html = load_data(html_path+file)\n",
    "    \n",
    "    if html == 'file not found':\n",
    "        print(html)\n",
    "        break\n",
    "        \n",
    "    result = kaggle_scrape_datasets(html)\n",
    "    \n",
    "    #print(result)\n",
    "    print(len(result))\n",
    "    \n",
    "    csv_file = file.replace('.html', '.csv')\n",
    "    \n",
    "    #store_data(result, path+csv_file, toJson=True)\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv(csv_path+csv_file,encoding='utf-8-sig')\n",
    "    #print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle csv and drop duplicates\n",
    "\n",
    "path = '../data/repositories/kaggle/index/csv_datasets/'\n",
    "files_all = os.listdir(path)\n",
    "csv = '../data/repositories/kaggle/kaggle_index.csv'\n",
    "\n",
    "li = []\n",
    "\n",
    "for file in files_all:\n",
    "    df = pd.read_csv(os.path.join(path, file), index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "print('count (with duplicates):', len(df))\n",
    "\n",
    "df = df.drop_duplicates(['link'])\n",
    "print('count (without duplicates):', len(df))\n",
    "\n",
    "df.to_csv(csv, encoding='utf-8-sig', index=False)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competitions_completed.html\n",
      "419\n",
      "                                            link  \\\n",
      "0                      /c/stanford-covid-vaccine   \n",
      "1       /c/rsna-str-pulmonary-embolism-detection   \n",
      "2                                    /c/lish-moa   \n",
      "3           /c/conways-reverse-game-of-life-2020   \n",
      "4  /c/lyft-motion-prediction-autonomous-vehicles   \n",
      "\n",
      "                                                name  \\\n",
      "0  OpenVaccine: COVID-19 mRNA Vaccine Degradation...   \n",
      "1              RSNA STR Pulmonary Embolism Detection   \n",
      "2              Mechanisms of Action (MoA) Prediction   \n",
      "3                 Conway's Reverse Game of Life 2020   \n",
      "4     Lyft Motion Prediction for Autonomous Vehicles   \n",
      "\n",
      "                                         description    category  \\\n",
      "0  Urgent need to bring the COVID-19 vaccine to m...    Research   \n",
      "1  Classify Pulmonary Embolism cases in chest CT ...    Featured   \n",
      "2  Can you improve the algorithm that classifies ...    Research   \n",
      "3      Reverse the arrow of time in the Game of Life  Playground   \n",
      "4  Build motion prediction models for self-drivin...    Featured   \n",
      "\n",
      "           date       teams              kind    prize  \n",
      "0  2 months ago  1636 Teams                    $25,000  \n",
      "1   a month ago   784 Teams  Code Competition  $30,000  \n",
      "2    2 days ago  4384 Teams  Code Competition  $30,000  \n",
      "3    2 days ago   188 Teams  Code Competition     Swag  \n",
      "4    7 days ago   937 Teams  Code Competition  $30,000  \n"
     ]
    }
   ],
   "source": [
    "# scrape competitions from html-code\n",
    "\n",
    "html_path = '../data/repositories/kaggle/index/html/'\n",
    "csv_path = '../data/repositories/kaggle/kaggle_competitions.csv'\n",
    "html_files = [\n",
    "    'competitions_completed.html',\n",
    "]\n",
    "\n",
    "#html_files = files\n",
    "\n",
    "def kaggle_scrape_competitions(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    result = []\n",
    "    \n",
    "    partial = soup.find_all('ul', class_=\"mdc-list\")[-1]\n",
    "    #print(len(partial), partial)\n",
    "    items = partial.find_all('li', {\"role\": \"listitem\"})\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        #print(i, item)\n",
    "        #try:\n",
    "        if True:\n",
    "            link = item.select('a.sc-pdOLj')[0].get('href')\n",
    "            name = item.select('div.sc-Axmtr')[0].text.strip()\n",
    "            sublines = item.select('span.sc-fznzOf')\n",
    "            description = sublines[0].text.strip()\n",
    "            # 'subline': 'Research • 2 months ago • Code Competition • 1636 Teams'\n",
    "            subline = sublines[1].text.strip()\n",
    "            subline = subline.split('•')\n",
    "            category = date = teams = kind = ''\n",
    "            for i, cell in enumerate(subline):\n",
    "                cell = cell.strip()\n",
    "                if i == 0:\n",
    "                    category = cell\n",
    "                if 'ago' in cell:\n",
    "                    date = cell\n",
    "                if 'Code Competition' in cell:\n",
    "                    kind = cell\n",
    "                if 'Teams' in cell:\n",
    "                    teams = cell\n",
    "            prize = item.select('div.sc-pjUyM')[0].text.strip()\n",
    "\n",
    "            result.append({\n",
    "                'link': link,\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'category': category,\n",
    "                'date': date,\n",
    "                'teams': teams,\n",
    "                'kind': kind,\n",
    "                'prize': prize,\n",
    "            })\n",
    "        #except:\n",
    "        #    print('an error occured')\n",
    "        #    break\n",
    "\n",
    "        #print(result)\n",
    "        #break\n",
    "    return result\n",
    "\n",
    "for file in html_files:\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    html = load_data(html_path+file)\n",
    "    \n",
    "    if html == 'file not found':\n",
    "        print(html)\n",
    "        break\n",
    "        \n",
    "    result = kaggle_scrape_competitions(html)\n",
    "    \n",
    "    #print(result)\n",
    "    print(len(result))\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv(csv_path,encoding='utf-8-sig')\n",
    "    print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
