date_closed;description;link;organisation;subtitle;tags;teams;title;type
2015-05-05 01:59:00;"IMPORTANT NOTE: This competition is only open to students of the MITx free, online course 15.071x - The Analytics Edge. What makes online news articles popular? Newspapers and online news aggregators like Google News need to understand which news articles will be the most popular, so that they can prioritize the order in which stories appear. In this competition, you will predict the popularity of a set of New York Times blog articles from the time period September 2014-December 2014. The following screenshot shows an example of the New York Times technology blog ""Bits"" homepage:  Many blog articles are published each day, and the New York Times has to decide which articles should be featured. In this competition, we challenge you to develop an analytics model that will help the New York Times understand the features of a blog post that make it popular. To download the data and learn how this competition works, please be sure to read the ""Data"" page, as well as the ""Evaluation"" page, which can both be found in the panel on the left. Acknowledgements This competition is brought to you by MITx and edX.";https://www.kaggle.com/c/15-071x-the-analytics-edge-competition-spring-2015;;Test your analytics skills by predicting which New York Times blog articles will be the most popular;['auc'];2,920;15.071x - The Analytics Edge (Spring 2015);Research prediction Competition
2015-04-04 02:02:14;"IMPORTANT NOTE: This competition is only open to students of 15.071x - The Analytics Edge (https://www.edx.org/course/analytics-edge-mitx-15-071x) What makes online news articles popular? Newspapers and online news aggregators like Google News need to understand which news articles will be the most popular, so that they can prioritize the order in which stories appear. In this competition, you will predict the popularity of a set of New York Times blog articles from the time period September 2014-December 2014.  The following screenshot shows an example of the New York Times technology blog ""Bits"" homepage:  Many blog articles are published each day, and the New York Times has to decide which articles should be featured. In this competition, we challenge you to develop an analytics model that will help the New York Times understand the features of a blog post that make it popular.  To download the data and learn how this competition works, please be sure to read the ""Data"" page, as well as the ""Evaluation"" page, which can both be found in the panel on the left. Acknowledgements This competition is brought to you by MITx and edX.";https://www.kaggle.com/c/15-071x-the-analytics-edge-spring-20152;;Test your analytics skills by predicting which New York Times blog articles will be the most popular.;['auc'];;15.071x - The Analytics Edge (Spring 2015);Research prediction Competition
2019-01-17 00:59:00;This isn't your classic decoder ring puzzle found in a cereal box. There's a twist. Welcome to the Ciphertext Challenge! In this competition, we've encrypted parts of a well-known dataset -- the 20 Newsgroups dataset -- with several simple, classic ciphers.  This dataset is commonly used as a multi-class and NLP sample set, noted for its small size, varied nature, and the first-hand look it offers into the deep existential horrors of the 90s-era internet. With 20 fairly distinct classes and lots of clues, it allows for a wide variety of successful approaches. We've made the problem a little harder to solve. Fabulous Kaggle swag will go to the top competitors - the highest-scoring teams (which might be the first to crack the code!), and the most popular kernel.  Note that this is a short competition, so use your submissions wisely. * = Note: It is possible to apply a number of techniques using ONLY the ciphertext. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice.  You can view and download the unencrypted dataset from Jason Rennie's homepage.  In the words of the host: The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. If you use the dataset in a scientific publication, please reference (at a minimum) the above website. Photo by U.S. Air Force photo/Don Branum;https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge;Kaggle;V8g{9827$A${?^*?}$$v7*.yig$w9.8};['text data', 'multiclass classification', 'macrofscore'];142;20 Newsgroups Ciphertext Challenge;Playground prediction Competition
2019-11-13 00:59:00;Self-driving technology presents a rare opportunity to improve the quality of life in many of our communities. Avoidable collisions, single-occupant commuters, and vehicle emissions are choking cities, while infrastructure strains under rapid urban growth. Autonomous vehicles are expected to redefine transportation and unlock a myriad of societal, environmental, and economic benefits. You can apply your data analysis skills in this competition to advance the state of self-driving technology. Lyft, whose mission is to improve people’s lives with the world’s best transportation, is investing in the future of self-driving vehicles. Level 5, their self-driving division, is working on a fleet of autonomous vehicles, and currently has a team of 450+ across Palo Alto, London, and Munich working to build a leading self-driving system (they’re hiring!). Their goal is to democratize access to self-driving technology for hundreds of millions of Lyft passengers. From a technical standpoint, however,  the bar to unlock technical research and development on higher-level autonomy functions like perception, prediction, and planning is extremely high. This implies technical R&D on self-driving cars has traditionally been inaccessible to the broader research community. This dataset aims to democratize access to such data, and foster innovation in higher-level autonomy functions for everyone, everywhere. By conducting a competition, we hope to encourage the research community to focus on hard problems in this space—namely, 3D object detection over semantic maps.  In this competition, you will build and optimize algorithms based on a large-scale dataset. This dataset features the raw sensor camera inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a restricted geographic area.  If successful, you’ll make a significant contribution towards stimulating further development in autonomous vehicles and empowering communities around the world.;https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;Lyft;Can you advance the state of the art in 3D object detection?;['image data', 'custom metric'];547;Lyft 3D Object Detection for Autonomous Vehicles;Featured prediction Competition
2020-05-28 01:59:00;Can a computer learn complex, abstract tasks from just a few examples? Current machine learning techniques are data-hungry and brittle—they can only make sense of patterns they've seen before. Using current methods, an algorithm can gain new skills by exposure to large amounts of data, but cognitive abilities that could broadly generalize to many tasks remain elusive. This makes it very challenging to create systems that can handle the variability and unpredictability of the real world, such as domestic robots or self-driving cars. However, alternative approaches, like inductive programming, offer the potential for more human-like abstraction and reasoning. The Abstraction and Reasoning Corpus (ARC) provides a benchmark to measure AI skill-acquisition on unknown tasks, with the constraint that only a handful of demonstrations are shown to learn a complex task. It provides a glimpse of a future where AI could quickly learn to solve new problems on its own. The Kaggle Abstraction and Reasoning Challenge invites you to try your hand at bringing this future into the present! This competition is hosted by François Chollet, creator of the Keras neural networks library. Chollet’s paper on measuring intelligence provides the context and motivation behind the ARC benchmark. In this competition, you’ll create an AI that can solve reasoning tasks it has never seen before. Each ARC task contains 3-5 pairs of train inputs and outputs, and a test input for which you need to predict the corresponding output with the pattern learned from the train examples. If successful, you’ll help bring computers closer to human cognition and you'll open the door to completely new AI applications!;https://www.kaggle.com/c/abstraction-and-reasoning-challenge;Abstraction and Reasoning Corpus;Create an AI capable of solving reasoning tasks it has never seen before;['artificial intelligence', 'meanbesterroratk'];914;Abstraction and Reasoning Challenge;Research Code Competition
2013-11-23 00:59:00;Since everyone moves differently and accelerometers are fast becoming ubiquitous, this competition is designed to investigate the feasibility of using accelerometer data as a biometric for identifying users of mobile devices. Seal has collected accelerometer data from several hundred users over a period of several months during normal device usage. To collect the data, we published an app on Googles’s Android PlayStore that samples accelerometer data in the background and posts it to a central database for analysis. We have uploaded approximately 60 million unique samples of accelerometer data collected from 387 different devices. These are split into equal sets for training and test. Samples in the training set are labeled with the unique device from which the data was collected. The test set is demarcated into 90k sequences of consecutive samples from one device.  A file of test questions is provided in which you are asked to determine whether the accelerometer data came from the proposed device.;https://www.kaggle.com/c/accelerometer-biometric-competition;;Recognize users of mobile devices from accelerometer data;['auc'];630;Accelerometer Biometric Competition;Research prediction Competition
2012-09-30 03:00:00;$500  1st place $350  2nd place $150  3rd place;https://www.kaggle.com/c/acm-sf-chapter-hackathon-big;;Predict which BestBuy product a mobile web visitor will be most interested in based on their search query or behavior over 2 years (7 GB).;['custom metric'];24;Data Mining Hackathon on BIG DATA (7GB) Best Buy mobile web site;Research prediction Competition
2012-09-30 03:00:00;Two years of mobile behavior, 67 million clicks, 27 million searches, 8 million users, 1 million products         There will also be a  Visualization Contest that can be entered from either track. For more details on the event, go to: http://www.sfbayacm.org/DM-Hackathon-2012-10 Data Provided by:   Cloud Compute Sponsors:;https://www.kaggle.com/c/acm-sf-chapter-hackathon-small;;Getting Started - Predict which Xbox game a visitor will be most interested in based on their search query.  (20 MB);['custom metric'];96;Data Mining Hackathon on (20 mb) Best Buy mobile web site - ACM SF Bay Area Chapter;Research prediction Competition
2014-07-15 01:59:00;Consumer brands often offer discounts to attract new shoppers to buy their products. The most valuable customers are those who return after this initial incented purchase.  With enough purchase history, it is possible to predict which shoppers, when presented an offer, will buy a new item. However, identifying the shopper who will become a loyal buyer -- prior to the initial purchase -- is a more challenging task.  The Acquire Valued Shoppers Challenge asks participants to predict which shoppers are most likely to repeat purchase. To aid with algorithmic development, we have provided complete, basket-level, pre-offer shopping history for a large set of shoppers who were targeted for an acquisition campaign. The incentive offered to that shopper and their post-incentive behavior is also provided. This challenge provides almost 350 million rows of completely anonymised transactional data from over 300,000 shoppers. It is one of the largest problems run on Kaggle to date.;https://www.kaggle.com/c/acquire-valued-shoppers-challenge;;Predict which shoppers will become repeat buyers;['auc'];949;Acquire Valued Shoppers Challenge;Featured prediction Competition
2019-07-09 01:59:00;To assess the impact of climate change on Earth's flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the VIGIA project, which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery. This is a kernels-only competition, meaning you must submit predictions using Kaggle Kernels. Read the basics here. Acknowledgments Kaggle is hosting this competition for the machine learning community to use for fun and practice. The original version of this data can be found here, with details in the following paper: Efren López-Jiménez, Juan Irving Vasquez-Gomez, Miguel Angel Sanchez-Acevedo, Juan Carlos Herrera-Lozada, Abril Valeria Uriarte-Arcia, Columnar Cactus Recognition in Aerial Images using a Deep Learning Approach. Ecological Informatics. 2019. Acknowledgements to Consejo Nacional de Ciencia y Tecnología. Project cátedra 1507. Instituto Politècnico Nacional. Universidad de la Cañada. Contributors: Eduardo Armas Garca, Rafael Cano Martnez and Luis Cresencio Mota Carrera. J.I. Vasquez-Gomez, JC. Herrera Lozada. Abril Uriarte, Miguel Sanchez.;https://www.kaggle.com/c/aerial-cactus-identification;Kaggle;Determine whether an image contains a columnar cactus;['earth and nature', 'image data', 'plants', 'auc'];1,225;Aerial Cactus Identification;Playground Code Competition
2014-10-22 01:59:00;Advances in rapid, low cost analysis of soil samples using infrared spectroscopy, georeferencing of soil samples, and greater availability of earth remote sensing data provide new opportunities for predicting soil functional properties at unsampled locations. Soil functional properties are those properties related to a soil’s capacity to support essential ecosystem services such as primary productivity, nutrient and water retention, and resistance to soil erosion. Digital mapping of soil functional properties, especially in data sparse regions such as Africa, is important for planning sustainable agricultural intensification and natural resources management.  Diffuse reflectance infrared spectroscopy has shown potential in numerous studies to provide a highly repeatable, rapid and low cost measurement of many soil functional properties. The amount of light absorbed by a soil sample is measured, with minimal sample preparation, at hundreds of specific wavebands across a range of wavelengths to provide an infrared spectrum (Fig. 1). The measurement can be typically performed in about 30 seconds, in contrast to conventional reference tests, which are slow and expensive and use chemicals. Conventional reference soil tests are calibrated to the infrared spectra on a subset of samples selected to span the diversity in soils in a given target geographical area. The calibration models are then used to predict the soil test values for the whole sample set. The predicted soil test values from georeferenced soil samples can in turn be calibrated to remote sensing covariates, which are recorded for every pixel at a fixed spatial resolution in an area, and the calibration model is then used to predict the soil test values for each pixel. The result is a digital map of the soil properties. This competition asks you to predict 5 target soil functional properties from diffuse reflectance infrared spectroscopy measurements. Acknowledgements This competition is sponsored by the Africa Soil Information Service.;https://www.kaggle.com/c/afsis-soil-properties;;Predict physical and chemical properties of soil using spectral measurements;['mcrmse'];1,231;Africa Soil Property Prediction Challenge;Research prediction Competition
2016-02-12 00:59:00;"Instead of waking to overlooked ""Do not disturb"" signs, Airbnb travelers find themselves rising with the birds in a whimsical treehouse, having their morning coffee on the deck of a houseboat, or cooking a shared regional breakfast with their hosts. New users on Airbnb can book a place to stay in 34,000+ cities across 190+ countries. By accurately predicting where a new user will book their first travel experience, Airbnb can share more personalized content with their community, decrease the average time to first booking, and better forecast demand. In this recruiting competition, Airbnb challenges you to predict in which country a new user will make his or her first booking. Kagglers who impress with their answer (and an explanation of how they got there) will be considered for an interview for the opportunity to join Airbnb's Data Science and Analytics team.  Wondering if you're a good fit? Check out this article on how Airbnb scaled data science to all sides of their organization, and visit their careers page for more on Airbnb's mission to create a world that inspires human connection.";https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;Airbnb;Where will a new guest book their first travel experience?;['tabular data', 'recommender systems', 'hotels and accommodations', 'ndcg@{k}'];1,458;Airbnb New User Bookings;Recruitment prediction Competition
2018-11-15 00:59:00;Airbus is excited to challenge Kagglers to build a model that detects all ships in satellite images as quickly as possible. Can you find them even in imagery with clouds or haze?  Here’s the backstory: Shipping traffic is growing fast.  More ships increase the chances of infractions at sea like environmentally devastating ship accidents, piracy, illegal fishing, drug trafficking, and illegal cargo movement. This has compelled many organizations, from environmental protection agencies to insurance companies and national government authorities, to have a closer watch over the open seas.   Airbus offers comprehensive maritime monitoring services by building a meaningful solution for wide coverage, fine details, intensive monitoring, premium reactivity and interpretation response. Combining its proprietary-data with highly-trained analysts, they help to support the maritime industry to increase knowledge, anticipate threats, trigger alerts, and improve efficiency at sea. A lot of work has been done over the last 10 years to automatically extract objects from satellite images with significative results but no effective operational effects.  Now Airbus is turning to Kagglers to increase the accuracy and  speed of automatic ship detection. Algorithm Speed Prize: After the Kaggle challenge is complete,  competitors may submit their model via a private Kaggle kernel for a speed evaluation based upon the inference time on over 40.000  images chips (typical size of a full satellite image) to win a special algorithm speed prize.   If you're interested to explore more Airbus data, you are welcomed to check out the OneAtlas Sandbox. And for more insights on our Maritime Surveillance capabilities, have a look at Airbus Intelligence page.;https://www.kaggle.com/c/airbus-ship-detection;Airbus;Find ships on satellite images as quickly as possible;['image data', 'intersectionoverunionobjectsegmentationbeta'];882;Airbus Ship Detection Challenge;Featured prediction Competition
2020-07-21 01:59:00;"That file you downloaded may contain hidden messages that aren’t part of its regular contents. The same technology employed for digital watermarking is also misused by crime rings. Law enforcement must now use steganalysis to detect these messages as part of their investigations. Machine learning is an important tool in the discovery of this secret data.  Current methods produce unreliable results, raising false alarms. One reason for inaccuracy is the many different devices and processing combinations. Yet, detection models are trained on a homogeneous dataset. To increase accuracy, researchers must put data hidden within digital images “into the wild” (hence the name ALASKA) to mimic real world conditions. In the competition, you’ll create an efficient and reliable method to detect secret data hidden within innocuous-seeming digital images. Rather than limiting the data source, these images have been acquired with as many as 50 different cameras (from smartphone to full-format high end) and processed in different fashions. Successful entries will include robust detection algorithms with minimal false positives. The IEEE WIFS (Workshop on Information Forensics and Security) is eager to make this happen again, as a follow up to the ALASKA#1 Challenge. WIFS is an annual event where researchers gather to discuss emerging challenges, exchange fresh ideas, and share state-of-the-art results and technical expertise in the areas of information security and forensics. WIFS has teamed up with Troyes University of Technology, CRIStAL Lab, Lille University, and CNRS to enable more accurate steganalysis.  Law enforcement officers need better methods to combat criminals using hidden messages. The data science community and other researchers can help with better automated detection. More accurate methods could help catch criminals whose communications are hidden in plain sight.  The challenge is organized by Rémi COGRANNE (UTT), Patrick BAS (CRIStAL / CNRS) and Quentin Giboulot (UTT) ; in addition to Kaggle, we have been greatly helped by the following sponsors:";https://www.kaggle.com/c/alaska2-image-steganalysis;Troyes University of Technology;Detect secret data hidden within digital images;['custom metric'];1,095;ALASKA2 Image Steganalysis;Research prediction Competition
2012-01-09 00:59:59;"The Algorithmic Trading Challenge is a forecasting competition which aims to encourage the development of new models to predict the stock market's short-term response following large trades. Contestants are asked to derive empirical models to predict the  behaviour of bid and ask prices following such ""liquidity shocks"". Modelling market resiliency will improve trading strategy evaluation methods by increasing the realism of backtesting simulations, which currently assume zero market resiliency.";https://www.kaggle.com/c/AlgorithmicTradingChallenge;;Develop new models to accurately predict the market response to large trades.;['rmse'];111;Algorithmic Trading Challenge;Featured prediction Competition
2016-12-13 00:59:00;When you’ve been devastated by a serious car accident, your focus is on the things that matter the most: family, friends, and other loved ones. Pushing paper with your insurance agent is the last place you want your time or mental energy spent. This is why Allstate, a personal insurer in the United States, is continually seeking fresh ideas to improve their claims service for the over 16 million households they protect.  Allstate is currently developing automated methods of predicting the cost, and hence severity, of claims. In this recruitment challenge, Kagglers are invited to show off their creativity and flex their technical chops by creating an algorithm which accurately predicts claims severity. Aspiring competitors will demonstrate insight into better ways to predict claims severity for the chance to be part of Allstate’s efforts to ensure a worry-free customer experience. New to Kaggle? This competition is a recruiting competition, your chance to get a foot in the door with the hiring team at Allstate.;https://www.kaggle.com/c/allstate-claims-severity;Allstate Insurance;How severe is an insurance claim?;['regression', 'tabular data', 'mae'];3,045;Allstate Claims Severity;Recruitment prediction Competition
2014-05-20 01:59:00;As a customer shops an insurance policy, he/she will receive a number of quotes with different coverage options before purchasing a plan. This is represented in this challenge as a series of rows that include a customer ID, information about the customer, information about the quoted policy, and the cost. Your task is to predict the purchased coverage options using a limited subset of the total interaction history. If the eventual purchase can be predicted sooner in the shopping window, the quoting process is shortened and the issuer is less likely to lose the customer's business. Using a customer’s shopping history, can you predict what policy they will end up choosing?;https://www.kaggle.com/c/allstate-purchase-prediction-challenge;Allstate Insurance;Predict a purchased policy based on transaction history;['categorizationaccuracy'];1,565;Allstate Purchase Prediction Challenge;Featured prediction Competition
2013-08-01 01:59:00;When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. This access may allow an employee to read/manipulate resources through various applications or web portals. It is assumed that employees fulfilling the functions of a given role will access the same or similar resources. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work (e.g. not able to log into a reporting portal). A knowledgeable supervisor then takes time to manually grant the needed access in order to overcome access obstacles. As employees move throughout a company, this access discovery/recovery cycle wastes a nontrivial amount of time and money. There is a considerable amount of data regarding an employee’s role within an organization and the resources to which they have access. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access. Objective The objective of this competition is to build a model, learned using historical data, that will determine an employee's access needs, such that manual access transactions (grants and revokes) are minimized as the employee's attributes change over time. The model will take an employee's role information and a resource code and will return whether or not access should be granted. Partners This competition is hosted in collaboration with the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2013);https://www.kaggle.com/c/amazon-employee-access-challenge;;Predict an employee's access needs, given his/her job role;['auc'];1,686;Amazon.com - Employee Access Challenge;Featured prediction Competition
2013-11-16 00:59:00;"Welcome to the American Meteorological Society 2013-2014 Solar Energy Prediction Contest! This contest is organized by the American Meteorological Society Committees on Artificial Intelligence Applications to Environmental Science, Probability and Statistics, and Earth and Energy. Prizes are sponsored by EarthRisk Technologies, Inc. Motivation Renewable energy sources, such as solar and wind, offer many environmental advantages over fossil fuels for electricity generation, but the energy produced by them fluctuates with changing weather conditions. Electric utility companies need accurate forecasts of energy production in order to have the right balance of renewable and fossil fuels available. Errors in the forecast could lead to large expenses for the utility from excess fuel consumption or emergency purchases of electricity from neighboring utilities. Power forecasts typically are derived from numerical weather prediction models, but statistical and machine learning techniques are increasingly being used in conjunction with the numerical models to produce more accurate forecasts. Objective  The goal of this contest is to discover which statistical and machine learning techniques provide the best short term predictions of solar energy production. Contestants will predict the total daily incoming solar energy at 98 Oklahoma Mesonet sites, which will serve as ""solar farms"" for the contest. Input numerical weather prediction data for the contest comes from the NOAA/ESRL Global Ensemble Forecast System (GEFS) Reforecast Version 2. Data include all 11 ensemble members and the forecast timesteps 12, 15, 18, 21, and 24. Locations of the Mesonet sites relative to the GEFS data are shown in the above figure. Training data will come from 1994-2007. Public testing data will be from 2008-2009. Private testing data for a more recent period will be used for the final evaluation. Acknowledgements Daily solar energy data were provided by the Oklahoma Mesonet with the assistance of Dr. Jeffrey Basara. The GEFS Reforecast Version 2 data were developed and provided by Dr. Thomas Hamill. The contest is being administered by David John Gagne and Dr. Amy McGovern of the University of Oklahoma. About Our Sponsor EarthRisk Technologies creates a market advantage for its clients by uniquely quantifying weather data.  Our company is a research pioneer that analyzes extreme weather risk at lead times longer than one week.  Our techniques enhance competitive business decisions. TempRisk, the company’s first product suite, is a web-based platform that utilizes historical data, machine learning and predictive analytics to project risk for extreme winter cold and summer heat up to 40 days before it occurs. These patent-pending algorithms were developed in conjunction with Scripps Institution of Oceanography at the University of California San Diego.  Energy producers and commodity investment firms currently employ TempRisk in their daily operations. Our customers require a uniquely objective quantitative methods for extreme event prediction.  Our products are continuously developing thanks to ongoing support from customer-partners including large energy companies, investment firms, and reinsurance advisors. EarthRisk's leadership team is excited to be deeply engaged with the American Meteorological Society.  In addition to our engagement with the Committee on Artificial Intelligence Applications to Environmental Science, we're also active on the AMS Energy Committee, the Board on Private Sector Meteorology, the Financial Weather/Climate Risk Management Committee, and the Weather Enterprise Economic Evaluation Team.  We have a true passion for advancing meteorological methods through the intelligent application of technology and are proud to be part of the Solar Energy Prediction Contest!";https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest;;Forecast daily solar energy with an ensemble of weather models;['mae'];160;AMS 2013-2014 Solar Energy Prediction Contest;Research prediction Competition
2019-09-08 01:59:00;"Imagine being able to detect blindness before it happened. Millions of people suffer from diabetic retinopathy, the leading cause of blindness among working aged adults. Aravind Eye Hospital in India hopes to detect and prevent this disease among people living in rural areas where medical screening is difficult to conduct. Successful entries in this competition will improve the hospital’s ability to identify potential patients. Further, the solutions will be spread to other Ophthalmologists through the 4th Asia Pacific Tele-Ophthalmology Society (APTOS) Symposium Currently, Aravind technicians travel to these rural areas to capture images and then rely on highly trained doctors to review the images and provide diagnosis. Their goal is to scale their efforts through technology; to gain the ability to automatically screen images for disease and provide information on how severe the condition may be. In this synchronous Kernels-only competition, you'll build a machine learning model to speed up disease detection. You’ll work with thousands of images collected in rural areas to help identify diabetic retinopathy automatically. If successful, you will not only help to prevent lifelong blindness, but these models may be used to detect other sorts of diseases in the future, like glaucoma and macular degeneration.   Get started today!";https://www.kaggle.com/c/aptos2019-blindness-detection;Asia Pacific Tele-Ophthalmology Society (APTOS);Detect diabetic retinopathy to stop blindness before it's too late;['image data', 'multiclass classification', 'healthcare', 'medicine', 'quadraticweightedkappa'];2,931;APTOS 2019 Blindness Detection;Featured Code Competition
2012-05-01 01:59:59;"The William and Flora Hewlett Foundation (Hewlett) is sponsoring the Automated Student Assessment Prize (ASAP).  Hewlett is appealing to data scientists and machine learning specialists to help solve an important social problem.  We need fast, effective and affordable solutions for automated grading of student-written essays. Hewlett is sponsoring the following prizes:  $60,000:  1 place $30,000:  2 place $10,000:  3 place  You are provided access to hand scored essays, so that you can build, train and test scoring engines against a wide field of competitors.  Your success depends upon how closely you can deliver scores to those of human expert graders.  While we believe that these financial incentives are important, we also intend to introduce top performers both to leading vendors in the industry and/or an established base of interested buyers.  Hewlett is opening the field of automated student assessment to you.  We want to induce a breakthrough that is both personally satisfying and game-changing for improving public education. Today, state departments of education are developing new forms of testing and grading methods, to assess the new common core standards.  In this environment the need for more sophisticated and affordable options is vital.  For example, we know that essays are an important expression of academic achievement, but they are expensive and time consuming for states to grade them by hand.  So, we are frequently limited to multiple-choice standardized tests.  We believe that automated scoring systems can yield fast, effective and affordable solutions that would allow states to introduce essays and other sophisticated testing tools.  We believe that you can help us pave the way towards a breakthrough.  ASAP is designed to achieve the following goals:  Challenge developers of automated student assessment systems to demonstrate their current capabilities. Compare the efficacy and cost of automated scoring to that of human graders. Reveal product capabilities to state departments of education and other key decision makers interested in adopting them.  The graded essays are selected according to specific data characteristics.  On average, each essay is approximately 150 to 550 words in length.  Some are more dependent upon source materials than others.  This range of essay type is provided so that we can better understand the strengths of your solution.  It is our intent to showcase quality and reliability, based on how well you can match expert human graders for each essay. You will be provided with training data for each essay prompt.  The number of training essays does vary.  For example, the lowest amount of training data is 1,190 essays, randomly selected from a total of 1,982.  The data will contain ASCII formatted text for each essay followed by one or more human scores, and (where necessary) a final resolved human score.  Where it is relevant, you are provided with more than one human score, so that you may evaluate the reliability of the human scorers, but - keep in mind - that you will be predicting to the resolved score.  Also, please note that most essays are scored using a holistic scoring rubric.  However, one data set uses a trait scoring rubric.  The variability is intended to test the limits of your scoring engine’s capabilities. Following a period of 3 months to build and/or train your engine, you will be provided with test data that will contain new essays, randomly selected for blind evaluation.  However, you will notice that the rater and resolved score columns will be blank.  You will be asked to supply, based on your engine's predictions for each essay, your score in the resolved score column and then submit your new data set on this site. As part of the file that you will submit with your predictive scores, you will be asked to submit additional information.  We would like to understand both the time and capital that you’ve spent developing your engine, the profile of your team (or you as an individual if you are working alone) and the projected cost to implement your solution on a larger scale, along with any known limitations.  Basically, you will have the opportunity to present your case for who you are, why your model is commercially viable and to what extent you can use your model to satisfy the interests of potential buyers.  This other information will not be used to determine any prize rewards, and it is optional.  But, if you provide it, it will be used to evaluate whether or not your model should be presented to state departments of education and others who stand to benefit from your work. Also, please note that it is our intention to stage other follow-on ASAP phases in the months ahead.  We are starting with graded essays and will follow with new data:  Phase 1: Demonstration for long-form constructed response (essays); Phase 2: Demonstration for short-form constructed response (short answers); Phase 3: Demonstration for symbolic mathematical/logic reasoning (charts/graphs).  In every instance, we seek to drive innovation for new solutions to automated student assessment.  We hope that you will enjoy this process.  May the best model win!";https://www.kaggle.com/c/asap-aes;;Develop an automated scoring algorithm for student-written essays.;['custom metric'];153;The Hewlett Foundation: Automated Essay Scoring;Featured prediction Competition
2012-09-06 01:59:00;"This competition has completed, congratulations to the preliminary winners and the other participants!  The William and Flora Hewlett Foundation (Hewlett Foundation) is sponsoring the Automated Student Assessment Prize (ASAP) in hopes of discovering new tools to support schools and teachers. The competition aspires to solve the problem of the high cost and the slow turnaround of hand scoring thousands of written responses in standardized tests.  As a result many schools exclude written responses in favor of multiple-choice questions, which are less able to assess students’ critical reasoning and writing skills.  ASAP has been designed to help determine whether computerized systems are capable of grading written content accurately for schools and teachers to adopt those solutions.  ASAP aspires to inform key decision makers, who are already considering adopting these systems, by delivering a fair, impartial and open series of trials to test current capabilities and to drive greater awareness when outcomes warrant further consideration. Critical reasoning is one of a suite of skills that experts believe students must be taught to succeed in the new century. The Hewlett Foundation makes grants to educators and nonprofit organizations in support of these skills, which it calls “deeper learning.” They include the mastery of core academic content, critical reasoning and problem solving, working collaboratively, communicating effectively, and learning how to learn independently. With ASAP, Hewlett is appealing to data scientists to help solve an important problem in the field of educational testing.  Hewlett is sponsoring the following prizes as part of Phase Two: $50,000:  1 place $25,000:  2 place $15,000:  3 place $  7,500:  4 place $  2,500:  5 place In May of this year, $100,000 in prizes was rewarded for ASAP, Phase One, and we have launched ASAP, Phase Two, with the same intentions.  During Phase One, we focused on systems to support the grading of student written essays. This time, we’re offering a similar competition, only focused on short answer responses.  We welcome you to learn more about our previous phase at www.kaggle.com/c/asap-aes During Phase Two, you are provided access to graded short answer responses and their corresponding prompts, so that you can build, train and test your scoring engines against a wide field of competitors. Your success depends upon how closely you can align your scores to those of human expert graders.  While we believe that a pool of $100,000 in potential financial incentives are important, we also intend to secure and distribute your solutions to the public, in hopes of elevating the field of automated assessment through your contributions.  We want you to induce a breakthrough that is both personally satisfying and game-changing for improving public education. We have already learned that automated assessment systems can yield fast, effective and affordable solutions that would allow states to introduce new testing tools capable of assessing deeper measures of learning.  We believe that you can help us pave the way towards better student assessment.  ASAP is designed to achieve the following goals:  Challenge developers of student assessment systems to demonstrate their current capabilities. Reveal the efficacy and cost of alternative scoring systems to support teachers. Promote the capabilities of effective scoring systems to state departments of education and other key decision makers, when those advantages have been proven to support student and teacher interests.  The Phase Two graded content is selected according to specific characteristics.  On average, each answer is approximately 50 words in length.  Some are more dependent upon source materials than others, and the answers cover a broad range of disciplines (from English Language Arts to Science).  The range of answer types is provided so that we can better understand the strengths of your solution.  It is our intent to showcase quality and reliability, based on how well you can align with expert human graders for each response. You will be provided with training data for each prompt.  Most training sets will consist of about 1,800 responses that have been randomly selected from a sample of approximately 3,000.  The number of training data may vary.  The data will contain ASCII formatted text for each response followed by two hand scores.  The first score is the final score and the one that you are trying to predict. The second score was used to determine reliability of the first score. The second score did not in any way influence the first (final) score. You are provided with both scores, so that you may evaluate the reliability of the hand scoring.  Further instruction and clarification regarding the data is available on the DATA tab. Following a period of 2.5 months to train your scoring engine, you will be provided with test data that will contain approximately 6,000 new responses (600 per data set), randomly selected for blind evaluation.  However, you will notice that the score columns will be blank.  You will be asked to supply, based on your engine's predictions for each response, your score for each response and to submit your new scored data set on this site. As part of the ZIP file that you will submit with your predictive scores, you will be asked to submit a technical METHODS PAPER. We would like to understand your specific approach to developing your scorig engine, along with any known limitations. Basically, you will have the opportunity to present your scoring engine to the world, so that others may build upon it.  Your technical METHODS PAPER will not be used to determine any prize rewards, but it is a required component of your final submission. Also, please note that it is our intention to continue staging other follow-on ASAP phases in the months ahead.  We have started with graded essays (Phase 1), and we are now focusing on short answers (Phase 2); we are developing plans for a third phase, and we’re planning to launch a phase to demonstrate efficacy of systems capable of offering formative feedback as part of classroom applications:  Phase 1:  Demonstration for long-form constructed response (essays);  Phase 2:  Demonstration for short-form constructed response (short answers); Phase 3:  Demonstration for symbolic mathematical/logic reasoning (charts/graphs).  In every instance, we seek to drive innovation for new solutions to student assessment, to support teachers in evaluating critical reasoning skills.  We hope that you will enjoy this process.  May the best model win!";https://www.kaggle.com/c/asap-sas;;Develop a scoring algorithm for student-written short-answer responses.;['custom metric'];151;The Hewlett Foundation: Short Answer Scoring;Prospect prediction Competition
2019-12-20 00:59:00;Q: How much does it cost to cool a skyscraper in the summer? A: A lot! And not just in dollars, but in environmental impact. Thankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That’s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don’t work with different building types. In this competition, you’ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies. About the Host  Founded in 1894, ASHRAE serves to advance the arts and sciences of heating, ventilation, air conditioning refrigeration and their allied fields. ASHRAE members represent building system design and industrial process professionals around the world. With over 54,000 members serving in 132 countries, ASHRAE supports research, standards writing, publishing and continuing education - shaping tomorrow’s built environment today. Banner photo by Federico Beccari on Unsplash;https://www.kaggle.com/c/ashrae-energy-prediction;ASHRAE;How much energy will a building consume?;['tabular data', 'energy', 'rmsle'];3,614;ASHRAE - Great Energy Predictor III;Featured prediction Competition
2015-02-10 00:59:00;In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.  For this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.;https://www.kaggle.com/c/avazu-ctr-prediction;;Predict whether a mobile ad will be clicked;['logloss'];1,602;Click-Through Rate Prediction;Featured prediction Competition
2015-07-29 01:59:00;In Russia, if you're looking to sell a tractor, a designer dress, a vintage lunchbox, or even a house, your first stop will likely be Avito.ru. As the largest general classified website in Russia, Avito connects buyers and sellers across the world's biggest country. Sellers are highly motivated to place ads on Avito, hoping to gain attention from the site's 70 million unique monthly visitors. There are three different types of ads available to sellers on Avito: regular, highlighted, and context.  Context ads are seen as the best way to target users with goods and services. Currently, Avito uses general statistics on ad performance to drive the placement of context ads. Their existing model ignores individual user behavior, making it difficult to predict which ad will be the most relevant for (and earn the most clicks from) each potential buyer.  In this competition, Avito is challenging you to improve on their model by predicting if individual users will click a given context ad. To create the most robust model and fun competition possible, Avito has provided eight comprehensive relational datasets for you to explore. This competition will help Avito more accurately predict click-through rates for their ads, creating a world where both buyers and sellers win.;https://www.kaggle.com/c/avito-context-ad-clicks;Avito;Predict if context ads will earn a user's click;['tabular data', 'marketing', 'logloss'];413;Avito Context Ad Clicks;Featured prediction Competition
2018-06-28 01:59:00;When selling used goods online, a combination of tiny, nuanced details in a product description can make a big difference in drumming up interest. Details like:   And, even with an optimized product listing, demand for a product may simply not exist–frustrating sellers who may have over-invested in marketing. Avito, Russia’s largest classified advertisements website, is deeply familiar with this problem. Sellers on their platform sometimes feel frustrated with both too little demand (indicating something is wrong with the product or the product listing) or too much demand (indicating a hot item with a good description was underpriced). In their fourth Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive.;https://www.kaggle.com/c/avito-demand-prediction;Avito;Predict demand for an online classified ad;['image data', 'text data', 'tabular data', 'rmse'];1,871;Avito Demand Prediction Challenge;Featured prediction Competition
2016-07-12 01:59:00;Online marketplaces make it a breeze for users to both find and buy unique treasures or unload their dusty record collections in the spirit of spring cleaning. As one of the world's largest and fastest growing online classifieds, Avito hosts high volumes of listings and competitive sellers often go to great lengths to get their wares noticed.   For some sellers, this means posting the same ad several times with slightly altered text or photos taken from different angles. To ensure that buyers can easily find what they're looking for without sifting through dozens of deceptively identical ads, Avito is asking Kagglers to develop a model that can automatically spot duplicate ads. With more accurate duplicate ad detection, Avito will make it much easier for buyers to find and make their next purchase with an honest seller.;https://www.kaggle.com/c/avito-duplicate-ads-detection;Avito;Can you detect duplicitous duplicate ads?;['internet', 'auc'];546;Avito Duplicate Ads Detection;Featured prediction Competition
2014-09-01 01:59:00;Avito.ru is the largest general classified website in Russia that helps connect buyers with sellers across all Russian territories. There are more than 22 million active ads on Avito and each day a huge number of ads are added or modified. The efficiency of Avito depends heavily on the content quality -- when buyers can quickly find relevant content, sellers can sell their items in hours. The larger and more popular Avito becomes the more attractive it becomes to sell illicit items or services. Some items that people try to sell are completely illegal while others might seem allowable but are still prohibited by our rules. This is why all new or modified ads are thoroughly moderated by our team of human moderators. The moderators can remove the ad if it conflicts with the Russian legislation or with the internal rules of AVITO.ru. However, with our growth it becomes more and more challenging to thoroughly moderate all ads. This is where machine learning comes into play. The objective of this challenge is to create a predictive model that will learn from moderators' answers how to classify if an ad contains illicit content or not.;https://www.kaggle.com/c/avito-prohibited-content;Avito;Predict which ads contain illicit content;['ap@{k}'];284;The Hunt for Prohibited Content;Featured prediction Competition
2012-04-16 01:59:59;Writer identification is a very active research field. It is of a primordial importance in forensic document examination when it helps experts in delibirating on the authenticity of a certain document. This is a follow-up contest of the last year'  Arabic Writer Identification Contest. As we mentioned in the last edition,    we have significantly augmented the number of writers (we have more than 200 writers in this new database). we will not be providing any side-information (eg. number of documents per writer), as this is not necessarly known in a real forensic casework. we will only provide binary images, as color and gray-level images might transform this into a pen identification task.  This competition is organized in conjunction with the International Conference of Frontiers in Handwriting Recognition ICFHR2012 which will be held in Bari, Italy in September 18-20.;https://www.kaggle.com/c/awic2012;;Identify which writer wrote which documents.;['categorizationaccuracy'];42;ICFHR 2012 - Arabic Writer Identification;Research prediction Competition
2015-03-17 00:59:00;"For automobile insurers, telematics represents a growing and valuable way to quantify driver risk. Instead of pricing decisions on vehicle and driver characteristics, telematics gives the opportunity to measure the quantity and quality of a driver's behavior. This can lead to savings for safe or infrequent drivers, and transition the burden to policies that represent increased liability. AXA has provided a dataset of over 50,000 anonymized driver trips. The intent of this competition is to develop an algorithmic signature of driving type. Does a driver drive long trips? Short trips? Highway trips? Back roads? Do they accelerate hard from stops? Do they take turns at high speed? The answers to these questions combine to form an aggregate profile that potentially makes each driver unique. For this competition, Kaggle participants must come up with a ""telematic fingerprint"" capable of distinguishing when a trip was driven by a given driver. The features of this driver fingerprint could help assess risk and form a crucial piece of a larger telematics puzzle.";https://www.kaggle.com/c/axa-driver-telematics-analysis;;Use telematic data to identify a driver signature;['multiclass classification', 'tabular data', 'auc'];1,524;Driver Telematics Analysis;Featured prediction Competition
2013-10-02 01:59:00;"The Big Data Combine engineered by BattleFin are rapid fire, live tryouts for computer scientists with elite predictive analytic skills intent on monetizing their models. The first stage of the competition is a predictive modeling competition that requires participants to develop a model that predicts stock price movements using sentiment data provided by RavenPack. Traders, analysts and investors are always looking for techniques to better predict price movements.  Knowing whether a security will increase or decrease allows traders to make better investment decisions and manage risk more effectively.  This competition is designed to identify people with the talent to create a predictive model using financial data. Competitors are given intraday trading data showing stock price movements at 5 minute intervals and asked to predict the change two hours in the future. The winners of the predictive modeling phase are invited to the ""live"" Big Data Combine tryouts in Miami, FL. Up to 12 finalists will be selected to compete in the live event in Miami. The lucky few will pitch their predictive model to expert judges and an engaged audience. They have only three minutes to present in non-technical terms three items: personal background, predictive model description, and how they would use there model to make money in finance. If their model and presentation impresses our judges, they will be eligible to work with BattleFin and Deltix to convert their predictive model into a trading strategy. Master of Ceremonies (""MC"") Matt Iseman - Actor, Comedian, Host of American Ninja Warrior Mr. Iseman has hosted the game shows Scream Play on E! and Casino Night on the GSN. He currently appears as a regular cast member on the home makeover show Clean House, and its companion outtakes show, Clean House Comes Clean, both on the Style Network. Additionally, he hosted season 2 and 3 of American Ninja Warrior on the channel G4. He also has worked episodically in television shows including The Drew Carey Show, NCIS, and General Hospital. He has appeared on the syndicated MAD TV, Comedy Central’s Premium Blend, Fox’s The Best Damn Sports Show Period, and Fox News Channel’s Red Eye w/ Greg Gutfeld. Mr. Iseman was also the host of Sports Soup, a spin-off of E!'s The Soup, on Versus. Style Network and Versus are owned by Comcast. Iseman began working with American Ninja Warrior (G4) in 2010. He uses his great athleticism and work as a comedian to add his style to the show with Johnny Moseley (American Professional Freestyle Skier), and Angela Sun (Sideline Correspondent). Judges Ilya Gorelik, CEO of Deltix Inc. CEO, Ilya Gorelik is responsible for setting the strategic direction of the company, as well as overseeing global product development, sales and marketing.Ilya has more than 15 years of experience managing large-scale software projects and teams. He was one of the key development leaders of PTC, where he worked from 1989 to 1998, attaining the position of Senior VP of Engineering and Chief Technology Officer. From 1998 until 2000, Ilya was Senior VP of Product Strategy and Development and Chief Scientist for FirePond. From 2000 until founding Deltix in 2005, Ilya worked as Advisory CTO for HighRoads and several other software technology companies. Ilya has a Ph.D. in ComputationalMechanics from Moscow Technical University, he received an MS in Mechanical Engineering from Minsk Technical University. Peter Hafez, Head Quantitative Research at RavenPack Peter is an award-winning expert in the field of applied news analytics and has consulted numerous leading trading and investment firms on how to take advantage of news analytics in financial markets. Peter has more than 10 years of experience in quantitative finance with companies such as Standard & Poor's, Credit Suisse First Boston, and Saxo Bank. He is a recognized speaker at conferences on behavioral finance and algorithmic trading. Peter holds a Master's degree in Quantitative Finance from City University's Cass Business School along with an undergraduate degree in Economics from Copenhagen University. Nabyl Charania, Managing Director at Rokk3r Labs Nabyl is a Co-Founder and Managing Director at Rokk3r Labs.  Rokk3r Labs is Miami based Venture Capital firm with 35 employees.  Rokk3r Labs fuses entrepreneurial and professional talent to help entrepreneurs create ideas, prototypes, products and generally invests in disruptive companies designed for the modern hyper-connected world. Prior, he was a Founder and Managing Director at Decipher Labs Inc. He graduated from University of Waterloo in 2000. Zeid Barakat, Co-Founder at Flyberry Capital LLC  Zeid is a Co-Founder of Flyberry Capital a Boston based hedge fund that utilizes a Big Data strategy. Zeid works with ‘best-in class’ MIT & Harvard computer scientists and machine learning experts to develop proprietary trading strategies, focused on event-based arbitrage. In charge of dfining corporate growth strategy,business development, marketing, managing Board of Advisers, and fundraising. He is an Entrepreneur in high-tech companies, focused on novel approaches to biotechnology, healthcare and financial services. He earned MBA degree in General Management and Entrepreneurship from MIT in 2008. About BattleFin BattleFin is a tournament platform that crowdsources the world's best investment talent. The firm uses rapid fire, real capital tournaments to democratically identify up and coming investment talent. BattleFin has recently been featured in Bloomberg Business Week in an article titles ""The Hedge Fund Hunger Games"". The firm is passionate about leveling the playing field in finance so that anyone with an internet connection can participate in its tournaments. The firm specializes in finding the hedge fund managers of tomorrow. To learn more about BattleFin visit BattleFin.com. About Deltix Founded in 2005 and with more than 50 staff, Deltix has established itself as a leader in the growing domain of software and services for quantitative research and systematic automated trading. The Deltix Product Suite is an end-to-end platform for all phases of the alpha discovery and trading life-cycle; including data collection and aggregation, model development, back-testing, optimization, simulation, and deployment to production trading. About RavenPack Financial firms are overloaded with information and have turned to computers to read news and other media. What would take days for an investment professional to read and interpret takes computers only a few milliseconds. Now financial institutions can react much faster to the ever increasing amounts of news and information available for making investment decisions. Powered by a proprietary text analysis platform, RavenPack the tournament data sponsor, analyzes novel and relevant stories published by major news sources to look for key scheduled and unexpected geopolitical, macro-economic and corporate events, topics and opinions that indicate changes in market sentiment. Sampled news sources represent the most reliable and authoritative publishers of business and financial news.RavenPack continuously analyzes relevant information from Dow Jones Newswires, regional editions for the Wall Street Journal, and Barron’s to produce real time news sentiment scores and events from entities across multiple asset classes, including currencies, commodities, organizations, companies, sectors, and industries.";https://www.kaggle.com/c/battlefin-s-big-data-combine-forecasting-challenge;;Predict short term movements in stock prices using news and sentiment data provided by RavenPack;['mae'];424;The Big Data Combine Engineered by BattleFin;Research prediction Competition
2013-10-31 00:59:00;"Imagine an energy feedback system that displays not only your total power consumption, but also continuously shows real-time usage, broken down by electrical appliance. Such a system could provide personalized and cost-effective energy saving recommendations. For example, it could report, ""Based on your usage patterns, you could save $215 per year by switching to a more efficient heating unit, which will pay for itself in 27 months."" The challenge in this scenario is to sense end-uses of energy to provide feedback at the fine-grained, appliance level. There has been substantial prior research in this area [1,5,6,7,8], however most of this work has concentrated on the use of power consumption patterns and using changes in power draw as features to identify what appliance is being used and how much energy it is consuming. We recommend the reader refer to [2,3,9] for a detailed overview of machine learning features for energy disaggregation. A more recent approach to estimate appliance usage is to examine the Electromagnetic Interference (EMI) that most consumer electronic appliances produce as identifying signatures [4]. This EMI is measured using a special sensor built at the Ubicomp Lab at the University of Washington as part of Sidhant Gupta's thesis work. The figure below shows an example of EMI captured from a home. The plot is in frequency domain and shows the signatures of various appliances.  The presence or absence of such EMI signatures would ideally tell us when a particular appliance is in use. However, due to the large numbers of appliances in a home, the solution is not straightforward. Machine learning is required not only to make an inference about the appliance class given a particular signature, but probabilistic models are needed that take into account, for example, human appliance usage patterns (think using coffee machine and toaster in morning vs. lights in evening), weather patterns (very unlikely that AC came on during winters), and appliance electrical model. The signature of an appliance can also drift or vary over time due to operating conditions and the mode in which they are used (for instance, a washing machine has many modes). We encourage participants to review [4] to better understand the use of EMI for electrical appliance use detection and classification. Videos and Slides Here are a few lab quality videos that may helo you grasp the big picture: Video of the signal: http://youtu.be/o-SqO8y8XUAVideo of the technology applied to energy monitoring: http://www.youtube.com/watch?v=dcPI1Cp0VZI Slides from conference talk for ElectriSense can be accessed here: http://homes.cs.washington.edu/~sidhant/slides/ElectriSense_PDF.pdf References 1. Berges, M., Goldman, E., Matthews, H.S., and Soibelman, L. Training Load Monitoring Algorithms on Highly Sub-Metered Home Electricity Consumption Data. Tsinghua Science & Technology 13, Supple, 0 (2008), 406–411. 2. Carrie Armel, K., Gupta, A., Shrimali, G., and Albert, A. Is disaggregation the holy grail of energy efficiency? The case of electricity. Energy Policy 52, (2012), 213–234. 3. Froehlich, J., Larson, E., Gupta, S., Cohn, G., Reynolds, M., and Patel, S. Disaggregated End-Use Energy Sensing for the Smart Grid. IEEE Pervasive Computing 10, 1 (2011), 28–39. 4. Gupta, S., Reynolds, M., and Patel, S. ElectriSense: Single-Point Sensing Using EMI for Electrical Event Detection and Classification in the home. Ubicomp 2010, (2010). 5. Hart, G. Nonintrusive appliance load monitoring. Proceedings of the IEEE, (1992). 6. Laughman, C., Lee, K., and Cox, R. Power signature analysis. IEEE Power and Energy, april 2003 (2003). 7. Leeb, S.B., Shaw, S.R., and Kirtley, J.L. Transient Event Detection in Spectral Envelope Estimates. IEEE Transactions on Power Delivery 10, 3 (1995), 1200–1210. 8. Norford, L.K. and Leeb, S.B. Non-intrusive electrical load monitoring in commercial buildings based on steady-state and transient load-detection algorithms. Energy and Buildings 24, 1 (1996), 51–64. 9. Zeifman, M., Ph, D., and Roth, K. Non-Intrusive Appliance Load Monitoring ( NIALM ): Review and Outlook * Fraunhofer : A Leading Force in Applied R & D. Consumer Electronics, January (2011).";https://www.kaggle.com/c/belkin-energy-disaggregation-competition;;Disaggregate household energy consumption into individual appliances;['custom metric'];165;Belkin Energy Disaggregation Competition;Featured prediction Competition
2012-05-01 01:59:59;The Benchmark Bond Trade Price Challenge is a competition to predict the next price that a US corporate bond might trade at. Contestants are given information on the bond including current coupon, time to maturity and a reference price computed by Benchmark Solutions.  Details of the previous 10 trades are also provided.;https://www.kaggle.com/c/benchmark-bond-trade-price-challenge;;Develop models to accurately predict the trade price of a bond.;['wmae'];263;Benchmark Bond Trade Price Challenge;Featured prediction Competition
2020-03-17 00:59:00;Challenge and dataset summary available at https://arxiv.org/abs/2010.00170 Bengali is the 5th most spoken language in the world with hundreds of million of speakers. It’s the official language of Bangladesh and the second most spoken language in India. Considering its reach, there’s significant business and educational interest in developing AI that can optically recognize images of the language handwritten. This challenge hopes to improve on approaches to Bengali recognition.  Optical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to English’s 250 graphemic units). Bangladesh-based non-profit Bengali.AI is focused on helping to solve this problem. They build and release crowdsourced, metadata-rich datasets and open source them through research competitions. Through this work, Bengali.AI hopes to democratize and accelerate research in Bengali language technologies and to promote machine learning education. For this competition, you’re given the image of a handwritten Bengali grapheme and are challenged to separately classify three constituent elements in the image: grapheme root, vowel diacritics, and consonant diacritics. By participating in the competition, you’ll hopefully accelerate Bengali handwritten optical character recognition research and help enable the digitalization of educational resources. Moreover, the methods introduced in the competition will also empower cousin languages in the Indian subcontinent. Acknowledgements:    Apurba: Apurba is the exclusive sponsor of Bengali.AI for this competition. Apurba Technologies Inc. is founded by a group of technology veterans who have been working at the cutting edge of software development in Silicon Valley for many years. Apart from its many ventures, Apurba is a pioneer in Bengali NLP research today and is accelerating AI research in Bangladesh through its contributions.     Intelligent Machines Limited: Intelligent Machines Limited is the technical partner of Bengali.AI for this competition and is providing compute support to Bangladeshi students. IML is an Artificial Intelligence and Advanced Analytics startup offering customized solutions to businesses in Bangladesh. IML believes in the strength of Bangladeshi talented resources and in the possibility of a far greater and developed Bangladesh in the coming days.;https://www.kaggle.com/c/bengaliai-cv19;Bengali.AI;Classify the components of handwritten Bengali;['image data', 'multiclass classification', 'weightedcategorizationaccuracy'];2,059;Bengali.AI Handwritten Grapheme Classification;Research Code Competition
2019-12-13 00:59:00;We’ve all been there: Stuck at a traffic light, only to be given mere seconds to pass through an intersection, behind a parade of other commuters. Imagine if you could help city planners and governments anticipate traffic hot spots ahead of time and reduce the stop-and-go stress of millions of commuters like you. Geotab provides a wide variety of aggregate datasets gathered from commercial vehicle telematics devices. Harnessing the insights from this data has the power to improve safety, optimize operations, and identify opportunities for infrastructure challenges. The dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.  This competition is being hosted in partnership with BigQuery, a data warehouse for manipulating, joining, and querying large scale tabular datasets. BigQuery also offers BigQuery ML, an easy way for users to create and run machine learning models to generate predictions through a SQL query interface. Kaggle recently released a BigQuery integration within our kernels notebook environment, and this starter kernel gives you a great starting point for how to use BQ & BQML. You’re encouraged to use your data savvy, resourcefulness & intuition to find and join in additional external datasets that will increase your models’ predictive power. Alright, stop waiting and get started! Acknowledgments  A big thanks to Geotab for providing the dataset for this competition! Geotab is advancing security, connecting commercial vehicles to the internet and providing web-based analytics to help customers better manage their fleets. Geotab’s open platform and Marketplace, offering hundreds of third-party solution options, allows both small and large businesses to automate operations by integrating vehicle data with their other data assets. As an IoT hub, the in-vehicle device provides additional functionality through IOX Add-Ons. Processing billions of data points a day, Geotab leverages data analytics and machine learning to help customers improve productivity, optimize fleets through the reduction of fuel consumption, enhance driver safety, and achieve strong compliance to regulatory changes. Geotab’s products are represented and sold worldwide through Authorized Geotab Resellers. To learn more, please visit www.geotab.com and follow us @GEOTAB and on LinkedIn.;https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;Google BigQuery;Can you predict wait times at major city intersections?;['regression', 'tabular data', 'geospatial analysis', 'cities and urban areas', 'rmse'];436;BigQuery-Geotab Intersection Congestion;Playground prediction Competition
2015-05-30 01:59:00;Get started on this competition through Kaggle Scripts Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world. The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.  Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Hadi Fanaee Tork using data from Capital Bikeshare. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Fanaee-T, Hadi, and Gama, Joao, Event labeling combining ensemble detectors and background knowledge, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.;https://www.kaggle.com/c/bike-sharing-demand;Kaggle;Forecast use of a city bikeshare system;['tabular data', 'cycling', 'rmsle'];3,242;Bike Sharing Demand;Playground prediction Competition
2015-05-02 01:59:00;This competition uses the billion-word benchmark corpus provided by Chelba et al. for language modeling. Rather than ask participants to create a classic language model and evaluate sentence probabilities -- a task which is difficult to faithfully score in Kaggle's supervised ML setting -- we have introduced a variation on the language modeling task. For each sentence in the test set, we have removed exactly one word. Participants must create a model capable of inserting back the correct missing word at the correct location in the sentence. Submissions are scored using an edit distance to allow for partial credit. We extend our thanks to authors who created this corpus and shared it for the research community to use. Please cite this paper if you use this dataset in your research: Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn: One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling, CoRR, 2013. Note: the train/test split used in this competition is different than the published version used for language modeling. If you are creating full language models and scoring perplexity, you should download the official version of the corpus from the authors' website.;https://www.kaggle.com/c/billion-word-imputation;Kaggle;Find and impute missing words in the billion word corpus;['text data', 'linguistics', 'levenshteinmean'];87;Billion Word Imputation;Playground prediction Competition
2012-06-16 01:59:59;"The objective of the competition is to help us build as good a model as possible so that we can, as optimally as this data allows, relate molecular information, to an actual biological response. We have shared the data in the comma separated values (CSV) format. Each row in this data set represents a molecule. The first column contains experimental data describing an actual biological response; the molecule was seen to elicit this response (1),  or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are calculated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The descriptor matrix has  been normalized.";https://www.kaggle.com/c/bioresponse;;Predict a biological response of molecules from their chemical properties;['logloss'];698;Predicting a Biological Response;Featured prediction Competition
2020-09-16 01:59:00;Do you hear the birds chirping outside your window? Over 10,000 bird species occur in the world, and they can be found in nearly every environment, from untouched rainforests to suburbs and even cities. Birds play an essential role in nature. They are high up in the food chain and integrate changes occurring at lower levels. As such, birds are excellent indicators of deteriorating habitat quality and environmental pollution. However, it is often easier to hear birds than see them. With proper sound detection and classification, researchers could automatically intuit factors about an area’s quality of life based on a changing bird population.  There are already many projects underway to extensively monitor birds by continuously recording natural soundscapes over long periods. However, as many living and nonliving things make noise, the analysis of these datasets is often done manually by domain experts. These analyses are painstakingly slow, and results are often incomplete. Data science may be able to assist, so researchers have turned to large crowdsourced databases of focal recordings of birds to train AI models. Unfortunately, there is a domain mismatch between the training data (short recording of individual birds) and the soundscape recordings (long recordings with often multiple species calling at the same time) used in monitoring applications. This is one of the reasons why the performance of the currently used AI models has been subpar.  To unlock the full potential of these extensive and information-rich sound archives, researchers need good machine listeners to reliably extract as much information as possible to aid data-driven conservation. The Cornell Lab of Ornithology’s Center for Conservation Bioacoustics (CCB)’s mission is to collect and interpret sounds in nature. The CCB develops innovative conservation technologies to inspire and inform the conservation of wildlife and habitats globally. By partnering with the data science community, the CCB hopes to further its mission and improve the accuracy of soundscape analyses. In this competition, you will identify a wide variety of bird vocalizations in soundscape recordings. Due to the complexity of the recordings, they contain weak labels. There might be anthropogenic sounds (e.g., airplane overflights) or other bird and non-bird (e.g., chipmunk) calls in the background, with a particular labeled bird species in the foreground. Bring your new ideas to build effective detectors and classifiers for analyzing complex soundscape recordings! If successful, your work will help researchers better understand changes in habitat quality, levels of pollution, and the effectiveness of restoration efforts. Reliable machine listeners would also allow conservationists to deploy more recording units worldwide and would enable data-driven conservation at a scale not yet possible. The eventual conservation outcomes could greatly improve the quality of life for many living organisms—birds and human beings included.;https://www.kaggle.com/c/birdsong-recognition;Cornell Lab of Ornithology;Build tools for bird population monitoring;['audio data', 'meanfscorebeta'];1,390;Cornell Birdcall Identification;Research Code Competition
2013-04-18 01:59:00;There is a $10,000 prize pool for this competition, with prizes awarded to the top 3 places:  1st place: $6,500  2nd place: $2,500  3rd place: $1,000;https://www.kaggle.com/c/bluebook-for-bulldozers;;"Predict the auction sale price for a piece of heavy equipment to create a ""blue book"" for bulldozers.";['rmsle'];474;Blue Book for Bulldozers;Featured prediction Competition
2016-04-19 01:59:00;As a global specialist in personal insurance, BNP Paribas Cardif serves 90 million clients in 36 countries across Europe, Asia and Latin America. In a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers.  In this challenge, BNP Paribas Cardif is providing an anonymized database with two categories of claims:     Kagglers are challenged to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore provide a better service to its customers.;https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;;Can you accelerate BNP Paribas Cardif's claims management process?;['binary classification', 'banking', 'tabular data', 'logloss'];2,920;BNP Paribas Cardif Claims Management;Featured prediction Competition
2016-11-12 00:59:00;A good chocolate soufflé is decadent, delicious, and delicate. But, it's a challenge to prepare. When you pull a disappointingly deflated dessert out of the oven, you instinctively retrace your steps to identify at what point you went wrong. Bosch, one of the world's leading manufacturing companies, has an imperative to ensure that the recipes for the production of its advanced mechanical components are of the highest quality and safety standards. Part of doing so is closely monitoring its parts as they progress through the manufacturing processes.  Because Bosch records data at every step along its assembly lines, they have the ability to apply advanced analytics to improve these manufacturing processes. However, the intricacies of the data and complexities of the production line pose problems for current methods. In this competition, Bosch is challenging Kagglers to predict internal failures using thousands of measurements and tests made for each component along the assembly line. This would enable Bosch to bring quality products at lower costs to the end user.;https://www.kaggle.com/c/bosch-production-line-performance;Bosch;Reduce manufacturing failures;['binary classification', 'tabular data', 'manufacturing', 'matthewscorrelationcoefficient'];1,370;Bosch Production Line Performance;Featured prediction Competition
2013-11-10 23:59:00;Boston Data Festival is hosting a Hackathon on Sunday 11/10/13 from 10 am to 6 pm. The event will take place at Hack/Reduce (275 Third Street, Cambridge, MA). The goal of the Hackathon is to predict the directional accuracy of a stock prices. The following cash prices will be awarded!  1st place: $500 2nd place: $250 Best submission using Matlab: $250 Self-respect from doing better than a monkey throwing darts: Priceless.  During the Hackathon every participant can use a free Matlab license. Acknowledgements We thank MathWorks for providing free licences for competitors during the competition.;https://www.kaggle.com/c/boston-data-festival-hackathon;;Can you make a better prediction than a monkey with a dart?;['auc'];21;Boston Data Festival Hackathon;Playground prediction Competition
2019-04-12 01:59:00;CareerCon 2019 is upon us! CareerCon is a digital event all about landing your first data science job — and registration is now open! Ahead of the event, we have a fun competition to get you started. See below for a unique challenge and opportunity to share your resume with select CareerCon sponsors.  ___________________________________ The Competition Robots are smart… by design. To fully understand and properly navigate a task, however, they need input about their environment. In this competition, you’ll help robots recognize the floor surface they’re standing on using data collected from Inertial Measurement Units (IMU sensors). We’ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity. Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won’t fall down on the job.  Special thanks for making this competition possible: The data for this competition has been collected by Heikki Huttunen and Francesco Lomio from the Department of Signal Processing and Damoon Mohamadi, Kaan Celikbilek, Pedram Ghazi and Reza Ghabcheloo from the Department of Automation and Mechanical   Engineering both from Tampere University, Finland. We at Kaggle would like thank them all for kindly donating the data that has made this competition possible!;https://www.kaggle.com/c/career-con-2019;Kaggle;Compete to get your resume in front of our sponsors;['tabular data', 'signal processing', 'robotics', 'categorizationaccuracy'];1,449;CareerCon 2019 - Help Navigate Robots;Recruitment prediction Competition
2017-09-28 01:59:00;As with any big purchase, full information and transparency are key. While most everyone describes buying a used car as frustrating, it’s just as annoying to sell one, especially online. Shoppers want to know everything about the car but they must rely on often blurry pictures and little information, keeping used car sales a largely inefficient, local industry. Carvana, a successful online used car startup, has seen opportunity to build long term trust with consumers and streamline the online buying process. An interesting part of their innovation is a custom rotating photo studio that automatically captures and processes 16 standard images of each vehicle in their inventory. While Carvana takes high quality photos,  bright reflections and cars with similar colors as the background cause automation errors, which requires a skilled photo editor to change.  In this competition, you’re challenged to develop an algorithm that automatically removes the photo studio background. This will allow Carvana to superimpose cars on a variety of backgrounds. You’ll be analyzing a dataset of photos, covering different vehicles with a wide variety of year, make, and model combinations.;https://www.kaggle.com/c/carvana-image-masking-challenge;Carvana;Automatically identify the boundaries of the car in an image;['image data', 'automobiles and vehicles', 'dice'];735;Carvana Image Masking Challenge;Featured prediction Competition
2019-12-10 00:59:00;Is there a cat in your dat? A common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured. Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:  binary features low- and high-cardinality nominal features low- and high-cardinality  ordinal features (potentially) cyclical features  This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community. If you're not sure how to get started, you can check out the Categorical Variables  section of Kaggle's Intermediate Machine Learning course.  Have Fun!;https://www.kaggle.com/c/cat-in-the-dat;Kaggle;Binary classification, with every feature a categorical;['binary classification', 'tabular data', 'categorical data', 'auc'];1,342;Categorical Feature Encoding Challenge;Playground prediction Competition
2020-04-01 01:59:00;Can you find more cat in your dat? We loved the participation and engagement with the first Cat in the Dat competition. Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:  binary features low- and high-cardinality nominal features low- and high-cardinality  ordinal features (potentially) cyclical features  This follow-up competition offers an even more challenging dataset so that you can continue to build your skills with the common machine learning task of encoding categorical variables.  This challenge adds the additional complexity of feature interactions, as well as missing data. This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community. If you're not sure how to get started, you can check out the Categorical Variables  section of Kaggle's Intermediate Machine Learning course.  Have Fun!;https://www.kaggle.com/c/cat-in-the-dat-ii;Kaggle;Binary classification, with every feature a categorical (and interactions!);['binary classification', 'categorical data', 'auc'];1,161;Categorical Feature Encoding Challenge II;Playground prediction Competition
2013-09-03 01:59:00;"The problem of attributing causes to effects is pervasive in science, medicine, economy and almost every aspects of our everyday life involving human reasoning and decision making. What affects your health? the economy? climate changes? The gold standard to establish causal relationships is to perform randomized controlled experiments. However, experiments are costly while non-experimental ""observational"" data collected routinely around the world are readily available. Unraveling potential cause-effect relationships from such observational data could save a lot of time and effort. Consider for instance a target variable B, like occurence of ""lung cancer"" in patients. The goal would be to find whether a factor A, like ""smoking"", might cause B. The objective of the challenge is to rank pairs of variables {A, B} to prioritize experimental verifications of the conjecture that A causes B. As is known, ""correlation does not mean causation"". More generally, observing a statistical dependency between A and B does not imply that A causes B or that B causes A;  A and B could be consequences of a common cause. But, is it possible to determine from the joint observation of samples of two variables A and B that A should be a cause of B? There are  new algorithms that have appeared in the literature in the past few years that tackle this problem. This challenge is an opportunity to evaluate them and propose new techniques to improve on them. We provide hundreds of pairs of real variables with known causal relationships from domains as diverse as chemistry, climatology, ecology, economy, engineering, epidemiology, genomics, medicine, physics. and sociology. Those are intermixed with controls (pairs of independent variables and pairs of variables that are dependent but not causally related) and semi-artificial cause-effect pairs (real variables mixed in various ways to produce a given outcome). This challenge is limited to pairs of variables deprived of their context. Thus constraint-based methods relying on conditional independence tests and/or graphical models are not applicable. The goal is to push the state-of-the art in complementary methods, which can eventually disambiguate Markov equivalence classes. If you are skeptical that this is possible, try this quiz: Examine the plot below of values of variable B plotted as a function of values of variable A. Can you guess which one is a cause of the other? Hint: Some non-linear functions are non-invertible.   July 1: A new data release was made to address a normalization problem and the deadline was extended. Scores on the public leaderboard prior to July 1 were decreased by 0.5. Please make new submissions with the new validation set. The competition is open to new teams.";https://www.kaggle.com/c/cause-effect-pairs;;Given samples from a pair of variables A, B, find whether A is a cause of B.;['custom metric'];266;Cause-effect pairs;Research prediction Competition
2017-12-15 00:59:00;Rules Update: The CDiscount team has updated their rules to allow for use of this dataset for research and academic purposes only. To access the data, go to rules and accept the terms to download the data. Cdiscount.com generated nearly 3 billion euros last year, making it France’s largest non-food e-commerce company. While the company already sells everything from TVs to trampolines, the list of products is still rapidly growing. By the end of this year, Cdiscount.com will have over 30 million products up for sale. This is up from 10 million products only 2 years ago. Ensuring that so many products are well classified is a challenging task. Currently, Cdiscount.com applies machine learning algorithms to the text description of the products in order to automatically predict their category. As these methods now seem close to their maximum potential, Cdiscount.com believes that the next quantitative improvement will be driven by the application of data science techniques to images. In this challenge you will be building a model that automatically classifies the products based on their images. As a quick tour of Cdiscount.com's website can confirm, one product can have one or several images. The data set Cdiscount.com is making available is unique and characterized by superlative numbers in several ways:  Almost 9 million products: half of the current catalogue More than 15 million images at 180x180 resolution More than 5000 categories: yes this is quite an extreme multi-class classification!;https://www.kaggle.com/c/cdiscount-image-classification-challenge;Cdiscount;Categorize e-commerce photos;['multiclass classification', 'categorizationaccuracy'];626;Cdiscount’s Image Classification Challenge;Featured prediction Competition
2016-02-02 00:59:00;This is a Masters competition. You must be a Kaggle Master to participate. Cervical cancer is the third most common cancer in women worldwide, affecting over 500,000 women and resulting in approximately 275,000 deaths every year. After reading these statistics, you may be surprised to hear that cervical cancer is potentially preventable and curable. Cervical cancer can be prevented through early administration of the HPV vaccine and regular pap smear screenings, which indicate the presence of precancerous cells. It is also sometimes curable by the removal of the early-stage cancerous tissue that is identified through pap smears. Screening and early treatment can lead to potential cures in about 95% of women at risk for cervical cancer. Most women in the US have access to cervical cancer screening, yet 4,000 women die every year from cervical cancer in the US and it is estimated that 30% of US women do not receive regular pap screenings. We know little about who these women are and why they are not getting screened. Prior research suggests that lower screening rates are associated with low income, low education, lack of interaction with the healthcare system, and lack of health insurance. But research also shows that even in women with access to healthcare fail to get this preventive test, indicating that barriers like lack of education and not being comfortable with the procedure are influencing their behavior (Patient Survey).  There are many patient advocacy programs on the importance of pap smears in cervical cancer prevention. However, these widespread programs may not be reaching or effectively speaking to the most vulnerable populations. If one could better identify these women, education campaigns could target them with content that speaks directly to their unique risk factors. Identifying predictors of not receiving pap smears will provide important information to stakeholders in cervical cancer prevention who run awareness programs. With this Masters competition, Genentech is asking you to join their mission to help prevent cervical cancer. Given a dataset of de-identified health records, your challenge is to predict which women will not be screened for cervical cancer on the recommended schedule. Identifying at-risk populations will make education and other intervention efforts more effective, ideally ultimately reducing the number of women who die from this disease. About Genentech Founded more than 35 years ago, Genentech is a leading biotechnology company that discovers, develops, manufactures and commercializes medicines to treat patients with serious or life-threatening medical conditions. The company, a member of the Roche Group, has headquarters in South San Francisco, California. For additional information about the company, please visit http://www.gene.com. Acknowledgements The dataset for this competition is provided by Symphony Health Solutions.   @Jotform.Show(35);https://www.kaggle.com/c/cervical-cancer-screening;;Help prevent cervical cancer by identifying at-risk populations;['auc'];40;Cervical Cancer Screening;Featured prediction Competition
2013-05-25 01:59:00;Example baseline submissions are available as part of the pylearn2 python package available at https://github.com/lisa-lab/pylearn2 The baseline submissions for this contest are in pylearn2/scripts/icml_2013_wrepl/emotions Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set.;https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;;Learn facial expressions from an image;['categorizationaccuracy'];56;Challenges in Representation Learning: Facial Expression Recognition Challenge;Research prediction Competition
2013-05-25 01:59:00;In this contest, competitors will design systems to learn about two modalities of data: images and text. The provided training data is Louis von Ahn's Small ESP Game Dataset, containing images and word tags for these images. Competitors should train their system to associate images to sets of word tags. At test time, the system is presented with two possible sets of word tags for an image, and must determine which is the correct set of word tags. Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manual labeling of the test set.;https://www.kaggle.com/c/challenges-in-representation-learning-multi-modal-learning;;The multi-modal learning challenge;['auc'];24;Challenges in Representation Learning: Multi-modal Learning;Research prediction Competition
2013-05-25 01:59:00;We are also providing a dataset of approx. 130,000 unsupervised examples that contestants can use to improve their models. The unsupervised data is a CSV file in the same format as the private test set (i.e. without the labels). The extra data comes from a distribution that is very similar to the training/test set distribution. We provide example code for this contest as part of the pylearn2 package at  https://github.com/lisa-lab/pylearn2 For this contest, look at the pylearn2/scripts/icml_2013_wrepl/black_box directory.;https://www.kaggle.com/c/challenges-in-representation-learning-the-black-box-learning-challenge;;Competitors train a classifier on a dataset that is not human readable, without knowledge of what the data consists of.;['categorizationaccuracy'];211;Challenges in Representation Learning: The Black Box Learning Challenge;Research prediction Competition
2019-08-29 01:59:00;Think you can use your data science smarts to make big predictions at a molecular level? This challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules. Researchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science.   This competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. Winning teams will have an opportunity to partner with this multi-university research program on an academic publication Your Challenge In this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant). Once the competition finishes, CHAMPS would like to invite the top teams to present their work, discuss the details of their models, and work with them to write a joint research publication which discusses an open-source implementation of the solution. About Scalar Coupling Using NMR to gain insight into a molecule’s structure and dynamics depends on the ability to accurately predict so-called “scalar couplings”. These are effectively the magnetic interactions between a pair of atoms. The strength of this magnetic interaction depends on intervening electrons and chemical bonds that make up a molecule’s three-dimensional structure. Using state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows. A fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior.  Ultimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease. Join the CHAMPS Scalar Coupling challenge to apply predictive analytics to chemistry and chemical biology.;https://www.kaggle.com/c/champs-scalar-coupling;CHAMPS (CHemistry And Mathematics in Phase Space);Can you measure the magnetic interactions between a pair of atoms?;['regression', 'tabular data', 'chemistry', 'custom metric'];2,749;Predicting Molecular Properties;Featured prediction Competition
2010-11-17 21:00:00;"When predicting the outcome of chess games, you typically need two things; a rating system wherein the current ability of each player is estimated based on past results, and a model for estimating the expected score for each player, once you know their ratings.Most rating systems use some methodology to determine initial ""seed"" ratings for the pool of players, and then update those ratings based on ongoing results.  The most famous approach is the Elo approach, where the applied change to a player's rating is proportional to the amount by which they exceed their aggregate expected score across all their recent games.  The scaling factor is known as the ""K-factor"", and for the official ratings used throughout the world, the K-factor is highest for new players and lowest for topmost players.  But there are many other approaches: the Ken Thompson approach takes each player's most recent 100 games and calculates the rating that would be most likely to lead to that performance.  The Mark Glickman approach is similar to Elo but introduces additional parameters for each player, tracking the level of confidence and level of volatility for each player's rating, and then using these parameters to determine which K-factor to apply.The initial seed ratings are typically determined through a simultaneous calculation: a start rating is assumed for each player, then a ""performance rating"" is calculated for each player based on their results and the ratings of their opponents, and then those performance ratings are fed back into another iteration as the start ratings.  This is allowed to run until it converges upon a stable set of ratings.  This was the methodology used to calculate initial ratings for most major rating systems.  In fact this is the overall approach taken by the Jeff Sonas Chessmetrics rating calculation, and is used not just to calculate initial ratings but in fact to calculate all ratings.There is a general convention in chess rating systems whereby the difference in two ratings is used for calculating expected score when two players face each other.  Of course it could just as well be the ratio of the two ratings, or some other more complex relationship that depends on the magnitude of the ratings and not just their relative difference.Here are some links to articles existing on rating systems:Elo and Ken ThompsonGlickoChessmetricsMicrosoft TrueSkillJeff Moser has a C# implementation of Elo and TrueSkill on Github. has posted a Java implementation of Glicko on Github.";https://www.kaggle.com/c/chess;;This competition aims to discover whether other approaches can predict the outcome of chess games more accurately than the workhorse Elo rating system.;['rmse'];252;Chess ratings - Elo versus the Rest of the World;Featured prediction Competition
2011-05-04 17:00:00;"The Elo rating system was developed by the Hungarian physicist Arpad Elo in the 1950's and adopted by the world chess federation (FIDE) in 1970.  For more than four decades the FIDE Elo system has served as the primary yardstick in the world for measuring the strength of chess players.  FIDE ratings are used for determining invitations to chess tournaments including the world championship cycle, calculating specific pairings in most chess tournaments, and granting titles such as International Master (IM) or Grandmaster (GM).  In fact the Elo system is so popular that it has been adapted to many other applications beyond chess, including team sports rankings, other board games, and online video game systems.However, despite the popularity of the Elo system, it has never really been demonstrated that the Elo approach is technically superior to other approaches.  Much of the appeal of the Elo system comes from its simplicity and familiarity, and it was ideally suited to a time when the computation of ratings was a significant practical challenge even for an annual list of a few hundred players.  Elo's formula was derived theoretically, in an era without large amounts of historical data or anything approaching today's computing power.  With the benefit of powerful computers and large game databases, we can easily investigate approaches that might do better than Elo at predicting chess results.  Such an investigation could have major implications on the theory and practice of ratings methodology, both for chess and also for the world beyond chess.As an initial step in this process, Kaggle held an ""Elo versus the Rest of the World"" contest in the fall of 2010, requiring participants to develop predictive models that could forecast the results of chess games with great accuracy.  The contest was immensely popular among both chess enthusiasts and data scientists, drawing more than 3,500 submissions from 258 participating teams across 41 countries.  Although the prize fund was minimal, there was tremendous competitive drive spurring many participants to sustained effort.  For instance, despite a restriction keeping teams from making more than two submissions per day, there were twenty teams that made more than 50 submissions across the entire duration of the contest.  The underlying data is so simple and straightforward that it is very easy to begin playing with prediction models, but the overall problem of maximizing accuracy is so challenging that even the massive efforts of the first contest were not sufficient to identify a clearly superior approach.  There was a wide variety in the methodologies of just the top ten prizewinners, all of whom documented their approaches in significant detail after the completion of the contest.  The benchmark submission of the Elo formula finished far, far, behind, in 141st place out of 258, and there were 39 teams whose predictions were at least 5% more accurate than the Elo system.  It is clear that the Elo system is not the most accurate one, but it remains unclear which system is superior.It is no longer a question of ""Elo versus the Rest of the World""; we must now hold a second contest and focus on finding a suitable replacement for Elo.  Possibly the best approach will be a modification of the Elo approach, or perhaps it will be something completely novel.  This second contest will have several significant improvements relative to the first contest.  It will have a more robust scoring function, one selected after extensive analysis of the results from the first contest as well as consultation with worldwide leaders in chess rating theory and categorical data analysis.  Further, FIDE has provided a complete dataset of multiple years of game-by-game results that were used for calculating the official FIDE ratings.  Until now it has never been possible to perform this level of analysis, because the dataset had not been assembled, and this contest website is the only place in the world where the data is available  The second contest provides more than 30 times as many games as the first contest did, and a much larger population of chess players as well, reflecting the whole distribution of player strength rather than just the fraction of top players covered by the first contest.";https://www.kaggle.com/c/ChessRatings2;;This contest, sponsored by professional services firm Deloitte, will find the most accurate system to predict chess outcomes, and FIDE will also bring a top finisher to Athens to present their system;['custom metric'];181;Deloitte/FIDE Chess Rating Challenge;Featured prediction Competition
2014-10-19 01:59:00;CIFAR-10  is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Kaggle is hosting a CIFAR-10 leaderboard for the machine learning community to use for fun and practice. You can see how your approach compares to the latest research methods on Rodrigo Benenson's classification results page.  Please cite this technical report if you use this dataset: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.;https://www.kaggle.com/c/cifar-10;Kaggle;Identify the subject of 60,000 labeled images;['categorizationaccuracy'];231;CIFAR-10 - Object Recognition in Images;Playground prediction Competition
2019-04-26 01:59:00;Ciphertext Challenge II: The Challengening! It's baaaaaaack! In our first ciphertext competition, we hunted the wilds of the '90s-era internet. This time around, we're exploring the dark slow-broadband-y wastelands of 2011, with the Movie Review Dataset. In 2011 most of the internet hadn't even been invented yet*, so wow, you're in for a treat. Again, simple classic ciphers have been used to encrypt this dataset. Your mission this time: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Also, there are some new ciphers in play this time, which will involve some meta-puzzling. Enjoy! Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the LB (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best competition-related kernels, in both visualization and cryptanalysis, based on upvotes. Go ahead. Get cracking! * - This is not true. Acknowledgements Maas, A., Daly, R., Pham, P., Huang, D., Ng, A. and Potts, C. (2011). Learning Word Vectors for Sentiment Analysis: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. [online] Portland, Oregon, USA: Association for Computational Linguistics, pp. 142–150. Available here.;https://www.kaggle.com/c/ciphertext-challenge-ii;Kaggle;553398 418126 467884 411 374106 551004 356535 539549 487091 290502 121468 556912 469347 515719 201909 101;['internet', 'text data', 'categorizationaccuracy'];74;Ciphertext Challenge II;Playground prediction Competition
2019-09-06 01:59:00;"Ciphertext Challenge III: Wherefore Art Thou, Simple Ciphers? We've done the 2010's, the 1990s… now it's time for the 80s. The 1580s!! In this new decryption competition's dataset, we've gone from perfectly respectable sources of electronic horror to a time before computers—heck, before calculus was called ""calculus""!  Shakespeare's plays are encrypted, and we time travelers must un-encrypt them so people can do innovative stage productions with intricate makeup, costumes, and possibly—possibly!—Leonardo DiCaprio. Think about it, folks: Leo.* As in previous ciphertext challenges, simple classic ciphers have been used to encrypt this dataset, along with a slightly less simple surprise that expands our definition of ""classic"" into the modern age. The mission is the same: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Meta-puzzles and difficulty await! Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the leaderboard (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best competition-related kernels, in both visualization and cryptanalysis, based on upvotes.  Last, the coveted ""Phil Prize""—for the team that correctly deduces the form AND key of the final cipher—is up for grabs again. Go ahead. Get cracking! * - Leo! Acknowledgements Many thanks to Kaggler LiamLarson for their excellent Shakespeare dataset.";https://www.kaggle.com/c/ciphertext-challenge-iii;;BRBTvl0LNstxQLyxulCEEq1czSFje0Z6iajczo6ktGmitTE=;['text data', 'categorizationaccuracy'];103;Ciphertext Challenge III;Playground prediction Competition
2012-10-16 01:59:00;The most expensive political campaign season in US history is well underway, with experts predicting that spending from both the Republican and Democratic camps will  exceed $5.8 billion before races for president and Congress are decided in November. Needless to say, campaign finance is a big business. And where big money leads the way, big data is never too far behind. That's why the Center for Investigative Reporting, the nation's largest non-profit investigative reporting organization, is teaming up with Investigative Reporters and Editors, Inc. -- the world's leading investigative and data journalism trade group -- to offer this Prospect challenge in the hopes of answering one simple question: What can some of the world's most  brilliant data scientists teach us about finding hidden patterns, interesting connections and ultimately compelling stories in a treasure trove of data about federal campaign contributions? Data journalists have been examining federal campaign finance records for decades, finding patterns and trends that have forced resignations and reforms. But despite many noteworthy successes, journalists' imaginations are limited by our skills. We're not  mathematicians and machine learning experts! We might learn from a source, for instance, that a particular congressman has begun receiving campaign contributions from a new and unusual donor. But a carefully tuned anomaly detection system might reveal those  patterns before our sources would ever notice them. What kinds of ideas are we looking for with this Prospect challenge? We want to see how sophisticated clustering algorithms can help spot donors that coordinate their operations, giving to the same candidates at the same times. We want to see how classification  or anomaly detection systems can find donations that are particularly interesting or unusual. We want to know how you would mix up campaign contribution data with other sources -- lobbying records, congressional votes, federal contracts -- to find patterns  that journalists are missing. Novel approaches to analysis, ideas for useful tools, and data visualizations will all be considered. What we're looking for is new ideas and approaches. To give you a sense of what journalists have done so far, we've included several examples of some of the most interesting campaign finance reporting around, along with some tipsheets that explain how journalists approach campaign finance data and what to look for.;https://www.kaggle.com/c/cir-prospect;;Find hidden patterns, connections, and ultimately compelling stories in a treasure trove of data about US federal campaign contributions;['rmse'];;Follow the Money: Investigative Reporting Prospect;Prospect prediction Competition
2011-10-13 01:59:59;Risk varies widely from customer to customer, and a deep understanding of different risk factors helps predict the likelihood and cost of insurance claims. The goal of this competition is to better predict Bodily Injury Liability Insurance claim payments based on the characteristics of the insured customer’s vehicle. Many factors contribute to the frequency and severity of car accidents including how, where and under what conditions people drive, as well as what they are driving.  Bodily Injury Liability Insurance covers other people’s bodily injury or death for which the insured is responsible.   The goal of this competition is to predict Bodily Injury Liability Insurance claim payments based on the characteristics of the insured’s vehicle.;https://www.kaggle.com/c/ClaimPredictionChallenge;Allstate Insurance;A key part of insurance is charging each customer the appropriate price for the risk they represent.;['normalizedgini'];102;Allstate Claim Prediction Challenge;Featured prediction Competition
2014-05-06 01:59:00;Understanding the brain structure and some of its disease alterations is key to research on the treatment of epilepsy, Alzheimer's disease, and other neuropathologies, as well as understanding the general function of the brain and its learning capabilities. The brain contains nearly 100 billion neurons with an average 7000 synaptic connections.  Recovering the exact wiring of the brain (connectome) at this neural level is therefore a daunting task. Traditional neuroanatomic methods of axonal tracing cannot scale up to very large networks. Could there be alternative methods to recovering neural network structures from patterns of neural activity? [Learn more ...] Today's cutting edge optical imaging of neural activity (using fluorescent calcium indicators) provides a tool to monitor the activity of tens of thousands of neurons simultaneously. Mathematical algorithms capable of discovering network structures are faced with the challenge of solving a new inverse problem: recover the neural network structure of a living system given the observation of a very large population of neurons. A promising way to experimentally proceed is to use neuronal cultures. Such cultures consist in a number of individual cells (dissected and dissociated from actual brain tissue) that are plated on a cover glass and maintained for several weeks in vitro. These living neuronal networks typically contain on the order of few thousand cells. One can then monitor their activity by fluorescence imaging, reconstruct their connectivity from activity data and, finally, compare the reconstructed circuitry with the real one. However, to fully understand the degree of accuracy of the reconstruction one needs first to procure superior reconstruction algorithms: this is where you can help by entering this competition! Monitoring changes in effective connectivity patterns of a network during behavior promises to advance our understanding of learning and intelligence. This challenge will stimulate research on network-structure learning from neurophysiological data, including causal discovery methods. [Learn more...]  Brain of the zebrafish in action. Today's cutting edge neurophysiology multi-electrode recording tools are capable of  recording (and even stimulating) of the order of 100 neurons. Optical imaging of neural activity using fluorescent calcium indicator molecules (calcium imaging) increases the number of neurons recorded by three orders of magnitude. Recently, researchers have been able to record in vivo the activity of the brain of a zebrafish embryo in 80% of its 100,000 neurons. This video comes from the work of Arens et al. Nature  485,  471–477 (May 2012). Acknowledgements This competition is brought to you by ChaLearn. See our credits page.;https://www.kaggle.com/c/connectomics;;Reconstruct the wiring between neurons from fluorescence imaging of neural activity;['auc'];143;CONNECTOMICS;Research prediction Competition
2014-03-03 00:59:00;"The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state:  Any live cell with fewer than two live neighbours dies, as if by underpopulation. Any live cell with two or three live neighbours lives on to the next generation. Any live cell with more than three live neighbours dies, as if by overpopulation. Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction.  These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics.  As Wikipedia tells it,  Ever since its publication, Conway's Game of Life has attracted much interest, because of the surprising ways in which the patterns can evolve. Life provides an example of emergence and self-organization. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that ""design"" and ""organization"" can spontaneously emerge in the absence of a designer. For example, philosopher and cognitive scientist Daniel Dennett has used the analogue of Conway's Life ""universe"" extensively to illustrate the possible evolution of complex philosophical constructs, such as consciousness and free will, from the relatively simple set of deterministic physical laws governing our own universe.  The emergence of order from simple rules begs an interesting question--what happens if we set time backwards? This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse.  Is the chaotic start of Life predictable from its orderly ends?  We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board. Although some people have examined this problem, it is unknown (at least, to us...) just how difficult this will be.";https://www.kaggle.com/c/conway-s-reverse-game-of-life;Kaggle;Reverse the arrow of time in the Game of Life;['mae'];141;Conway's Reverse Game of Life;Playground prediction Competition
2020-12-01 00:59:00;"This is a relaunch of a previous competition, Conway's Reverse Game of Life, with the following changes:  The grid size is larger (25 vs. 25) and the grid wraps around from top to bottom and left to right Submissions are solved forward by the appropriate number of steps, so that any correct starting solution will achieve a maximum score. This article contains the stepping function that is used for this competition.  Obligatory Disclaimer: A lot has changed since the original competition was launched 6 years ago. With the change from ""exact starting point"" to  ""any correct starting point"", it is possible to get a perfect score. We just don't know how difficult that will be. Use it as a fun learning experience, and don't spoil it for others by posting perfect solutions! ~~~~~~~~~ The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state:  Overpopulation: if a living cell is surrounded by more than three living cells, it dies. Stasis: if a living cell is surrounded by two or three living cells, it survives. Underpopulation: if a living cell is surrounded by fewer than two living cells, it dies. Reproduction: if a dead cell is surrounded by exactly three cells, it becomes a live cell.  These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics. As Wikipedia states Ever since its publication, Conway's Game of Life has attracted much interest, because of the surprising ways in which the patterns can evolve. Life provides an example of emergence and self-organization. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that ""design"" and ""organization"" can spontaneously emerge in the absence of a designer. For example, philosopher and cognitive scientist Daniel Dennett has used the analogue of Conway's Life ""universe"" extensively to illustrate the possible evolution of complex philosophical constructs, such as consciousness and free will, from the relatively simple set of deterministic physical laws governing our own universe.  The emergence of order from simple rules begs an interesting question—what happens if we set time backwards? This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse. Is the chaotic start of Life predictable from its orderly ends? We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board.  This is a Code Competition. Refer to Code Requirements for details.";https://www.kaggle.com/c/conways-reverse-game-of-life-2020;Kaggle;Reverse the arrow of time in the Game of Life;['simulations', 'board games', 'custom metric'];188;Conway's Reverse Game of Life 2020;Playground Code Competition
2018-09-20 01:59:00;The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?   Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.   In Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need. While this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines. To improve on PMT, the IDB (the largest source of development financing for Latin America and the Caribbean) has turned to the Kaggle community. They believe that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance. Beyond Costa Rica, many countries face this same problem of inaccurately assessing social need. If Kagglers can generate an improvement, the new algorithm could be implemented in other countries around the world. This is a Kernels-Only Competition, so you must submit your code through Kernels, rather than uploading .csv predictions. You can create private Kernels and even share/edit your work with teammates by adding them as collaborators.;https://www.kaggle.com/c/costa-rican-household-poverty-prediction;Inter-American Development Bank;Can you identify which households have the highest need for social welfare assistance?;['multiclass classification', 'tabular data', 'macrofscore'];618;Costa Rican Household Poverty Level Prediction;Playground Code Competition
2015-10-01 01:59:00;Recruit Ponpare is Japan's leading joint coupon site, offering huge discounts on everything from hot yoga, to gourmet sushi, to a summer concert bonanza. Ponpare's coupons open doors for customers they've only dreamed of stepping through. They can learn difficult to acquire skills, go on unheard of adventures, and dine like (and with) the stars. Investing in a new experience is not cheap. We fear wasting our time and money on a product or service that we may not enjoy or fully understand. Ponpare takes the high price out of this equation, making it easier for you to take the leap towards your first sky-dive or diamond engagement ring. Using past purchase and browsing behavior, this competition asks you to predict which coupons a customer will buy in a given period of time. The resulting models will be used to improve Ponpare's recommendation system, so they can make sure their customers don't miss out on their next favorite thing.;https://www.kaggle.com/c/coupon-purchase-prediction;Recruit Holdings;Predict which coupons a customer will buy;['multiclass classification', 'tabular data', 'marketing', 'map@{k}'];1,072;Coupon Purchase Prediction;Featured prediction Competition
2020-03-26 00:59:00;This week 1 forecasting task is now closed for submissions. Click here to visit the week 2 version, and make a submission there. This is one of the two complementary forecasting tasks to predict COVID-19 spread. This task is based on various regions across the world. To start on a single state-level subcomponent, please see the companion forecasting task for California, USA. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching two companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 by region, the primary goal isn't to produce accurate forecasts. It’s to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-1;Kaggle;Forecast daily COVID-19 spread in regions around world;['covid19', 'tabular data', 'mcrmsle'];544;COVID19 Global Forecasting (Week 1);Research Code Competition
2020-04-06 05:56:00;This week 2 forecasting task is now closed for submissions. Click here to visit the week 3 version, and make a submission there. This is week 2 of Kaggle's COVID19 forecasting series, following the Week 1 competition. This is the 2nd of at least 4 competitions we plan to launch in this series. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-2;Kaggle;Forecast daily COVID-19 spread in regions around world;['covid19', 'tabular data', 'mcrmsle'];215;COVID19 Global Forecasting (Week 2);Research Code Competition
2020-04-09 01:59:00;This week 3 forecasting task is now closed for submissions. Click here to visit the week 4 version, and make a submission there. This is week 3 of Kaggle's COVID19 forecasting series, following the Week 2 competition. This is the 3rd of at least 4 competitions we plan to launch in this series. All of the prior discussion forums have been migrated to this competition for continuity. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-3;Kaggle;Forecast daily COVID-19 spread in regions around world;['covid19', 'tabular data', 'mcrmsle'];452;COVID19 Global Forecasting (Week 3);Research Code Competition
2020-04-16 01:59:00;This is week 4 of Kaggle's COVID-19 forecasting series, following the Week 3 competition. This is the 4th competition we've launched in this series. All of the prior discussion forums have been migrated to this competition for continuity. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 15 and May 14 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-4;Kaggle;Forecast daily COVID-19 spread in regions around world;['covid19', 'tabular data', 'mcrmsle'];472;COVID19 Global Forecasting (Week 4);Research Code Competition
2020-05-12 01:59:00;This is week 5 of Kaggle's COVID-19 forecasting series, following the Week 4 competition. This competition has some changes from prior weeks - be sure to check the Evaluation and Data pages for more details. All of the prior discussion forums have been migrated to this competition for continuity. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves developing quantile estimates intervals for confirmed cases and fatalities between May 12 and June 7 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-5;Kaggle;Forecast daily COVID-19 spread in regions around world;['covid19', 'tabular data', 'weightedpinballloss'];173;COVID19 Global Forecasting (Week 5);Research Code Competition
2020-03-26 00:59:00;This is one of the two complementary forecasting tasks to predict COVID-19 spread. This one is based on a single state-level subcomponent in California, USA. Our intent in having this region-specific version is to offer a more manageable starting point for the global forecasting task. To start on the global version, please see the companion forecasting task. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching two companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 in California, the primary goal isn't to produce accurate forecasts. It’s to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;Kaggle;Forecast daily COVID-19 spread in California, USA;['covid19', 'tabular data', 'mcrmsle'];190;COVID19 Local US-CA Forecasting (Week 1);Research Code Competition
2012-09-25 01:59:00;A significant proportion of web usage relates to discussions, research, and purchase of consumer products. Currently, hundreds of thousands of blogs, forums, product review sites, and e-commerce merchants currently exist, in part, to service consumer's need  to access product related information and demand to share experiences with products. The goal of this competition is to determine the state-of-the-art methods to automatically recognize product mentions in such textual content and to also disambiguate which product(s) in product catalogs are being referenced. Specifically, the task is to  automatically identify all mentions of consumer products in a largely user generated collection of web-content, and to correctly identify the product(s) that each product mention refers to from a large catalog of products. The datasets provided includes hundreds  of thousands of text items, a product catalog with over fifteen million products, and hundreds of manually annotated product mentions to support data-driven approaches. The prize pool for the contest is $10,000 and is divided as follows: $6,000 for first, $3,000 for second and $1,000 for third place submissions. Note that the contest is colocated with the ICDM-2012 conference. There will be a workshop on the contest results on December 10th.;https://www.kaggle.com/c/cprod1;;Identify product mentions within a largely user-generated web-based corpus and disambiguate the mentions against a large product catalog.;['custom metric'];29;CPROD1: Consumer PRODucts contest #1;Research prediction Competition
2014-09-24 01:59:00;Display advertising is a billion dollar effort and one of the central uses of machine learning on the Internet. However, its data and methods are usually kept under lock and key. In this research competition, CriteoLabs is sharing a week’s worth of data for you to develop models predicting ad click-through rate (CTR). Given a user and the page he is visiting, what is the probability that he will click on a given ad?  The goal of this challenge is to benchmark the most accurate ML algorithms for CTR estimation. All winning models will be released under an open source license. As a participant, you are given a chance to access the traffic logs from Criteo that include various undisclosed features along with the click labels.;https://www.kaggle.com/c/criteo-display-ad-challenge;;Predict click-through rates on display ads;['logloss'];717;Display Advertising Challenge;Research prediction Competition
2015-07-07 01:59:00;So many of our favorite daily activities are mediated by proprietary search algorithms. Whether you're trying to find a stream of that reality TV show on cat herding or shopping an eCommerce site for a new set of Japanese sushi knives, the relevance of search results is often responsible for your (un)happiness. Currently, small online businesses have no good way of evaluating the performance of their search algorithms, making it difficult for them to provide an exceptional customer experience. The goal of this competition is to create an open-source model that can be used to measure the relevance of search results. In doing so, you'll be helping enable small business owners to match the experience provided by more resource rich competitors. It will also provide more established businesses a model to test against. Given the queries and resulting product descriptions from leading eCommerce sites, this competition asks you to evaluate the accuracy of their search algorithms. Make a first submission with this Python benchmark on Kaggle scripts.   The dataset for this competition was created using query-result pairings enriched on the CrowdFlower platform. They are sponsoring this competition as an investment in the open-source data science community. A dataset collected, cleaned, and labeled by CrowdFlower can make your supervised machine learning dreams come true.;https://www.kaggle.com/c/crowdflower-search-relevance;Figure Eight;Predict the relevance of search results from eCommerce sites;['internet', 'tabular data', 'quadraticweightedkappa'];1,324;Crowdflower Search Results Relevance;Featured prediction Competition
2013-12-02 00:59:00;"In this competition you are provided a set of tweets related to the weather. The challenge is to analyze the tweet and determine whether it has a positive, negative, or neutral sentiment, whether the weather occurred in the past, present, or future, and what sort of weather the tweet references. It's a lot to mine from so few characters, but if the going gets tough you can always blame the weather... ""Please knock out the power giant storm that is passing thru....please."" -Tweet #74096 CrowdFlower We are excited to team up with CrowdFlower on the first of what we hope will be many fun machine learning projects. CrowdFlower is debuting a new open data library and we're always looking for an excuse to have a competition. Why is this exciting? Sweet, sweet Labels. Data repositories sometimes have more in common with a landfill than a library. They're home to tattered piles of spreadsheets in odd formats with nary a shred of documentation to tell the GDP of Chile from the migratory patterns of North American goldfinches. If creating value from this digital exhaust is a defining theme of the big data explosion, most repositories leave you choking on the diesel fumes of data disappointment. Such data is great if you are doing a report on the GDP of Chile, but not so useful if you are doing machine learning, or its red-headed step child, data science. Crowdflower's data sets provide the thing that makes so many repositories fall short - data paired with labels. One can decide whether two English sentences are related, make judgments about yogurt chatter, or rank emotions on tweets about nuclear energy. It's all about the (wo)manpower to label what these bytes actually mean. The Open Data Library CrowdFlower Open Data Library is a repository of real data set samples that developers, researchers and data scientists can download and use to test and improve algorithms. Our mission is to encourage users to explore the possibilities and power of crowdsourcing. Open Data is free, available to anyone, and ready-to-use with CrowdFlower’s Platform. New data sets are continuously added to CrowdFlower Open Data Library as users of the CrowdFlower Platform opt-in to share their data with the crowdsourcing community. Sample data sets currently available include tweets for sentiment and topic analysis, word combinations to test similarities, sentence combinations to test related topics, and more. Learn more at www.crowdflower.com.";https://www.kaggle.com/c/crowdflower-weather-twitter;Figure Eight;What can a #machine learn from tweets about the #weather?;['rmse'];258;Partly Sunny with a Chance of Hashtags;Playground prediction Competition
2012-12-12 01:00:00;An important part of succeeding as an insurance company is having a good understanding of which of the company’s current customers will be with the company into the future.  Every customer comes with a different risk profile and it is critical to plan appropriately for that future risk.  The goal of this competition is to predict which current customers will still be with the company in 6 months, given many of the customer’s characteristics.;https://www.kaggle.com/c/customer-retention;;Predict which of our current customers will stay insured with us for an entire policy term.;['logloss'];12;Will I Stay or Will I Go?;Masters prediction Competition
2018-06-12 01:59:00;When you're driving, how important is it to be able to quickly tell the difference between a person vs. a stop sign? It's a hugely important, but typically very simple, distinction that you would make reflexively. Autonomous vehicles are not able to do this quite as effortlessly.  This challenge, hosted by the 2018 CVPR workshop on autonomous driving (WAD), asks you to help give autonomously driven vehicles the same edge. Using an unprecedented dataset, you're asked to segment movable objects, such as cars and pedestrians, at instance level within image frames.  By participating in this competition, you'll be helping to further our understand of the current status of computer vision algorithms in solving environmental perception problems for autonomous driving. This challenge is a truly unique opportunity to work on a tremendously high value and high profile problem. The dataset presented here contains over 10 times more fine-labeled images than the largest public dataset of its type. Acknowledgements This competition is hosted by the 2018 CVPR workshop on autonomous driving (WAD), with dataset and evaluation metric contributed by Baidu Inc.;https://www.kaggle.com/c/cvpr-2018-autonomous-driving;CVPR 2018 WAD;Can you segment each objects within image frames captured by vehicles?;['custom metric'];141;CVPR 2018 WAD Video Segmentation Challenge;Research prediction Competition
2012-12-17 00:59:00;The $20,000 USD prize will be split between the top three finishers for the Observing Dark Worlds challenge as follows:  First place       $12,000 Second place  $5,000 Third place      $3,000  To be eligible for prize money, you must reveal your model code and documentation under GPLv3 and agree to your First Name, First Initial of Last (Family) Name, City and Country being listed on the website. But wait, there's more ... Recruiting Competition Do you want to put your hard-earned scientific know-how to work using real-world data sets in a first-class research environment? Itching to test your skills against a new challenge? Winton Capital Management are looking for motivated, creative scientists to join their cutting edge research teams in Oxford, London, Zurich and Hong Kong. Winton is a world leader in applying advanced scientific methods to today's finance problems in a  collaborative -- dare we say even academic -- environment that rewards innovation and robust research discoveries. Research activities at Winton include analyzing historical data for trends and correlations, applying proprietary pattern recognition and signal processing techniques to financial data, and of course, developing novel tests and tools to unlock the hidden  worlds of markets and their prediction. Winton will review the top entries and offer interviews to the creators of those submissions which are exceptional. Send your resumes to WintonRecruiting@kaggle.com for consideration. Please  note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions. Winton will review the resumes and code of the top participants before deciding whether to offer an interview.;https://www.kaggle.com/c/DarkWorlds;;Can you find the Dark Matter that dominates our Universe? Winton Capital offers you the chance to unlock the secrets of dark worlds.;['custom metric'];351;Observing Dark Worlds;Recruitment prediction Competition
2017-04-13 01:59:00;In the United States, lung cancer strikes 225,000 people every year, and accounts for $12 billion in health care costs. Early detection is critical to give patients the best chance at recovery and survival. One year ago, the office of the U.S. Vice President spearheaded a bold new initiative, the Cancer Moonshot, to make a decade's worth of progress in cancer prevention, diagnosis, and treatment in just 5 years. In 2017, the Data Science Bowl will be a critical milestone in support of the Cancer Moonshot by convening the data science and medical communities to develop lung cancer detection algorithms. Using a data set of thousands of high-resolution lung scans provided by the National Cancer Institute, participants will develop algorithms that accurately determine when lesions in the lungs are cancerous. This will dramatically reduce the false positive rate that plagues the current detection technology, get patients earlier access to life-saving interventions, and give radiologists more time to spend with their patients. This year, the Data Science Bowl will award $1 million in prizes to those who observe the right patterns, ask the right questions, and in turn, create unprecedented impact around cancer screening care and prevention. The funds for the prize purse will be provided by the Laura and John Arnold Foundation. Visit DataScienceBowl.com to: • Sign up to receive news about the competition• Learn about the history of the Data Science Bowl and past competitions• Read our latest insights on emerging analytics techniques  Acknowledgments The Data Science Bowl is presented by  Competition Sponsors Laura and John Arnold FoundationCancer Imaging Program of the National Cancer InstituteAmerican College of RadiologyAmazon Web ServicesNVIDIA Data Support Providers National Lung Screening TrialThe Cancer Imaging ArchiveDiagnostic Image Analysis Group, Radboud UniversityLahey Hospital & Medical CenterCopenhagen University Hospital Supporting Organizations  Bayes ImpactBlack Data Processng AssociatesCode the ChangeData Community DCDataKindGalvanizeGreat Minds in STEMHortonworksINFORMSLesbians Who TechNSBESociety of Asian Scientists & EngineersSociety of Women EngineersUniversity of Texas Austin, Business Analytics Program,McCombs School of BusinessUS Dept. of Health and Human ServicesUS Food and Drug AdministrationWomen in TechnologyWomen of Cyberjutsu;https://www.kaggle.com/c/data-science-bowl-2017;Booz Allen Hamilton;Can you improve lung cancer detection?;['image data', 'binary classification', 'healthcare', 'logloss'];1,972;Data Science Bowl 2017;Featured prediction Competition
2018-04-17 01:59:00;Spot Nuclei. Speed Cures. Imagine speeding up research for almost every disease, from lung cancer and heart disease to rare disorders. The 2018 Data Science Bowl offers our most ambitious mission yet: create an algorithm to automate nucleus detection. We’ve all seen people suffer from diseases like cancer, heart disease, chronic obstructive pulmonary disease, Alzheimer’s, and diabetes. Many have seen their loved ones pass away. Think how many lives would be transformed if cures came faster. By automating nucleus detection, you could help unlock cures faster—from rare disorders to the common cold. Want a snapshot about the 2018 Data Science Bowl? View this video. Why nuclei? Identifying the cells’ nuclei is the starting point for most analyses because most of the human body’s 30 trillion cells contain a nucleus full of DNA, the genetic code that programs each cell. Identifying nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work. By participating, teams will work to automate the process of identifying nuclei, which will  allow for more efficient drug testing, shortening the 10 years it takes for each new drug to come to market. Check out this video overview to find out more. What will participants do? Teams will create a computer model that can identify a range of nuclei across varied conditions. By observing patterns, asking questions, and building a model, participants will have a chance to push state-of-the-art technology farther. Visit DataScienceBowl.com to:  • Sign up to receive news about the competition • Learn about the history of the Data Science Bowl and past competitions • Read our latest insights on emerging analytics techniques;https://www.kaggle.com/c/data-science-bowl-2018;Booz Allen Hamilton;Find the nuclei in divergent images to advance medical discovery;['biology', 'custom metric'];3,634;2018 Data Science Bowl;Featured prediction Competition
2020-01-23 00:59:00;Illuminate Learning. Ignite Possibilities. Uncover new insights in early childhood education and how media can support learning outcomes. Participate in our fifth annual Data Science Bowl, presented by Booz Allen Hamilton and Kaggle. PBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life.  In this challenge, you’ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes. Data Science Bowl is the world’s largest data science competition focused on social good. Each year, this competition gives Kagglers a chance to use their passion to change the world. Over the last four years, more than 50,000+ Kagglers have submitted over 114,000+ submissions, to improve everything from lung cancer and heart disease detection to ocean health.  For more information on the Data Science Bowl, please visit DataScienceBowl.com Where does the data for the competition come from? The data used in this competition is anonymous, tabular data of interactions with the PBS KIDS Measure Up! app. Select data, such as a user’s in-app assessment score or their path through the game, is collected by the PBS KIDS Measure Up! app, a game-based learning tool.  PBS KIDS is committed to creating a safe and secure environment that family members of all ages can enjoy. The PBS KIDS Measure Up! app does not collect any personally identifying information, such as name or location. All of the data used in the competition is anonymous. To view the full PBS KIDS privacy policy, please visit: pbskids.org/privacy. No one will be able to download the entire data set and the participants do not have access to any personally identifiable information about individual users. The Data Science Bowl and the use of data for this year’s competition has been reviewed to ensure that it meets requirements of applicable child privacy regulations by PRIVO, a leading global industry expert in children’s online privacy. What is the PBS KIDS Measure Up! app? In the PBS KIDS Measure Up! app, children ages 3 to 5 learn early STEM concepts focused on length, width, capacity, and weight while going on an adventure through Treetop City, Magma Peak, and Crystal Caves. Joined by their favorite PBS KIDS characters, children can also collect rewards and unlock digital toys as they play. To learn more about PBS KIDS Measure Up!, please click here. PBS KIDS and the PBS KIDS Logo are registered trademarks of PBS. Used with permission. The contents of PBS KIDS Measure Up! were developed under a grant from the Department of Education. However, those contents do not necessarily represent the policy of the Department of Education, and you should not assume endorsement by the Federal Government. The app is funded by a Ready To Learn grant (PR/AWARD No. U295A150003, CFDA No. 84.295A) provided by the Department of Education to the Corporation for Public Broadcasting.;https://www.kaggle.com/c/data-science-bowl-2019;Booz Allen Hamilton;Uncover the factors to help measure how young children learn;['education', 'video games', 'people', 'quadraticweightedkappa'];3,493;2019 Data Science Bowl;Featured Code Competition
2019-04-24 01:59:00;Welcome In this competition you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition.  CareerVillage.org is a nonprofit that crowdsources career advice for underserved youth. Founded in 2011 in four classrooms in New York City, the platform has now served career advice from 25,000 volunteer professionals to over 3.5M online learners. The platform uses a Q&A style similar to StackOverflow or Quora to provide students with answers to any question about any career.  In this Data Science for Good challenge, CareerVillage.org, in partnership with Google.org, is inviting you to help recommend questions to appropriate volunteers. To support this challenge, CareerVillage.org has supplied five years of data.  Problem Statement The U.S. has almost 500 students for every guidance counselor. Underserved youth lack the network to find their career role models, making CareerVillage.org the only option for millions of young people in America and around the globe with nowhere else to turn.  To date, 25,000 volunteers have created profiles and opted in to receive emails when a career question is a good fit for them. This is where your skills come in. To help students get the advice they need, the team at CareerVillage.org needs to be able to send the right questions to the right volunteers. The notifications sent to volunteers seem to have the greatest impact on how many questions are answered. Your objective: develop a method to recommend relevant questions to the professionals who are most likely to answer them.  Criteria for Measuring Solutions Performance: How well does the solution match professionals to the questions they would be motivated to answer? CareerVillage.org will not be able to live-test every submission, so a strong entry will clearly articulate why it will be effective at motivating answers. Easy to implement: The CareerVillage.org team wants to put the winning submissions to work, quickly.  A good entry will be well documented and easy to test in production. Extensibility: In the future, CareerVillage.org aims to add more data features and to accommodate new objectives. Winning submissions should allow for this and other augmentations to be added in the future.;https://www.kaggle.com/c/data-science-for-good-careervillage;CareerVillage.org;Match career advice questions with professionals in the field;['education', 'people'];;Data Science for Good: CareerVillage.org;Analytics  Competition
2019-06-22 01:59:00;"Data Science for Good: City of Los Angeles Help the City of Los Angeles to structure and analyze its job descriptions The City of Los Angeles faces a big hiring challenge: 1/3 of its 50,000 workers are eligible to retire by July of 2020. The city has partnered with Kaggle to create a competition to improve the job bulletins that will fill all those open positions. Problem Statement The content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.   The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to: (1) identify language that can negatively bias the pool of applicants; (2) improve the diversity and quality of the applicant pool; and/or (3) make it easier to determine which promotions are available to employees in each job class. How to Participate  Accept the Rules   Accept the competition rules.     Make Your Submission   Follow the submission instructions.  WIth your help, Los Angeles will overcome a wave of retirements and fill those jobs with a strong and diverse workforce. Good luck and happy Kaggling! Do you think companies can find better candidates by improving their job postings? We hope to create an open-sourced body of work focused on this topic by hosting another Data Science for Good competition, this time in partnership with the City of Los Angeles.";https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;City of Los Angeles;Help the City of Los Angeles to structure and analyze its job descriptions;['nlp', 'image data', 'text data', 'employment'];;Data Science for Good: City of Los Angeles;Analytics  Competition
2015-01-01 00:59:00;"hosting a meetup on Scikit-learn We encourage participants to post code via the ""Tutorials"" link on the left.  Don't worry about accuracy or whether your code is perfect.  The aim here is to explore sklearn by using it.    Its implementation is high quality due to s Meetup Information Thursday, March 7, 2013,  “Learning in Python with scikit-learn"" by Andreas Mueller   ""Parallel and large scale learning with scikit-learn"" by Olivier Grisel   notebook interface How to perform scalable text feature extraction with the Hashing Trick and hyper parameters tuning How to optimize memory usage with memory mapping How to approximate kernel Support Vector Machines for large scale datasets A short introduction to Ensembles with model averaging and Random Forests   by day and a Python machine learning hacker by night. He is interested in applications to Natural Language Processing, Computer Vision and predictive modelling.";https://www.kaggle.com/c/data-science-london-scikit-learn;Kaggle;Scikit-learn is an open-source machine learning library for Python. Give it a try here!;['categorizationaccuracy'];190;Data Science London + Scikit-learn;Getting Started prediction Competition
2015-03-17 00:59:00;Plankton are critically important to our ecosystem, accounting for more than half the primary productivity on earth and nearly half the total carbon fixed in the global carbon cycle. They form the foundation of aquatic food webs including those of large, important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.  Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of an underwater imagery sensor. This towed, underwater camera system captures microscopic, high-resolution images over large study areas. The images can then be analyzed to assess species populations and distributions. Manual analysis of the imagery is infeasible – it would take a year or more to manually analyze the imagery volume captured in a single day. Automated image classification using machine learning tools is an alternative to the manual approach. Analytics will allow analysis at speeds and scales previously thought impossible. The automated system will have broad applications for assessment of ocean and ecosystem health. The National Data Science Bowl challenges you to build an algorithm to automate the image identification process. Scientists at the Hatfield Marine Science Center and beyond will use the algorithms you create to study marine food webs, fisheries, ocean conservation, and more. This is your chance to contribute to the health of the world’s oceans, one plankton at a time. Acknowledgements The National Data Science Bowl is presented bywith data provided by the Hatfield Marine Science Center at Oregon State University.;https://www.kaggle.com/c/datasciencebowl;Booz Allen Hamilton;Predict ocean health, one plankton at a time;['image data', 'multiclass classification', 'water bodies', 'custom metric'];1,049;National Data Science Bowl;Featured prediction Competition
2015-10-15 01:59:00;"Online media companies rely more and more on paid advertising to keep their lights on and their content engines humming. ""Native advertising"" is a popular alternative to the unsightly banner ads and infuriating pop-ups of Internet Advertising 1.0. Native ads mimic the core content of the site they're advertising on, ideally avoiding any interruption of the user's experience.  When native advertising is done right, users aren't desperately scanning an ad for a hidden ""x"". In fact, they don't even know they're viewing one. To pull this off, native ads need to be just as interesting, fun, and informative as the unpaid content on a site. Dato is sponsoring this competition with the noble goal of making native advertising live up to its name. With a dataset of over 300,000 raw HTML files containing text, links, and downloadable images, they also want to give Kagglers a challenge that encourages creativity. Given the HTML of websites served to users of StumbleUpon, your challenge is to identify the paid content disguised as just another internet gem you've stumbled upon. If media companies can better identify poorly designed native ads, they can keep them off your feed and out of your user experience.  For details on using GraphLab Create for the competition, check out Dato's post. Acknowledgements   The dataset for this competition was generously provided by StumbleUpon.";https://www.kaggle.com/c/dato-native;;Predict which web pages served by StumbleUpon are sponsored;['binary classification', 'tabular data', 'marketing', 'auc'];273;Truly Native?;Featured prediction Competition
2014-07-28 01:59:00;Understanding how the human brain works is a primary goal in neuroscience research. Non-invasive functional neuroimaging techniques, such as magnetoencephalography (MEG), are able to capture the brain activity as multiple timeseries. When a subject is presented a stimulus and the concurrent brain activity is recorded, the relation between the pattern of recorded signal and the category of the stimulus may provide insights on the underlying mental process. Among the approaches to analyse the relation between brain activity and stimuli, the one based on predicting the stimulus from the concurrent brain recording is called brain decoding. The goal of this competition is to predict the category of a visual stimulus presented to a subject from the concurrent brain activity. The brain activity is captured with an MEG device which records 306 timeseries at 1KHz of the magnetic field associated with the brain currents. The categories of the visual stimulus for this competition are two: face and scrambled face. A stimulus and the concurrent MEG recording is called trial and thousands of randomized trials were recorded from multiple subjects. The trials of some of the subjects, i.e. the train set, are provided to create prediction models. The remaining trials, i.e. the test set, belong to different subjects and they will be used to score the prediction models. Because of the variability across subjects in brain anatomy and in the patterns of brain activity, a certain degree of difference is expected between the data of different subjects and thus between the train set and the test set.  Bibliography Full details of the neuroscientific experiment in which the data were collected are described in:  Front. Hum. Neurosci.  http://www.frontiersin.org/Journal/10.3389/fnhum.2011.00076/abstract  A brief survey of the scientific literature on the problem of decoding across subjects, together with the description of the train set of this competition and a preliminary solution in terms of transfer learning, are described in:  Pattern Recognition in Neuroimaging, 2014 International Workshop on ieeexplore.ieee.orghttp://arxiv.org/abs/1404.4175  Conference This competition is associated with the the 19th International Conference on Biomagnetism, Biomag 2014. The Biomag conference will be held in Halifax, Canada, August 24-28, 2014. Organization This competition is organized by Emanuele Olivetti, Mostafa Kia and Paolo Avesani (NeuroInformatics Lab, Fondazione Bruno Kessler and Università di Trento, IT). Acknowledgements The awards of this competition are funded by Elekta Oy, MEG International Services Ltd (MISL), Fondazione Bruno Kessler, and Besa. We would also like to thank Daniel Wakeman (Martinos Center, MGH, USA), Richard Henson (MRC/CBU, Cambridge, UK), Ole Jensen (Donders Institute, NL), Nathan Weisz (University of Trento, IT) and Alexandre Gramfort (Telecom ParisTech, CNRS, CEA / Neurospin) for their contributions in preparing this competition.;https://www.kaggle.com/c/decoding-the-human-brain;;Predict visual stimuli from MEG recordings of human brain activity;['categorizationaccuracy'];267;DecMeg2014 - Decoding the Human Brain;Research prediction Competition
2020-04-24 00:17:00;This competition is closed for submissions. Participants' selected code submissions were re-run by the host on a privately-held test set and the private leaderboard results have been finalized. Late submissions will not be opened, due to an inability to replicate the unique design of this competition. Deepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online. These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rights—especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion. Identifying manipulated media is a technically demanding and rapidly evolving challenge that requires collaborations across the entire tech industry and beyond.  AWS, Facebook, Microsoft, the Partnership on AI’s Media Integrity Steering Committee, and academics have come together to build the Deepfake Detection Challenge (DFDC). The goal of the challenge is to spur researchers around the world to build innovative new technologies that can help detect deepfakes and manipulated media. Challenge participants must submit their code into a black box environment for testing. Participants will have the option to make their submission open or closed when accepting the prize. Open proposals will be eligible for challenge prizes as long as they abide by the open source licensing terms. Closed proposals will be proprietary and not be eligible to accept the prizes. Regardless of which track is chosen, all submissions will be evaluated in the same way. Results will be shown on the leaderboard.  The PAI Steering Committee has emphasized the need to ensure that all technical efforts incorporate attention to how the resulting code and products based on it can be made as accessible and useful as possible to key frontline defenders of information quality such as journalists and civic leaders around the world. The DFDC results will be a contribution to this effort and building a robust response to the emergent threat deepfakes pose globally.;https://www.kaggle.com/c/deepfake-detection-challenge;Deepfake Detection Challenge;Identify videos with facial or voice manipulations;['video data', 'logloss'];2,265;Deepfake Detection Challenge;Featured Code Competition
2013-12-22 00:59:00;Understanding customer loyalty is an important part of any business. The ability to predict ahead of time when a customer is likely to churn can enable early intervention processes to be put in place, and ultimately a reduction in customer churn.  This competition seeks a solution for predicting which current customers of an insurance company will leave in 12 months time, and when. This competition is now closed to new entrants.;https://www.kaggle.com/c/deloitte-churn-prediction;;Predict which customers will leave an insurance company in the next 12 months.;['mcauc'];37;As the World Churns;Masters prediction Competition
2015-12-01 00:59:00;This is a Masters competition. You must be a Kaggle Master to participate. Property rental prices are a key economic indicator, often signaling significant changes in things like unemployment rate or income. Accurately predicting rental prices would help organizations offering public and commercial services with the ability to better plan for and price these services. Weekly rental values for properties vary due to a broad mix of factors. Some measures are objective, like proximity to hospitals, schools, transport, and coastline. Others are more subjective, like the aesthetic value of your backyard garden. The rental market in Western Australia is unusually diverse and difficult to predict due to the region's varied landscape and small, widely spread population. Currently, automated valuation models are used for over 90% of residential property estimates in Western Australia. Using data on location, property, zoning, past sales, and more, the goal of this competition is to improve on existing models by accurately estimating the weekly market rental value for residential properties across Western Australia. @JotForm.Show(34);https://www.kaggle.com/c/deloitte-western-australia-rental-prices;;Predict rental prices for properties across Western Australia;['rmsle'];59;Western Australia Rental Prices;Featured prediction Competition
2018-09-25 01:59:00;This competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset.  You are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores. What's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost? This is a great competition to explore different models and improve your skills in forecasting.;https://www.kaggle.com/c/demand-forecasting-kernels-only;Kaggle;Predict 3 months of item sales at different stores;['tabular data', 'smape'];461;Store Item Demand Forecasting Challenge;Playground Code Competition
2015-10-06 01:59:00;Optical Character Recognition (OCR) is the process of getting type or handwritten documents into a digitized format. If you've read a classic novel on a digital reading device or had your doctor pull up old healthcare records via the hospital computer system, you've probably benefited from OCR. OCR makes previously static content editable, searchable, and much easier to share. But, a lot of documents eager for digitization are being held back. Coffee stains, faded sun spots, dog-eared pages, and lot of wrinkles are keeping some printed documents offline and in the past.  This competition challenges you to give these documents a machine learning makeover. Given a dataset of images of scanned text that has seen better days, you're challenged to remove the noise. Improving the ease of document enhancement will help us get that rare mathematics book on our e-reader before the next beach vacation. We've kicked off the fun with a few handy scripts to get you started on the dataset. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was created by RM.J. Castro-Bleda, S. España-Boquera, J. Pastor-Pellicer, F. Zamora-Martinez. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science;https://www.kaggle.com/c/denoising-dirty-documents;Kaggle;Remove noise from printed text;['image data', 'rmse'];161;Denoising Dirty Documents;Playground prediction Competition
2012-09-22 01:59:00;"How Does the Prize Work? The $10,000 prize pool will be split between the top two finishers for the classification challenge and the top visualization submission as follows: First place : $7,000 Second place:  $2,500 Visualization Prospect: $500 But wait, there's more... Recruiting Competition Do you want a chance to build a crucial component of Web 2.0 infrastructure: the defense against spam and abuse? Do you live and breathe machine learning and data-mining? We're looking for someone whose passion lies in the invention and application of cutting-edge machine learning and data-mining techniques. Impermium is an engineering-driven startup dedicated to helping consumer web sites protect themselves from ""social spam,""  account hacking, bot attacks, and more. Impermium will review the top entries and offer interviews to the creators of those submissions which are exceptional.    Due to visa issuance delays,  Impermium is not able to sponsor new H1B applicants but are happy to support H1B transfers, permanent residents and US citizens.";https://www.kaggle.com/c/detecting-insults-in-social-commentary;;Predict whether a comment posted during a public discussion is considered insulting to one of the participants.;['auc'];50;Detecting Insults in Social Commentary;Recruitment prediction Competition
2015-07-28 01:59:00;Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people.  The US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment. Currently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment. Clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient. The need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible – ideally resulting in models with realistic clinical potential. The winning models will be open sourced to maximize the impact such a model can have on improving DR detection. Acknowledgements This competition is sponsored by the California Healthcare Foundation.  Retinal images were provided by EyePACS, a free platform for retinopathy screening.;https://www.kaggle.com/c/diabetic-retinopathy-detection;;Identify signs of diabetic retinopathy in eye images;['image data', 'binary classification', 'quadraticweightedkappa'];660;Diabetic Retinopathy Detection;Featured prediction Competition
2018-02-27 00:59:00;Who's a good dog? Who likes ear scratches? Well, it seems those fancy deep neural networks don't have all the answers. However, maybe they can answer that ubiquitous question we all ask when meeting a four-legged stranger: what kind of good pup is that? In this playground competition, you are provided a strictly canine subset of ImageNet in order to practice fine-grained image categorization. How well you can tell your Norfolk Terriers from your Norwich Terriers? With 120 breeds of dogs and a limited number training images per class, you might find the problem more, err, ruff than you anticipated.  Acknowledgments We extend our gratitude to the creators of the Stanford Dogs Dataset for making this competition possible: Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li.;https://www.kaggle.com/c/dog-breed-identification;Kaggle;Determine the breed of a dog in an image;['image data', 'multiclass classification', 'animals', 'multiclassloss'];1,282;Dog Breed Identification;Playground prediction Competition
2014-02-02 00:59:00;In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat.  This is easy for humans, dogs, and cats. Your computer will find it a bit more difficult.  Deep Blue beat Kasparov at chess in 1997.Watson beat the brightest trivia minds at Jeopardy in 2011.Can you tell Fido from Mittens in 2013? The Asirra data set Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords. Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface: Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research.  Image recognition attacks While random guessing is the easiest form of attack, various forms of image recognition can allow an attacker to make guesses that are better than random. There is enormous diversity in the photo database (a wide variety of backgrounds, angles, poses, lighting, etc.), making accurate automatic classification difficult. In an informal poll conducted many years ago, computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art. For reference, a 60% classifier improves the guessing probability of a 12-image HIP from 1/4096 to 1/459. State of the art The current literature suggests machine classifiers can score above 80% accuracy on this task [1]. Therfore, Asirra is no longer considered safe from attack.  We have created this contest to benchmark the latest computer vision and deep learning approaches to this problem. Can you crack the CAPTCHA? Can you improve the state of the art? Can you create lasting peace between cats and dogs? Okay, we'll settle for the former.  Acknowledgements We extend our thanks to Microsoft Research for providing the data for this competition.;https://www.kaggle.com/c/dogs-vs-cats;Kaggle;Create an algorithm to distinguish dogs from cats;['categorizationaccuracy'];213;Dogs vs. Cats;Playground prediction Competition
2017-03-03 00:59:00;In 2013, we hosted one of our favorite for-fun competitions:  Dogs vs. Cats. Much has since changed in the machine learning landscape, particularly in deep learning and image analysis. Back then, a tensor flow was the diffusion of the creamer in a bored mathematician's cup of coffee. Now, even the cucumber farmers are neural netting their way to a bounty. Much has changed at Kaggle as well. Our online coding environment Kernels didn't exist in 2013, and so it was that we approached sharing by scratching primitive glpyhs on cave walls with sticks and sharp objects. No more. Now, Kernels have taken over as the way to share code on Kaggle. IPython is out and Jupyter Notebook is in. We even have TensorFlow. What more could a data scientist ask for? But seriously, what more? Pull requests welcome.  We are excited to bring back the infamous Dogs vs. Cats classification problem as a playground competition with kernels enabled. Although modern techniques may make light of this once-difficult problem, it is through practice of new techniques on old datasets that we will make light of machine learning's future challenges.;https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition;Kaggle;Distinguish images of dogs from cats;['image data', 'binary classification', 'animals', 'logloss'];1,314;Dogs vs. Cats Redux: Kernels Edition;Playground prediction Competition
2018-04-26 01:59:00;Founded in 2000 by a high school teacher in the Bronx, DonorsChoose.org empowers public school teachers from across the country to request much-needed materials and experiences for their students. At any given time, there are thousands of classroom requests that can be brought to life with a gift of any amount. DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website.  Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they need to solve:  How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as efficiently as possible How to increase the consistency of project vetting across different volunteers to improve the experience for teachers How to focus volunteer time on the applications that need the most assistance  The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval. With an algorithm to pre-screen applications, DonorsChoose.org can auto-approve some applications quickly so that volunteers can spend their time on more nuanced and detailed project ​vetting processes, including doing more to help teachers develop projects that qualify for specific funding opportunities.  Your machine learning algorithm can help more teachers get funded more quickly, and with less cost to DonorsChoose.org, allowing them to channel even more funding directly to classrooms across the country.  Getting Started with Kernels Get familiar with the competition data and the machine learning objective quickly using Kernels. Google's engineering education team has put together a starter tutorial implementing benchmark linear classification model.  Acknowledgments Machine Learning Crash Course was created by Google's engineering education team in partnership with numerous Machine Learning subject matter experts across Google.;https://www.kaggle.com/c/donorschoose-application-screening;DonorsChoose.org;Predict whether teachers' project proposals are accepted;['binary classification', 'crowdfunding', 'auc'];580;DonorsChoose.org Application Screening;Playground prediction Competition
2018-11-27 00:59:00;Hungry for a new competition? Give thanks for this opportunity to avoid those awkward family political dinner discussions and endless holiday movie marathons over the Thanksgiving break. Spend time with your Kaggle family instead to find the real turkey!  In this competition you are tasked with finding the turkey sound signature from pre-extracted audio features. A simple binary problem, or is it? What does a turkey really sound like? How many sounds are similar? Will you be able to find the turkey or will you go a-fowl?  This is a short, fun, holiday, playground competition. Please, do not ruin the fun for yourself and for everyone by using a model trained on the answers.  Don't be a turkey!;https://www.kaggle.com/c/dont-call-me-turkey;Kaggle;Thanksgiving Edition: Find the turkey in the sound bite;['binary classification', 'tabular data', 'animals', 'auc'];267;Don't call me turkey!;Playground prediction Competition
2019-05-08 01:59:00;Long ago, in the distant, fragrant mists of time, there was a competition… It was not just any competition. It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples… without overfitting. Data scientists ― including Kaggle's very own Will Cukierski ― competed by the hundreds. Legends were made. (Will took 5th place, and eventually ended up working at Kaggle!) People overfit like crazy. It was a Kaggle-y, data science-y madhouse. So… we're doing it again. Don't Overfit II: The Overfittening This is the next logical step in the evolution of weird competitions. Once again we have 20,000 rows of continuous variables, and a mere handful of training samples. Once again, we challenge you not to overfit. Do your best, model without overfitting, and add, perhaps, to your own legend. In addition to bragging rights, the winner also gets swag.  Enjoy! Acknowledgments We hereby salute the hard work that went into the original competition, created by Phil Brierly. Thank you!;https://www.kaggle.com/c/dont-overfit-ii;Kaggle;A Fistful of Samples;['binary classification', 'tabular data', 'auc'];2,330;Don't Overfit! II;Playground prediction Competition
2012-01-06 00:59:59;"One of the biggest challenges of an auto dealership purchasing a used car at an auto auction is the risk of that the vehicle might have serious issues that prevent it from being sold to customers. The auto community calls these unfortunate purchases ""kicks"". Kicked cars often result when there are tampered odometers, mechanical issues the dealer is not able to address, issues with getting the vehicle title from the seller, or some other unforeseen problem. Kick cars can be very costly to dealers after transportation cost, throw-away repair work, and market losses in reselling the vehicle. Modelers who can figure out which cars have a higher risk of being kick can provide real value to dealerships trying to provide the best inventory selection possible to their customers. The challenge of this competition is to predict if the car purchased at the Auction is a Kick (bad buy).";https://www.kaggle.com/c/DontGetKicked;;Predict if a car purchased at auction is a lemon;['gini'];570;Don't Get Kicked!;Featured prediction Competition
2016-06-28 01:59:00;Imagine a world where we can use satellite images to help find better access to clean water, prevent poaching of wildlife, predict storms more efficiently, optimize traffic patterns more readily, and inform human behaviors to mitigate the spread of disease.  Thanks to a marked increase of satellites in orbit, we will be able to capture images – and the information contained within – of nearly every place on Earth, every day by 2017. However, our ability to analyze datasets of these images has not advanced as quickly. Changes from day to day in images of the same location are subtle, can be hard to detect, and are difficult to understand in terms of their significance. In this competition, Draper provides a unique dataset of images taken at the same locations over 5 days. Kagglers are challenged to predict the chronological order of the photos taken at each location. Accurately doing so could uncover approaches that have a global impact on commerce, science, and humanitarian works.;https://www.kaggle.com/c/draper-satellite-image-chronology;;Can you put order to space and time?;['image data', 'maspearmanr'];399;Draper Satellite Image Chronology;Featured prediction Competition
2020-03-25 00:59:00;PROJECT OVERVIEW Develop a methodology to calculate an average historical emissions factor of electricity generated for a sub-national region, using remote sensing data and techniques. The Environmental Insights Explorer team at Google is keen to gather insights on ways to improve calculations of global emissions factors for sub-national regions. The ultimate goal of this challenge is to test if calculations of emissions factors using remote sensing techniques are possible and on par with calculations of emissions factors from current methodologies. PROBLEM STATEMENT Current emissions factors methodologies are based on time-consuming data collection and may include errors derived from a lack of access to granular datasets, inability to refresh data on a frequent basis, overly general modeling assumptions, and inaccurate reporting of emissions sources like fuel consumption.  This begs the question: What if there was a different way to calculate or measure emissions factors? We’re challenging the Kaggle community to see if it’s possible to use remote sensing techniques to better model emissions factors. You will develop a methodology to calculate an average historical emissions factor for electricity generation in a sub-national region. We’ve provided an initial list of datasets covering the geographic boundary of Puerto Rico to serve as the foundation for this analysis. As an island, there are fewer confounding factors from nearby areas. Puerto Rico also offers a unique fuel mix and distinctive energy system layout that should make it easier to isolate pollution attributable to power generation in the remote sensing data.  Participants will be tasked with developing a methodology to calculate an average annual historical emissions factor for the sub-national region. Participants will also be asked to provide an explanation of the conditions that would result in a higher/lower emissions factor, as well as a recommendation for how the methodology could be applied to calculate the emissions factor of electricity for another geospatial area using similar techniques. Bonus points will be awarded for smaller time slices of the average historical emissions factors, such as one per month for the 12-month period, and additional bonus points will be awarded for participants that develop methodologies for calculating marginal emissions factors for the sub-national region. HOW TO PARTICIPATE To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will only review the most recent entry. To be valid, a submission must be contained in one or more notebook, and made public on or before the submission deadline. Participants are free to use any datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Earth Engine or Kaggle for the submission to be valid.;https://www.kaggle.com/c/ds4g-environmental-insights-explorer;;Exploring alternatives for emissions factor calculations;['geospatial analysis', 'environment', 'pollution'];;DS4G: Environmental Insights Explorer;Analytics  Competition
2012-04-29 14:00:00;Hosted by  Data Science London and Data Science Global as part of a Big Data Week event, and organised by Kaggle, the first ever global data science hackathon will take place at the same time in several cities around the world spanning a 24-hour period. During this time the data scientists  will compete with each other for cash prizes using a large dataset provided by the Cook County, Illinois, local government. The challenge for the hackathon is to build with better more accurate predictive models of metropolitan air pollution. The EPA’s Air Quality Index is used daily by people suffering from asthma and other respiratory diseases to avoid dangerous levels of outdoor  air pollutants, which can trigger attacks. According to the World Health Organisation there are now estimated to be 235 million people suffering from asthma. Globally, it is now the most common chronic disease among children, with incidence in the US doubling  since 1980.  The model we build could be used as the basis for an early warning system that is capable of accurately predicting dangerous levels of air pollutants on an hourly basis. Data Science Global is a non-profit organization dedicated to bringing together the world’s communities of data scientists, artists, technologists and visionaries.  For our inaugural event, we are hosting a global data science hackathon.   It will be taking  place simultaneously in cities around the world: London, New York, Boston, Chicago, San Francisco, Melbourne, Canberra, Sydney and Turku, Finland, as well as remote participants competing directly through Kaggle.  You can join in the live webcast from the  participating venues at datascienceglobal.org;https://www.kaggle.com/c/dsg-hackathon;;Build a local early warning systems to accurately predict dangerous levels of air pollutants on an hourly basis.;['mae'];110;EMC Data Science Global Hackathon (Air Quality Prediction);Featured prediction Competition
2017-03-08 00:59:00;The proliferation of satellite imagery has given us a radically improved understanding of our planet. It has enabled us to better achieve everything from mobilizing resources during disasters to monitoring effects of global warming. What is often taken for granted is that advancements such as these have relied on labeling features of significance like building footprints and roadways fully by hand or through imperfect semi-automated methods. As these large, complex datasets continue to increase exponentially in number, the Defence Science and Technology Laboratory (Dstl) is seeking novel solutions to alleviate the burden on their image analysts. In this competition, Kagglers are challenged to accurately classify features in overhead imagery. Automating feature labeling will not only help Dstl make smart decisions more quickly around the defense and security of the UK, but also bring innovation to computer vision methodologies applied to satellite imagery.;https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;Defence Science & Technology Laboratory;Can you train an eye in the sky?;['image data', 'multiclass classification', 'custom metric'];419;Dstl Satellite Imagery Feature Detection;Featured prediction Competition
2011-10-01 01:59:59;Going grocery shopping, we all have to do it, some even enjoy it, but can you predict it? Dunnhumby is looking to build a model to better predict when supermarket shoppers will next visit the store and how much they will spend. The modelling data set consists of details of every visit made by 100,000 customers over a year from April 2010 to March 31st 2011. Each visit is stamped with the date and the customer’s spend in that visit. We also have details of their next visit after March 31st. Your challenge is to predict the visit_date and visit_spend of this next visit for each customer_id in the modelling test set. For more details on how you're scored, click here;https://www.kaggle.com/c/dunnhumbychallenge;;Going grocery shopping, we all have to do it, some even enjoy it, but can you predict it? dunnhumby is looking to build a model to better predict when supermarket shoppers will next visit the store and how much they will spend.;['custom metric'];276;dunnhumby's Shopper Challenge;Featured prediction Competition
2019-02-27 00:59:00;Imagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner! Right now, Elo, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key.  Elo has built machine learning models to understand the most important aspects and preferences in their customers’ lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in. In this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers’ lives and help Elo reduce unwanted campaigns, to create the right experience for customers.;https://www.kaggle.com/c/elo-merchant-category-recommendation;Elo;Help understand customer loyalty;['regression', 'banking', 'tabular data', 'rmse'];4,127;Elo Merchant Category Recommendation;Featured prediction Competition
2012-09-02 01:59:00;The EMC source code classification challenge requires you to classify source code files according to the projects they belong to. Given a set of source code files collected from various open source projects, how well can unseen source code files from the same set of open source projects can be classified? Possible real-world applications:  Protecting intellectual property Data Loss Protection (DLP) Automatic categorization of source code repositories;https://www.kaggle.com/c/emc-data-science;;Match source code files to the open source code project;['custom metric'];86;EMC Israel Data Science Challenge;Research prediction Competition
2012-04-16 01:59:00;The aim of the contest is to determine how people may be identified based on their eye movement characteristic. The organizers provide all interested participants dataset of eye movements' recordings in CSV format. After downloading the training dataset, participants may analyze it to prepare their own classification models and try to classify sapmles in test dataset. This in an official competition for BTAS 2012 (The Fifth IEEE International Conference on Biometrics: Theory, Applications and Systems, September 23-27, Washington DC, USA) and all results will be published during that conference (and of course on this web page as well). . All data needed is ready to download! You only need to have some experience in data classification and... take your chance.;https://www.kaggle.com/c/emvic;;Determine how people may be identified based on their eye movement characteristic.;['custom metric'];46;Eye Movements Verification and Identification Competition;Research prediction Competition
2010-05-25 20:00:00;"The Eurovision Song Contest is an annual competition broadcast worldwide which is open to entrants from active members of the European Broadcasting Union (EBU).  This is not defined by Europe's geographical bounds; for example Israel and Cyprus compete in the contest.  This year's contest is being held in Oslo, Norway.  The location is usually the country of the previous year's winner.  The number of entrants changes on a yearly basis, due to flux in EBU membership, political shifts such as the dissolution of the Former Soviet Union and Yugoslavia, and desire (or lack thereof) of countries to participate.   Each country must submit one song to represent it, which must not have been previously commercially released.  All songs are, however, made public by a deadline several weeks before the contest, and this year's entrants are all known already with details such as title, language and artist included in the database provided.   Since 1999 songs may be sung in any language, whereas prior to this point (up to 1998 inclusive) songs had to be sung in one of the entrant country's national languages.Although the voting format has changed over time, the modern positional voting system has been in place since 1975.  Voting countries award scores of 1, 2, 3, 4, 5, 6, 7, 8, 10, and 12 to competitors, with 12 allocated to the favourite song.  Each score can only be awarded once; ie: each voting country can only allocate scores to 10 countries, with all other countries automatically allocated a score of 0.  These scores are then added up to determine a competitor's final score.  No country can vote for itself.Although mass televoting has been the primary mode of final voting since its implementation in 1998, each country also has a back-up jury in the event of failure of the televoting system.  Some countries also supplement televotes with sms votes.Despite the introduction of semi-finals in 2004, the four same countries - Germany, Spain, United Kingdom and France - automatically qualify for the final each year.  This means that they do not compete in the semi-final.  Additionally, the host country - this year, Norway - is also exempt from performing in the semi-final, automatically qualifying for the final.  However, these five countries do vote in the semi-final, specifically France, Germany and Spain in semi-final 1, and United Kingdom and Norway in semi-final 2 this year.";https://www.kaggle.com/c/Eurovision2010;;This competition requires contestants to forecast the voting for this year's Eurovision Song Contest in Norway on May 25th, 27th and 29th.;['ae'];22;Forecast Eurovision Voting;Featured prediction Competition
2013-02-21 00:59:00;We (the competition hosts) are excited to sponsor the Event Recommendation Engine Challenge, which asks you to predict what events our users will be interested in based on events they’ve responded to in the past, user demographic information, and what events  they’ve seen and clicked on in our app. The insights you discover from this data, and the algorithms the winners create, will allow us to improve our event recommendation algorithm, a core part of our applications and a key element in improving user experience. This is the first competition launching under the  Kaggle Startup Program!;https://www.kaggle.com/c/event-recommendation-engine-challenge;;Predict what events our users will be interested in based on user actions, event metadata, and demographic information.;['custom metric'];223;Event Recommendation Engine Challenge;Featured prediction Competition
2016-06-11 01:59:00;Planning your dream vacation, or even a weekend escape, can be an overwhelming affair. With hundreds, even thousands, of hotels to choose from at every destination, it's difficult to know which will suit your personal preferences. Should you go with an old standby with those pillow mints you like, or risk a new hotel with a trendy pool bar?   Expedia wants to take the proverbial rabbit hole out of hotel search by providing personalized hotel recommendations to their users. This is no small task for a site with hundreds of millions of visitors every month! Currently, Expedia uses search parameters to adjust their hotel recommendations, but there aren't enough customer specific data to personalize them for each user. In this competition, Expedia is challenging Kagglers to contextualize customer data and predict the likelihood a user will stay at 100 different hotel groups. The data in this competition is a random selection from Expedia and is not representative of the overall statistics.;https://www.kaggle.com/c/expedia-hotel-recommendations;;Which hotel type will an Expedia customer book?;['tabular data', 'recommender systems', 'hotels and accommodations', 'map@{k}'];1,971;Expedia Hotel Recommendations;Featured prediction Competition
2013-11-05 00:59:00;Expedia is the world’s largest online travel agency (OTA) and powers search results for millions of travel shoppers every day. In this competitive market matching users to hotel inventory is very important since users easily jump from website to website. As such, having the best ranking of hotels (“sort”) for specific users with the best integration of price competitiveness gives an OTA the best chance of winning the sale. For this contest, Expedia has provided a dataset that includes shopping and purchase data as well as information on price competitiveness. The data are organized around a set of “search result impressions”, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website. In addition to impressions from the existing algorithm, the data contain impressions where the hotels were randomly sorted, to avoid the position bias of the existing algorithm. The user response is provided as a click on a hotel or/and a purchase of a hotel room. Appended to impressions are the following: 1) Hotel characteristics2) Location attractiveness of hotels3) User’s aggregate purchase history4) Competitive OTA information Models will be scored via performance on a hold-out set.;https://www.kaggle.com/c/expedia-personalized-sort;;Learning to rank hotels to maximize purchases;['ndcg@{k}'];336;Personalize Expedia Hotel Searches - ICDM 2013;Featured prediction Competition
2012-11-22 00:59:00;There is no monetary prize for this competition and no points will be awarded. Highly ranked contestants who indicate their interest will be considered by Facebook for further interviews based on their work in the competition and additional background.;https://www.kaggle.com/c/facebook-ii;Facebook;Round II of the Facebook Recruiting Competition.;['auc'];111;Facebook II - Mapping the Internet;Recruitment prediction Competition
2013-12-21 00:59:00;"Looking for a data science position at Facebook?  After two successful prior Kaggle competitions, Facebook continues their mission to identify the best data scientists and software engineers that Kaggle has to offer. In this third installment, they seek candidates who have experience text mining large amounts of data.  This competition tests your text skills on a large dataset from the Stack Exchange sites.  The task is to predict the tags (a.k.a. keywords, topics, summaries), given only the question text and its title. The dataset contains content from disparate stack exchange sites, containing a mix of both technical and non-technical questions.  Positions are available in Menlo Park, Seattle, New York City, and London; candidates must have, or be eligible to obtain, authorization to work in the US or UK. Please note: you must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions. Crawling stack exchange sites to look up answers is not permitted. Facebook will review the code of the top participants before deciding whether to offer an interview.  This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Facebook, check the box ""Allow host to contact me"" when you make your first entry. Acknowledgements We thank Stack Exchange (and its users) for generously releasing the source dataset through its Creative Commons Data Dumps. All data is licensed under the cc-by-sa license.";https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;Facebook;Identify keywords and tags from millions of text questions;['meanfscore'];366;Facebook Recruiting III - Keyword Extraction;Recruitment prediction Competition
2015-06-09 01:59:00;"Ever wonder what it's like to work at Facebook? Facebook and Kaggle are launching an Engineering competition for 2015. Trail blaze your way to the top of the leader board to earn an opportunity at interviewing for a role as a software engineer, working on world class Machine Learning problems.  In this competition, you'll be chasing down robots for an online auction site. Human bidders on the site are becoming increasingly frustrated with their inability to win auctions vs. their software-controlled counterparts. As a result, usage from the site's core customer base is plummeting.  In order to rebuild customer happiness, the site owners need to eliminate computer generated bidding from their auctions. Their attempt at building a model to identify these bids using behavioral data, including bid frequency over short periods of time, has proven insufficient.  The goal of this competition is to identify online auction bids that are placed by ""robots"", helping the site owners easily flag these users for removal from their site to prevent unfair auction activity.  The data in this competition comes from an online platform, not from Facebook.  Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.";https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot;Facebook;Predict if an online bid is made by a machine or a human;['internet', 'binary classification', 'tabular data', 'auc'];983;Facebook Recruiting IV: Human or Robot?;Recruitment prediction Competition
2016-07-07 01:59:00;Ever wonder what it's like to work at Facebook? Facebook and Kaggle are launching a machine learning engineering competition for 2016. Trail blaze your way to the top of the leaderboard to earn an opportunity at interviewing for one of the 10+ open roles as a software engineer, working on world class machine learning problems.  The goal of this competition is to predict which place a person would like to check in to. For the purposes of this competition, Facebook created an artificial world consisting of more than 100,000 places located in a 10 km by 10 km square. For a given set of coordinates, your task is to return a ranked list of the most likely places. Data was fabricated to resemble location signals coming from mobile devices, giving you a flavor of what it takes to work with real data complicated by inaccurate and noisy values. Inconsistent and erroneous location data can disrupt experience for services like Facebook Check In. We highly encourage competitors to be active on Kaggle Scripts. Your work there will be thoughtfully included in the decision making process. Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.;https://www.kaggle.com/c/facebook-v-predicting-check-ins;Facebook;Identify the correct place for check ins;['internet', 'multiclass classification', 'tabular data', 'geography', 'map@{k}'];1,209;Facebook V: Predicting Check Ins;Recruitment prediction Competition
2012-07-11 01:59:59;Please note: You must compete as an  in recruiting competitions.  You may only use the data provided to make your predictions.  Facebook will review the code of the top participants before deciding whether to offer an interview.;https://www.kaggle.com/c/FacebookRecruiting;Facebook;Show them your talent, not just your resume.;['custom metric'];418;Facebook Recruiting Competition;Recruitment prediction Competition
2017-01-07 01:00:00;The objective of this task is to predict keypoint positions on face images. This can be used as a building block in several applications, such as:  tracking faces in images and video analysing facial expressions detecting dysmorphic facial signs for medical diagnosis biometrics / face recognition  Detecing facial keypoints is a very challenging problem.  Facial features vary greatly from one individual to another, and even for a single individual, there is a large amount of variation due to 3D pose, size, position, viewing angle, and illumination conditions. Computer vision research has come a long way in addressing these difficulties, but there remain many opportunities for improvement. This getting-started competition provides a benchmark data set and an  R tutorial to get you going on analysing face images. Get started with R >> Acknowledgements The data set for this competition was graciously provided by  Dr. Yoshua Bengio of the University of Montreal. James Petterson.;https://www.kaggle.com/c/facial-keypoints-detection;Kaggle;Detect the location of keypoints on face images;['image data', 'rmse'];175;Facial Keypoints Detection;Getting Started prediction Competition
2018-01-16 00:59:00;Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming. The problem becomes more complex as retailers add new locations with unique needs, new products, ever transitioning seasonal tastes, and unpredictable product marketing. Corporación Favorita, a large Ecuadorian-based grocery retailer, knows this all too well. They operate hundreds of supermarkets, with over 200,000 different products on their shelves. Corporación Favorita has challenged the Kaggle community to build a model that more accurately forecasts product sales. They currently rely on subjective forecasting methods with very little data to back them up and very little automation to execute plans. They’re excited to see how machine learning could better ensure they please customers by having just enough of the right products at the right time.;https://www.kaggle.com/c/favorita-grocery-sales-forecasting;Corporación Favorita;Can you accurately predict sales for a large grocery chain?;['regression', 'food', 'tabular data', 'custom metric'];1,674;Corporación Favorita Grocery Sales Forecasting;Featured prediction Competition
2015-03-24 00:59:00;Elite chess players are rated, ranked, analyzed, and compared in many ways. Classical methods of ranking chess players have focused on game histories, paying particular attention to the relative strength of the players involved. This includes the popular FIDE Elo score, which was the focus of one of Kaggle's first ever competitions - Elo vs. the Rest of the World. Recent work on chess analysis has focused on intrinsic performance ratings, where one assesses skill based on the quality of decisions rather than the outcomes of games. For an example of this kind of approach, see this draft by Kenneth Regan. Two advantages of an intrinsic approach are an increased sample size (there are many more moves than games) and the ability to approach new challenges, such as determining whether a player is cheating by performing moves above their skill level. This competition challenges Kagglers to determine players' FIDE Elo ratings at the time a game is played, based solely on the moves in one game. Do a player's moves reflect their absolute skill? Does the opponent matter? How closely does one game reflect intrinsic ability? How well can an algorithm do? Does computational horsepower increase accuracy? Let's find out! You do not need to be a chess expert -- or even know how to play chess -- to attempt this competition. You do need patience and a computer that doesn't mind some heat. The dataset includes 50,000 games between elite, ranked players. As a getting-started computational bonus, Kaggle has run these games through a chess engine to score each move. Good luck finding Elo!;https://www.kaggle.com/c/finding-elo;Kaggle;Predict a chess player's FIDE Elo rating from one game;['tabular data', 'board games', 'mae'];157;Finding Elo;Playground prediction Competition
2015-10-13 01:59:00;"Like last year's Higgs Boson Machine Learning Challenge, this competition deals with the  physics at the Large Hadron Collider (LHC). However, the subject of last year's challenge, the Higgs Boson, was already known to exist. The aim of this year's challenge is to find a phenomenon that is not already known to exist – charged lepton flavour violation – thereby helping to establish ""new physics"".  Flavours of Physics 101 The laws of nature ensure that some physical quantities, such as energy or momentum, are conserved. From Noether’s theorem, we know that each conservation law is associated with a fundamental symmetry. For example, conservation of energy is due to the time-invariance (the outcome of an experiment would be the same today or tomorrow) of physical systems. The fact that physical systems behave the same, regardless of where they are located or how they are oriented, gives rise to the conservation of linear and angular momentum. Symmetries are also crucial to the structure of the Standard Model of particle physics, our present theory of interactions at microscopic scales. Some are built into the model, while others appear accidentally from it. In the Standard Model, lepton flavour, the number of electrons and electron-neutrinos, muons and muon-neutrinos, and tau and tau-neutrinos, is one such conserved quantity.  Interestingly, in many proposed extensions to the Standard Model, this symmetry doesn’t exist, implying decays that do not conserve lepton flavour are possible. One decay searched for at the LHC is τ → μμμ (or τ → 3μ). Observation of this decay would be a clear indication of the violation of lepton flavour and a sign of long-sought new physics. Competition Design You will be working with real data from the LHCb experiment at the LHC, mixed with simulated datasets of the decay. The metric used in this challenge includes checks that physicists do in their analysis to make sure the results are unbiased. These checks have been built into the competition design to help ensure that the results will be useful for physicists in future studies.  To get started, review the Data Page, and be sure to download the Starter Kit. The Starter Kit will help you to get used to the unique submission procedure for this competition. Competition Video Tutorial You've got lots of questions. Researchers at CERN & LCHb have the answers.  - What is the goal of this competition? (1:56) - Why is finding τ → μμμ exciting? (2:18) - What are flavours? (4:10) - Why use machine learning to find τ → μμμ? (4:57) - How did you decide on the size of the dataset? (5:31) - Why is weighted AUC the evaluation metric? (6:09) - Why use Ds → φπ data for the Agreement Test? (7:53) - Why do we need a Correlation Check? (8:44) - How will the competition results impact what you do? (11:38) - How will the competition results be used at CERN? (12:17) Resources Flavour of Physics, Research Documentation Roel Aaij et al., Search for the lepton flavour violating decay τ → µµµ, 2015, JHEP, 1502:121, 2015 New approaches for boosting to uniformity Acknowledgements This competition is brought to you by:                             Co-sponsored by:  Additional support from:";https://www.kaggle.com/c/flavours-of-physics;;Identify a rare decay phenomenon;['binary classification', 'tabular data', 'physics', 'custom metric'];673;Flavours of Physics: Finding τ  →  μμμ;Featured prediction Competition
2018-09-25 01:59:00;The European Organization for Nuclear Research is the world’s largest high energy physics laboratory.  LHCb is an experiment set up to explore what happened after the Big Bang that allowed matter to survive and build the Universe we inhabit today.  The Yandex School of Data Analysis (YSDA) is a free Master’s-level program in Computer Science and Data Analysis, which is offered by Yandex since 2007. The aim of the School is to train specialists in data analysis and information retrieval to be able to solve cutting edge industry problems as well as fundamental research challenges. YSDA is associated member of LHCb since December 2014.  Yandex Data Factory are the Machine Learning and data analytics experts that use data science to improve business’ operations, revenues and profitability. By building upon the real-time personalisation and predictive analytics technology of parent company, Yandex, the fourth largest search engine in the world, Yandex Data Factory helps clients improve their business awareness through the exploitation of their own data. Yandex Data Factory’s proven data science and technology continually analyses, tests, refines and reapplies hundreds of hypotheses to the customers’ datasets to determine the best next course of action. It offers tailored, scalable, SaaS-driven Machine Learning services to a wide variety of data-reliant verticals, such as retail, financial services, travel and telecoms, who wish to use their data for purposes such as improving personalisation, segmentation, churn prevention or fraud detection. Yandex Data Factory was founded in 2014 by Yandex and is headquartered in Amsterdam, operating throughout Europe.  Intel (NASDAQ: INTC) is a world leader in computing innovation. The company designs and builds the essential technologies that serve as the foundation for the world’s computing devices. As a leader in corporate responsibility and sustainability, Intel also manufactures the world’s first commercially available “conflict-free” microprocessors. Additional information about Intel is available at http://newsroom.intel.com and http://blogs.intel.com.  The University of Zurich is one of the leading research universities in Europe and offers the widest range of degree programs in Switzerland. It was founded in 1833 and currently has seven faculties: Philosophy, Human Medicine, Economic Sciences, Law, Mathematics and Natural Sciences, Theology and Veterinary Medicine.   Warwick is one of the UK's leading universities, with an acknowledged reputation for excellence in research and teaching, for innovation, and for links with business and industry.  Institute of Nuclear Physics, Polish Academy of Sciences. Founded in 1955 Institute of Nuclear Physics has become leading Particle Physics research institution and ranked as class A+ by Polish Ministry of Higher Education.  Consistently ranked as one of Russia’s top universities, the Higher School of Economics is a leader in Russian education and one of the preeminent economics and social sciences universities in eastern Europe and Eurasia.;https://www.kaggle.com/c/flavours-of-physics-kernels-only;;Identify a rare decay phenomenon;['custom metric'];64;Flavours of Physics: Finding τ  →  μμμ (Kernels Only);Playground Code Competition
2013-03-12 00:59:00;Flight Quest Phase 1 Winners         1. Xavier Conort        1. Hong Cao        1. Clifton Phua       1. Ghim-Eng Yap        1. Kenny Chua             Team Gxav &* used a mixture of gradient boosting and random forest models to predict gate and runway arrival times. With average errors of 4.2 and 3.2 minutes for gate and runway arrivals, respectively, this translates to 40% and 45% improvements over the standard industry benchmark estimates. Key to their success was careful feature selection with their final models using only 58 and 84 features for gate and runway arrivals, respectively, from the total 258 features they painstakingly constructed and optimized.              2. Jonathan Peters        2. Pawel Jankiewicz             Team As High As Honor used a two-step approach that combined the results of a generalized linear model that encoded intuition about important variables with refinements derived from a random forest model. The team capitalized on the success of the linear model to add the effects of multiple variables and cleanly resolve issues of missing data.            3. Gabor Takacs           Team Taki used a six layer model relying on successive ridge regressions and gradient boosting machines to model both gate and runway arrival times. This approach used 56 features extracted from the raw data, with all but two coming from the test day data.             4. Sergey Kozub           Team Sun’s approach to predicting gate and runway arrival times relied on creating a derived data set with new variables encoding information about the aircraft, airport, airway, gate, hour, and flight path times. Important features used in this model include aircraft GPS position, ASDI flight plans the direction from which airplanes approached airport runways.            5. Jacques Kvam   Jacques Kvam’s approach for predicting runway and gate arrival times used gradient boosting for a model using 10,000 trees and a whopping 1,102 features trained on 260,000 flights. Most significant among these included the distance between the final waypoint and the arrival airport. Many weather features were important as well including temporary vertical visibility and wind speed at the arrival airport.            Honorable Mention: Matt Berseth          Team __mtb__ used random forest and gradient boosted models to estimate runway and gate arrival times. The final solution included over 100 different individual models, each focused on a narrow set of features (i.e. wind/weather, flight plan, aircraft's current location, etc.). These individual models were blended together to generate the final estimates. The training data was created by randomly selecting eight cutoff times for each day in the training period. A separate cross validation data set was used to select hyper-parameters.         Think you can change the future of flight? Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays. There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with “real time business intelligence,”—information available in the cockpit that would allow them to make adjustments to their flight patterns.  Your challenge, should you decide to accept it:  Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. Tweet;https://www.kaggle.com/c/flight;;Think you can change the future of flight?;['custom metric'];172;GE Flight Quest;GE Quests prediction Competition
2014-02-24 00:59:00;Flight Quest Phase 2 Winners Final Prizes   1stJose Fonollosa    2ndSergey Kozub    3rdWillem Mestrom    4thDmytro Lystopad  Read more about the winners » Milestone Prize   Roman A. Prokopenko     The submissions to the Final Phase leaderboard closed on Sunday 11:59 pm UTC February 23, 2014.  The teams on the leaderboard carried forward from the Main Phase of the Flight Quest 2 competition.;https://www.kaggle.com/c/flight2-final;;Final Phase of Flight Quest 2;['custom metric'];33;Flight Quest 2: Flight Optimization, Final Phase;GE Quests prediction Competition
2014-01-19 00:59:00;These pages describe the Main phase of this competition, which is now closed. Click here to visit the final phase of Flight Quest 2.  Think you can change the future of flight? Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays. There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with “real time business intelligence” — information available in the cockpit that would allow them to make adjustments to their flight patterns.  Your challenge, should you decide to accept it:  Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. For background on the competition structure, visit the Basic Structure page. Be sure to check the Forums regularly to stay on top of the latest competition news.  Download data »  Make a submission »;https://www.kaggle.com/c/flight2-main;;Optimize flight routes based on current weather and traffic.;['custom metric'];121;Flight Quest 2: Flight Optimization, Main Phase;GE Quests prediction Competition
2013-09-26 01:59:00;These pages describe the first Milestone phase of this competition, which is now closed. Click here to visit the current phase of Flight Quest 2. Think you can change the future of flight? Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays. There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with “real time business intelligence” — information available in the cockpit that would allow them to make adjustments to their flight patterns.  Your challenge, should you decide to accept it:  Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. For background on the competition structure, visit the Basic Structure page. Be sure to check the Forums regularly to stay on top of the latest competition news.  Download data »  Make a submission »;https://www.kaggle.com/c/flight2-milestone;;Optimize flight routes based on current weather and traffic.;['custom metric'];129;Flight Quest 2: Flight Optimization, Milestone Phase;GE Quests prediction Competition
2020-05-12 01:59:00;Tensor Processing Units (TPUs) are Now Available on Kaggle Tensor Processing Unit (TPU) quotas are now available on Kaggle, at no cost to you! TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try. The latest Tensorflow release (TF 2.1) was focused on TPUs and they’re now supported both through the Keras high-level API and at a lower level, in models using a custom training loop. We can’t wait to see how your solutions are accelerated by TPUs! The Challenge It’s difficult to fathom just how vast and diverse our natural world is. There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish – and astonishingly, over 400,000 different types of flowers. In this competition, you’re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we’re sticking to just over 100 types).  To get started with TPUs:  Read the TPU documentation one-pager Then jump right into the Getting Started Notebook for this competition  Quick note: a TPU is a network-connected accelerator and requires a couple extra lines in your code. Flipping the TPU switch in your notebook will not, by itself, accelerate your code.  Have Questions? Martin Görner, Google Developer Advocate and author of Tensorflow without a PhD will be actively engaged in the competition forum. If you have a question or need help troubleshooting, that’s the best place to find help.;https://www.kaggle.com/c/flower-classification-with-tpus;Google Cloud TPU;Use TPUs to classify 104 types of flowers;['tpu', 'image data', 'plants', 'macrofscore'];848;Flower Classification with TPUs;Playground Code Competition
2018-09-25 01:59:00;Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing. In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from cartographic variables. The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type. This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. This competition originally ran in 2015. We are relaunching it as a kernels-only version here. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science;https://www.kaggle.com/c/forest-cover-type-kernels-only;Kaggle;Use cartographic variables to classify forest categories;['tabular data', 'forestry', 'categorizationaccuracy'];359;Forest Cover Type (Kernels Only);Playground Code Competition
2015-05-12 01:59:00;Get started on this competition with Kaggle Scripts. No data download or local environment needed! Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing. In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type. This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science;https://www.kaggle.com/c/forest-cover-type-prediction;Kaggle;Use cartographic variables to classify forest categories;['multiclass classification', 'forestry', 'categorizationaccuracy'];1,692;Forest Cover Type Prediction;Playground prediction Competition
2018-08-01 13:59:00;Some sounds are distinct and instantly recognizable, like a baby’s laugh or the strum of a guitar. Other sounds aren’t clear and are difficult to pinpoint. If you close your eyes, can you tell which of the sounds below is a chainsaw versus a blender? Moreover, we often experience a mix of sounds that create an ambience – like the clamoring of construction, a hum of traffic from outside the door, blended with loud laughter from the room, and the ticking of the clock on your wall. The sound clip below is of a busy food court in the UK. Partly because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. Currently, a lot of manual effort is required for tasks like annotating sound collections and providing captions for non-speech events in audiovisual content. To tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 370,000 Creative Commons Licensed sounds) and Google Research’s Machine Perception Team (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this competition. You’re challenged to build a general-purpose automatic audio tagging system using a dataset of audio files covering a wide range of real-world environments. Sounds in the dataset include things like musical instruments, human sounds, domestic sounds, and animals from Freesound’s library, annotated using a vocabulary of more than 40 labels from Google’s AudioSet ontology. To succeed in this competition your systems will need to be able to recognize an increased number of sound events of very diverse nature, and to leverage subsets of training data featuring annotations of varying reliability (see Data section for more information).;https://www.kaggle.com/c/freesound-audio-tagging;;Can you automatically recognize sounds from a wide range of real-world environments?;['map@{k}'];558;Freesound General-Purpose Audio Tagging Challenge;Research prediction Competition
2019-06-18 00:22:00;One year ago, Freesound and Google’s Machine Perception hosted an audio tagging competition challenging Kagglers to build a general-purpose auto tagging system. This year they’re back and taking the challenge to the next level with multi-label audio tagging, doubled number of audio categories, and a noisier than ever training set. If you like raising your ML game, this challenge is for you.  Here's the background: Some sounds are distinct and instantly recognizable, like a baby’s laugh or the strum of a guitar. Other sounds are difficult to pinpoint. If you close your eyes, could you tell the difference between the sound of a chainsaw and the sound of a blender? Because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. A significant amount of manual effort goes into tasks like annotating sound collections and providing captions for non-speech events in audiovisual content. To tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 400,000 Creative Commons Licensed sounds) and Google Research’s Machine Perception Team  (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this new competition. To win this competition, Kagglers will develop an algorithm to tag audio data automatically using a diverse vocabulary of 80 categories. If successful, your systems could be used for several applications, ranging from automatic labelling of sound collections to the development of systems that automatically tag video content or recognize sound events happening in real time. Ready to raise your game? Join the competition! Note, this competition is similar in nature to this competition  with a new dataset, and multi-class labels.  Organizers  Eduardo Fonseca, MTG-UPF, Barcelona Manoj Plakal, Google's Sound Understanding, New York Frederic Font, MTG-UPF, Barcelona Dan Ellis, Google's Sound Understanding, New York   This is a Kernels-only competition. Refer to Kernels Requirements for details.;https://www.kaggle.com/c/freesound-audio-tagging-2019;Freesound;Automatically recognize sounds and apply tags of varying natures;['audio data', 'weightedlabelrankingaverageprecision'];880;Freesound Audio Tagging 2019;Research Code Competition
2019-02-21 21:04:00;The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.  RStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have. In this competition, you’re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.;https://www.kaggle.com/c/ga-customer-revenue-prediction;RStudio;Predict how much GStore customers will spend;['regression', 'tabular data', 'rmse'];3,611;Google Analytics Customer Revenue Prediction;Featured prediction Competition
2014-04-05 01:59:00;Understanding how and why we are here is one of the fundamental questions for the human race. Part of the answer to this question lies in the origins of galaxies, such as our own Milky Way. Yet questions remain about how the Milky Way (or any of the other ~100 billion galaxies in our Universe) was formed and has evolved. Galaxies come in all shapes, sizes and colors: from beautiful spirals to huge ellipticals. Understanding the distribution, location and types of galaxies as a function of shape, size, and color are critical pieces for solving this puzzle.  The Whirlpool Galaxy (M51). Credit: NASA and European Space Agency With each passing day telescopes around and above the Earth capture more and more images of distant galaxies. As better and bigger telescopes continue to collect these images, the datasets begin to explode in size. In order to better understand how the different shapes (or morphologies) of galaxies relate to the physics that create them, such images need to be sorted and classified. Kaggle has teamed up with Galaxy Zoo and Winton Capital to produce the Galaxy Challenge, where participants will help classify galaxies into categories.   Image Credit: ESA/Hubble & NASA Galaxies in this set have already been classified once through the help of hundreds of thousands of volunteers, who collectively classified the shapes of these images by eye in a successful citizen science crowdsourcing project. However, this approach becomes less feasible as data sets grow to contain of hundreds of millions (or even billions) of galaxies. That's where you come in. This competition asks you to analyze the JPG images of galaxies to find automated metrics that reproduce the probability distributions derived from human classifications. For each galaxy, determine the probability that it belongs in a particular class. Can you write an algorithm that behaves as well as the crowd does? Contributors: D. Harvey, C. Lintott, T. Kitching, P. Marshall, K. Willett, Galaxy Zoo  Acknowledgments The Contributors and the rest of the Galaxy Zoo and Kaggle teams would like to say a big thank you to Winton Capital for helping make this happen. Without their support, we would have not been able to make this competition go ahead.;https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge;;Classify the morphologies of distant galaxies in our Universe;['rmse'];326;Galaxy Zoo - The Galaxy Challenge;Research prediction Competition
2012-10-31 01:00:00;This is the Wind Forecasting track of Global Energy Forecasting Competition 2012 (GEFCom2012).This competition will bring together state-of-the-art techniques for energy forecasting, serve as the bridge to connect academic  research and industry practice, promote analytics in power engineering education, and prepare the industry to overcome forecasting challenges in the smart grid world. The total prize pool for the wind forecasting track is  $7,500. GEFCom is not a paper contest. Instead, this is a competition that requires participants to develop models and submit forecasts based on a given data set. Accuracy of the forecasts will be one of the evaluation criteria. In addition to accuracy,  the participants are also required to submit a report describing the methodology, findings and models. Selected entries will be invited to IEEE PES General Meeting 2013 in Vancouver, Canada to present their methodologies and results. The team that finishes  at the top of the leaderboard will win a cash prize. However overall winners of the competition will be determined by the GEFCom Award Committee after the presentations based on forecasting accuracy, clarity of documentation, rigors of the approach, interpretability  of the models and practicality to the industry. A few winning entries will be invited to submit the report in scientific paper format to prestigious scholarly journals, such as International Journal of Forecasting and IEEE Transactions on Smart Grid. The topic for the wind forecasting track is focused on mimicking the operation 48-hour ahead prediction of hourly power generation at 7 wind farms, based on historical measurements and additional wind forecast information (48-hour ahead predictions of wind  speed and direction at the sites). The data is available for period ranging from the 1st hour of 2009/7/1 to the 12th hour of 2012/6/28. The period between 2009/7/1 and 2010/12/31 is a model identification and training period, while the remainder of the dataset, that is, from 2011/1/1 to 2012/6/28, is there for the evaluation. The training period is there to be used for designing and estimating  models permiting to predicting wind power generation at lead times from 1 to 48 hours ahead, based on past power observations and/or available meteorological wind forecasts for that period. Over the evaluation part, it is aimed at mimicking real operational  conditions. For that, a number of 48-hour periods with missing power observations where defined. All these power observations are to be predicted. These periods are defined as following. The first period with missing observations is that from 2011/1/1 at 01:00  until 2011/1/3 at 00:00. The second period with missing observations is that from 2011/1/4 at 13:00 until 2011/1/6 at 12:00. Note that to be consistent, only the meteorological forecasts for that period that would actually be available in practice are given.  These two periods then repeats every 7 days until the end of the dataset. Inbetween periods with missing data, power observations are available for updating the models.;https://www.kaggle.com/c/GEF2012-wind-forecasting;;A wind power forecasting problem: predicting hourly power generation up to 48 hours ahead at 7 wind farms;['rmse'];133;Global Energy Forecasting Competition 2012 - Wind Forecasting;Research prediction Competition
2019-04-23 01:59:00;Can you help end gender bias in pronoun resolution?   Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding, and the resolution of ambiguous pronouns is a longstanding challenge.   Unfortunately, recent studies have suggested gender bias among state-of-the-art coreference resolvers.  Google AI Language aims to improve gender-fairness in modeling by releasing the Gendered Ambiguous Pronouns (GAP) dataset, containing gender-balanced pronouns (50% of its examples containing feminine pronouns, and 50% containing masculine pronouns).   In this two-stage competition, Kagglers are challenged to build pronoun resolution systems that perform equally well regardless of pronoun gender. Stage two's final evaluation will use a new dataset following the same format. To encourage gender-fair modeling, the ratio of masculine to feminine examples in the official test data will not be known ahead of time.    ----------    Please cite the original paper if you use GAP in your work:   @inproceedings{webster2018gap,   title =     {Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns},   author =    {Webster, Kellie and Recasens, Marta and Axelrod, Vera and Baldridge, Jason},   booktitle = {Transactions of the ACL},   year =      {2018},   pages =     {to appear}, };https://www.kaggle.com/c/gendered-pronoun-resolution;Google Research;Pair pronouns to their correct entities;['nlp', 'text data', 'multiclassloss'];838;Gendered Pronoun Resolution;Research prediction Competition
2014-03-04 00:59:00;Seasonal influenza, commonly referred to as “the flu”, affects 5-20% of the United States population ever year, causing over 200,000 people to be hospitalized from associated complications. Influenza is a contagious respiratory illness, which can range in severity from mild cases with cold-like symptoms to death. Flu epidemics are fast-moving and spread rapidly due to rapid viral reproduction and short generation times (time from when an infected person infects another), which makes them very difficult to control. Additionally, there are several different strains of the influenza virus and new viruses constantly evolving. All together, this poses a significant challenge when it comes to predicting when, where and at what level of severity the flu will strike during the flu season. The objective of this competition is to build an algorithm that helps predict where the occurrence, peak and severity of influenza in a given season. Enter Now! This competition is only open to Masters-level participants who meet the eligibility criteria. Visit the Enter the Competition page to view the eligibility criteria and request entrance. * Image courtesy of CDC;https://www.kaggle.com/c/genentech-flu-forecasting;;Predict when, where and how strong the flu will be;['rmsle'];50;Flu Forecasting;Masters prediction Competition
2019-08-28 21:23:00;"This competition is closed and no longer accepting submissions. The private leaderboard has been finalized as of 8/28/2019.  Important Warning: This competition has an experimental format and submission style (images as submission). Competitors must use generative methods to create their submission images and are not permitted to make submissions that include any images already classified as dogs or altered versions of such images.  To enforce and prevent cheating, we reserve the right to: (a) Visually inspect all participants' submitted images, (b) review any submitted source code, (c) use these reviews to identify violators or determine winners, and (d) disqualify participants from the competition who are found in violation. This is also specified in the competition's rules  Use your training skills to create images, rather than identify them. You’ll be using GANs, which are at the creative frontier of machine learning. You might think of GANs as robot artists in a sense—able to create eerily lifelike images, and even digital worlds.  ""You might not think that programmers are artists, but programming is an extremely creative profession. It’s logic-based creativity. '' -   John Romero  A generative adversarial network (GAN) is a class of machine learning system invented by Ian Goodfellow in 2014. Two neural networks compete with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set. In this competition, you’ll be training generative models to create images of dogs. Only this time… there’s no ground truth data for you to predict. Here, you’ll submit the images and be scored based on how well those images are classified as dogs from pre-trained neural networks.  Take these images, for example. Can you tell which are real vs. generated?  Trick question; they are all generated! Why dogs? We chose dogs because, well, who doesn’t love looking at photos of adorable pups? Moreover, dogs can be classified into many sub-categories (breed, color, size), making them ideal candidates for image generation. Generative methods (in particular, GANs) are currently used in various places on Kaggle for data augmentation. Their potential is vast; they can learn to mimic any distribution of data across any domain: photographs, drawings, music, and prose. If successful, not only will you help advance the state of the art in generative image creation, but you’ll enable us to create more experiments across a variety of domains in the future.  This is a Kernels-only competition. Refer to Kernels Requirements for details.";https://www.kaggle.com/c/generative-dog-images;Kaggle;Experiment with creating puppy pics;['custom metric'];927;Generative Dog Images;Research Code Competition
2012-04-11 01:59:59;One-shot-learning Gesture Recognition Humans are capable of recognizing patterns like hand gestures after seeing just ONE example. Can machines do that too? This challenge is one of four CHALEARN Gesture Challenge events        This challenge round is over, round 2 has started follow that link to enter. Motivations You will never need a remote controller anymore, you will never need a light switch. Lying in bed in the dark, you will point to the ceiling to turn on the light, you will wave your hand to increase the temperature, you will make a T with your hands to turn on the TV set. You and your loved ones will feel safer at home, in parking lots, in airports: nobody will be watching, but computers will detect distressed people and suspicious activities. Computers will teach you how to effectively use gestures to enhance speech, to communicate with people who do not speak your language, to speak with deaf people, and you will easily learn many other sign languages to comminicate under water, to referee sports, etc. All that thanks to gesture recognition!   Kinect (TM) This is a challenge on gesture and sign language recognition using a Kinect camera.  by providing an affordable 3D camera, which records both an RGB image and a depth image (using an infrared sensor). The challenge focuses on hand gestures. Applications include man-machine communication, translating sign languages for the deaf, video surveillance, and computer gaming. Check out some examples.   One-shot-learning Every application needs a specialized gesture vocabulary. If we want gesture recognition to become part of everyday life, we need gesture recognition machines, which easily get tailored to new gesture vocabularies. This is why the focus of the challenge is on “one-shot-learning” of gestures, which means learning to recognize new categories of gestures from a single video clip of each gesture. The gestures will be drawn from a small vocabulary of gestures, generally related to a particular task, for instance, hand signals used by divers, finger codes to represent numerals, signals used by referees, or marchalling signals to guide vehicles or aircrafts.   Protocol sketch This competition consists of two main components: a development phase (December 7, 2011 to April 6, 2012) and a final evaluation phase (April 7, 2012 to April 10. 2012):    During the development phase of the competition, the participants build a learning system capable of learning from a single training example a gesture classification problem. To that end, they get development data consisting of several batches of gestures, each split into a training set (of one example for each gesture) and a test set of short sequences of one to 5 gestures. Each batch contains gestures from a different small vocabulary of 8 to 15 gestures, for instance diving signals, signs of American Sign Language representing small animals, Italian gestures, etc. The test data labels are provided for such development data so the participants can self-evaluate their systems. To evaluate their progress and compare themselves with others, they can use provided validation data, for which one training example is provided for each gesture token in each batch, but the test data labels are withheld. The prediction results on validation test data can be submitted on-line to get immediate feed-back. A real-time leaderboard shows participants their current standing based on their validation set predictions. During the final evaluation phase of the competition, the participants get to perform similar tasks as those of the validation data on new final evaluation data revealed at the end of the development phase. The participants have a few days to train their systems and upload their predictions. Prior to the end of the development phase, Kaggle will make available a software vault so the participants can upload executable code for their best learning system, which they will then use to train their models and make predictions on the final evaluation test data. This will allow the competition organizers to check their results and ensure the fairness of the competition. Note that participation is NOT conditiioned on submitting code or disclosing methods. See the rules for details. If any of the top ranking participants opted not to submit their learning system for verification, he/she will have to demonstrate the performance of his/her system in person on another verification set.  Any statistically significant deviation between the performance on the final evaluation set and the verification set may result in disqualification.     Sponsors This challenge is organized by CHALEARN and is sponsored in part by Microsoft (Kinect for Xbox 360). Other sponsors include  Texas Instrument.;https://www.kaggle.com/c/GestureChallenge;;Develop a Gesture Recognizer for Microsoft Kinect (TM);['custom metric'];47;CHALEARN Gesture Challenge;Featured prediction Competition
2012-09-12 01:59:00;This is Round 2 - Check out the results of the first round!      NEW:  data annotations and  examples of algorithms (in Matlab).    This competition is identitical to the first round of the  CHALEARN gesture challenge, the only difference is that is will be judged on  new fresh final evaluation data. Keep informed of new data releases and new events, sign up to the  gesturechallenge group. Similarly, there will be a development phase (May 7, 2012 to September 6, 2012) and a final evaluation phase (September 7, 2012 to September 10. 2012):    Development phase: Create a learning system capable of learning from a single training example a gesture classification problem. Practice with development data (a large database of 50,000 labeled gestures is available)  and submit predictions on-line on validation data to get immediate feed-back on the leaderboard. Recommended: towards the end of the development phase, submit your code for verification purpose. See the evaluation section. Final evaluation phase: Make predictions on the new final evaluation data revealed at the end of the development phase. The participants will have 4 days to train their systems and upload their predictions.    Sponsors This challenge is organized by CHALEARN and is sponsored in part by Microsoft (Kinect for Xbox 360). Other sponsors include Texas Instrument.;https://www.kaggle.com/c/GestureChallenge2;;Develop a Gesture Recognizer for Microsoft Kinect (TM);['custom metric'];30;CHALEARN Gesture Challenge 2;Research prediction Competition
2012-02-26 01:00:00;Insert your description here;https://www.kaggle.com/c/getting-started;;Create a forum for New Users;['rmse'];;Getting Started;Featured prediction Competition
2016-12-02 00:59:00;Get out your dowsing rods, electromagnetic sensors, … and gradient boosting machines. Kaggle is haunted and we need your help. After a month of making scientific observations and taking careful measurements, we’ve determined that 900 ghouls, ghosts, and goblins are infesting our halls and frightening our data scientists. When trying garlic, asking politely, and using reverse psychology didn't work, it became clear that machine learning is the only answer to banishing our unwanted guests.  So now the hour has come to put the data we’ve collected in your hands. We’ve managed to identify 371 of the ghastly creatures, but need your help to vanquish the rest. And only an accurate classification algorithm can thwart them. Use bone length measurements, severity of rot, extent of soullessness, and other characteristics to distinguish (and extinguish) the intruders. Are you ghost-busters up for the challenge?;https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;;Can you classify monsters haunting Kaggle?;['multiclass classification', 'tabular data', 'categorizationaccuracy'];763;Ghouls, Goblins, and Ghosts... Boo!;Playground prediction Competition
2011-12-16 00:59:59;Banks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit.  Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years. The goal of this competition is to build a model that borrowers can use to help make the best financial decisions. Historical data are provided on 250,000 borrowers and the prize pool is $5,000 ($3,000 for first, $1,500 for second and $500 for third).;https://www.kaggle.com/c/GiveMeSomeCredit;;Improve on the state of the art in credit scoring by predicting the probability that somebody will experience financial distress in the next two years.;['auc'];924;Give Me Some Credit;Featured prediction Competition
2012-10-31 01:00:00;The competition is financially sponsored by IEEE Power & Energy Society. The top 1 place of the final leadership board will get $500. Competition award winners will be determined by the Award Committee of GEFCom2012 with break down of $4000, $2000 and $1000  for 1st, 2nd and 3rd place respectively.;https://www.kaggle.com/c/global-energy-forecasting-competition-2012-load-forecasting;;A hierarchical load forecasting problem: backcasting and forecasting hourly loads (in kW) for a US utility with 20 zones.;['wrmse'];103;Global Energy Forecasting Competition 2012 - Load Forecasting;Research prediction Competition
2020-08-19 03:59:00;"Open up your pantry and you’re likely to find several wheat products. Indeed, your morning toast or cereal may rely upon this common grain. Its popularity as a food and crop makes wheat widely studied. To get large and accurate data about wheat fields worldwide, plant scientists use image detection of ""wheat heads""—spikes atop the plant containing grain. These images are used to estimate the density and size of wheat heads in different varieties. Farmers can use the data to assess health and maturity when making management decisions in their fields. However, accurate wheat head detection in outdoor field images can be visually challenging. There is often overlap of dense wheat plants, and the wind can blur the photographs. Both make it difficult to identify single heads. Additionally, appearances vary due to maturity, color, genotype, and head orientation. Finally, because wheat is grown worldwide, different varieties, planting densities, patterns, and field conditions must be considered. Models developed for wheat phenotyping need to generalize between different growing environments. Current detection methods involve one- and two-stage detectors (Yolo-V3 and Faster-RCNN), but even when trained with a large dataset, a bias to the training region remains. The Global Wheat Head Dataset is led by nine research institutes from seven countries: the University of Tokyo, Institut national de recherche pour l’agriculture, l’alimentation et l’environnement, Arvalis, ETHZ, University of Saskatchewan, University of Queensland, Nanjing Agricultural University, and Rothamsted Research. These institutions are joined by many in their pursuit of accurate wheat head detection, including the Global Institute for Food Security, DigitAg, Kubota, and Hiphen. In this competition, you’ll detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. You will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China. Wheat is a staple across the globe, which is why this competition must account for different growing conditions. Models developed for wheat phenotyping need to be able to generalize between environments. If successful, researchers can accurately estimate the density and size of wheat heads in different varieties. With improved detection farmers can better assess their crops, ultimately bringing cereal, toast, and other favorite dishes to your table.  This is a Code Competition. Refer to Code Requirements for details.";https://www.kaggle.com/c/global-wheat-detection;University of Saskatchewan;Can you help identify wheat heads using image analysis?;['image data', 'plants', 'custom metric'];2,245;Global Wheat Detection;Research Code Competition
2018-08-31 01:59:00;"Introduction Google AI (Google’s AI research arm, tasked with advancing AI for everyone) is challenging you to build an algorithm that detects objects automatically using an absolutely massive training dataset ― one with more varied and complex bounding-box annotations and object classes than ever before. Here's the background. Computers are getting better and better at vision. But in a few critical ways, they still can't match a human’s intuitive perception. For example, what do you see when you look at this photo?  Most of us would answer, “a sandy beach, the ocean, a few people walking, some trees, grass, and buildings…a woman walking her dog right there! Oh yeah, and there is a man holding a plastic cup.” Can a computer provide as precise an image description? Google AI wants to further push the capabilities of computer vision. We hope that providing very large training set will stimulate research into more sophisticated object and relationship detection models that will exceed current state-of-the-art performance. The results of this Challenge will be presented at a workshop at the European Conference on Computer Vision 2018.  Object Detection Track Object detection is a central task in computer vision, with applications ranging across search, robotics, self-driving cars, and many others.  As deep network solutions become deeper and more complex, they are often limited by the amount of training data available. With this in mind, to spur advances in analyzing and understanding images, Google AI has publicly released the Open Images dataset.  Open Images follows the tradition of PASCAL VOC, ImageNet and COCO, now at an unprecedented scale. The Open Images Challenge is based on Open Images dataset. The training set of the Challenge contains:  12M bounding-box annotations for 500 object classes on 1.7M training images Images of complex scenes with several objects–an average of 7 boxes per image Highly varied images that contain brand new objects like “fedora” and “snowman” Class hierarchy that reflects the relationships between classes of Open Images.   In this track of the Challenge, you are asked to build the best performing algorithm for automatically detecting objects.  Please refer to the Open Images Challenge page for additional details on the dataset. In addition to this Object Detection track, the Challenge also includes a Visual Relationship Detection track to detect pairs of objects in particular relations, e.g. ""woman playing guitar,"" ""beer on table,"" ""dog inside car"", ""man holding coffee"", etc. The Visual Relationship Detection track is available here.      Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.";https://www.kaggle.com/c/google-ai-open-images-object-detection-track;Google Research;Detect objects in varied and complex images.;['custom metric'];454;Google AI Open Images - Object Detection Track;Featured prediction Competition
2018-08-31 01:59:00;"Introduction Google AI (Google’s AI research arm, tasked with advancing AI for everyone) is challenging you to build an algorithm that detects objects automatically using an absolutely massive training dataset ― one with more varied and complex bounding-box annotations and object classes than ever before. Here's the background. Computers are getting better and better at vision. But in a few critical ways, they still can't match a human’s intuitive perception. For example, what do you see when you look at this photo?  Most of us would answer, “a sandy beach, the ocean, a few people walking, some trees, grass, and buildings…a woman walking her dog right there! Oh yeah, and there is a man holding a plastic cup.” Can a computer provide as precise an image description? Google AI wants to further push the capabilities of computer vision. We hope that providing very large training set will stimulate research into more sophisticated object and relationship detection models that will exceed current state-of-the-art performance. The results of this Challenge will be presented at a workshop at the European Conference on Computer Vision 2018.  Visual Relationship Detection Track Identifying different objects (man and cup) is an important problem on its own, but identifying the relationship between them (holding) is critical for many real world use cases. In this Visual Relationship Detection track Challenge you’re asked to build an algorithm that detects pairs of objects in particular relations: things like ""woman playing guitar,"" ""beer on table,"" or ""dog inside car."" The Challenge dataset includes both object bounding boxes and visual relationship annotations. The training set contains annotations for 329 distinct relationship triplets, occurring a total of 374,768 times.   In this track of the Challenge, you are asked to build the best performing algorithm for automatically detecting relationships triplets.  Please refer to the Open Images Challenge page for additional details on the dataset. This competition is one of two tracks in the Open Images Challenge.  Find the Object Detection track of this competition using the entire training set here.       Example of ‘man playing guitar’  Radiofiera - Villa Cordellina Lombardi, Montecchio Maggiore (VI) - agosto 2010 by Andrea Sartorati     Example of ‘chair at table’  Epic Fireworks - Loads A Room by Epic Fireworks";https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track;Google Research;Detect pairs of objects in particular relationships.;['custom metric'];232;Google AI Open Images - Visual Relationship Track;Featured prediction Competition
2020-03-19 16:00:00;Update: this competition has been cancelled on account of the COVID-19 pandemic.  As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.    In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results. As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!  This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.  If you want to extend your analysis then try out our Analytics Competition here;https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['tabular data', 'basketball', 'logloss'];;Google Cloud & NCAA® ML Competition 2020-NCAAM;Featured prediction Competition
2020-03-20 16:00:00;Update: this competition has been cancelled on account of the COVID-19 pandemic.  As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results. As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!  This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here. If you want to extend your analysis then try out our Analytics Competition here;https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['tabular data', 'basketball', 'logloss'];;Google Cloud & NCAA® ML Competition 2020-NCAAW;Featured prediction Competition
2020-02-11 00:59:00;"Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.  Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well…yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.   Unfortunately, it’s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That’s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects. In this competition, you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a ""common-sense"" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get! Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.";https://www.kaggle.com/c/google-quest-challenge;Google;Improving automated understanding of complex question answer content;['nlp', 'text data', 'mcspearmanr'];1,571;Google QUEST Q&A Labeling;Featured Code Competition
2015-09-01 01:59:00;"Think back to this morning: turning off the alarm, getting dressed, brushing your teeth, making coffee, drinking coffee, and locking the door as you left for work. Now imagine doing all those things again, without the use of your hands.  Patients who have lost hand function due to amputation or neurological disabilities wake up to this reality everyday. Restoring a patient's ability to perform these basic activities of daily life with a brain-computer interface (BCI) prosthetic device would greatly increase their independence and quality of life. Currently, there are no realistic, affordable, or low-risk options for neurologically disabled patients to directly control external prosthetics with their brain activity.  Recorded from the human scalp, EEG signals are evoked by brain activity. The relationship between brain activity and EEG signals is complex and poorly understood outside of specific laboratory tests. Providing affordable, low-risk, non-invasive BCI devices is dependent on further advancements in interpreting EEG signals.  This competition challenges you to identify when a hand is grasping, lifting, and replacing an object using EEG data that was taken from healthy subjects as they performed these activities. Better understanding the relationship between EEG signals and hand movements is critical to developing a BCI device that would give patients with neurological disabilities the ability to move through the world with greater autonomy.  Acknowledgements This competition is sponsored by the WAY Consortium (Wearable interfaces for hAnd function recoverY; FP7-ICT-288551).";https://www.kaggle.com/c/grasp-and-lift-eeg-detection;;Identify hand motions from EEG recordings;['multiclass classification', 'mcauc'];378;Grasp-and-Lift EEG Detection;Research prediction Competition
2016-08-31 01:59:00;Planning a celebration is a balancing act of preparing just enough food to go around without being stuck eating the same leftovers for the next week. The key is anticipating how many guests will come. Grupo Bimbo must weigh similar considerations as it strives to meet daily consumer demand for fresh bakery products on the shelves of over 1 million stores along its 45,000 routes across Mexico.  Currently, daily inventory calculations are performed by direct delivery sales employees who must single-handedly predict the forces of supply, demand, and hunger based on their personal experiences with each store. With some breads carrying a one week shelf life, the acceptable margin for error is small. In this competition, Grupo Bimbo invites Kagglers to develop a model to accurately forecast inventory demand based on historical sales data. Doing so will make sure consumers of its over 100 bakery products aren’t staring at empty shelves, while also reducing the amount spent on refunds to store owners with surplus product unfit for sale.;https://www.kaggle.com/c/grupo-bimbo-inventory-demand;;Maximize sales and minimize returns of bakery goods;['food', 'tabular data', 'rmsle'];1,963;Grupo Bimbo Inventory Demand;Featured prediction Competition
2013-05-12 01:00:00;"Predict how successful a product will be after its launch hack/reduce and  dunnhumby announce the Product launch challenge, as part of a one day hackathon. This competition asks you to predict how successful each of a number of product launches will be 26 weeks after the launch, based only on information up to the 13th week after the launch.  The training set and question set each contain by week for each launch:  The category of the product, such as Bread, Coffee or Video Games The number of stores selling the product The number of units sold that week The number of distinct customers who have bought the product (cumulative) The number of distinct customers who have bought the product at least twice (cumulative) Cumulative units sold to a number of different customer groups: Convenience at home, Family Focussed, Finest, Grab and Go, Shoppers On A Budget, Traditional Homes, Watching the Waistline, Least Price Sensitive, Price Sensitive, Splurge and Save, and Very Price Sensitive.   Competition begins: Saturday, May 11, 9am EDT (1:00pm UTC) Competition ends: Saturday, May 11, 7pm EDT (11:00pm UTC) This competition awards 25% the ranking points of a standard competition, but does not count towards tiers. The top remote participant in the Kaggle competition will receive a ""Prize Winner"" achievement on their profile, in addition to the three local winners.";https://www.kaggle.com/c/hack-reduce-dunnhumby-hackathon;;The success or failure of a new product launch is often evident within the first few weeks of sales. Can you predict a product's destiny?;['rmsle'];108;dunnhumby & hack/reduce Product Launch Challenge;Featured prediction Competition
2020-09-23 01:59:00;"Ahoy there! There's halite to be had and ships to be deployed! Are you ready to navigate the skies and secure your territory? Halite by Two Sigma (""Halite"") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.  Created by Two Sigma in 2016, more than 15,000 people around the world have participated in a Halite challenge. Players apply advanced algorithms in a dynamic, open source game setting. The strategic depth and immersive, interactive nature of Halite games make each challenge a unique learning environment. Halite IV builds on the core game design of Halite III with a number of key changes that shift the focus of the game towards tighter competition on a smaller board. New game features include regenerating halite, shipyard creation, no more ship movement costs, and stealing halite from other players! So dust off your halite meters and fasten your seatbelts. The fourth season of Halite is about to begin!";https://www.kaggle.com/c/halite;Kaggle;Collect the most halite during your match in space;['video games', 'simulations', 'custom metric'];1,139;Halite by Two Sigma;Featured Simulation Competition
2012-08-27 06:00:00;Winning Analysis and Viz to be featured with credit in an upcoming Harvard Business Review - Vision Statement section.  In addition, there will be cash awards for the winning entries: 1st: $ 1500 2nd: $750 3rd: $250;https://www.kaggle.com/c/harvard-business-review-vision-statement-prospect;;Your Analysis and/or Visualization featured in the Harvard Business Review;['rmse'];;Harvard Business Review 'Vision Statement' Prospect;Prospect prediction Competition
2020-07-28 01:59:00;"Note: Put your heads together to solve programming challenges. Google's coding competition, Hash Code, has just finished for 2020. Use this online qualifier from 2019 to keep your skills sharp for future competitions! As the saying goes, ""a picture is worth a thousand words.""  We agree – photos are an important part of contemporary digital and cultural life. How we experience photos largely depends on the story they’re arranged to tell. The same shots could be a monotonous series of snaps or form a narrative masterpiece. Approximately 2.5 billion people around the world carry a camera – in the form of a smartphone – in their pocket every day. We tend to make good use of it, too, taking more photos than ever (back in 2017, Google Photos announced it was backing up more than 1.2 billion photos and videos per day)! The rise of digital photography creates an interesting challenge: what should we do with all of these photos? In this competition, you will compose a slideshow out of a photo collection. Given a list of photos and the tags associated with each photo, you are challenged to arrange the photos into a slideshow that is as interesting as possible (the evaluation section explains what we mean by “interesting”) Will your slideshow tell a good story or be a major snoozefest?";https://www.kaggle.com/c/hashcode-photo-slideshow;Google;Optimizing a photo album from Hash Code 2019;['internet', 'optimization', 'custom metric'];89;Hash Code Archive - Photo Slideshow Optimization;Playground Code Competition
2015-01-08 00:59:00;Every year Santa has to satisfy a grueling toy-production schedule. Toy orders arrive all year long and the Elfin Workers Union is stronger than ever. Let's help Santa develop a job scheduling algorithm that meets his toy target while keeping his elves healthy and happy.  Problem Description In this job scheduling problem, you will assign which elves work on which toys, at what time, and for how long. The goal is to complete all of the toys as early as possible, scaled by the natural log of the number of elves that work. Thus the objective S is S=tf∗log(1+ne) where  tf is the last minute the final toy is complete, from reference date Jan 1, 2014 at 12:00 AM ne is the number of unique elves that were needed to build the toys  Toys There are 10 million toys that will need to be built by the elves. Each toy is described by an id, the time the order for the toy arrives in Santa's workshop, and the amount of time it takes to build the toy.  Work on toys cannot start before the order comes in but can start any time after it comes in. Once work on a toy starts, it must continue until the toy is complete, and it must be performed by only one elf. As a result, an elf cannot start work one day, stop, and then resume the next morning, or have a different elf resume the work. All toys must be completely built for the submission to be valid. Submissions with incomplete toys or where work starts too soon or too late will result in an invalid scoring exception. Working Conditions Santa's Workshop opens for the year on January 1, 2014 at 9:00 am North Pole Time. Sanctioned elf working hours are every day, 7 days a week, from 9:00 to 19:00 (10 hours per day). Work outside of these hours are unsanctioned and penalized. Every minute worked during unsanctioned hours must be compensated with a rest period of equivalent time during sanctioned hours. If an elf works from 14:00-19:33, the next time he can work is the following day at 9:33. Thus 33 minutes overtime results in 33 minutes rest time. Submissions that have elves returning to work before the appropriate amount of rest time has passed will result in an invalid scoring exception. An elf with no accrued resting period may start work at any time. Elves There are 900 elves in Santa's Workshop. Each elf is described by     An elf's productivity rating determines how efficiently he builds a toy. A productivity rating of 1.0 means a 120-min toy takes 120 minutes to build. A 1.25-rating means a 120-min toy takes him only 120-min/1.25 = 96 minutes to build. Minimum and maximum values for the productivity rating are 0.25 and 4.0, respectively. All elves start the year with a productivity rating of 1.0. An elf’s productivity rating changes as he completes toys. Ratings are held constant during the building of a toy and updated once the toy is complete. The rating is calculated per the required time for a toy, not per the time he spends on a toy. The time used in the productivity calculation will be toy_duration/elf_starting_productivity, e.g.: a 0.5-elf working on a 120-min toy uses 240 minutes in his productivity calculation. If a 1.0-rated elf is assigned to work on a 120-min toy for 180 minutes, his productivity rating will only take into account the 120 minutes of needed work. For each hour worked outside of the sanctioned hours, the rest period will apply (see Working Conditions described above).  For every hour worked on actively building a toy during sanctioned work hours, an elf's productivity increases as p=p′∗(1.02)n where p is the updated productivity, p' is the previous productivity, and n is the number of hours (not minutes, can be a decimal value) worked that contributed to the building of the toy. Work performed during unsanctioned hours decrease an elf's productivity: p=p′∗(0.9)m where m is the number of hours (not minutes, can be a decimal value) worked that contributed to the building of the toy. In practice, the productivity is updated in a single step once work is over as p=p′∗(1.02)n∗(0.9)m Acknowledgements This competition is brought to you by;https://www.kaggle.com/c/helping-santas-helpers;;Jingle bells, Santa tells ...;['custom metric'];437;Helping Santa's Helpers;Featured prediction Competition
2020-05-27 01:59:00;"The Herbarium 2020 FGVC7 Challenge is to identify vascular plant species from a large, long-tailed collection herbarium specimens provided by the New York Botanical Garden (NYBG). The Herbarium 2020 dataset contains over 1M images representing over 32,000 plant species. This is a dataset with a long tail; there are a minimum of 3 specimens per species, however, some species are represented by more than a hundred specimens. This dataset only contains vascular land plants which includes lycophytes, ferns, gymnosperms, and flowering plants. The extinct forms of lycophytes are the major component of coal deposits, ferns are indicators of ecosystem health, gymnosperms provide major habitats for animals, and flowering plants provide all of our crops, vegetables, and fruits.        The teams with the most accurate models will be contacted, with the intention of using them on the un-named plant collections in the NYBG herbarium collection, and assessed by the NYBG plant specialists. Background The New York Botanical Garden (NYBG) herbarium contains more than 7.8 million plant and fungal specimens. Herbaria are a massive repository of plant diversity data.  These collections not only represent a vast amount of plant diversity, but since herbarium collections include specimens dating back hundreds of years, they provide snapshots of plant diversity through time.  The integrity of the plant is maintained in herbaria as a pressed, dried specimen; a specimen collected nearly two hundred years ago by Darwin looks much the same as one collected a month ago by an NYBG botanist.  All specimens not only maintain their morphological features but also include collection dates and locations, and the name of the person who collected the specimen.  This information, multiplied by millions of plant collections, provides the framework for understanding plant diversity on a massive scale and learning how it has changed over time. About This is an FGVC competition hosted as part of the FGVC7 workshop at CVPR 2020 and sponsored by NYBG. Details of this competition are mirrored on the github page. Please post in the forum or open an issue if you have any questions or problems with the dataset.";https://www.kaggle.com/c/herbarium-2020-fgvc7;Fine-Grained Visual Categorization 7;Identify plant species from herbarium specimens. Data from New York Botanical Garden.;['image data', 'macrofscore'];153;Herbarium 2020 - FGVC7;Research prediction Competition
2013-04-04 09:00:00;"Please note: This competition is over! The leaderboard now displays the final results. This means that the only people can download the data or make submissions are people who accepted the competition rules prior to 06:59:59 UTC on October 4, 2012. Individuals who had accepted to rules but not yet formed a team at that date may join a team or create their own team (consisting of them only). No teams may merge at this point. ------------------ More than 71 million individuals in the United States are admitted to hospitals each year, according to the latest survey from the American Hospital Association. Studies have concluded that in 2006 well over $30 billion was spent on unnecessary hospital admissions. Is there a better way? Can we identify earlier those most at risk and ensure they get the treatment they need? The Heritage Provider Network (HPN) believes that the answer is ""yes”. To achieve its goal of developing a breakthrough algorithm that uses available patient data to predict and prevent unnecessary hospitalizations, HPN is sponsoring the Heritage Health Prize Competition (the “Competition”). HPN believes that incentivized competition is the best way to achieve the radical breakthroughs necessary to begin fixing America’s health care system. The winning team will create an algorithm that predicts how many days a patient will spend in a hospital in the next year. Once known, health care providers can develop new care plans and strategies to reach patients before emergencies occur, thereby reducing the number of unnecessary hospitalizations. This will result in increasing the health of patients while decreasing the cost of care. In short, a winning solution will change health care delivery as we know it – from an emphasis on caring for the individual after they get sick to a true health care system. The Competition runs for two years and offers a US $3 million Grand Prize, as well as six Milestone Prizes totaling $230,000, which are awarded in varying amounts at three designated intervals during the Competition.";https://www.kaggle.com/c/hhp;;Identify patients who will be admitted to a hospital within the next year using historical claims data. (Enter by 06:59:59 UTC Oct 4 2012);['rmsle'];1,350;Heritage Health Prize;Featured prediction Competition
2014-09-16 01:59:00;"Discovery of the long awaited Higgs boson was announced July 4, 2012 and confirmed six months later. 2013 saw a number of prestigious awards, including a Nobel prize. But for physicists, the discovery of a new particle means the beginning of a long and difficult quest to measure its characteristics and determine if it fits the current model of nature. A key property of any particle is how often it decays into other particles. ATLAS is a particle physics experiment taking place at the Large Hadron Collider at CERN that searches for new particles and processes using head-on collisions of protons of extraordinarily high energy. The ATLAS experiment has recently observed a signal of the Higgs boson decaying into two tau particles, but this decay is a small signal buried in background noise.  The goal of the Higgs Boson Machine Learning Challenge is to explore the potential of advanced machine learning methods to improve the discovery significance of the experiment. No knowledge of particle physics is required. Using simulated data with features characterizing events detected by ATLAS, your task is to classify events into ""tau tau decay of a Higgs boson"" versus ""background.""  The winning method may eventually be applied to real data and the winners may be invited to CERN to discuss their results with high energy physicists. Acknowledgements   This competition is brought to you by       Additional support from:";https://www.kaggle.com/c/higgs-boson;;Use the ATLAS experiment to identify the Higgs boson;['custom metric'];1,784;Higgs Boson Machine Learning Challenge;Featured prediction Competition
2019-03-31 00:59:00;"In this competition, you must create an algorithm to identify metastatic cancer in small image patches taken from larger digital pathology scans. The data for this competition is a slightly modified version of the PatchCamelyon (PCam) benchmark dataset (the original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates).   PCam is highly interesting for both its size, simplicity to get started on, and approachability. In the authors' words:   [PCam] packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. Models can easily be trained on a single GPU in a couple hours, and achieve competitive scores in the Camelyon16 tasks of tumor detection and whole-slide image diagnosis. Furthermore, the balance between task-difficulty and tractability makes it a prime suspect for fundamental machine learning research on topics as active learning, model uncertainty, and explainability.     Acknowledgements   Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Bas Veeling, with additional input from Babak Ehteshami Bejnordi, Geert Litjens, and Jeroen van der Laak.  You may view and download the official Pcam dataset from  GitHub. The data is provided under the CC0 License, following the license of Camelyon16. If you use PCam in a scientific publication, please reference the following papers: [1] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling. ""Rotation Equivariant CNNs for Digital Pathology"".  arXiv:1806.03962 [2] Ehteshami Bejnordi et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA: The Journal of the American Medical Association, 318(22), 2199–2210. doi:jama.2017.14585 Photo by Ousa Chea";https://www.kaggle.com/c/histopathologic-cancer-detection;Kaggle;Identify metastatic tissue in histopathologic scans of lymph node sections;['cancer', 'medicine', 'research', 'auc'];1,157;Histopathologic Cancer Detection;Playground prediction Competition
2010-08-02 14:32:00;"This contest focuses on using the nucleotide sequence of the Reverse Transcriptase (RT) and Protease (PR) to predict the patient's short-term progression. For the non-Biologist: the nucleotide sequence is the blueprint of the protein, which is the workhorse of the cell.  The RT enzyme is responsible for copying the HIV-1 genome within the cell. As the HIV-1 genome is translated it is in one long string of amino acids; the PR protein cuts this string into the numerous functional units - required by the HIV life-cycle. These are the proteins that are targeted by most HIV-1 drugs since they are mostly unique to the HIV-1 life-cycle.  Along with the HIV-1 viral sequences I have provided the two common clinical indicators used to determine the ""general health"" of an HIV-1 infected individual: Viral Load and CD4+ cell counts.  The CD4+ cell count is an estimate of the number of white-blood-cells in 1 mL of blood while the viral load is the number of viral particles in that same mL.  In this dataset the viral load is represented in a log-10 scale.  The higher the number the more ""active"" the immune system. Paradoxically higher CD4 counts imply both a healthier individual but also a higher amount of viral reproduction (the virus primarily replicates in CD4 cells).If you're interested in learning more about the HIV lifecycle and HIV treatments, here are some extra resources:http://en.wikipedia.org/wiki/HIVhttp://www.youtube.com/watch?v=RO8MP3wMvqg&feature=relatedhttp://www.hiv.lanl.gov/content/sequence/HIV/HIVTools.htmlhttp://en.wikipedia.org/wiki/HIV_therapy#Treatment";https://www.kaggle.com/c/hivprogression;;This contest requires competitors to predict the likelihood that an HIV patient's infection will become less severe, given a small dataset and limited clinical information.;['mce'];107;Predict HIV Progression;Featured prediction Competition
2018-08-30 01:59:00;Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.  Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities. While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.;https://www.kaggle.com/c/home-credit-default-risk;Home Credit Group;Can you predict how capable each applicant is of repaying a loan?;['banking', 'tabular data', 'auc'];7,190;Home Credit Default Risk;Featured prediction Competition
2016-04-26 01:59:00;Shoppers rely on Home Depot’s product authority to find and buy the latest products and to get timely solutions to their home improvement needs. From installing a new ceiling fan to remodeling an entire kitchen, with the click of a mouse or tap of the screen, customers expect the correct results to their queries – quickly. Speed, accuracy and delivering a frictionless customer experience are essential. In this competition, Home Depot is asking Kagglers to help them improve their customers' shopping experience by developing a model that can accurately predict the relevance of search results. Search relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. Currently, human raters evaluate the impact of potential changes to their search algorithms, which is a slow and subjective process. By removing or minimizing human input in search relevance evaluation, Home Depot hopes to increase the number of iterations their team can perform on the current search algorithms.;https://www.kaggle.com/c/home-depot-product-search-relevance;;Predict the relevance of search results on homedepot.com;['tabular data', 'rmse'];2,122;Home Depot Product Search Relevance;Featured prediction Competition
2016-02-09 00:59:00;Before asking someone on a date or skydiving, it's important to know your likelihood of success. The same goes for quoting home insurance prices to a potential customer. Homesite, a leading provider of homeowners insurance, does not currently have a dynamic conversion rate model that can give them confidence a quoted price will lead to a purchase.  Using an anonymized database of information on customer and sales activity, including property and coverage information, Homesite is challenging you to predict which customers will purchase a given quote. Accurately predicting conversion would help Homesite better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments.;https://www.kaggle.com/c/homesite-quote-conversion;;Which customers will purchase a quoted insurance plan?;['binary classification', 'tabular data', 'auc'];1,755;Homesite Quote Conversion;Featured prediction Competition
2013-02-18 01:00:00;Hospital Quest Winners         1. Russ Graney        1. Mike Galbo        1. Janan Rajeevikaran   Integrated with the discharge management process, this app automatically generates lists of available post-acute care providers. This minimizes administrative tasks for social workers and enables them to devote more time to patients.         2. Sabrina Casucci, Dapeng Cao, Theresa Guarrera, David Lavergne, Nicolette McGeorge, Judith Tiferes Wang, and Yuan Zhou (Buffalo, NY, USA)   This app provides a mobile solution to the complex problems of discharge management. By supporting better communication throughout the discharge management process, it introduces patient and caregiver choice into the discharge discussion and supports timely communication between the hospital care team and community based care providers.         3. Dr Philip Xiu        3. Ivan Wong        3. Alex Fargus       3. Dr Alain Vuylsteke   This app enables a more efficient approach to managing porter resources in a care facility, dispatching porter resources to the areas of greatest need.          4. Jon Gautsch   By grouping primary care physicians by “permission levels” and patients with a “priority score,” this app is designed to streamline scheduling between primary care physicians and specialists. This allows specialists to schedule more referrals from physicians whose referrals lead to the most surgeries and ultimately, more profit for specialty offices.         5. Mark Kizelshteyn        5. Molly Lafferty    This app facilitates the communication of discharge instructions in a visual, plain-language manner, helping patients to better follow with their post-discharge care plan.         6. Matt Scantland     This app streamlines the medication pre-authorization process by consolidating and automating requests from health plans and delivering the information to care providers.         7. Chris Nunes   This tablet-based app delivers a robust set of tools to patients, allowing them to interface with the hospital to learn more about their care, understand ownership of their own health and records, participate in quality and error control, and provide feedback on their experience.         8. Kerry McLuckie        8. Colin N. Young        8. Catharine G. Clark       8. David Clark   This app provides workflow checklists for standard healthcare procedures to improve operational flow, provide vital information for the patient and hospital staff, reduce mistakes and improve the patient experience.         LSU. Scott Keisler   This idea focused on increasing the use of geographic information and technologies to improve scheduling.     Think it’s possible to make hospital visits hassle-free? GE does. Despite every good intention, far too often, frustration and confusion are common reactions to a hospital visit. Many factors must come together to avoid things like long wait times, poor communication, repetitive paperwork, procedure delays, damaged or lost equipment, delayed discharge, and more. It is estimated that there is $100 billion wasted annually in healthcare inefficiencies, distracting facilities from their primary focus ‐ patient care. Now more than ever, we have the ability to improve on the efficiencies within hospitals. Think you’ve got the cure? Your challenge:  Contribute to the design of the ultimate patient experience. While medicine should be left to the professionals, many aspects of hospital operations are ripe for rethinking. In this Quest, focus on operational (non-medical) solutions that can promote an improved health care system experience for patient and family. Tweet;https://www.kaggle.com/c/hospital;;Think it’s possible to make hospital visits hassle-free? GE does.;['rmse'];;GE Hospital Quest;GE Quests prediction Competition
2015-05-16 01:59:00;"For agriculture, it is extremely important to know how much it rained on a particular field. However, rainfall is variable in space and time and it is impossible to have rain gauges everywhere. Therefore, remote sensing instruments such as radar are used to provide wide spatial coverage. Rainfall estimates drawn from remotely sensed observations will never exactly match the measurements that are carried out using rain gauges, due to the inherent characteristics of both sensors. Currently, radar observations are ""corrected"" using nearby gauges and a single estimate of rainfall is provided to users who need to know how much it rained. This competition will explore how to address this problem in a probabilistic manner.  Knowing the full probabilistic spread of rainfall amounts can be very useful to drive hydrological and agronomic models -- much more than a single estimate of rainfall.  Image courtesy of NOAA Unlike a conventional Doppler radar, a polarimetric radar transmits radio wave pulses that have both horizontal and vertical orientations. Because rain drops become flatter as they increase in size and because ice crystals tend to be elongated vertically, whereas liquid droplets tend to be flattened, it is possible to infer the size of rain drops and the type of hydrometeor from the differential reflectivity of the two orientations. In this competition, you are given polarimetric radar values and derived quantities at a location over the period of one hour. You will need to produce a probabilistic distribution of the hourly rain gauge total. More details are on the data page. This competition is sponsored by the Artificial Intelligence Committee of the American Meteorological Society. The Climate Corporation has kindly agreed to sponsor the prizes.";https://www.kaggle.com/c/how-much-did-it-rain;;Predict probabilistic distribution of hourly rain given polarimetric radar measurements;['tabular data', 'crps'];319;How Much Did It Rain?;Research prediction Competition
2015-12-08 00:59:00;After incorporating feedback from the Kaggle community, as well as scientific and educational partners, the Artificial Intelligence Committee of the American Meteorological Society is excited to be running a second iteration of the How Much Did It Rain? competition. How Much Did It Rain? II is focused on solving the same core rain measurement prediction problem, but approaches it with a new and improved dataset and evaluation metric. This competition will go even further towards building a useful educational tool for universities, as well as making a meaningful contribution to continued meteorological research. Competition Description Rainfall is highly variable across space and time, making it notoriously tricky to measure. Rain gauges can be an effective measurement tool for a specific location, but it is impossible to have them everywhere. In order to have widespread coverage, data from weather radars is used to estimate rainfall nationwide. Unfortunately, these predictions never exactly match the measurements taken using rain gauges. Recently, in an effort to improve their rainfall predictors, the U.S. National Weather Service upgraded their radar network to be polarimetric. These polarimetric radars are able to provide higher quality data than conventional Doppler radars because they transmit radio wave pulses with both horizontal and vertical orientations.   Dual pulses make it easier to infer the size and type of precipitation because rain drops become flatter as they increase in size, whereas ice crystals tend to be elongated vertically. In this competition, you are given snapshots of polarimetric radar values and asked to predict the hourly rain gauge total. A word of caution: many of the gauge values in the training dataset are implausible (gauges may get clogged, for example). More details are on the data page. Acknowledgements This competition is sponsored by the Artificial Intelligence Committee of the American Meteorological Society. Climate Corporation is providing the prize pool.;https://www.kaggle.com/c/how-much-did-it-rain-ii;;Predict hourly rainfall using data from polarimetric radars;['regression', 'tabular data', 'mae'];587;How Much Did It Rain? II;Research prediction Competition
2019-01-11 00:59:00;In this competition, Kagglers will develop models capable of classifying mixed patterns of proteins in microscope images. The Human Protein Atlas will use these models to build a tool integrated with their smart-microscopy system to identify a protein's location(s) from a high-throughput image. Proteins are “the doers” in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells. Images visualizing proteins in cells are commonly used for biomedical research, and these cells could hold the key for the next breakthrough in medicine. However, thanks to advances in high-throughput microscopy, these images are generated at a far greater pace than what can be manually evaluated. Therefore, the need is greater than ever for automating biomedical image analysis to accelerate the understanding of human cells and disease.   Nature Methods has indicated interest in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team would like to invite top performing teams to join as co-authors in the writing of this paper. Top performing teams will also be eligible to compete for the special prize. Additional information for both the special prize and co-authoring for Nature Methods will become available through the Discussion posts once the main competition is complete.    Acknowledgements  The Human Protein Atlas is a Sweden-based initiative aimed at mapping all human proteins in cells, tissues and organs. All the data in the knowledge resource is open access to allow anyone to pursue exploration of the human proteome. In a recent publication, the Human Protein Atlas team has demonstrated the promise of both citizen science and artificial intelligence approaches in describing the location of human proteins in images, however current results are yet to approach expert-level annotations (Sullivan et al, Nature Biotechnology, Oct 2018).;https://www.kaggle.com/c/human-protein-atlas-image-classification;Human Protein Atlas;Classify subcellular protein patterns in human cells;['classification', 'image data', 'macrofscore'];2,169;Human Protein Atlas Image Classification;Featured prediction Competition
2019-03-01 00:59:00;After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food. To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized. In this competition, you’re challenged to build an algorithm to identify individual whales in images. You’ll analyze Happywhale’s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you’ll help to open rich fields of understanding for marine mammal population dynamics around the globe. Note, this competition is similar in nature to this competition  with an expanded and updated dataset.  We'd like to thank Happywhale  for providing this data and problem. Happywhale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.;https://www.kaggle.com/c/humpback-whale-identification;Kaggle;Can you identify a whale by its tail?;['image data', 'animals', 'map@{k}'];2,129;Humpback Whale Identification;Featured prediction Competition
2013-04-15 02:00:00;In addition to $1000, the;https://www.kaggle.com/c/icdar2013-gender-prediction-from-handwriting;;Predict if a handwritten document has been produced by a male or a female writer;['logloss'];189;ICDAR2013 - Gender Prediction from Handwriting;Research prediction Competition
2013-04-21 01:59:00;IAPR/ICDAR award to be presented during ICDAR2013 that will be held in Washington, DC.;https://www.kaggle.com/c/icdar2013-stroke-recovery-from-offline-data;;Predict the trajectory of a handwritten signature;['rmse'];40;ICDAR2013 - Handwriting Stroke Recovery from Offline Data;Research prediction Competition
2015-08-25 01:59:00;Imagine you're planning a summer holiday to Iceland: you read a travel blog on your smartphone on the subway to work, search for hotels on your laptop during lunch, browse Reykjavik restaurants on a tablet while half-watching TV after dinner, and then download a travel book to your e-reader to skim before bed.   As consumers move across devices to complete online tasks, their identity becomes fragmented. Marketers, hoping to target them with meaningful messages, recommendations, and customized experiences, aren't always able to discern when activity on different devices is tied to one user vs. many users. Given usage data and a set of fabricated non-personally-identifiable IDs, this competition tasks you with making individual user connections across a variety of digital devices. Improving marketers' ability to identify individual users as they switch between devices means you'll see relevant messages wherever you go, making it easy for you to plan the best, most fjord-filled trip ever.  Acknowledgements  The competition dataset and prize pool have been generously provided by Drawbridge in sponsorship of the ICDM 2015 conference.;https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;;Identify individual users across their digital devices;['multiclass classification', 'tabular data', 'meanfscorebeta'];338;ICDM 2015: Drawbridge Cross-Device Connections;Featured prediction Competition
2019-10-04 01:59:00;Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren’t thinking about the data science that determined your fate. Embarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. “Press 1 if you really tried to spend $500 on cheddar cheese.” While perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection,  you can get on with your chips without the hassle. IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge. In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions  and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.  If successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives. Acknowledgements:   Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions.  Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry.  Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually.  Header Photo by Tim Evans on Unsplash;https://www.kaggle.com/c/ieee-fraud-detection;IEEE Computational Intelligence Society;Can you detect fraud from customer transactions?;['binary classification', 'tabular data', 'auc'];6,381;IEEE-CIS Fraud Detection;Research prediction Competition
2020-03-27 00:59:00;Competition Description While It's pretty easy for people to identify subtle differences in photos, computers still have a ways to go. Visually similar items are tough for computers to count, like this overlapping bunch of bananas   Or consider this photo of a family of foxes camouflaged in the wild - where do the foxes end and where does the grass begins?    To solve this problem and enhance the state of the art in object detection and classification, the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) began in 2010. This year, Kaggle is excited and honored to be the new home of the official ImageNet Object Localization competition. Participants are challenged with identifying all objects within an image, so those images can then be classified and annotated.   Already, because of this competition, there’s been a 4.2× reduction in image classification error (from 28.2% to 6.7%) and a 1.7× reduction in localization error (from 42.5% to 25.3%) between 2010 and 2014 alone. Can you improve the accuracy even further?  Competition Overview The validation and test data will consist of 150,000 photographs, collected from Flickr and other search engines, hand labeled with the presence or absence of 1000 object categories. The 1000 object categories contain both internal nodes and leaf nodes of ImageNet, but do not overlap with each other.  A random subset of 50,000 of the images with labels will be released as the training set along with a list of the 1000 categories. The remaining images will be used as the test set.  The validation and test data for this competition are not contained in the ImageNet training data.;https://www.kaggle.com/c/imagenet-object-localization-challenge;ImageNet;Identify the objects in images;['image data', 'custom metric'];75;ImageNet Object Localization Challenge;Research prediction Competition
2018-05-31 01:59:00;As shoppers move online, it would be a dream come true to have products in photos classified automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, royal blue vs turquoise in color. Many of today’s general-purpose recognition machines simply cannot perceive such subtle differences between photos, yet these differences could be important for shopping decisions. Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Wish, and Malong Technologies to challenge the data science community to help push the state of the art in automatic image classification. In this competition, FGVC workshop organizers with Wish and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign attribute labels for fashion images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;;Image classification of fashion products.;['meanfscorebeta'];212;iMaterialist Challenge (Fashion) at FGVC5;Research prediction Competition
2017-07-08 01:59:00;As shoppers move online, it’d be a dream come true to have product attributes in photos detected automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine grained attribute labels may look very similar, for example, royal blue vs turquoise in color. Many of today’s general-purpose recognition machines simply can’t perceive such subtle differences between photos. Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC4 workshop. As part of this workshop, CVPR is partnering with Google to challenge the data science community to help push the state of the art in automatic image classification. In this competition, FGVC workshop organizers and Google challenge you to develop algorithms that will help with the an important step towards automatic product detection–accurately assigning attribute labels for product images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC4 workshop. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/imaterialist-challenge-FGVC2017;Google;Can you assign accurate description labels to images of apparel products?;['clothing and accessories', 'image data', 'meanbesterroratk'];28;iMaterialist Challenge at FGVC 2017;Research prediction Competition
2018-05-31 01:59:00;As shoppers move online, it’d be a dream come true to have products in photos classified automatically. But, automatic product recognition is challenging because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, ball chair vs egg chair for furniture, or dutch oven vs french oven for cookware. Many of today’s general-purpose recognition machines simply can’t perceive such subtle differences between photos, yet these differences could be important for shopping decisions. Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Malong Technologies and Wish to challenge the data science community to help push the state of the art in automatic image classification. In this competition, FGVC5 workshop organizers and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product recognition – to accurately assign category labels for furniture and home goods images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/imaterialist-challenge-furniture-2018;;Image Classification of Furniture & Home Goods.;['meanbesterroratk'];428;iMaterialist Challenge (Furniture) at FGVC5;Research prediction Competition
2019-06-11 01:59:00;Designers know what they are creating, but what, and how, do people really wear their products? What combinations of products are people using? In this competition, we challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign segmentations and attribute labels for fashion images.  Visual analysis of clothing is a topic that has received increasing attention in recent years. Being able to recognize apparel products and associated attributes from pictures could enhance the shopping experience for consumers, and increase work efficiency for fashion professionals. We present a new clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.   While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders’ needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 92 related fine-grained attributes. Secondly, a total of 50K clothing images (10K with both segmentation and fine-grained attributes, 40K with apparel instance segmentation) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation. Individuals/Teams with top submissions will be invited to present their work live at the FGVC6 workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) 2019 Checkout the iMaterialist-Fashion Competition Github repo for the specifics of the dataset.  Acknowledgments The iMat-Fashion Challenge 2019 is sponsored by Google AI, CVDF, Samasource and Fashionpedia.;https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;Fine-Grained Visual Categorization 6;Fine-grained segmentation task for fashion and apparel;['custom metric'];241;iMaterialist (Fashion) 2019 at FGVC6;Research prediction Competition
2020-05-27 01:59:00;Designers know what they are creating, but what, and how, do people really wear their products? What combinations of products are people using? In this competition, we challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign segmentations and attribute labels for fashion images.  Visual analysis of clothing is a topic that has received increasing attention in recent years. Being able to recognize apparel products and associated attributes from pictures could enhance the shopping experience for consumers, and increase work efficiency for fashion professionals. We present a clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.   While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders’ needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 294 related fine-grained attributes. Secondly, a total of 50K clothing images (with both segmentation masks and fine-grained attributes) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation. Individuals/Teams with top submissions will be invited to present their work live at the FGVC7 workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) 2020. Acknowledgments The iMat-Fashion Challenge 2020 is sponsored by Google AI, CVDF, Fashionpedia and Hearst Magazine.;https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;Fine-Grained Visual Categorization 7;Fine-grained segmentation task for fashion and apparel;['custom metric'];56;iMaterialist (Fashion) 2020 at FGVC7;Research prediction Competition
2019-06-11 00:09:06;The Metropolitan Museum of Art in New York, also known as The Met, has a diverse collection of over 1.5M objects of which over 200K have been digitized with imagery. The online cataloguing information is generated by Subject Matter Experts (SME) and includes a wide range of data. These include, but are not limited to: multiple object classifications, artist, title, period, date, medium, culture, size, provenance, geographic location, and other related museum objects within The Met’s collection. While the SME-generated annotations describe the object from an art history perspective, they can also be indirect in describing finer-grained attributes from the museum-goer’s understanding. Adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects.    About This is an FGVCx competition hosted as part of the FGVC6 workshop at CVPR 2019. View the github page for more details.  This is a Kernels-only competition. Refer to Kernels Requirements for details.;https://www.kaggle.com/c/imet-2019-fgvc6;Fine-Grained Visual Categorization 6;Recognize artwork attributes from The Metropolitan Museum of Art;['image data', 'art', 'meanfscorebeta'];521;iMet Collection 2019 - FGVC6;Research Code Competition
2020-05-29 01:59:00;The Metropolitan Museum of Art in New York, also known as The Met, has a diverse collection of over 1.5M objects of which over 200K have been digitized with imagery. Can you help find the significant attributes to identify a specific work of art? Help advance this research in this notebook competition.    The online cataloguing information is generated by subject matter experts and includes a wide range of data. These include, but are not limited to: multiple object classifications, artist, title, period, date, medium, culture, size, provenance, geographic location, and other related museum objects within The Met’s collection. While the annotations describe the object from an art history perspective, they can also be indirect in describing finer-grained attributes for the museum-goer’s understanding. Adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects.  This is a Code Competition. Refer to Code Requirements for details.  About This is an FGVCx competition hosted as part of the FGVC7 workshop at CVPR 2020.;https://www.kaggle.com/c/imet-2020-fgvc7;Fine-Grained Visual Categorization 7;Recognize artwork attributes from The Metropolitan Museum of Art;['meanfscorebeta'];96;iMet Collection 2020 - FGVC7;Research Code Competition
2018-06-05 01:59:00;As part of the FGVC5 workshop at CVPR 2018 we are conducting the iNat Challenge 2018 large scale species classification competition. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features a large number of fine-grained categories with high class imbalance. The iNat Challenge 2018 dataset contains over 8,000 species, with a combined training and validation set of 450,000 images that have been collected and verified by multiple users from iNaturalist. The dataset features many visually similar species, captured in a wide variety of situations, from all over the world. Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC5 workshop.  The iNat Challenge 2018 is sponsored by Microsoft.;https://www.kaggle.com/c/inaturalist-2018;;Long tailed classification challenge spanning 8,000 species.;['meanbesterroratk'];59;iNaturalist Challenge at FGVC5;Research prediction Competition
2019-06-11 01:59:00;As part of the FGVC6 workshop at CVPR 2019 we are conducting the iNat Challenge 2019 large scale species classification competition,  sponsored by Microsoft. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features a large number of fine-grained categories. Previous versions of the challenge have focused on classifying large numbers of species. This year features a smaller number of highly similar categories captured in a wide variety of situations, from all over the world. In total, the iNat Challenge 2019 dataset contains 1,010 species, with a combined training and validation set of 268,243 images that have been collected and verified by multiple users from iNaturalist. Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC6 workshop.  Participants who make a submission that beats the sample submission can fill out this form to receive $150 in Google Cloud credits.   Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/inaturalist-2019-fgvc6;Fine-Grained Visual Categorization 6;Fine-grained classification spanning a thousand species;['meanbesterroratk'];214;iNaturalist 2019 at FGVC6;Research prediction Competition
2017-07-08 01:59:00;With so much diversity, accurately classifying animals and plants is a tough challenge. Check out the photos below. Alpaca or Llama? Donkey or mule? Roses or kale?  It’s estimated that our planet contains several million species of plants and animals–many that look really similar to each other. Because of this, a lot of species in the natural world are too hard to classify without an expert. As part of the FGVC4 workshop at CVPR 2017 we are conducting the iNat Challenge 2017 large scale species classification competition, sponsored by Google. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features fine-grained categories, big class imbalances, and large numbers of classes. The iNat Challenge 2017 dataset contains 5,089 species, with a combined training and validation set of 675,000 images that have been collected and verified by multiple users from inaturalist.org. The dataset features many visually similar species, captured in a wide variety of situations, from all over the world.  Example images, along with their unique GBIF ID numbers (where available), can be viewed here. Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC4 workshop.  Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/inaturalist-challenge-at-fgvc-2017;Google;Fine-grained classification challenge spanning 5,000 species.;['image data', 'animals', 'plants', 'meanbesterroratk'];50;iNaturalist Challenge at FGVC 2017;Research prediction Competition
2018-11-13 00:59:00;"Making products that work for people all over the globe is an important value at Google AI. In the field of classification, this means developing models that work well for regions all over the world.  Today, the dataset a model is trained on greatly dictates the performance of that model. A system trained on a dataset that doesn’t represent a broad range of localities could perform worse on images drawn from geographic regions underrepresented in the training data. Google and the industry at large are working to create more diverse & representative datasets. But it is also important for the field to make progress in understanding how to build models when the data available may not cover all audiences a model is meant to reach. Google AI is challenging Kagglers to develop models that are robust to blind spots that might exist in a data set, and to create image recognition systems that can perform well on test images drawn from different geographic distributions than the ones they were trained on. By finding ways to teach image classifiers to generalize to new geographic and cultural contexts, we hope the community will make even more progress in inclusive machine learning that benefits everyone, everywhere. Note: This competition is run in two stages. Refer to the FAQ for an explanation of how this works & the Timeline for specific dates.  This competition is a part of the NIPS 2018 competition track. Winners will be invited to attend and present their solutions at the workshop.     Shankar et al. ""No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World"" NIPS 2017 Workshop on Machine Learning for the Developing World";https://www.kaggle.com/c/inclusive-images-challenge;Google Research;Stress test image classifiers across new geographic distributions;['image data', 'multiclass classification', 'meanfscorebeta'];468;Inclusive Images Challenge;Research prediction Competition
2010-10-10 04:28:00;Traders, analysts, investors and hedge funds are always looking for techniques to     better predict stock price movements. The 2010 INFORMS Data Mining Contest takes     aim at this goal, requiring participants to build models that predict the movement     of stock prices over the next 60 minutes.       Knowing whether a stock will increase or decrease allows traders to make better     investment decisions. Moreover, good predictive models allow traders to better understand     what drives stock prices, supporting better risk management. The results of this     contest could have a big impact on the finance industry.;https://www.kaggle.com/c/informs2010;;The goal of this contest is to predict short term movements in stock prices. The winners of this contest will be honoured of the INFORMS Annual Meeting in Austin-Texas (November 7-10).;['auc'];145;INFORMS Data Mining Contest 2010;Featured prediction Competition
2015-02-25 00:59:00;"This BCI Challenge is being proposed as part of the IEEE Neural Engineering Conference (NER2015). Participation is open without restriction, and the winners will be selected among participants who have submitted an abstract to the conference (see rules). Problem Description As humans think, we produce brain waves. These brain waves can be mapped to actual intentions. In this competition, you are given the brain wave data of people with the goal of spelling a word by only paying attention to visual stimuli. The goal of the competition is to detect errors during the spelling task, given the subject's brain waves.  The Setup The “P300-Speller” is a well-known brain-computer interface (BCI) paradigm which uses Electroencephalography (EEG) and the so-called P300 response evoked by rare and attended stimuli in order to select items displayed on a computer screen. In this experiment, each subject was presented with letters and numbers (36 possible items displayed on a matrix) to spell words. Each item of a word is selected one at a time, by flashing screen items in group and in random order. The selected item is the one for which the online algorithm could most likely recognize the typical target response. The goal of this challenge is to determine when the selected item is not the correct one by analyzing the brain signals after the subject received feedback. Experimental Design For each participant, a prototypical target response was learned from a short calibration session prior to the test sessions. In test sessions, the spelling performance is highly dependent upon the subject’s attentional effort towards the target item and his/her simultaneous effort to ignore the flashes of the irrelevant items. Since subjects' attention might fluctuate, performance does too (e.g. over time, with fatigue). Two copy-spelling conditions were used, corresponding to short and long trials, respectively:  A fast mode, more error-prone condition (each item was flashed 4 times); A slower, less error-prone one (each item was flashed 8 times).  At each trial, after the last flash, the subject was instructed to keep looking at the screen and wait for the feedback. The feedback consisted in the selected item, displayed in the middle of the screen in large font. Even if the feedback was incorrect, the subject was asked to then look at the next target.  Download sample video Twenty-six healthy subjects took part in this study (13 male, mean age = 28.8±5.4 (SD), range 20-37). All subjects reported normal or corrected-to-normal vision and had no previous experience with the P300-Speller paradigm or any other BCI application. Subject’s brain activity was recorded with 56 passive Ag/AgCl EEG sensors (VSM-CTF compatible system) whose placement followed the extended 10-20 system. Their signals were sampled at 600 Hz and were all referenced to the nose. The ground electrode was placed on the shoulder and impedances were kept below 10 kΩ. The subjects had to go through five copy spelling sessions. Each session consisted of twelve 5-letter words, except the fifth which consisted of twenty 5-letter words. Objective In this paradigm and BCI in general, at least in situations where a discrete feedback can be presented to the user, the EEG evoked response to the feedback can be recorded and processed online in order to evaluate whether the item selection was correct or not. This decision, if reliable, could then be used to improve the BCI performance by implementing some error correction strategy. One possible strategy for online error detection and correction has been proposed in Perrin et al. 2012. Most of the data for this competition come from this study and this paper should be cited whenever the competition data will be used and results reported. In this competition, participants are asked to submit an Error Potential detection algorithm, capable of detecting the erroneous feedbacks online and to generalize across subjects (transfer learning). Perrin, M., Maby, E., Daligault, S., Bertrand, O., & Mattout, J. Objective and subjective evaluation of online error correction during P300-based spelling. Advances in Human-Computer Interaction, 2012, 4. (link) Acknowledgements  This competition is brought to you by";https://www.kaggle.com/c/inria-bci-challenge;;A spell on you if you cannot detect errors!;['auc'];260;BCI Challenge @ NER 2015;Research prediction Competition
2017-08-15 01:59:00;Whether you shop from meticulously planned grocery lists or let whimsy guide your grazing, our unique food rituals define who we are. Instacart, a grocery ordering and delivery app, aims to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you. Instacart’s data science team plays a big part in providing this delightful shopping experience. Currently they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open sourced this data - see their blog post on 3 Million Instacart Orders, Open Sourced. In this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user’s next order. They’re not only looking for the best model, Instacart’s also looking for machine learning engineers to grow their team. Winners of this competition will receive both a cash prize and a fast track through the recruiting process. For more information about exciting opportunities at Instacart, check out their careers page here or e-mail their recruiting team directly at ml.jobs@instacart.com.;https://www.kaggle.com/c/instacart-market-basket-analysis;Instacart;Which products will an Instacart consumer purchase again?;['food', 'meanfscore'];2,622;Instacart Market Basket Analysis;Featured prediction Competition
2019-06-21 01:59:00;"Welcome to Instant (well, almost) Gratification! In 2015, Kaggle introduced Kernels as a resource to competition participants. It was a controversial decision to add a code-sharing tool to a competitive coding space. We thought it was important to make Kaggle more than a place where competitions are solved behind closed digital doors. Since then, Kernels has grown from its infancy--essentially a blinking cursor in a docker container--into its teenage years. We now have more compute, longer runtimes, better datasets, GPUs, and an improved interface. We have iterated and tested several Kernels-only (KO) competition formats with a true holdout test set, in particular deploying them when we would have otherwise substituted a two-stage competition. However, the experience of submitting to a Kernels-only competition has typically been asynchronous and imperfect; participants wait many days after a competition has concluded for their selected Kernels to be rerun on the holdout test dataset, the leaderboard updated, and the winners announced. This flow causes heartbreak to participants whose Kernels fail on the unseen test set, leaving them with no way to correct tiny errors that spoil months of hard work. Say Hello to Synchronous KO We're now pleased to announce general support for a synchronous Kernels-only format. When you submit from a Kernel, Kaggle will run the code against both the public test set and private test set in real time. This small-but-substantial tweak improves the experience for participants, the host, and Kaggle:  With a truly withheld test set, we are practicing proper, rigorous machine learning. We will be able to offer more varieties of competitions and intend to run many fewer confusing two-stage competitions. You will be able to see if your code runs successfully on the withheld test set and have the leeway to intervene if it fails. We will run all submissions against the private data, not just selected ones. Participants will get the complete and familiar public/private scores available in a traditional competition. The final leaderboard can be released at the end of the competition, without the delay of rerunning Kernels.  This competition is a low-stakes, trial-run introduction to our new synchronous KO implementation. We want to test that the process goes smoothly and gather feedback on your experiences. While it may feel like a normal KO competition, there are complicated new mechanics in play, such as the selection logic of Kernels that are still running when the deadline passes. Since the competition also presents an authentic machine learning problem, it will also award Kaggle medals and points. Have fun, good luck, and welcome to the world of synchronous Kernels competitions!";https://www.kaggle.com/c/instant-gratification;Kaggle;A synchronous Kernels-only competition;['binary classification', 'tabular data', 'auc'];1,832;Instant Gratification;Featured Code Competition
2016-11-01 00:59:00;7. You read that correctly. That's the start to a real integer sequence, the powers of primes. Want something easier? How about the next number in 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55? If you answered 89, you may enjoy this challenge. Your computer may find it considerably less enjoyable. The On-Line Encyclopedia of Integer Sequences is a 50+ year effort by mathematicians the world over to catalog sequences of integers. If it has a pattern, it's probably in the OEIS, and probably described with amazing detail. This competition challenges you create a machine learning algorithm capable of guessing the next number in an integer sequence. While this sounds like pattern recognition in its most basic form, a quick look at the data will convince you this is anything but basic! Acknowledgments Kaggle is hosting this competition for the data science community to use for fun and education. We thank the OEIS and its contributors for cataloging this data.;https://www.kaggle.com/c/integer-sequence-learning;Kaggle;1, 2, 3, 4, 5, 7?!;['tabular data', 'categorizationaccuracy'];284;Integer Sequence Learning;Playground prediction Competition
2017-06-22 01:59:00;Cervical cancer is so easy to prevent if caught in its pre-cancerous stage that every woman should have access to effective, life-saving treatment no matter where they live. Today, women worldwide in low-resource settings are benefiting from programs where cancer is identified and treated in a single visit. However, due in part to lacking expertise in the field, one of the greatest challenges of these cervical cancer screen and treat programs is determining the appropriate method of treatment which can vary depending on patients’ physiological differences.  Especially in rural parts of the world, many women at high risk for cervical cancer are receiving treatment that will not work for them due to the position of their cervix. This is a tragedy: health providers are able to identify high risk patients, but may not have the skills to reliably discern which treatment which will prevent cancer in these women. Even worse, applying the wrong treatment has a high cost. A treatment which works effectively for one woman may obscure future cancerous growth in another woman, greatly increasing health risks. Currently, MobileODT offers a Quality Assurance workflow to support remote supervision which helps healthcare providers make better treatment decisions in rural settings. However, their workflow would be greatly improved given the ability to make real-time determinations about patients’ treatment eligibility based on cervix type. In this competition, Intel is partnering with MobileODT to challenge Kagglers to develop an algorithm which accurately identifies a woman’s cervix type based on images. Doing so will prevent ineffectual treatments and allow healthcare providers to give proper referral for cases that require more advanced treatment.  Competition Partner MobileODT has developed and sells the Enhanced Visual Assessment (EVA) System, a digital toolkit for health care workers of every level to provide expert services to patients, anchored at the point-of-care by an FDA-approved, intelligent, mobile-phone based medical device. Combining the algorithmic power of biomedical optics with the computational capabilities and connectivity of mobile phones, MobileODT's connected, intelligent medical systems can be used everywhere, under nearly any conditions. MobileODT's first product, the FDA approved EVA System for colposcopy, is in use by health providers in 31 hospital systems across the US, and in 22 countries, to better screen and treat women for cervical cancer and to conduct forensic colposcopy.;https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;Intel;Which cancer treatment will be most effective?;['image data', 'multiclass classification', 'healthcare', 'multiclassloss'];848;Intel & MobileODT Cervical Cancer Screening;Featured prediction Competition
2015-09-29 01:59:00;Kaggle is first and foremost a community. If you're here to learn, it is from the community. If you're here to compete, it's against the community. If you're here to share, it's with our community. Continuing the theme, we've just launched a product called Scripts. The concept is simple: we'll host, version, & run your code for you. Why? The community does a tremendous amount of data science tinkering each day on Kaggle. The fruits of this labor are too often lost on old drives, never checked into version control, never shared, never critiqued, never reproduced, never visualized, never assimilated into a broader knowledge. Scripts is not a better IDE or better database or new language. It is the beginning of communal input, collaboration, and useful workflow features for the data science that's done on Kaggle. You don't need a full tutorial to learn how to use scripts (it has one button and runs code). In lieu of yet another iris dataset tutorial, we invite you to kick the tires and try it out. Punish our servers by setting ntree = 500 in that random forest call. You deserve it. To add to the experience, we're giving out a truckload of extremely prestigious prizes. The details on Scripts:   Currently supports Python, R, and Julia, with more on the way  Has all the cool data science packages installed. Just import them.  Can run for 20 minutes and use 512 MB of disk space  Can't access the internet (sorry, spammers and botnets)  Are open source and public by default  Can be forked  Can directly submit to competitions  Can be hidden  Are part of competitions, but will soon take on a life of their own   Scripts Showcase - Intros to dplyr  Our friend and data science educator Kevin Markham (the man behind the dataschool.io and the scikit-learn video series) kicks off the scripts action with two introductions to Hadley Wickham's dplyr package for data manipulation in R. Part 1 covers the basics, while part 2 covers useful new features in versions 0.3 and 0.4. If you're still using plyr, this is a great chance to jump to its faster, sleeker, more consistent cousin. Introduction to dplyr (Part 1) Introduction to dplyr (Part 2)  You can view site-wide scripts at https://www.kaggle.com/scripts. We're also in the habit of naming naming our favorite scripts of the week on the blog.;https://www.kaggle.com/c/introducing-kaggle-scripts;Kaggle;Your code deserves better;['rmse'];;Introducing Kaggle Scripts;Playground prediction Competition
2017-08-16 01:59:00;Tangles of kudzu overwhelm trees in Georgia while cane toads threaten habitats in over a dozen countries worldwide. These are just two invasive species of many which can have damaging effects on the environment, the economy, and even human health. Despite widespread impact, efforts to track the location and spread of invasive species are so costly that they’re difficult to undertake at scale. Currently, ecosystem and plant distribution monitoring depends on expert knowledge. Trained scientists visit designated areas and take note of the species inhabiting them. Using such a highly qualified workforce is expensive, time inefficient, and insufficient since humans cannot cover large areas when sampling.  Because scientists cannot sample a large quantity of areas, some machine learning algorithms are used in order to predict the presence or absence of invasive species in areas that have not been sampled. The accuracy of this approach is far from optimal, but still contributes to approaches to solving ecological problems. In this playground competition, Kagglers are challenged to develop algorithms to more accurately identify whether images of forests and foliage contain invasive hydrangea or not. Techniques from computer vision alongside other current technologies like aerial imaging can make invasive species monitoring cheaper, faster, and more reliable. Acknowledgments Data providers: Christian Requena Mesa, Thore Engel, Amrita Menon, Emma Bradley.;https://www.kaggle.com/c/invasive-species-monitoring;;Identify images of invasive hydrangea;['image data', 'plants', 'auc'];512;Invasive Species Monitoring;Playground prediction Competition
2019-06-08 01:59:00;Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to nearby areas we are faced with an interesting probem: how do you classify a species in a new region that you may not have seen in previous training data? In order to tackle this problem, we have prepared a challenge where the training data and test data are from different regions, namely The American Southwest and the American Northwest. The species seen in each region overlap, but are not identical, and the challenge is to classify the test species correctly. To this end, we will allow training on our American Southwest data (from CaltechCameraTraps), on iNaturalist 2017/2018 data, and on simulated data generated from Microsoft AirSim. We have provided a taxonomy file mapping our classes into the iNat taxonomy. This is an FGVCx competition as part of the FGVC6 workshop at CVPR 2019, and is sponsored by Microsoft AI for Earth.  There is a github page for the competition here. Please open an issue if you have questions or problems with the dataset. If you use this dataset in publication, please cite: @article{beery2019iwildcam,  title={The iWildCam 2019 Challenge Dataset},  author={Beery, Sara and Morris, Dan and Perona, Pietro},  journal={arXiv preprint arXiv:1907.07617},  year={2019} }   Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/iwildcam-2019-fgvc6;Fine-Grained Visual Categorization 6;Categorize animals in the wild;['image data', 'multiclass classification', 'macrofscore'];336;iWildCam 2019 - FGVC6;Playground prediction Competition
2020-05-27 01:59:00;Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automatic species classification in camera trap images. However, as we try to expand the scope of these models we are faced with an interesting problem: how do we train models that perform well on new (unseen during training) camera trap locations? Can we leverage data from other modalities, such as citizen science data and remote sensing data?    In order to tackle this problem, we have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to classify species in the test cameras correctly. To explore multimodal solutions, we allow competitors to train on the following data: (i) our camera trap training set (data provided by WCS), (ii) iNaturalist 2017-2019 data, and (iii) multispectral imagery (from Landsat 8) for each of the camera trap locations. On the competition GitHub page we provide the multispectral data, a taxonomy file mapping our classes into the iNat taxonomy, a subset of iNat data mapped into our class set, and a camera trap detection model (the MegaDetector) along with the corresponding detections. If you use this dataset in publication, please cite: @article{beery2020iwildcam,     title={The iWildCam 2020 Competition Dataset},     author={Beery, Sara and Cole, Elijah and Gjoka, Arvi},     journal={arXiv preprint arXiv:2004.10340},     year={2020} }  This is an FGVCx competition as part of the FGVC7 workshop at CVPR 2020, and is sponsored by Microsoft AI for Earth and Wildlife Insights. There is a GitHub page for the competition here. Please open an issue if you have questions or problems with the dataset.    You can find the iWildCam 2018 Competition here, and the iWildCam 2019 Competition here. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/iwildcam-2020-fgvc7;Fine-Grained Visual Categorization 7;Categorize animals in the wild;['biology', 'multiclass classification', 'categorizationaccuracy'];121;iWildCam 2020 - FGVC7;Research prediction Competition
2020-06-23 01:59:00;"It only takes one toxic comment to sour an online discussion. The Conversation AI team, a research initiative founded by Jigsaw and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. If these toxic contributions can be identified, we could have a safer, more collaborative internet. In the previous 2018 Toxic Comment Classification Challenge, Kagglers built multi-headed models to recognize toxicity and several subtypes of toxicity. In 2019, in the Unintended Bias in Toxicity Classification Challenge, you worked to build toxicity models that operate fairly across a diverse range of conversations. This year, we're taking advantage of Kaggle's new TPU support and challenging you to build multilingual models with English-only training data. Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results ""translate"" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages.  As our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.  To get started with TPUs:  Read the TPU documentation one-pager Then jump right into the Getting Started Notebooks for this competition  Quick note: a TPU is a network-connected accelerator and requires a couple extra lines in your code. Flipping the TPU switch in your notebook will not, by itself, accelerate your code.";https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification;Jigsaw/Conversation AI;Use TPUs to identify toxicity comments across multiple languages;['tpu', 'text data', 'languages', 'auc'];1,621;Jigsaw Multilingual Toxic Comment Classification;Featured Code Competition
2018-03-21 00:59:00;Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments. The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content). In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful. Disclaimer: the dataset for this competition contains text that may be considered profane, vulgar, or offensive.;https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;Jigsaw/Conversation AI;Identify and classify toxic online comments;['text data', 'mcauc'];4,550;Toxic Comment Classification Challenge;Featured prediction Competition
2019-07-18 21:35:00;"Can you help detect toxic comments ― and minimize unintended model bias? That's your challenge in this competition. The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.  Last year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.  Here’s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. ""gay""), even when those comments were not actually toxic (such as ""I am a gay woman""). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users. In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive. Acknowledgments The Conversation AI team would like to thank Civil Comments for making this dataset available publicly and the Online Hate Index Research Project at D-Lab, University of California, Berkeley, whose labeling survey/instrument informed the dataset labeling. We'd also like to thank everyone who has contributed to Conversation AI's research, especially those who took part in our last competition, the success of which led to the creation of this challenge.    This is a Kernels-only competition. Refer to Kernels Requirements for details.";https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;Jigsaw/Conversation AI;Detect toxicity across a diverse range of conversations;['nlp', 'text data', 'custom metric'];3,165;Jigsaw Unintended Bias in Toxicity Classification;Featured Code Competition
2012-10-07 02:00:00;This competition has completed. Congratulations to the winners along with all the other participants!  CareerBuilder.com is proud to sponsor the Job Recommendation Engine Challenge, which asks you to predict what jobs its users will apply to based on their previous applications, demographic information, and work history. The insights you discover from this data, and the algorithms the winners create, will allow CareerBuilder to improve its job recommendation algorithm, a core part of its website and a key element in improving user experience.     There will also be a data visualization prospect towards the end of this contest.;https://www.kaggle.com/c/job-recommendation;;Predict which jobs users will apply to;['custom metric'];81;Job Recommendation Challenge;Prospect prediction Competition
2013-04-04 01:59:00;"Kaggle Startup Programplease apply      Successful models will incorporate some analysis of the impact of including different keywords or phrases, as well as making use of the structured data fields like location, hours or company.  Some of the structured data shown (such as category) is 'inferred'  by Adzuna's own processes, based on where an ad came from or its contents, and may not be ""correct"" but is representative of the real data. You will be provided with a training data set on which to build your model, which will include all variables including salary.  A second data set will be used to provide feedback on the public leaderboard.  After approximately 6 weeks, Kaggle will release  a final data set that does not include the salary field to participants, who will then be required to submit their salary predictions against each job for evaluation.";https://www.kaggle.com/c/job-salary-prediction;Adzuna;Predict the salary of any UK job ad based on its contents;['mae'];289;Job Salary Prediction;Featured prediction Competition
2013-02-26 21:30:00;"Two of Kaggle's very own are presenting an introductory tutorial at Strata 2013. Targeted for those with basic programming experience, it will cover the end-to-end analysis of predictive data problems. The tutorial is comprised of four sections, the last of which is this, a hands-on Kaggle competition in which participants can experience firsthand the joys of creating a model and the sorrows of overfitting. Competition ends: 9:00am Tuesday, 02/26/2013 Location: Ballroom AB  Competition Starts: Approximately 11:15 AM PT (2:15PM ET), 02/26/2013 Competition Ends: 12:30 PM PT (3:30 PM ET), 02/26/2013 Open to the public! Yes, you can participate in this for-fun competition without attending the tutorial.   For fun? You heard correctly; there's no Kaggle points or money up for grabs here. Isn't getting to lunch early a big enough motivation?  Unlimited* submissions! Show our servers who's boss! *for small values of unlimited Where's the data? To prevent head starts, the data will be available at the start of the competition. About the presenters Ben Hamner has worked with machine learning problems in a variety of different domains, including natural language processing, computer vision, web classification, and neuroscience. Prior to joining Kaggle, he applied machine learning to improve brain-computer interfaces as a Whitaker Fellow at the École Polytechnique Fédérale de Lausanne in Lausanne, Switzerland. He graduated with BSE in Biomedical Engineering, Electrical Engineering, and Math from Duke University. William Cukierski has a bachelor’s degree in physics from Cornell University and a Ph.D. in biomedical engineering from Rutgers University, where he studied applications of machine learning in cancer research.";https://www.kaggle.com/c/just-the-basics-strata-2013;Kaggle;Live from Santa Clara, CA - Core Data Science Skills with Kaggle’s Top Competitors;['auc'];49;Just the Basics - Strata 2013;Getting Started prediction Competition
2013-03-01 02:00:00;Missed the one hour  Just the Basics tutorial competition? Didn't get to implement that method you had in mind? Too many coffee breaks has your brain in Beautiful Mind mode? This is the after-party competition. Same data. Same problem. More time! You have until the close of Strata to have fun with the problem. Competition Starts: approximately 12:30 PM PT (3:30 PM ET), 02/26/2013 Competition Ends: 5:00 PM PT (8:00 PM ET), 02/28/2013;https://www.kaggle.com/c/just-the-basics-the-after-party;Kaggle;Live from Santa Clara, CA;['auc'];48;Just the Basics - Strata 2013 After-party;Getting Started prediction Competition
2019-12-03 00:59:00;"Overview Welcome to Kaggle's third annual Machine Learning and Data Science Survey ― and our second-ever survey data challenge.  You can read our executive summary here. This year, as in 2017 and 2018, we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live for three weeks in October, and after cleaning the data we finished with 19,717 responses! There's a lot to explore here. The results include raw numbers about who is working with data, what’s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published the data in as raw a format as possible without compromising anonymization, which makes it an unusual example of a survey dataset.  Challenge This year Kaggle is launching the second annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community. In our third year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey. For that reason, we’re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world.   The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A “story” could be defined any number of ways, and that’s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!  Submissions will be evaluated on the following:   Composition - Is there a clear narrative thread to the story that’s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.  Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.   Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible   To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid.   How to Participate To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will review the last (most recent) entry.  No submission is necessary for the Weekly Notebook Award. To be eligible, a notebook must be public and use the 2019 Data Science Survey as a data source.  Submission deadline: 11:59PM UTC, December 2nd, 2019.  Survey Methodology  This survey received 19,717 usable respondents from 171 countries and territories. If a country or territory received less than 50 respondents, we grouped them into a group named “Other” for anonymity. We excluded respondents who were flagged by our survey system as “Spam”. Most of our respondents were found primarily through Kaggle channels, like our email list, discussion forums and social media channels. The survey was live from October 8th to October 28th. We allowed respondents to complete the survey at any time during that window.  The median response time for those who participated in the survey was approximately 10 minutes. Not every question was shown to every respondent. You can learn more about the different segments we used in the survey_schema.csv file.  In general, respondents with more experience were asked more questions and respondents with less experience were asked less questions. To protect the respondents’ identity, the answers to multiple choice questions have been separated into a separate data file from the open-ended responses. We do not provide a key to match up the multiple choice and free form responses. Further, the free form responses have been randomized column-wise such that the responses that appear on the same row did not necessarily come from the same survey-taker. Multiple choice single response questions fit into individual columns whereas multiple choice multiple response questions were split into multiple columns. Text responses were encoded to protect user privacy and countries with fewer than 50 respondents were grouped into the category ""other"".  Data has been released under a CC 2.0 license: https://creativecommons.org/licenses/by/2.0/";https://www.kaggle.com/c/kaggle-survey-2019;;The most comprehensive dataset available on the state of ML and data science;['jobs and career', 'survey analysis'];;2019 Kaggle ML & DS Survey;Analytics  Competition
2019-12-18 00:59:00;"Bored of MNIST? The goal of this competition is to provide a simple extension to the classic MNIST competition we're all familiar with. Instead of using Arabic numerals, it uses a recently-released dataset of Kannada digits. Kannada is a language spoken predominantly by people of Karnataka in southwestern India. The language has roughly 45 million native speakers and is written using the Kannada script. Wikipedia  This competition uses the same format as the MNIST competition in terms of how the data is structured, but it's different in that it is a synchronous re-run Kernels competition. You write your code in a Kaggle Notebook, and when you submit the results, your code is scored on both the public test set, as well as a private (unseen) test set. Technical Information All details of the dataset curation has been captured in the paper titled: Prabhu, Vinay Uday. ""Kannada-MNIST: A new handwritten digits dataset for the Kannada language."" arXiv preprint arXiv:1908.01242 (2019) The github repo of the author can be found here. On the originally-posted dataset, the author suggests some interesting questions you may be interested in exploring. Please note, although this dataset has been released in full, the purpose of this competition is for practice, not to find the labels to submit a perfect score. In addition to the main dataset, the author also disseminated an additional real world handwritten dataset (with 10k images), termed as the 'Dig-MNIST dataset' that can serve as an out-of-domain test dataset. It was created with the help of volunteers that were non-native users of the language, authored on a smaller sheet and scanned with different scanner settings compared to the main dataset. This 'dig-MNIST' dataset serves as a more difficult test-set (An accuracy of 76.1% was reported in the paper cited above) and achieving ~98+% accuracy on this test dataset would be rather commendable. Acknowledgments Kaggle thanks Vinay Prabhu for providing this interesting dataset for a Playground competition. Image reference: https://www.researchgate.net/figure/speech-for-Kannada-numbers_fig2_313113588";https://www.kaggle.com/c/Kannada-MNIST;Kaggle;MNIST like datatset for Kannada handwritten digits;['image data', 'computer vision', 'categorizationaccuracy'];1,214;Kannada MNIST;Playground Code Competition
2013-06-13 01:59:00;The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom. Microsoft Academic Search is an open platform that provides a variety of metrics and experiences for the research community, in addition to literature search. It covers more than 50 million publications and over 19 million authors across a variety of domains, with updates added each week. One of the main challenges of providing this service is caused by author-name ambiguity. This KDD Cup task challenges participants to determine which authors in a given data set are duplicates.;https://www.kaggle.com/c/kdd-cup-2013-author-disambiguation;;Identify which authors correspond to the same person;['meanfscore'];237;KDD Cup 2013 - Author Disambiguation Challenge (Track 2);Featured prediction Competition
2013-06-27 01:59:00;THIS COMPETITION IS COMPLETE. CONGRATULATIONS TO THE PRELIMINARY WINNERS!  The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom. Microsoft Academic Search is an open platform that provides a variety of metrics and experiences for the research community, in addition to literature search. It covers more than 50 million publications and over 19 million authors across a variety of domains, with updates added each week. One of the main challenges of providing this service is caused by author-name ambiguity. On one hand, there are many authors who publish under several variations of their own name.  On the other hand, different authors might share a similar or even the same name. As a result, the profile of an author with an ambiguous name tends to contain noise, resulting in papers that are incorrectly assigned to him or her. This KDD Cup task challenges participants to determine which papers in an author profile were truly written by a given author.;https://www.kaggle.com/c/kdd-cup-2013-author-paper-identification-challenge;;Determine whether an author has written a given paper;['custom metric'];552;KDD Cup 2013 - Author-Paper Identification Challenge (Track 1);Featured prediction Competition
2014-07-16 01:59:00;DonorsChoose.org is an online charity that makes it easy to help students in need through school donations. At any time, thousands of teachers in K-12 schools propose projects requesting materials to enhance the education of their students. When a project reaches its funding goal, they ship the materials to the school. The 2014 KDD Cup asks participants to help DonorsChoose.org identify projects that are exceptionally exciting to the business, at the time of posting. While all projects on the site fulfill some kind of need, certain projects have a quality above and beyond what is typical. By identifying and recommending such projects early, they will improve funding outcomes, better the user experience, and help more students receive the materials they need to learn. Successful predictions may require a broad range of analytical skills, from natural language processing on the need statements to data mining and classical supervised learning on the descriptive factors around each project. About KDD KDD 2014 is a premier interdisciplinary conference that brings together researchers and practitioners from all aspects of data science, data mining, knowledge discovery, large-scale data analytics, and big data. This year's KDD features 4 keynotes, 151 Research Track papers, 44 Industry & Government Track papers, 24 workshops, 12 tutorials, and more. Acknowledgements Data and logistical support has been graciously provided by DonorsChoose.org. DonorsChoose.org is an online charity and 501(c)(3) nonprofit organization that makes it easy for anyone to help students in need. Public school teachers from every corner of America post classroom project requests, and donors can give any amount to the project that most inspires them.;https://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose;;Predict funding requests that deserve an A+;['auc'];472;KDD Cup 2014 - Predicting Excitement at DonorsChoose.org;Research prediction Competition
2012-06-02 01:59:59;"BACKGROUND Online social networking services have become tremendously popular in recent years, with popular social networking sites like Facebook, Twitter, and Tencent Weibo adding thousands of enthusiastic new users each day to their existing billions  of actively engaged users. Since its launch in April 2010, Tencent Weibo, one of the largest micro-blogging websites in China, has become a major platform for building friendship and sharing interests online. Currently, there are more than 200 million registered  users on Tencent Weibo, generating over 40 million messages each day. This scale benefits the Tencent Weibo users but it can also flood users with huge volumes of information and hence puts them at risk of information overload. Reducing the risk of information  overload is a priority for improving the user experience and it also presents opportunities for novel data mining solutions. Thus, capturing users’ interests and accordingly serving them with potentially interesting items (e.g. news, games, advertisements,  products), is a fundamental and crucial feature social networking websites like Tencent Weibo.  TASK 1 DESCRIPTION  The prediction task involves predicting whether or not a user will follow an item that has been recommended to the user. Items can be persons, organizations, or groups and will be defined more thoroughly below.  DATASETS First, we define some notations as follows: “Item”: An item is a specific user in Tencent Weibo, which can be a person, an organization, or a group, that was selected and recommended to other users. Typically, celebrities, famous organizations, or some well-known groups were selected  to form the ‘items set’ for recommendation. The size of this is about 6K items in the dataset.  Items are organized in categories; each category belongs to another category, and all together they form a hierarchy. For example, an item, a vip user Dr. Kaifu LEE, vip user: http://t.qq.com/kaifulee (wikipedia: http://en.wikipedia.org/wiki/Kai-Fu_Lee) represented as  science-and-technology.internet.mobile  We can see that categories in different levels are separated by a dot ‘.’, and the category information about an item can help enhance your model prediction. For example, if a user Peter follows kaifulee, he may be interested in the other items of the category  that kaifulee belongs to, and might also be interested in the items of the parent category of kaifulee’s category. “Tweet”: a “tweet” is the action of a user posting a message to the microblog system, or the posted message itself. So when one user is “tweeting“, his/her followers will see the “tweet”. “Retweet”: a user can repost a tweet and append some comments (or do nothing), to share it with more people (my followers). “Comment”: a user can add some comments to a tweet. The contents of the comments  will not be automatically pushed to his/her followers as ‘tweeting’ or ‘retweeting’,but will appear at the ‘comment history’ of the commented tweet. “Followee/follower”: If User B is followed by User A, B is a followee to A, and A is a follower to B. We describe the datasets as follows: The dataset represents a sampled snapshot of Tencent Weibo users’ preferences for various items –– the recommendation of items to users and the history of users’ ‘following’ history. It is of a larger scale compared to other publicly available  datasets ever released. Also it provides richer information in multiple domains such as user profiles, social graph, item category, which may hopefully evoke deeply thoughtful ideas and methodology. The users in the dataset, numbered in millions, are provided with rich information (demographics, profile keywords, follow history, etc.) for generating a good prediction model. To protect the privacy of the users, the IDs of both the users  and the recommended items are anonymized as random numbers such that no identification is revealed. Furthermore, their information, when in Chinese, will be encoded as random strings or numbers, thus no contestant who understands Chinese would get advantages.  Timestamps for recommendation are given for performing session analysis. Two datasets in 7 text files, downloadable: a) Training dataset : some fields are in the file rec_log_train.txt  b) Testing dataset: some fields are in the file rec_log_test.txt Format of the above 2 files: (UserId)\t(ItemId)\t(Result)\t(Unix-timestamp) Result: values are 1 or -1, where 1 represents the user UserId accepts the recommendation of item ItemId and follows it (i.e., adds it to his/her social network), and -1 represents the user rejects the recommended item. We provide the true values of the ‘Result’ field in rec_log_train.txt, whereas in  rec_log_test.txt, the true values of the ‘Result’ field are withheld (for simplicity, in the file they are always 0). Another difference from rec_log_test.txt to rec_log_train.txt  is that repeated recommended (UserId,ItemId) pairs were removed. c)      More fields of the training and the testing datasets about the user and the item are in the following 5 files:           i.              User profile data: user_profile.txt Each line contains the following information of a user: the year of birth, the gender, the number of tweets and the tag-Ids. It is important to note that information about the users to be recommended is  also in this file. Format: (UserId)\t(Year-of-birth)\t(Gender)\t(Number-of-tweet)\t(Tag-Ids) Year of birth is selected by user when he/she registered. Gender has an integer value of 0, 1, or 2, which represents “unknown”, “male”, or “female”, respectively. Number-of-tweet is an integer that represents the amount of tweets the user has posted. Tags are selected by users to represent their interests. If a user likes mountain climbing and swimming, he/she may select ""mountain climbing"" or ""swimming"" to be his/her tag. There are some users who select  nothing. The original tags in natural languages are not used here, each unique tag is encoded as an unique integer. Tag-Ids are in the form “tag-id1;tag-id2;...;tag-idN”. If a user doesn’t have tags, Tag-Ids will be ""0"".         ii.              Item data: item.txt Each line contains the following information of an item: its category and keywords. Format: (ItemId)\t(Item-Category)\t(Item-Keyword) Item-Category is a string “a.b.c.d”, where the categories in the hierarchy are delimited by the character “.”, ordered in top-down fashion (i.e., category ‘a’ is a parent category of ‘b’, and category ‘b’ is a parent category of ‘c’, and so on. Item-Keyword contains the keywords extracted from the corresponding Weibo profile of the person, organization, or group. The format is a string “id1;id2;…;idN”, where each unique keyword is encoded as an unique integer such that no real term is revealed.       iii.              User action data: user_action.txt The file user_action.txt contains the statistics about the ‘at’ (@) actions between the users in a certain number of recent days. Format: (UserId)\t(Action-Destination-UserId)\t(Number-of-at-action)\t(Number-of-retweet )\t(Number-of-comment) If user A wants to notify another user about his/her tweet/retweet/comment, he/she would use an ‘at’ (@) action to notify the other user, such as ‘@tiger’ (here the user to be notified is ‘tiger’).. For example, user A has retweeted user B 5 times, has “at” B 3 times, and has commented user B 6 times, then there is one line “A   B     3     5     6” in user_action.txt.        iv.              User sns data: user_sns.txt The file user_sns.txt contains each user’s follow history (i.e., the history of following another user). Note that the following relationship can be reciprocal. Format: (Follower-userid)\t(Followee-userid)          v.              User key word data: user_key_word.txt The file user_key_word.txt contains the keywords extracted from the tweet/retweet/comment by each user. Format: (UserId)\t(Keywords) Keywords is in the form “kw1:weight1;kw2:weight2;…kw3:weight3”. Keywords are extracted from the tweet/retweet/comment of a user, and can be used as features to better represent the user in your prediction model. The greater the weight, the more interested the user is with regards to the keyword. Every keyword is encoded as a unique integer, and the keywords of the users are from the same vocabulary as the Item-Keyword.  EVALUATION  Teams’ scores and ranks on the leaderboard are based on a metric calculated from the predicted results in submitted result file and the held out ground truth of a validation dataset whose instances were a fixed set sampled from the testing dataset in the  beginning and, until the last day of the competition (June 1, 2012) by then the scores and associated ranks on leaderboard are based on the predicted results and that of the rest of the testing dataset. This entails that the top-3 ranked teams at the time  when the competition ends are the winners. The log for forming the training dataset corresponds to earlier time than that of the testing dataset. The evaluation metric is average precision. For a detailed definition of the metric, please refer to the tab ‘Evaluation’.  PRIZES  The prizes for the 1, 2 and 3 winners for task 1 are US Dollars $5000, $2000, and $1000, respectively.";https://www.kaggle.com/c/kddcup2012-track1;;Predict which users (or information sources) one user might follow in Tencent Weibo.;['map@3'];656;KDD Cup 2012, Track 1;Featured prediction Competition
2012-06-02 01:59:59;TASK 2 DESCRIPTION  Search advertising has been one of the major revenue sources of the Internet industry for years. A key technology behind search advertising is to predict the click-through rate (pCTR) of ads, as the economic model behind search advertising requires pCTR values to rank ads and to price clicks. In this task, given the training instances derived from session logs of the Tencent proprietary search engine, soso.com, participants are expected to accurately predict the pCTR of ads in the testing instances. TRAINING DATA FILE     The training data file is a text file, where each line is a training instance derived from search session log messages. To understand the training data, let us begin with a description of search sessions.    A search session refers to an interaction between a user and the search engine. It contains the following ingredients: the user, the query issued by the user, some ads returned by the search engine and thus impressed (displayed) to the user, and zero or more ads that were clicked by the user. For clarity, we introduce a terminology here. The number of ads impressed in a session is known as the ’depth’. The order of an ad in the impression list is known as the ‘position’ of that ad. An Ad, when impressed, would be displayed as a short text known as ’title’, followed by a slightly longer text known as the ’description’, and a URL (usually shortened to save screen space) known as ’display URL’.    We divide each session into multiple instances, where each instance describes an impressed ad under a certain setting  (i.e., with certain depth and position values).  We aggregate instances with the same user id, ad id, query, and setting in order to reduce the dataset size. Therefore, schematically, each instance contains at least the following information:  UserID  AdID  Query  Depth  Position  Impression   the number of search sessions in which the ad (AdID) was impressed by the user (UserID) who issued the query (Query).  Click   the number of times, among the above impressions, the user (UserID) clicked the ad (AdID).    Moreover, the training, validation and testing data contain more information than the above list, because each ad and each user have some additional properties. We include some of these properties into the training, validation  and the testing instances, and put other properties in separate data files that can be indexed using ids in the instances. For more information about these data files, please refer to the section ADDITIONAL DATA FILES.  Finally, after including additional features, each training instance is a line consisting of fields delimited by the TAB character:  1. Click: as described in the above list.  2. Impression: as described in the above list.  3. DisplayURL: a property of the ad.  The URL is shown together with the title and description of an ad. It is usually the shortened landing page URL of the ad, but not always. In the data file,  this URL is hashed for anonymity.  4. AdID: as described in the above list.  5. AdvertiserID: a property of the ad.  Some advertisers consistently optimize their ads, so the title and description of their ads are more attractive than those of others’ ads.  6. Depth: a property of the session, as described above.    7. Position: a property of an ad in a session, as described above.  8. QueryID:  id of the query.  This id is a zero‐based integer value. It is the key of the data file 'queryid_tokensid.txt'. 9. KeywordID: a property of ads.  This is the key of  'purchasedkeyword_tokensid.txt'.  10. TitleID: a property of ads.  This is the key of 'titleid_tokensid.txt'.  11. DescriptionID: a property of ads.   This is the key of 'descriptionid_tokensid.txt'.  12. UserID  This is the key of 'userid_profile.txt'.  When we cannot identify the user, this field has a special value of 0.  ADDITIONAL DATA FILES There are five additional data files, as mentioned in the above section:  1. queryid_tokensid.txt  2. purchasedkeywordid_tokensid.txt  3. titleid_tokensid.txt  4. descriptionid_tokensid.txt  5. userid_profile.txt  Each line of the first four files maps an id to a list of tokens, corresponding to the query, keyword, ad title, and ad description, respectively. In each line, a TAB character separates the id and the token set.  A token can basically be a word in a natural language. For anonymity, each token is represented by its hash value.  Tokens are delimited by the character ‘|’.  Each line of ‘userid_profile.txt’ is composed of UserID, Gender, and Age, delimited by the TAB character. Note that not every UserID in the training and the testing set will be present in ‘userid_profile.txt’. Each field is described below:  1. Gender:  '1'  for male, '2' for female,  and '0'  for unknown.  2. Age:  '1'  for (0, 12],  '2' for (12, 18], '3' for (18, 24], '4'  for  (24, 30], '5' for (30,  40], and '6' for greater than 40.  TESTING DATASET The testing dataset shares the same format as the training dataset, except for the counts of ad impressions and ad clicks that are needed for computing the empirical CTR. A subset of the testing dataset is used to consistently rank submitted/updated results on the leaderboard. The testing dataset is used for picking the final winners. The log for forming the training dataset corresponds to earlier time than that of the testing dataset. EVALUATION Teams are expected to submit their result file in text format, in which each line corresponds to a line in the downloaded file with the same order, and there is only one field in each line: the predicted CTR. In the result file, the lines corresponding to the lines from validation dataset will be used to score for the ranking on the leaderboard during the competition except the last day (June 1, 2012), and the lines corresponding to the lines from testing dataset will be used for the ranking on the leaderboard on the day of June 1, 2012, and for picking the final winners. The performance of the prediction will be scored in terms of the AUC (for more details about AUC, please see ‘ROC graphs: Notes and practical considerations for researchers‘ by Tom Fawcett). For a detailed definition of the metric, please refer to the tab ‘Evalaution’. PRIZES Teams with the best performance scores will be the winners. The prizes for the 1, 2 and 3 winners for task 2 are US Dollars $5000, $2000, and $1000, respectively.;https://www.kaggle.com/c/kddcup2012-track2;;Predict the click-through rate of ads given the query and user information.;['custom metric'];163;KDD Cup 2012, Track 2;Featured prediction Competition
2017-12-18 00:59:00;"The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build an algorithm that predicts whether a subscription user will churn using a donated dataset from KKBOX. WSDM (pronounced ""wisdom"") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models. For a subscription business, accurately predicting churn is critical to long-term success. Even slight variations in churn can drastically affect profits. KKBOX is Asia’s leading music streaming service, holding the world’s most comprehensive Asia-Pop music library with over 30 million tracks. They offer a generous, unlimited version of their service to millions of people, supported by advertising and paid subscriptions. This delicate model is dependent on accurately predicting churn of their paid users. In this competition you’re tasked to build an algorithm that predicts whether a user will churn after their subscription expires. Currently, the company uses survival analysis techniques to determine the residual membership life time for each subscriber. By adopting different methods, KKBOX anticipates they’ll discover new insights to why users leave so they can be proactive in keeping users dancing. Winners will present their findings at the WSDM conference February 6-8, 2018 in Los Angeles, CA.  For more information on the conference, click here.";https://www.kaggle.com/c/kkbox-churn-prediction-challenge;KKBOX;Can you predict when subscribers will churn?;['binary classification', 'logloss'];574;WSDM - KKBox's Churn Prediction Challenge;Research prediction Competition
2017-12-18 00:59:00;"The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build a better music recommendation system using a donated dataset from KKBOX. WSDM (pronounced ""wisdom"") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models. Not many years ago, it was inconceivable that the same person would listen to the Beatles, Vivaldi, and Lady Gaga on their morning commute. But, the glory days of Radio DJs have passed, and musical gatekeepers have been replaced with personalizing algorithms and unlimited streaming services. While the public’s now listening to all kinds of music, algorithms still struggle in key areas. Without enough historical data, how would an algorithm know if listeners will like a new song or a new artist? And, how would it know what songs to recommend brand new users? WSDM has challenged the Kaggle ML community to help solve these problems and build a better music recommendation system. The dataset is from KKBOX, Asia’s leading music streaming service, holding the world’s most comprehensive Asia-Pop music library with over 30 million tracks. They currently use a collaborative filtering based algorithm with matrix factorization and word embedding in their recommendation system but believe new techniques could lead to better results. Winners will present their findings at the conference February 6-8, 2018 in Los Angeles, CA.  For more information on the conference, click here, and don't forget to check out the other KKBox/WSDM competition: KKBox Music Churn Prediction Challenge";https://www.kaggle.com/c/kkbox-music-recommendation-challenge;KKBOX;Can you build the best music recommendation system?;['auc'];1,081;WSDM - KKBox's Music Recommendation Challenge;Research prediction Competition
2016-06-14 01:59:00;Kobe Bryant marked his retirement from the NBA by scoring 60 points in his final game as a Los Angeles Laker on Wednesday, April 12, 2016. Drafted into the NBA at the age of 17, Kobe earned the sport’s highest accolades throughout his long career. Using 20 years of data on Kobe's swishes and misses, can you predict which shots will find the bottom of the net? This competition is well suited for practicing classification basics, feature engineering, and time series analysis. Practice got Kobe an eight-figure contract and 5 championship rings. What will it get you? Acknowledgements Kaggle is hosting this competition for the data science community to use for fun and education. For more data on Kobe and other NBA greats, visit stats.nba.com.;https://www.kaggle.com/c/kobe-bryant-shot-selection;Kaggle;Which shots did Kobe sink?;['binary classification', 'tabular data', 'basketball', 'logloss'];1,117;Kobe Bryant Shot Selection;Playground prediction Competition
2019-10-15 01:59:00;Build a model to transcribe ancient Kuzushiji into contemporary Japanese characters Imagine the history contained in a thousand years of books. What stories are in those books? What knowledge can we learn from the world before our time? What was the weather like 500 years ago? What happened when Mt. Fuji erupted? How can one fold 100 cranes using only one piece of paper? The answers to these questions are in those books. Japan has millions of books and over a billion historical documents such as personal letters or diaries preserved nationwide. Most of them cannot be read by the majority of Japanese people living today because they were written in “Kuzushiji”. Even though Kuzushiji, a cursive writing style, had been used in Japan for over a thousand years, there are very few fluent readers of Kuzushiji today (only 0.01% of modern Japanese natives). Due to the lack of available human resources, there has been a great deal of interest in using Machine Learning to automatically recognize these historical texts and transcribe them into modern Japanese characters. Nevertheless, several challenges in Kuzushiji recognition have made the performance of existing systems extremely poor. (More information in About Kuzushiji) This is where you come in. The hosts need help from machine learning experts to transcribe Kuzushiji into contemporary Japanese characters. With your help, Center for Open Data in the Humanities (CODH) will be able to develop better algorithms for Kuzushiji recognition. The model is not only a great contribution to the machine learning community, but also a great help for making millions of documents more accessible and leading to new discoveries in Japanese history and culture.  Hosts Center for Open Data  in the Humanities (CODH)  conducts research and development to enhance access to humanities data using state-of-the-art technology in informatics and statistics. The National Institute of Japanese Literature (NIJL) is an institution which strives to serve researchers in the field of Japanese literature as well as those working in various other humanities, by collecting in one location a vast storage of materials related to Japanese literature gathered from all corners of the country.  The National Institute of Informatics (NII) is Japan's only general academic research institution seeking to create future value in the new discipline of informatics. NII seeks to advance integrated research and development activities in information-related fields, including networking, software, and content. Official Collaborators Mikel Bober-Irizar (anokas) Kaggle Grandmaster and Alex Lamb (MILA. Quebec Artificial Intelligence Institute);https://www.kaggle.com/c/kuzushiji-recognition;ROIS-DS Center for Open Data in the Humanities;Opening the door to a thousand years of Japanese culture;['image data', 'multiclass classification', 'history', 'japan', 'custom metric'];293;Kuzushiji Recognition;Playground prediction Competition
2019-06-04 01:59:00;Did you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. In this competition, we present the largest worldwide dataset to date, to foster progress in this problem. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 200K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. This is the second edition of this challenge. Compared to the first edition, the new dataset is more comprehensive and diverse. See the Data tab for more in-depth discussion on the new released dataset. This challenge is organized in conjunction with the Landmark Retrieval Challenge. In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We encourage participants to use the training data from the recognition challenge (either from this year’s or last year’s dataset) to develop models which could be useful for the retrieval challenge.;https://www.kaggle.com/c/landmark-recognition-2019;Google;Label famous (and not-so-famous) landmarks in images;['custom metric'];281;Google Landmark Recognition 2019;Research prediction Competition
2020-09-30 01:59:00;Welcome to the third Landmark Recognition competition! This year, we have worked to set this up as a code competition and collected a new set of test images. Have you ever gone through your vacation photos and asked yourself: What was the name of that temple I visited in China? or Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. In the previous editions of this challenge (2018 and  2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring. This challenge is organized in conjunction with the Landmark Retrieval Challenge 2020, which was launched June 30, 2020. Both challenges are affiliated with the Instance-Level Recognition workshop in ECCV’20.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/landmark-recognition-2020;Google;Label famous (and not-so-famous) landmarks in images;['image data', 'computer vision', 'custom metric'];736;Google Landmark Recognition 2020;Research Code Competition
2018-05-30 01:59:00;[UPDATE] 2019 challenge launched: https://kaggle.com/c/landmark-recognition-2019 Did you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. In this competition, we present the largest worldwide dataset to date, to foster progress in this problem. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are a total of 15K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. This challenge is organized in conjunction with the Landmark Retrieval Challenge ( https://www.kaggle.com/c/landmark-retrieval-challenge ). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.;https://www.kaggle.com/c/landmark-recognition-challenge;Google;Label famous (and not-so-famous) landmarks in images;['image data', 'custom metric'];477;Google Landmark Recognition Challenge;Research prediction Competition
2019-06-04 01:59:00;Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph. In this competition, Kagglers are given query images and, for each query, are expected to retrieve all database images containing the same landmarks (if any). The competition will proceed in two phases: The 1st phase will use the same test and index sets as last year, while for phase 2 we will release a completely new dataset that contains 700K images with more than 100K unique landmarks. We hope that this release will accelerate progress in this important research problem. This challenge is organized in conjunction with the Landmark Recognition Challenge. In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge (either from this year’s or last year’s dataset) to develop models which could be useful for the retrieval challenge.;https://www.kaggle.com/c/landmark-retrieval-2019;Google;Given an image, can you find all of the same landmarks in a dataset?;['map@{k}'];144;Google Landmark Retrieval 2019;Research prediction Competition
2020-08-18 01:59:00;Welcome to the third Landmark Retrieval competition! This year, we have worked to set this up as a code competition and we have completely refreshed the test and index image sets.  Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph. In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the Landmark Recognition Challenge 2020. Both challenges will be discussed at the Instance-Level Recognition workshop in ECCV’20. In the previous editions of this challenge (2018 and  2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a representation learning format: rather than creating a submission file with retrieved images, you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality with mean average precision.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/landmark-retrieval-2020;Google;Given an image, can you find all of the same landmarks in a dataset?;['image data', 'computer vision', 'custom metric'];541;Google Landmark Retrieval 2020;Research Code Competition
2018-05-30 01:59:00;[UPDATE] 2019 challenge launched: https://kaggle.com/c/landmark-retrieval-2019 Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph. In this competition, Kagglers are given query images and, for each query, are expected to retrieve all database images containing the same landmarks (if any). The new dataset is the largest worldwide dataset for image retrieval research, comprising more than a million images of 15K unique landmarks. We hope that this release will accelerate progress in this important research problem. This challenge is organized in conjunction with the Landmark Recognition Challenge (https://www.kaggle.com/c/landmark-recognition-challenge). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.;https://www.kaggle.com/c/landmark-retrieval-challenge;Google;Given an image, can you find all of the same landmarks in a dataset?;['image data', 'map@{k}'];209;Google Landmark Retrieval Challenge;Research prediction Competition
2019-06-04 01:59:00;Forecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: when the event will occur, where it will occur, and how large it will be. In this competition, you will address when the earthquake will take place. Specifically, you’ll predict the time remaining before laboratory earthquakes occur from real-time seismic data.  If this challenge is solved and the physics are ultimately shown to scale from the laboratory to the field, researchers will have the potential to improve earthquake hazard assessments that could save lives and billions of dollars in infrastructure. This challenge is hosted by  Los Alamos National Laboratory which enhances national security by ensuring the safety of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.  Acknowledgments:     Geophysics Group: The competition builds on initial work from Bertrand Rouet-Leduc, Claudia Hulbert, and Paul Johnson. B. Rouet-Leduc prepared the data for the competition.    Department of Geosciences: Data are from experiments performed by  Chas Bolton, Jacques Riviere, Paul Johnson and Prof. Chris Marone.    Department of Physics & Astronomy: This competition stemmed from the DOE Council workshop “Information is in the Noise: Signatures of Evolving Fracture and Fracture Networks” held March 2018 that was organized by Prof. Laura J. Pyrak-Nolte.   Department of Energy Office of Science, Basic Energy Sciences, Chemical Sciences, Geosciences and Biosciences Division: The Geosciences core research.   Photo by Nik Shuliahin on Unsplash;https://www.kaggle.com/c/LANL-Earthquake-Prediction;Los Alamos National Laboratory;Can you predict upcoming laboratory earthquakes?;['earth science', 'physics', 'signal processing', 'mae'];4,521;LANL Earthquake Prediction;Research prediction Competition
2017-03-01 00:59:00;There are estimated to be nearly half a million species of plant in the world. Classification of species has been historically problematic and often results in duplicate identifications. Automating plant recognition might have many applications, including:       The objective of this playground competition is to use binary leaf images and extracted features, including shape, margin & texture, to accurately identify 99 species of plants. Leaves, due to their volume, prevalence, and unique characteristics, are an effective means of differentiating plant species. They also provide a fun introduction to applying techniques that involve image-based features. As a first step, try building a classifier that uses the provided pre-extracted features. Next, try creating a set of your own features. Finally, examine the errors you're making and see what you can do to improve. Acknowledgments Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from leaf images collected by  James Cope, Thibaut Beghin, Paolo Remagnino, & Sarah Barman of the Royal Botanic Gardens, Kew, UK. Charles Mallah, James Cope, James Orwell. Plant Leaf Classification Using Probabilistic Integration of Shape, Texture and Margin Features. Signal Processing, Pattern Recognition and Applications, in press. 2013. We thank the UCI machine learning repository for hosting the dataset.;https://www.kaggle.com/c/leaf-classification;Kaggle;Can you see the random forest for the leaves?;['image data', 'multiclass classification', 'multiclassloss'];1,597;Leaf Classification;Playground prediction Competition
2013-02-08 01:00:00;The leaderboard is a central fixture of the Kaggle experience. It provides context to the incredible work accomplished by the Kaggle data science community. To a competitor, the leaderboard is a dynamic, living, action-filled battle. Tactics come to life. Individuals leapfrog over each other.  Teams merge and blend submissions.  Some submit early and often, attempting to build up insurmountable leads. Others bide time, waiting to pounce minutes before the buzzer with their finest of forests.  We see the joys of regularization and the  agony of overfitting.  It's raw. It's beautiful. It's thousands of hours of collective human toil. It's boring. To an observer, the leaderboard is a spreadsheet.  They see funny team names, numbers with too many decimals, strange column titles, and none of the history behind the battle. We run a veritable nerd olympics, but instead of smashing the 100m world record, we're elbowing for a few decimal places of some esoteric quantity called a capped binomial deviance. It's faceless. It's cold. It fails to tell the story of the battle. And you know what that means? This means war. We're calling on you to bring the leaderboard to life.  Break out the  D3. Sacrifice an old PC to the javascript gods. Abandon all text, ye who enter here.  We're bootstrapping our own community to do what they do best, and that is doing things better. What kinds of submissions do we hope result from this competition? Maybe you know an API or two and can create a motion chart? Maybe you know the hot, new HTML5 canvas tricks? Maybe you know of an R package that  styles plots like The Economist or XKCD? Maybe you know Edward Tufte and can call in a favor? Be creative. Scrape profile photos. Examine team formation. Examine relative scores. Watch for edge cases, cluttered text, and all the gotchas that crop up when you juggle a leaderboard of 10 vs. 1000 teams.  We're looking for entries that convey the storyline behind the leaderboard.  Style and substance counts, as does reproducibility (sorry to the Bob Rosses of the world who want to hand draw their submission).  Web-readiness is appreciated, but we know better than to put such constraints on the Kaggle community.  Use whatever brush you wish to paint this masterpiece. Credits: We'd like to acknowledge Chris Mulligan at Columbia University for providing the impetus that put this prospect in motion. You can see his blog post or even check out a git repository of the code he used to do it. Image:;https://www.kaggle.com/c/leapfrogging-leaderboards;;Provide creative visualizations of the Kaggle leaderboard;['rmse'];;Leaping Leaderboard Leapfrogs;Research prediction Competition
2014-10-29 00:59:00;"Social Circles help users organize their personal social networks.  These are implemented as ""circles"" on Google+, and as ""lists"" on Facebook and Twitter. Each circle consists of a subset of a particular user's friends. Such circles may be disjoint, overlap, or be hierarchically nested. The goal of this competition is to automatically infer users' social circles. You are provided a set of users, each of whose circles must be inferred. To do this, participants have access to:  A list of the user's friends Anonymized Facebook profiles of each of those friends A network of connections between those friends (their ""ego network"")  To give you an idea of how to use this data, the problem of detecting social circles has been discussed (in an academic setting) in http://i.stanford.edu/~julian/pdfs/nips2012.pdf Those of you who've been around Kaggle for a while may remember that we called upon you to create this data set. We extend our thanks to those of you who helped out. As a reward, you might be able to find yourself in the data and gain a one-row advantage?!";https://www.kaggle.com/c/learning-social-circles;Kaggle;Model friend memberships to multiple circles;['custom metric'];202;Learning Social Circles in Networks;Playground prediction Competition
2014-09-03 01:59:00;A Fortune 100 company, Liberty Mutual Insurance has provided a wide range of insurance products and services designed to meet our customers' ever-changing needs for over 100 years. Within the business insurance industry, fire losses account for a significant portion of total property losses. High severity and low frequency, fire losses are inherently volatile, which makes modeling them difficult. In this challenge, your task is to predict the target, a transformed ratio of loss to total insured value, using the provided information. This will enable more accurate identification of each policyholder’s risk exposure and the ability to tailor the insurance coverage for their specific operation. Because we seek to tap innovation both inside and outside the company, certain eligible Liberty Mutual employees are encouraged to participate in this challenge for development purposes. Refer to the competition rules for the full details.;https://www.kaggle.com/c/liberty-mutual-fire-peril;;Predict expected fire losses for insurance policies;['normalizedweightedgini'];632;Liberty Mutual Group - Fire Peril Loss Cost;Featured prediction Competition
2015-08-29 01:59:00;A Fortune 100 company, Liberty Mutual Insurance has provided a wide range of insurance products and services designed to meet their customers' ever-changing needs for over 100 years. To ensure that Liberty Mutual’s portfolio of home insurance policies aligns with their business goals, many newly insured properties receive a home inspection. These inspections review the condition of key attributes of the property, including things like the foundation, roof, windows and siding. The results of an inspection help Liberty Mutual determine if the property is one they want to insure. In this challenge, your task is to predict a transformed count of hazards or pre-existing damages using a dataset of property information. This will enable Liberty Mutual to more accurately identify high risk homes that require additional examination to confirm their insurability.  Liberty Mutual is interested in hiring predictive modelers like you to work on one of many growing analytics teams within our company. As a member of Liberty Mutual’s advanced analytics community, you will have the opportunity to apply sophisticated, cutting-edge techniques, similar to those used in this competition, to large data sets in departments such as Actuarial, Product, Claims, Marketing, Distribution, Human Resources, and Finance. Click to view available positions. Because we seek to tap innovation both inside and outside the company, certain eligible Liberty Mutual employees are encouraged to participate in this challenge for development purposes. Refer to the competition rules for the full details.;https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;;Quantify property hazards before time of inspection;['housing', 'normalizedgini'];2,232;Liberty Mutual Group: Property Inspection Prediction;Featured prediction Competition
2020-12-01 00:59:00;The Connectivity Map, a project within the Broad Institute of MIT and Harvard,  the Laboratory for Innovation Science at Harvard (LISH), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS), present this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms. What is the Mechanism of Action (MoA) of a drug? And why is it important?  In the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short. How do we determine the MoAs of a new drug?  One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs. In this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells’ responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset. As is customary, the dataset has been split into testing and training subsets. Hence, your task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.     How to evaluate the accuracy of a solution?  Based on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic  loss function applied to each drug-MoA annotation pair. If successful, you’ll help to develop an algorithm to predict a compound’s MoA given its cellular signature, thus helping scientists advance the drug discovery process.      This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/lish-moa;Laboratory for Innovation Science at Harvard;Can you improve the algorithm that classifies drugs based on their biological activity?;['biology', 'tabular data', 'drugs and medications', 'genetics', 'meancolumnwiselogloss'];4,373;Mechanisms of Action (MoA) Prediction;Research Code Competition
2020-05-26 01:59:00;Think you can use your data science skills to make big predictions at a submicroscopic level? Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.  When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data. The University of Liverpool’s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you’ll use ion channel data to better model automatic identification methods. If successful, you’ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments. Technology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems. Acknowledgements: This would not be possible without the help of the Biotechnology and Biological Sciences Research Council (BBSRC).;https://www.kaggle.com/c/liverpool-ion-switching;University of Liverpool;Identify the number of channels open at each time point;['biology', 'macrofscore'];2,618;University of Liverpool - Ion Switching;Research prediction Competition
2014-03-15 00:59:00;This competition asks you to determine whether a loan will default, as well as the loss incurred if it does default. Unlike traditional finance-based approaches to this problem, where one distinguishes between good or bad counterparties in a binary way, we seek to anticipate and incorporate both the default and the severity of the losses that result. In doing so, we are building a bridge between traditional banking, where we are looking at reducing the consumption of economic capital, to an asset-management perspective, where we optimize on the risk to the financial investor. This competition is sponsored by researchers at Imperial College London.;https://www.kaggle.com/c/loan-default-prediction;;Constructing an optimal portfolio of loans;['mae'];672;Loan Default Prediction - Imperial College London;Research prediction Competition
2014-04-23 01:59:00;"We are pleased to announce the 4th edition of the Large Scale Hierarchical Text Classification (LSHTC) Challenge. The LSHTC Challenge is a hierarchical text classification competition, using very large datasets.  Hierarchies are becoming ever more popular for the organization of text documents, particularly on the Web. Web directories and Wikipedia are two examples of such hierarchies. Along with their widespread use comes the need for automated classification of new documents to the categories in the hierarchy. As the size of the hierarchy grows and the number of documents to be classified increases, a number of interesting machine learning problems arise. In particular, it is one of the rare situations where data sparsity remains an issue, despite the vastness of available data: as more documents become available, more classes are also added to the hierarchy, and there is a very high imbalance between the classes at different levels of the hierarchy. Additionally, the statistical dependence of the classes poses challenges and opportunities for new learning methods. The challenge is based on a large dataset created from Wikipedia. The dataset is multi-class, multi-label and hierarchical. The number of categories is roughly 325,000 and number of the documents is 2,400,000. This challenge builds upon a series of successful challenges on large-scale hierarchical text classification. More information can be found at http://lshtc.iit.demokritos.gr/ Very Large Scale Supervised Learning Track This track concerns multi-label classification based on the Wikipedia dataset. The hierarchy is a graph that can have cycles.  The number of categories is roughly 325,000 and the number of documents is 2,400,000. A document can appear in multiple classes. Organizers Ioannis Partalas, LIG, Grenoble, France Massih-Reza Amini, LIG, Grenoble, France Ion Androutsopoulos, AUEB, Athens, Greece Thierry Artières, LIP6, Paris, France Nicolas Baskiotis, LIP6, Paris, France Patrick Gallinari, LIP6, Paris, France Eric Gaussier, LIG, Grenoble, France Aris Kosmopoulos, NCSR ""Demokritos"" & AUEB, Athens, Greece George Paliouras, NCSR ""Demokritos"", Athens, Greece Acknowledgements Class-Y ANR project, University of Grenoble, University of Pierre and Marie Curie, NCSR ""Demokritos"", and Athens University of Economics and Business. We would also like to thank the Kaggle team for their support.";https://www.kaggle.com/c/lshtc;;Classify Wikipedia documents into one of 325,056 categories;['macrofscore'];119;Large Scale Hierarchical Text Classification;Research prediction Competition
2020-11-26 00:59:00;Autonomous vehicles (AVs) are expected to dramatically redefine the future of transportation. However, there are still significant engineering challenges to be solved before one can fully realize the benefits of self-driving cars. One such challenge is building models that reliably predict the movement of traffic agents around the AV, such as cars, cyclists, and pedestrians. The ridesharing company Lyft started Level 5 to take on the self-driving challenge and build a full self-driving system (they’re hiring!). Their previous competition tasked participants with identifying 3D objects, an important step prior to detecting their movement. Now, they’re challenging you to predict the motion of these traffic agents. In this competition, you’ll apply your data science skills to build motion prediction models for self-driving vehicles. You'll have access to the largest Prediction Dataset ever released to train and test your models. Your knowledge of machine learning will then be required to predict how cars, cyclists,and pedestrians move in the AV's environment. Lyft’s mission is to improve people’s lives with the world’s best transportation. They believe in a future where self-driving cars make transportation safer, environment-friendly and more accessible for everyone. Their goal is to accelerate development across the industry by sharing data with researchers. As a result of your participation, you can have a hand in propelling the industry forward and helping people around the world benefit from self-driving cars sooner.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;Lyft;Build motion prediction models for self-driving vehicles;['image data', 'automobiles and vehicles', 'transportation', 'tabular data', 'custom metric'];935;Lyft Motion Prediction for Autonomous Vehicles;Featured Code Competition
2020-07-01 01:59:00;Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? If you are interested in estimating the uncertainty distribution of the realized values of the same series, be sure to check out its companion competition How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses.  In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy. The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s. In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy. If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications. Acknowledgements Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.;https://www.kaggle.com/c/m5-forecasting-accuracy;University of Nicosia;Estimate the unit sales of Walmart retail goods;['time series analysis', 'custom metric'];5,558;M5 Forecasting - Accuracy;Featured prediction Competition
2020-07-01 01:59:00;Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the uncertainty distribution of the unit sales of various products sold in the USA by Walmart? This specific competition is the first of its kind, opening up new directions for both academic research and how uncertainty could be assessed and used in organizations. If you are interested in providing point (accuracy) forecasts for the same series, be sure to check out its companion competition. How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses.  In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy. The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s. In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days and to make uncertainty estimates for these forecasts. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy. If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications. Acknowledgements Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.;https://www.kaggle.com/c/m5-forecasting-uncertainty;University of Nicosia;Estimate the uncertainty distribution of Walmart unit sales.;['time series analysis', 'custom metric'];909;M5 Forecasting - Uncertainty;Featured prediction Competition
2015-09-01 01:59:00;Construction machines rely on a complex set of tubes to keep the forklift lifting, the loader loading, and the bulldozer from dozing off. Tubes can vary across a number of dimensions, including base materials, number of bends, bend radius, bolt patterns, and end types. Tubes come from a variety manufacturers, each having their own unique pricing model. This competition provides detailed tube, component, and annual volume datasets, and challenges you to predict the price a supplier will quote for a given tube assembly.;https://www.kaggle.com/c/machinery-tube-pricing;;Model quoted prices for industrial tube assemblies;['regression', 'tabular data', 'manufacturing', 'rmsle'];1,320;Machinery Tube Pricing;Featured prediction Competition
2015-04-18 01:59:00;"March 2018 Update:  when using this dataset, please cite http://arxiv.org/abs/1802.10135 In recent years, the malware industry has become a well organized market involving large amounts of money. Well funded, multi-player syndicates invest heavily in technologies and capabilities built to evade traditional protection, requiring anti-malware vendors to develop counter mechanisms for finding and deactivating them. In the meantime, they inflict real financial and emotional pain to users of computer systems.One of the major challenges that anti-malware faces today is the vast amounts of data and files which need to be evaluated for potential malicious intent. For example, Microsoft's real-time detection anti-malware products are present on over 160M computers worldwide and inspect over 700M computers monthly. This generates tens of millions of daily data points to be analyzed as potential malware. One of the main reasons for these high volumes of different files is the fact that, in order to evade detection, malware authors introduce polymorphism to the malicious components. This means that malicious files belonging to the same malware ""family"", with the same forms of malicious behavior, are constantly modified and/or obfuscated using various tactics, such that they look like many different files. In order to be effective in analyzing and classifying such large amounts of files, we need to be able to group them into groups and identify their respective families. In addition, such grouping criteria may be applied to new files encountered on computers in order to detect them as malicious and of a certain family. For this challenge, Microsoft is providing the data science community with an unprecedented malware dataset and encouraging open-source progress on effective techniques for grouping variants of malware files into their respective families. Acknowledgements This competition is hosted by WWW 2015 / BIG 2015 and the following Microsoft groups: Microsoft Malware Protection Center, Microsoft Azure Machine Learning and Microsoft Talent Management.  Microsoft contacts: Dr. Royi Ronen (royir@microsoft.com) and Corina Feuerstein (corinaf@microsoft.com)";https://www.kaggle.com/c/malware-classification;Microsoft;Classify malware into families based on file content and characteristics;['internet', 'text data', 'multiclass classification', 'custom metric'];377;Microsoft Malware Classification Challenge (BIG 2015);Research prediction Competition
2014-04-09 01:59:00;Each year, millions of people fill out a bracket to predict the outcome of the popular men’s college basketball tournament that tips off in March. While the odds of creating a perfect bracket are astronomical, these odds are made better by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media. How well can machine learning and statistical techniques improve the forecast? Presented by Intel, this competition will test how well predictions based on data stack up against a (jump) shot in the dark. We have assembled the basic elements necessary to get started with tournament prediction. The provided data covers nearly two decades of historical games, but you’re also encouraged to use data from external sources. To help turn all of that information into useful insight, Intel is making its big data technologies more affordable, available, and easier to use for everything from helping develop new scientific discoveries and business models to gaining the upper hand on good-natured predictions of sporting events. In stage one of this two-stage competition, participants will build and test their models against the previous five tournaments. In the second stage, participants will predict the outcome of the 2014 tournament. You don’t need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2014 results, for which you’ll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket. To sweeten the pot, Intel will present the team with the most accurate predictions a $15,000 cash prize. Get started today – predictions are due by Wednesday, March 19, 2014. Please visit the FAQs for more information.;https://www.kaggle.com/c/march-machine-learning-mania-2014;;Tip off college basketball by predicting the 2014 NCAA Tournament;['sports', 'basketball', 'logloss'];248;March Machine Learning Mania;Featured prediction Competition
2015-04-08 01:59:00;At Kaggle HQ and in offices across the country, March is a month when bracketology is in bloom. Back by popular demand, our second annual March Machine Learning Mania competition pits you against the millions of sports fans and office-pool bandwagoners who are hoping to win big by correctly predicting the outcome of the men's NCAA basketball tournament.  While the odds of forecasting a perfect bracket are astronomical, these odds are improved by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media. How well can machine learning and statistical techniques improve the forecast? Presented by HP Software's industry leading Big Data group and the HP Haven Big Data platform, this competition will test how well predictions based on data stack up against a (jump) shot in the dark.  This competition allows you to get creative with the datasets you use to create your model. We provide data covering three decades of historical games, but you're highly encouraged to pull in data from external sources.  The 50+ REST APIs from HP IDOL OnDemand are a great way to get started augmenting the dataset. Developer accounts are free and includes free monthly quota! Begin by extracting trending topics and identifying entities from the IDOL OnDemand news dataset (accessed via the Query Text Index API) or by analyzing public sentiment about players and teams using data from your social media feed.  In stage one of this two-stage competition, participants will build and test their models against the previous four tournaments. In the second stage, participants will predict the outcome of the 2015 tournament. You don’t need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2015 results, for which you’ll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket. HP is sponsoring $15,000 in cash prizes for the winners. Please visit the FAQs for more information. Acknowledgements March Machine Learning Mania 2015 is presented by HP. Please see About the sponsor to read more.;https://www.kaggle.com/c/march-machine-learning-mania-2015;;Predict the 2015 NCAA Basketball Tournament;['sports', 'tabular data', 'basketball', 'logloss'];340;March Machine Learning Mania 2015;Featured prediction Competition
2016-04-05 08:00:00;Update: although the tournament is over, we're continuing our analysis under the predictions dataset page. Back for its third year, March Machine Learning Mania challenges data scientists to predict winners and losers of the men's 2016 NCAA basketball tournament. You're provided data covering three decades of historical NCAA games and freely encouraged to use other sources of data to gain a winning edge.  In stage one of this two-stage competition, participants will build and test their models against the previous four tournaments. In the second stage, participants will predict the outcome of the 2016 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2016 results. Acknowledgments SAP is the presenting sponsor of March Machine Learning Mania 2016. Please see About the Sponsor to read more.;https://www.kaggle.com/c/march-machine-learning-mania-2016;;Predict the 2016 NCAA Basketball Tournament;['sports', 'tabular data', 'basketball', 'logloss'];596;March Machine Learning Mania 2016;Featured prediction Competition
2017-04-04 17:00:00;Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our fourth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2017 tournament. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2017 results.;https://www.kaggle.com/c/march-machine-learning-mania-2017;;Predict the 2017 NCAA Basketball Tournament;['sports', 'basketball', 'logloss'];441;March Machine Learning Mania 2017;Playground prediction Competition
2020-05-01 01:59:00;"There's a reason why it's called March Madness®. Upsets happen, underdogs become ""cinderellas,"" and games that analysts expected to be blowouts become nail-biters through the final seconds. A team's competitiveness is what keeps games exciting and the tournament truly ""mad.""  In addition to the predictive modeling competitions we typically host (NCAA Men's and Women’s), we are hosting a separate competition using Kaggle Notebooks that challenges you to present an exploratory analysis of the “Madness.” Can you quantify competitiveness? Can you explain ""cinderella…ness""?  Or perhaps, can you determine what dictates the ability of a team to “stay in the game” and increase their chance to win late in the contest? This may or may not be a scalar metric. It might be a clustering of types of competitiveness and then a rating within each. Does this metric have predictive power? The interpretation is up to you. Your challenge is to tell a data story about college basketball through a combination of both narrative text and data exploration. A “story” could be defined any number of ways, and that’s deliberate. You are to deeply explore (through data) the mania of the Men’s and Women’s NCAA College Basketball tournaments. That story can be examined in the macro (for example: How does “competitiveness” differ from the regular season to their decisions in the tournament?) or the micro (for example: Does effectively neutralizing an opponent’s star players increase their ability to “stay in the game”?).  This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!";https://www.kaggle.com/c/march-madness-analytics-2020;Google Cloud;Uncover the madness of March Madness®;['sports', 'basketball', 'logloss'];;Google Cloud & NCAA® March Madness Analytics;Analytics  Competition
2013-08-08 01:59:00;This is a private, invitation-only competition. The relevant information is provided only to contestants. The competition is closed to new entrants. Qualification for future private competitions is based solely on objective leaderboard performance in competitions.;https://www.kaggle.com/c/mastercard-data-cleansing-competition-finals;;Improve the quality of information within transaction data;['custom metric'];6;MasterCard - Data Cleansing Competition;Masters prediction Competition
2011-08-18 02:00:00;The Mapping Dark Matter is related to the GRavitational lEnsing Accuracy Testing (GREAT) challenges. The GREAT10 challenge is running from December 2010 to September 2011.  For more information on the GREAT challenges we refer the readers to the GREAT08 and GREAT10 Handbooks that describe the galaxy shape measurement process, as well as some existing approaches in more detail       The GREAT challenges are an extension of the Shear TEsting Programme (STEP). If people wish to analyses more simulated data and even real astronomical data we encourage you to investigate these pages STEP The Mapping Dark Matter Challenge is cross-posted at :   US Government Challenge Website  GREAT10 Challenge Website;https://www.kaggle.com/c/mdm;;Measure the small distortion in galaxy images caused by dark matter;['rmse'];70;Mapping Dark Matter;Featured prediction Competition
2016-12-02 00:59:00;Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective. Even after surgical removal of epilepsy, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for electrical brain activity (EEG) based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.  The Competition Transitioning from the Kaggle contests held on seizure detection and seizure prediction in 2014 that primarily involved long-term electrical brain activity recordings from dogs, the current contest focuses on seizure prediction using long-term electrical brain activity recordings from humans obtained from the world-first clinical trial of the implantable NeuroVista Seizure Advisory System. Human brain activity was recorded in the form of intracranial EEG (iEEG), which involves electrodes positioned on the surface of the cerebral cortex and the recording of electrical signals with an ambulatory monitoring system. These are long duration recordings, spanning multiple months up to multiple years and recording large numbers of seizures in some humans. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity.  Acknowledgments  This competition is sponsored by MathWorks, the National Institutes of Health (NINDS), the American Epilepsy Society and the University of Melbourne, and organised in partnership with the Alliance for Epilepsy Research, the University of Pennsylvania and the Mayo Clinic.       References;https://www.kaggle.com/c/melbourne-university-seizure-prediction;;Predict seizures in long-term human intracranial EEG recordings;['healthcare', 'diseases', 'auc'];477;Melbourne University AES/MathWorks/NIH Seizure Prediction;Research prediction Competition
2018-04-03 01:59:00;Google Cloud and NCAA® have teamed up to bring you this year’s version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness® during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.   In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results. This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.;https://www.kaggle.com/c/mens-machine-learning-competition-2018;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['basketball', 'logloss'];933;Google Cloud & NCAA® ML Competition 2018-Men's;Featured prediction Competition
2019-04-09 08:00:00;As a result of the continued collaboration between Google Cloud and the NCAA, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.   In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.  As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!  This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.;https://www.kaggle.com/c/mens-machine-learning-competition-2019;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['sports', 'basketball', 'logloss'];866;Google Cloud & NCAA® ML Competition 2019-Men's;Featured prediction Competition
2018-02-21 21:03:00;It can be hard to know how much something’s really worth. Small details can mean big differences in pricing. For example, one of these sweaters cost $335 and the other cost $9.99. Can you guess which one’s which?   Product pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs.  Mercari, Japan’s biggest community-powered shopping app, knows this problem deeply. They’d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari's marketplace. In this competition, Mercari’s challenging you to build an algorithm that automatically suggests the right product prices. You’ll be provided user-inputted text descriptions of their products, including details like product category name, brand name, and item condition. Note that, because of the public nature of this data, this competition is a “Kernels Only” competition. In the second stage of the challenge, files will only be available through Kernels and you will not be able to modify your approach in response to new data. Read more details in the data tab and Kernels FAQ page.;https://www.kaggle.com/c/mercari-price-suggestion-challenge;Mercari;Can you automatically suggest product prices to online sellers?;['rmsle'];2,382;Mercari Price Suggestion Challenge;Featured Code Competition
2017-07-11 01:59:00;Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler’s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams. . To ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler’s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world’s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler’s production lines.  In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler’s standards.;https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;Daimler;Can you cut the time a Mercedes-Benz spends on the test bench?;['regression', 'automobiles and vehicles', 'tabular data', 'r2score'];3,831;Mercedes-Benz Greener Manufacturing;Featured prediction Competition
2012-10-17 01:59:00;Help enable the development of safe, effective medicines. When developing new medicines it is important to identify molecules that are highly active toward their intended targets but not toward other targets that might cause side effects.  The objective of this competition is to identify the best statistical techniques for predicting biological activities of different molecules, both on- and off-target, given numerical descriptors generated from their chemical structures The challenge is based on 15 molecular activity data sets, each for a biologically relevant target. Each row corresponds to a molecule and contains descriptors derived from that molecule's chemical structure. In addition to the prediction competition, Merck is also hosting a  visualization challenge with a $2,000 prize for the most insightful and elegant graphical representations of the data. Prizes total $40,000.;https://www.kaggle.com/c/MerckActivity;;Help develop safe and effective medicines by predicting molecular activity.;['custom metric'];236;Merck Molecular Activity Challenge;Featured prediction Competition
2019-03-14 00:59:00;The malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways.   With more than one billion  enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security. As one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences. Can you help protect more than one billion machines from damage BEFORE it happens? Acknowledgements This competition is hosted by Microsoft, Windows Defender ATP Research, Northeastern University College of Computer and Information Science, and Georgia Tech Institute for Information Security & Privacy.      Microsoft contacts  Rob McCann (Robert.McCann@microsoft.com) Christian Seifert (chriseif@microsoft.com) Susan Higgs (Susan.Higgs@microsoft.com) Matt Duncan (Matthew.Duncan@microsoft.com)      Northeastern University contact  Mansour Ahmadi (m.ahmadi@northeastern.edu)      Georgia Tech contacts  Brendan Saltaformaggio (brendan@ece.gatech.edu) Taesoo Kim (taesoo@gatech.edu);https://www.kaggle.com/c/microsoft-malware-prediction;Microsoft;Can you predict if a machine will soon be hit with malware?;['auc'];2,426;Microsoft Malware Prediction;Research prediction Competition
2013-08-20 01:59:59;"It is important to gain a better understanding of bird behavior and population trends. Birds respond quickly to environmental change, and may also tell us about other organisms (e.g., insects they feed on), while being easier to detect. Traditional methods for collecting data about birds involve costly human effort. A promising alternative is acoustic monitoring. There are many advantages to recording audio of birds compared to human surveys, including increased temporal and spatial resolution and extent, applicability in remote sites, reduced observer bias, and potentially lower costs. However, it is an open problem for signal processing and machine learning to reliably identify bird sounds in real-world audio data collected in an acoustic monitoring scenario. Some of the major challenges include multiple simultaneously vocalizing birds, other sources of non-bird sound (e.g. buzzing insects), and background noise like wind, rain, and motor vehicles. The goal in this challenge is to predict the set of bird species that are present given a ten-second audio clip. This is a multi-label supervised classification problem. The training data consists of audio recordings paired with the set of species that are present. Background The audio dataset for this challenge was collected in the H. J. Andrews (HJA) Long-Term Experimental Research Forest, in the Cascade mountain range of Oregon. Since 2009, members of the OSU Bioacoustics group have collected over 10TB of audio data in HJA using Songmeter audio recording devices. A Songmeter has two omnidirectional microphones, and records audio in WAV format to flash memory. A Songmeter can be left in the field for several weeks at a time before either its batteries run out, or its memory is full. HJA has been the site of decades of experiments and data collection in ecology, geology and meteorology. This means, for example, that given an audio recording from a particular day and location in HJA, it is possible to look up the weather, vegetative composition, elevation, and much more. Such data enables unique discoveries through cross-examination, and long-term analysis.  Previous experiments on supervised classification using multi-instance and/or multi-label formulations have used audio data collected with song meters in HJA. The dataset for this competition is similar to, but perhaps more difficult than that dataset used in these prior works; in earlier work care was taken to avoid recordings with rain and loud wind, or no birds at all, and all of the recordings came from a single day. In this competition, you will consider a new dataset which includes rain and wind, and represents a sample from two years of audio recording at 13 different locations. Conference Attendance To participate in the conference, participants should email the following information to catherine.huang {at} intel.com no later than August 19, 2013: (1) the names of the team members (each person may belong to at most one team), (2) the name(s) of the host institutions of the researchers, (3) a 1-3 paragraph description of the approach used, (4) their submission score.  Those planning to attend the conference should additionally upload their source code to reproduce results.  model submission best practices Acknowledgements Collection and preparation of this dataset was partially funded by NSF grant DGE 0333257, NSF-CDI grant 0941748, NSF grant 1055113, NSF grant CCF-1254218, and the College of Engineering, Oregon State University. We would also like to thank Sarah Hadley, Jed Irvine, and others for their contributions in data collection and labeling.";https://www.kaggle.com/c/mlsp-2013-birds;;Predict the set of bird species present in an audio recording, collected in field conditions.;['auc'];79;MLSP 2013 Bird Classification Challenge;Research prediction Competition
2014-07-21 01:59:00;Schizophrenia is a severe and disabling mental illnesses which has no well-established, non-invasive diagnosis biomarker. Currently, due to its symptom overlap with other mental illnesses (like bipolar disorder) it can only be diagnosed subjectively, by process of elimination. This competition invites you to automatically diagnose subjects with schizophrenia based on multimodal features derived from their brain magnetic resonance imaging (MRI) scans. The features made available in this competition are a result from current state-of-the art developments in neuroimaging and MRI data processing. Two modalities of MRI scans are used to obtain these features: functional and structural MRI. One challenge in this competition is how to optimally combine this type of multimodal information and select features that enhance diagnosis. Optional additional information is provided that could be helpful with this particular aspect of the task. This is an official competition of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2014) Acknowledgements Collection of this dataset was made at the Mind Research Network under an NIH NIGMS Centers of Biomedical Research Excellence (COBRE) grant 5P20RR021938/P20GM103472 to Vince Calhoun (PI).;https://www.kaggle.com/c/mlsp-2014-mri;;Diagnose schizophrenia using multimodal features from MRI scans;['auc'];313;MLSP 2014 Schizophrenia Classification Challenge;Research prediction Competition
2018-09-25 01:59:00;"""There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side."" The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.  Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of Socher et al [2]. We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper: http://nlp.stanford.edu/sentiment/ There you will find have source code, a live demo, and even an online interface to help train the model. [1] Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124. [2] Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).";https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;Kaggle;Classify the sentiment of sentences from the Rotten Tomatoes dataset;['text data', 'multiclass classification', 'categorizationaccuracy'];410;Movie Review Sentiment Analysis (Kernels Only);Playground Code Competition
2012-08-10 01:59:59;The Million Song Dataset Challenge aims at being the best possible offline evaluation of a music recommendation system.  Any type of algorithm can be used: collaborative filtering, content-based methods, web crawling, even human oracles!* By relying on the  Million Song Dataset, the data for the competition is completely open: almost everything is known and possibly available. What is the task in a few words? You have: 1) the full listening history for 1M users, 2) half of the listening history for 110K users (10K validation set, 100K test set), and you must predict the missing half. How much easier can it get? The most straightforward approach to this task is pure collaborative filtering, but remember that there is a wealth of information available to you through the  Million Song Dataset. Go ahead, explore!  If you have questions, we recommend that you consult the  MSD Mailing List. Ready to start recommending?  Read through our  Getting Started tutorial. You can also look at this  open-source solution offered by a  contestant. For a more technical introduction to the MSD Challenge, see our  AdMIRe paper. (Please use this following  citation when referring to the contest in an academic setting.) * This contest is for computer models, but if you manage to get recommendations from humans for 110K listeners, we'd like to know how!   The Million Song Dataset Challenge is a joint effort between the  Computer Audition Lab at UC San Diego and LabROSA at Columbia University. The user data for the challenge, like much of the data in the Million Song Dataset, was generously donated by The Echo Nest, with additional data contributed by SecondHandSongs,  musiXmatch, and Last.fm. Follow-up evaluations will be conducted by IMIRSEL at the Graduate School of Library Information Science at UIUC as part of the Music Information Retrieval Evaluation eXchange (MIREX).;https://www.kaggle.com/c/msdchallenge;;Predict which songs a user will listen to.;['custom metric'];150;Million Song Dataset Challenge;Research prediction Competition
2017-10-03 01:59:00;A lot has been said during the past several years about how precision medicine and, more concretely, how genetic testing is going to disrupt the way diseases like cancer are treated. But this is only partially happening due to the huge amount of manual work still required. Memorial Sloan Kettering Cancer Center (MSKCC) launched this competition, accepted by the NIPS 2017 Competition Track,  because we need your help to take personalized medicine to its full potential.  Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers).  Currently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature. For this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists have manually annotated thousands of mutations. We need your help to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/msk-redefining-cancer-treatment;Kaggle;Predict the effect of Genetic Variants to enable Personalized Medicine;['text data', 'multiclass classification', 'genetics', 'multiclassloss'];1,386;Personalized Medicine: Redefining Cancer Treatment;Research prediction Competition
2013-08-26 01:59:00;The Multi-modal gesture recognition challenge, focused on gesture recognition from 2D and 3D video data using Kinect, is organized by ChaLearn in conjunction with ICMI 2013. Kinect is revolutionizing the field of gesture recognition given the set of input data modalities it provides, including RGB image, depth image (using an infrared sensor), and audio. Gesture recognition is genuinely important in many multi-modal interaction and computer vision applications, including image/video indexing, video surveillance, computer interfaces, and gaming. It also provides excellent benchmarks for algorithms. The recognition of continuous, natural signing is very challenging due to the multimodal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limitations such as spatial and temporal resolution and unreliable depth cues.  The Multi-modal Challenge workshop will be devoted to the presentation of most recent and challenging techniques from multi-modal gesture recognition. The committee encourages paper submissions in the following topics (but not limited to):  Multi-modal descriptors for gesture recognition Fusion strategies for gesture recognition Multi-modal learning for gesture recognition Data sets and evaluation protocols for multi-modal gesture recognition Applications of multi-modal gesture recognition  The results of the challenge will be discussed at the workshop. It features a quantitative evaluation of automatic gesture recognition from a multi-modal dataset recorded with Kinect (providing RGB images of face and body, depth images of face and body, skeleton information, joint orientation and audio sources), including about 15,000 Italian gestures from several users. The emphasis of this edition of the competition will be on multi-modal automatic learning of a vocabulary of 20 types of Italian gestures performed by several different users while explaining a history, with the aim of performing user independent continuous gesture recognition combined with audio information.  Additionally, the challenge includes a live competition of demos/systems of applications based on multi-modal gesture recognition techniques. Demos using data from different modalities and different kind of devices are welcome. The demos will be evaluated in terms of multi-modality, technical quality, and applicability. Best workshop papers and top three ranked participants of the quantitative evaluation will be invited to present their work at ICMI 2013 and their papers will be published in the ACM proceedings. Additionally, there will be travel grants (based on availability) and the possibility to be invited to present extended versions of their works to a special issue in a high impact factor journal. Moreover, all three top ranking participants in both, quantitative and qualitative challenges will be awarded with a ChaLearn winner certificate and an economic prize (based on availability). We will also announce a best paper and best student paper awards among the workshop contributions.;https://www.kaggle.com/c/multi-modal-gesture-recognition;;Recognize gesture sequences in video and depth data from Kinect;['custom metric'];53;Multi-modal Gesture Recognition;Research prediction Competition
2013-11-25 00:59:00;The Neural Information Processing Scaled for Bioacoustics (NIPS4B) bird song competition asks participants to identify which of 87 sound classes of birds and their ecosystem are present in 1000 continuous wild recordings from different places in Provence, France. The data is provided by the BIOTOPE society, which maintains the largest collection of wild recordings of birds in Europe. This challenge is a more complex task than the previous ICML4B challenge, in which 77 teams participated (see proceedings at sabiod.org). For more information about the Neural Information Processing Scaled for Bioacoustics workshop, please visit the official site. Organizers Pr. H. Glotin - Institut Universitaire de France, CNRS LSIS and USTV, glotin@univ-tln.fr O. Dufour - CNRS LSIS, FR Dr. Y. Bas - BIOTOPE, FR;https://www.kaggle.com/c/multilabel-bird-species-classification-nips2013;;Identify which of 87 classes of birds and amphibians are present into 1000 continuous wild sound recordings;['auc'];32;Multi-label Bird Species Classification - NIPS 2013;Research prediction Competition
2012-07-22 14:00:00;(Data will be made available 24 hours prior to the start of the contest) For more info http://musicdatascience.com/ hashtag #musicdata #ds_ldn #DSGhack    Proudly brought to you by;https://www.kaggle.com/c/MusicHackathon;;Can you predict if a listener will love a new song?;['rmse'];133;EMI Music Data Science Hackathon - July 21st - 24 hours;Featured prediction Competition
2018-09-26 01:59:00;In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations.  While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!  To learn how to handle large datasets with ease and solve this problem using TensorFlow, consider taking the Machine Learning with TensorFlow on Google Cloud Platform specialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To make this easier, head to Coursera.org/NEXTextended to claim this specialization for free for the first month!;https://www.kaggle.com/c/new-york-city-taxi-fare-prediction;Google Cloud;Can you predict a rider's taxi fare?;['regression', 'tabular data', 'rmse'];1,484;New York City Taxi Fare Prediction;Playground prediction Competition
2020-01-07 00:59:00;"“The running back takes the handoff… he breaks a tackle…spins… and breaks free! One man to beat! Past the 50-yard-line! To the 40! The 30! He! Could! Go! All! The! Way!” But will he?  American football is a complex sport. From the 22 players on the field to specific characteristics that ebb and flow throughout the game, it can be challenging to quantify the value of specific plays and actions within a play.  Fundamentally, the goal of football is for the offense to run (rush) or throw (pass) the ball to gain yards, moving towards, then across, the opposing team’s side of the field in order to score. And the goal of the defense is to prevent the offensive team from scoring.  In the National Football League (NFL), roughly a third of teams’ offensive yardage comes from run plays.. Ball carriers are generally assigned the most credit for these plays, but their teammates (by way of blocking), coach (by way of play call), and the opposing defense also play a critical role. Traditional metrics such as ‘yards per carry’ or ‘total rushing yards’ can be flawed; in this competition, the NFL aims to provide better context into what contributes to a successful run play. As an “armchair quarterback” watching the game, you may think you can predict the result of a play when a ball carrier takes the handoff - but what does the data say? In this competition, you will develop a model to predict how many yards a team will gain on given rushing plays as they happen. You'll be provided game, play, and player-level data, including the position and speed of players as provided in the NFL’s Next Gen Stats data. And the best part - you can see how your model performs from your living room, as the leaderboard will be updated week after week on the current season’s game data as it plays out.  Deeper insight into rushing plays will help teams, media, and fans better understand the skill of players and the strategies of coaches. It will also assist the NFL and its teams evaluate the ball carrier, his teammates, his coach, and the opposing defense, in order to make adjustments as necessary. Additionally, the winning model will be provided to the NFL’s Next Gen Stats group to potentially share with teams. You could help the NFL Network generate models to use during games, or for pre-game/post-game breakdowns.";https://www.kaggle.com/c/nfl-big-data-bowl-2020;The National Football League;How many yards will an NFL player gain after receiving a handoff?;['sports', 'football', 'crps'];2,038;NFL Big Data Bowl;Featured Code Competition
2020-01-03 00:59:00;"Welcome In this challenge, you're tasked to investigate the relationship between the playing surface and the injury and performance of National Football League (NFL) athletes and to examine factors that may contribute to lower extremity injuries. You'll also notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, the NFL’s annual Super Bowl competition designed to spur innovation in player health, safety and performance. The Challenge In the NFL, 12 stadiums have fields with synthetic turf.  Recent investigations of lower limb injuries among football athletes have indicated significantly higher injury rates on synthetic turf compared with natural turf (Mack et al., 2018; Loughran et al., 2019).  In conjunction with the epidemiologic investigations, biomechanical studies of football cleat-surface interactions have shown that synthetic turf surfaces do not release cleats as readily as natural turf and may contribute to the incidence of non-contact lower limb injuries (Kent et al., 2015).  Given these differences in cleat-turf interactions, it has yet to be determined whether player movement patterns and other measures of player performance differ across playing surfaces and how these may contribute to the incidence of lower limb injury.   Now, the NFL is challenging Kagglers to help them examine the effects that playing on synthetic turf versus natural turf can have on player movements and the factors that may contribute to lower extremity injuries.  NFL player tracking, also known as Next Gen Stats, is the capture of real time location data, speed and acceleration for every player, every play on every inch of the field. As part of this challenge, the NFL has provided full player tracking of on-field position for 250 players over two regular season schedules.  One hundred of the athletes in the study data set sustained one or more injuries during the study period that were identified as a non-contact injury of a type that may have turf interaction as a contributing factor to injury.  The remaining 150 athletes serve as a representative sample of the larger NFL population that did not sustain a non-contact lower-limb injury during the study period.  Details of the surface type and environmental parameters that may influence performance and outcome are also provided.  Your challenge is to characterize any differences in player movement between the playing surfaces and identify specific scenarios (e.g., field surface, weather, position, play type, etc.) that interact with player movement to present an elevated risk of injury.  More details on the entry criteria are available in Evaluation Tab. About The NFL The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country. The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries. For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com  Evaluation";https://www.kaggle.com/c/nfl-playing-surface-analytics;The National Football League;Can you investigate the relationship between the playing surface and the injury and performance of NFL athletes?;['sports', 'tabular data'];;NFL 1st and Future - Analytics;Analytics  Competition
2019-01-10 00:59:00;Welcome In this challenge you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. Instead, this challenge asks you to use data to propose specific rule modifications for the NFL that aim to reduce the occurrence of concussions during punt plays. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, presented by Arrow Electronics – the NFL’s annual Super Bowl competition designed to spur innovation in player health, safety and performance. The Challenge For the 2018 season, the NFL revised their kickoff rules in an effort to reduce the risk of injury during those plays. By examining injury reports, player position and velocity data, and game video, they were able to understand the game-play circumstances that may exacerbate the risk of injury to players.  This comprehensive review showed that over the course of all games during the 2015-2017 seasons, the kickoff represented only six percent of plays but 12 percent of concussions. Players had approximately four times the risk of concussion on returned kickoffs compared to running or passing plays. The changes to the kickoff rule aim to address the components that posed the most risk, like the use of a two-man wedge.  Now, the NFL is challenging Kagglers to help them perform the same examination, this time on punt play rules. They have provided data for all punt plays from the 2016 and 2017 NFL seasons that includes player rosters, on-field position data and video data, including the plays in which a player suffered a concussion.   Your challenge is to propose specific rule modifications (e.g. changes to the initial formation, tackling techniques, blocking rules etc.), supported by data, that may reduce the occurrence of concussions during punt plays. More details on the entry criteria are available in Overview tab > Evaluation.  About The NFL The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country. The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to look at anything and everything to protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries. For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com.  Evaluation;https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;The National Football League;Analyze NFL game data and suggest rules to improve player safety during punt plays;['health', 'sports', 'football'];;NFL Punt Analytics Competition;Analytics  Competition
2017-10-02 01:59:00;This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details. Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track. The competition on Adversarial Attacks and Defenses consist of three sub-competitions:  Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier. Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier. Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.  In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.;https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack;Google Brain;Create an image classifier that is robust to adversarial attacks;['image data', 'adversarial learning', 'custom metric'];107;NIPS 2017: Defense Against Adversarial Attack;Research prediction Competition
2017-10-02 01:59:00;This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details. Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track. The competition on Adversarial Attacks and Defenses consist of three sub-competitions:  Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier. Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier. Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.  In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.;https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack;Google Brain;Imperceptibly transform images in ways that fool classification models;['image data', 'adversarial learning', 'custom metric'];91;NIPS 2017: Non-targeted Adversarial Attack;Research prediction Competition
2017-10-02 01:59:00;This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details. Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track. The competition on Adversarial Attacks and Defenses consist of three sub-competitions:  Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier. Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier. Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.  In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.;https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack;Google Brain;Develop an adversarial attack that causes image classifiers to predict a specific target class;['image data', 'adversarial learning', 'custom metric'];65;NIPS 2017: Targeted Adversarial Attack;Research prediction Competition
2017-06-28 01:59:00;Steller sea lions in the western Aleutian Islands have declined 94 percent in the last 30 years. The endangered western population, found in the North Pacific, are the focus of conservation efforts which require annual population counts. Specially trained scientists at NOAA Fisheries Alaska Fisheries Science Center conduct these surveys using airplanes and unoccupied aircraft systems to collect aerial images. Having accurate population estimates enables us to better understand factors that may be contributing to lack of recovery of Stellers in this area. Currently, it takes biologists up to four months to count sea lions from the thousands of images NOAA Fisheries collects each year. Once individual counts are conducted, the tallies must be reconciled to confirm their reliability. The results of these counts are time-sensitive. In this competition, Kagglers are invited to develop algorithms which accurately count the number of sea lions in aerial photographs. Automating the annual population count will free up critical resources allowing NOAA Fisheries to focus on ensuring we hear the sea lion’s roar for many years to come. Plus, advancements in computer vision applied to aerial population counts may also greatly benefit other endangered species.  Resources Learn more about research being done to better understand what's going on with the endangered Steller sea lion populations by joining scientists on a research vessel to the western Aleutian Islands in the video below.;https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;NOAA;How many sea lions do you see?;['image data', 'animals', 'water bodies', 'mcrmse'];385;NOAA Fisheries Steller Sea Lion Population Count;Featured prediction Competition
2016-01-08 00:59:00;With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. Currently, only a handful of very experienced researchers can identify individual whales on sight while out on the water. For the majority of researchers, identifying individual whales takes time, making it difficult to effectively target whales for biological samples, acoustic recordings, and necessary health assessments.  To track and monitor the population, right whales are photographed during aerial surveys and then manually matched to an online photo-identification catalog. Customized software has been developed to aid in this process (DIGITS), but this still relies on a manual inspection of the potential comparisons, and there is a lag time for those images to be incorporated into the database. The current identification process is extremely time consuming and requires special training. This constrains marine biologists, who work under tight deadlines with limited budgets. This competition challenges you to automate the right whale recognition process using a dataset of aerial photographs of individual whales. Automating the identification of right whales would allow researchers to better focus on their conservation efforts. Recognizing a whale in real-time would also give researchers on the water access to potentially life-saving historical health and entanglement records as they struggle to free a whale that has been accidentally caught up in fishing gear. Acknowledgements MathWorks is sponsoring the competition prize pool. If your team is participating in this competition MathWorks is also providing complimentary software. Click here for more details on how to request your copy.  Thanks to Christin Khan and Leah Crowe from NOAA for hand labeling the images to create this one of a kind dataset and to the right whale research team at New England Aquarium for maintaining the photo-identification catalog. Without their continued efforts, none of this would be possible.;https://www.kaggle.com/c/noaa-right-whale-recognition;NOAA;Identify endangered right whales in aerial photographs;['image data', 'animals', 'water bodies', 'multiclassloss'];364;Right Whale Recognition;Research prediction Competition
2018-02-16 00:59:00;"Innovative materials design is needed to tackle some of the most important health, environmental, energy, social, and economic challenges of this century. In particular, improving the properties of materials that are intrinsically connected to the generation and utilization of energy is crucial if we are to mitigate environmental damage due to a growing global demand. Transparent conductors are an important class of compounds that are both electrically conductive and have a low absorption in the visible range, which are typically competing properties. A combination of both of these characteristics is key for the operation of a variety of technological devices such as photovoltaic cells, light-emitting diodes for flat-panel displays, transistors, sensors, touch screens, and lasers. However, only a small number of compounds are currently known to display both transparency and conductivity suitable enough to be used as transparent conducting materials.  Aluminum (Al), gallium (Ga), indium (In) sesquioxides are some of the most promising transparent conductors because of a combination of both large bandgap energies, which leads to optical transparency over the visible range, and high conductivities. These materials are also chemically stable and relatively inexpensive to produce. Alloying of these binary compounds in ternary or quaternary mixtures could enable the design of a new material at a specific composition with improved properties over what is current possible. These alloys are described by the formula (AlxGayInz)2NO3N; where x, y, and z can vary but are limited by the constraint x+y+z = 1. The total number of atoms in the";https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;Kaggle;Predict the key properties of novel transparent semiconductors;['chemistry', 'mcrmsle'];878;Nomad2018 Predicting Transparent Conductors;Research prediction Competition
2017-09-16 01:59:00;In this competition, Kaggle is challenging you to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables. Longtime Kagglers will recognize that this competition objective is similar to the ECML/PKDD trip time challenge we hosted in 2015. But, this challenge comes with a twist. Instead of awarding prizes to the top finishers on the leaderboard, this playground competition was created to reward collaboration and collective learning.  We are encouraging you (with cash prizes!) to publish additional training data that other participants can use for their predictions. We also have designated bi-weekly and final prizes to reward authors of kernels that are particularly insightful or valuable to the community.;https://www.kaggle.com/c/nyc-taxi-trip-duration;Kaggle;Share code and data to improve ride time predictions;['regression', 'tabular data', 'rmsle'];1,257;New York City Taxi Trip Duration;Playground prediction Competition
2012-07-04 01:59:00;The objective of the competition is to help us build as good a model as possible to predict monthly online sales of a product. Imagine the products are online  self-help programs following an initial advertising campaign. We have shared the data in the comma separated values (CSV) format.  Each row in this data set represents a different consumer product. The first 12 columns (Outcome_M1 through Outcome_M12) contains the monthly online sales for the first 12 months after the product launches.   Date_1 is the day number the major advertising campaign began and the product launched.   Date_2 is the day number the product was announced and a pre-release advertising campaign began. Other columns in the data set are features of the product and the advertising campaign.  Quan_x are quantitative variables and Cat_x are categorical variables. Binary categorical variables are measured as (1) if the product had the feature and (0) if it  did not.;https://www.kaggle.com/c/online-sales;;Predict the online sales of a consumer product based on a data set of product features.;['rmsle'];363;Online Product Sales;Featured prediction Competition
2019-10-02 01:59:00;"Introduction Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:  Object detection track for detecting bounding boxes around object instances, relaunched from 2018. Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018. Instance segmentation track for segmenting masks of objects in images, brand new for 2019.  Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding. Instance Segmentation Track In this track of the Challenge, you are asked to provide segmentation masks of objects. This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.   Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.  The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision. We are excited to partner with Open Images for this second year of competitions, including this brand new track!";https://www.kaggle.com/c/open-images-2019-instance-segmentation;Google Research;Outline segmentation masks of objects in images;['image data', 'custom metric'];193;Open Images 2019 - Instance Segmentation;Research prediction Competition
2019-10-02 01:59:00;Introduction Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:  Object detection track for detecting bounding boxes around object instances, relaunched from 2018. Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018. Instance segmentation track for segmenting masks of objects in images, brand new for 2019.  Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding. Object Detection Track In this track of the Challenge, you are asked to predict a tight bounding box around object instances. The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).   Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.  Please refer to the Open Images 2019 Challenge page for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you. The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision. We are excited to partner with Open Images for this second year of competitions. See link here for last year’s Object Detection competition.;https://www.kaggle.com/c/open-images-2019-object-detection;Google Research;Detect objects in varied and complex images;['image data', 'computer vision', 'custom metric'];558;Open Images 2019 - Object Detection;Research prediction Competition
2019-10-02 01:59:00;"Introduction Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:  Object detection track for detecting bounding boxes around object instances, relaunched from 2018. Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018. Instance segmentation track for segmenting masks of objects in images, brand new for 2019.  Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding. Visual Relationship Track In this track of the Challenge, you are asked to detect pairs of objects and the relationships that connect them. The training set contains 329 relationship triplets with 375k training samples. These include both human-object relationships (e.g. ""woman playing guitar"", ""man holding microphone""), object-object relationships (e.g. ""beer on table"", ""dog inside car""), and also considers object-attribute relationships (e.g.""handbag is made of leather"" and ""bench is wooden"").   Left: Example of ‘man playing guitar’ - Radiofiera - Villa Cordellina Lombardi, Montecchio Maggiore (VI) - agosto 2010 by Andrea Sartorati. Right: Example of ‘chair at table’ - Epic Fireworks - Loads A Room by Epic Fireworks  Please refer to the Open Images 2019 Challenge page for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you. The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision. We are excited to partner with Open Images for this second year of competitions. See link here for last year’s Visual Representation Detection competition.";https://www.kaggle.com/c/open-images-2019-visual-relationship;Google Research;Detect pairs of objects in particular relationships;['image data', 'computer vision', 'custom metric'];201;Open Images 2019 - Visual Relationship;Research prediction Competition
2020-08-14 18:00:00;"Introduction Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year the Open Images  Instance Segmentation competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details.  Participants are also welcome to submit to this playground competition beyond the context of RVC. Instance Segmentation Track In this track of the Challenge, you are asked to provide segmentation masks of objects. This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.   Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.  The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.";https://www.kaggle.com/c/open-images-instance-segmentation-rvc-2020;Google Research;Outline segmentation masks of objects in images;['image data', 'custom metric'];18;Open Images Instance Segmentation RVC 2020 edition;Playground prediction Competition
2020-08-14 18:00:00;Introduction Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year the Open Images Object Detection competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details.  Participants are also welcome to submit to this playground competition beyond the context of RVC. Object Detection Track In this track, you are asked to predict a tight bounding box around object instances. The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).   Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license. The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.;https://www.kaggle.com/c/open-images-object-detection-rvc-2020;Google Research;Detect objects in varied and complex images;['image data', 'custom metric'];89;Open Images Object Detection RVC 2020 edition;Playground prediction Competition
2020-10-07 01:59:00;Imagine one day, your breathing became consistently labored and shallow. Months later you were finally diagnosed with pulmonary fibrosis, a disorder with no known cause and no known cure, created by scarring of the lungs. If that happened to you, you would want to know your prognosis. That’s where a troubling disease becomes frightening for the patient: outcomes can range from long-term stability to rapid deterioration, but doctors aren’t easily able to tell where an individual may fall on that spectrum. Your help, and data science, may be able to aid in this prediction, which would dramatically help both patients and clinicians.  Current methods make fibrotic lung diseases difficult to treat, even with access to a chest CT scan. In addition, the wide range of varied prognoses create issues organizing clinical trials. Finally, patients suffer extreme anxiety—in addition to fibrosis-related symptoms—from the disease’s opaque path of progression. Open Source Imaging Consortium (OSIC) is a not-for-profit, co-operative effort between academia, industry and philanthropy. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis (IPF), fibrosing interstitial lung diseases (ILDs), and other respiratory diseases, including emphysematous conditions. Its mission is to bring together radiologists, clinicians and computational scientists from around the world to improve imaging-based treatments. In this competition, you’ll predict a patient’s severity of decline in lung function based on a CT scan of their lungs. You’ll determine lung function based on output from a spirometer, which measures the volume of air inhaled and exhaled. The challenge is to use machine learning techniques to make a prediction with the image, metadata, and baseline FVC as input. If successful, patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression;Open Source Imaging Consortium (OSIC);Predict lung function decline;['image data', 'healthcare', 'laplaceloglikelihood'];2,097;OSIC Pulmonary Fibrosis Progression;Featured Code Competition
2015-05-19 01:59:00;Get started on this competition through Kaggle Scripts The Otto Group is one of the world’s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate & Barrel (USA), Otto.de (Germany) and 3 Suisses (France). We are selling millions of products worldwide every day, with several thousand products being added to our product line. A consistent analysis of the performance of our products is crucial. However, due to our diverse global infrastructure, many identical products get classified differently. Therefore, the quality of our product analysis depends heavily on the ability to accurately cluster similar products. The better the classification, the more insights we can generate about our product range.  For this competition, we have provided a dataset with 93 features for more than 200,000 products. The objective is to build a predictive model which is able to distinguish between our main product categories. The winning models will be open sourced.;https://www.kaggle.com/c/otto-group-product-classification-challenge;;Classify products into the correct category;['internet', 'tabular data', 'multiclassloss'];3,505;Otto Group Product Classification Challenge;Featured prediction Competition
2017-01-19 00:59:00;The internet is a stimulating treasure trove of possibility. Every day we stumble on news stories relevant to our communities or experience the serendipity of finding an article covering our next travel destination. Outbrain, the web’s leading content discovery platform, delivers these moments while we surf our favorite sites.  Currently, Outbrain pairs relevant content with curious readers in about 250 billion personalized recommendations every month across many thousands of sites. In this competition, Kagglers are challenged to predict which pieces of content its global base of users are likely to click on. Improving Outbrain’s recommendation algorithm will mean more users uncover stories that satisfy their individual tastes.;https://www.kaggle.com/c/outbrain-click-prediction;Outbrain;Can you predict which recommended content each user will click?;['internet', 'tabular data', 'map@{k}'];978;Outbrain Click Prediction;Featured prediction Competition
2011-05-15 02:00:00;One of the main objectives of predictive modelling is to build a model that will give accurate predictions on unseen data.A necessary step in the building of models is to ensure that they have not overfit the training data, which leads to sub optimal predictions on new data.The purpose of this challenge is to stimulate research and highlight existing algorithms, techniques or strategies that can be used to guard against overfitting.In order to achieve this we have created a simulated data set with 200 variables and 20,000 cases. An ‘equation’ based on this data was created in order to generate a Target to be predicted. Given the all 20,000 cases, the problem is very easy to solve – but you only get given the Target value of 250 cases – the task is to build a model that gives the best predictions on the remaining 19,750 cases.This competition is of particular relevance to medical data analysis, where often the number of cases is severely restricted.;https://www.kaggle.com/c/overfitting;;With nearly as many variables as training cases, what are the best techniques to avoid disaster?;['auc'];259;Don't Overfit!;Featured prediction Competition
2014-01-27 00:59:00;It's that time of year again, when Santa and his helpers gear up for their big night. Last year's path recommendations were such a success that Santa is back for more. As the latest in a line of many data science converts, Santa is looking to you to help pack his sleigh.      Problem Description Given a list of presents, pack them in Santa's sleigh as compactly as possible and in the best order possible. The sleigh and presents are discretized and described in units of the fundamental length unit ℓℓ. The sleigh is 1000 x 1000 with infinite vertical extent as needed by your highest placed present. The cells of the sleigh go from 1 to 1000 for the length and width, and 1 to infinity in height. Presents come in random sizes and are represented by their extent in the x, y, and z dimensions. Each present has  PresentId,Dimension1,Dimension2,Dimension31,2,5,32,243,207,73 Present 1 is 2 x 5 x 3 ℓ3ℓ3 and Present 2 is 243 x 207 x 73 ℓ3ℓ3. Presents can be packed in any orientation provided they are parallel and perpendicular to the x-y-z axes, meaning they can be rotated in any direction by multiples of 90∘∘ but not, for example, by 60∘∘.   Please see the evaluation page to learn how your packing configurations will be scored and for the submission file schema. Acknowledgements This competition is brought to you by MathWorks, creators of MATLAB® and Simulink®. Learn more about MathWorks.;https://www.kaggle.com/c/packing-santas-sleigh;The MathWorks;"He's making a list, checking it twice; to fill up his sleigh, he needs your advice";['custom metric'];361;Packing Santa's Sleigh;Featured prediction Competition
2016-11-01 00:59:00;With an original Picasso carrying a 106 million dollar price tag, identifying an authentic work of art from a forgery is a high-stakes industry. While algorithms have gotten good at telling us if a still life is of a basket of apples or a sunflower bouquet, they aren't yet able to tell us with certainty if both paintings are by van Gogh.   In this playground competition, we're challenging Kagglers to examine pairs of paintings and determine if they are by the same artist. This is an excellent opportunity to improve your computer vision skills and engage with a unique dataset of art. From the movement of brushstrokes to the use of light and dark, successful algorithms will likely incorporate many aspects of a painter's unique style.  Resources  neural algorithm How Do We See Art: An Eye-Tracker Study  Acknowledgments Many of the images in this dataset were obtained from wikiart.org. Additional paintings were provided by artists whose contributions will be acknowledged at the close of the competition. This playground competition and its datasets were prepared by Small Yellow Duck (Kiri Nichol). This includes the design of the pairwise-evaluation scheme.;https://www.kaggle.com/c/painter-by-numbers;Kaggle;Does every painter leave a fingerprint?;['image data', 'auc'];41;Painter by Numbers;Playground prediction Competition
2014-04-01 06:00:00;The goal of PAKDD 2014 competition is to predict future malfunctional components of ASUS notebooks from historical data. This will help estimate how many products will require maintenance or repair services. ASUS has provided information on its laptop shipments as well as the laptops requiring maintenance or repair services. Participants will use this information to estimate how many of each module of a specific model will require maintenance or repair services. Acknowledgements The organizers of PAKDD would like to thank ASUS for sponsorship of this competition.;https://www.kaggle.com/c/pakdd-cup-2014;;Predict malfunctional components of ASUS notebooks;['mae'];607;PAKDD 2014 - ASUS Malfunctional Components Prediction;Research prediction Competition
2017-12-16 00:59:00;While long lines and frantically shuffling luggage into plastic bins isn’t a fun experience, airport security is a critical and necessary requirement for safe travel. No one understands the need for both thorough security screenings and short wait times more than U.S. Transportation Security Administration (TSA). They’re responsible for all U.S. airport security, screening more than two million passengers daily. As part of their Apex Screening at Speed Program, DHS has identified high false alarm rates as creating significant bottlenecks at the airport checkpoints. Whenever TSA’s sensors and algorithms predict a potential threat, TSA staff needs to engage in a secondary, manual screening process that slows everything down. And as the number of travelers increase every year and new threats develop, their prediction algorithms need to continually improve to meet the increased demand. Currently, TSA purchases updated algorithms exclusively from the manufacturers of the scanning equipment used. These algorithms are proprietary, expensive, and often released in long cycles. In this competition, TSA is stepping outside their established procurement process and is challenging the broader data science community to help improve the accuracy of their threat prediction algorithms. Using a dataset of images collected on the latest generation of scanners, participants are challenged to identify the presence of simulated threats under a variety of object types, clothing types, and body types. Even a modest decrease in false alarms will help TSA significantly improve the passenger experience while maintaining high levels of security. This is a two-stage competition. Please read our two-stage FAQs to understand more about what this means. All persons contained in the dataset are volunteers who have agreed to have their images used for this competition. The images may contain sensitive content. We kindly request that you conduct yourself with professionalism, respect, and maturity when working with this data.;https://www.kaggle.com/c/passenger-screening-algorithm-challenge;Department of Homeland Security;Improve the accuracy of the Department of Homeland Security's threat recognition algorithms;['image data', 'logloss'];518;Passenger Screening Algorithm Challenge;Featured prediction Competition
2019-04-10 03:09:00;Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. If homes can be found for them, many precious lives can be saved — and more happy families created. PetFinder.my has been Malaysia’s leading animal welfare platform since 2008, with a database of more than 150,000 animals. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare. Animal adoption rates are strongly correlated to the metadata associated with their online profiles, such as descriptive text and photo characteristics. As one example, PetFinder is currently experimenting with a simple AI tool called the Cuteness Meter, which ranks how cute a pet is based on qualities present in their photos. In this competition you will be developing algorithms to predict the adoptability of pets - specifically, how quickly is a pet adopted? If successful, they will be adapted into AI tools that will guide shelters and rescuers around the world on improving their pet profiles' appeal, reducing animal suffering and euthanization. Top participants may be invited to collaborate on implementing their solutions into AI tools for assessing and improving pet adoption performance, which will benefit global animal welfare.  Important Note Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output.      Photo by Krista Mangulsone on Unsplash;https://www.kaggle.com/c/petfinder-adoption-prediction;PetFinder.my;How cute is that doggy in the shelter?;['image data', 'text data', 'quadraticweightedkappa'];2,023;PetFinder.my Adoption Prediction;Featured Code Competition
2012-07-01 01:59:00;Get the data »  Practice Fusion is America's fastest growing Electronic Health Record (EHR) community, with more than 170,000 medical professional users treating 34 million patients in all 50 states. Practice Fusion’s EHR-driven research dataset is used to detect disease outbreaks, identify dangerous drug interactions and compare the effectiveness of competing treatments. In partnership with Kaggle, Practice Fusion is releasing 10,000 de-identified, HIPAA-compliant medical records to spur innovation into new uses of clinical data to improve public health and patient care. This dataset is one of the largest and richest sources of medical record data ever released and includes information on diagnoses, lab results, medications, allergies, immunizations, vital signs, and health behavior. Practice Fusion’s past data challenges have spawned a range of creative visualizations, applications, analyses, and even a few start-ups. In this year’s Analyze This!, Practice Fusion and Kaggle call on an ever-growing community of developers, designers, and data scientists interested in solving our nation’s most stubborn healthcare problems by tackling our two data challenges: the Prediction Challenge and the Open Challenge.    Prediction Challenge The first phase of the Prediction Challenge is  Kaggle Prospect in which you submit ideas for the best prediction problem! The Kaggle community is invited to vote on the most interesting and promising ideas. A panel of judges will select a winner from the top-voted 10 ideas for the second phase, a predictive modeling competition based on the winning idea. Kaggle Prospect submissions and voting will be available from the launch of Analyze This! to June 30. The predictive modeling phase of the Prediction Challenge will start no later than July 9, and end on Monday, September 10. The predictive modeling competition is now underway and can be found here.  Open Challenge Get the dataset and show us what you can find! Combine the Analyze This! dataset with one or more datasets from  www.data.gov. Use the mash-up however you like: map chronic disease across the country, create a personal health app, or a tool for running clinical trials. Share your results and analyses online! Submissions for the Open Challenge will be accepted from the launch of Analyze This! to Monday, September 10.;https://www.kaggle.com/c/pf2012;;Start digging into electronic health records and submit your ideas for the most promising, impactful or interesting predictive modeling competitions;['rmse'];;Practice Fusion Analyze This! 2012 - Prediction Challenge;Prospect prediction Competition
2012-09-11 01:59:59;Get the data »  Practice Fusion is America's fastest growing Electronic Health Record (EHR) community, with more than 170,000 medical professional users treating 34 million patients in all 50 states. Practice Fusion’s EHR-driven research dataset is used to detect disease outbreaks, identify dangerous drug interactions and compare the effectiveness of competing treatments. In partnership with Kaggle, Practice Fusion is releasing 10,000 de-identified, HIPAA-compliant medical records to spur innovation into new uses of clinical data to improve public health and patient care. This dataset is one of the largest and richest sources of medical record data ever released and includes information on diagnoses, lab results, medications, allergies, immunizations, vital signs, and health behavior. Practice Fusion’s past data challenges have spawned a range of creative visualizations, applications, analyses, and even a few start-ups. In this year’s Analyze This!, Practice Fusion and Kaggle call on an ever-growing community of developers, designers, and data scientists interested in solving our nation’s most stubborn healthcare problems by tackling our two data challenges: the Prediction Challenge and the Open Challenge.    Open Challenge   Get the dataset and show us what you can find! Combine the Analyze This! dataset with one or more datasets from  www.data.gov. Use the mash-up however you like: map chronic disease across the country, create a personal health app, or a tool for running clinical trials. Share your results and analyses online! Submissions for the Open Challenge will be accepted from the launch of Analyze This! to Monday, September 10.;https://www.kaggle.com/c/pf2012-at;;Start digging into electronic health records and submit your creative, insightful, and visually striking analyses.;['rmse'];;Practice Fusion Analyze This! 2012 - Open Challenge;Prospect prediction Competition
2012-09-11 01:59:59;In the first phase of this prediction challenge  Practice Fusion invited anyone with an interest in using electronic medical record data to improve public health to submit and vote on ideas for prediction problems based on a new dataset of 10,000 de-identified medical records. The votes are in and Shea Parkes' top voted submission has won. Practice Fusion is now sponsoring the second and final phase of the challenge inspired by the winning problem: Identify patients diagnosed with Type 2 Diabetes Mellitus. Over 25 million people, or nearly 8.3% of the entire United States population, have diabetes. Diabetes is also associated with a wide range of complications from heart disease and stroke to blindness and kidney disease. Predicting who has diabetes will lead to a better understanding of these complications and the common comorbidities that diabetics suffer. The Challenge: Given a de-identified data set of patient electronic health records, build a model to determine who has a diabetes diagnosis, as defined by ICD9 codes 250, 250.0, 250.*0 or 250.*2 (e.g., 250, 250.0, 250.00, 250.10, 250.52, etc).;https://www.kaggle.com/c/pf2012-diabetes;;Identify patients diagnosed with Type 2 Diabetes;['logloss'];145;Practice Fusion Diabetes Classification;Research prediction Competition
2011-11-21 00:59:59;Background We have a large collection of user-generated photos. We want to automatically pick out particularly enjoyable or impressive ones to highlight, especially travel-related, using only the meta-data associated with the images such as caption text, image dimensions  and approximate location in the world. We know from our preliminary experiments and intuition that certain words and places are correlated with good photos, and others are indicators of less enjoyable pictures, but we would like to develop an algorithm to  tie together these multiple signals. Objective Given anonymized information on thousands of photo albums, predict whether a human evaluator would mark them as 'good'.;https://www.kaggle.com/c/PhotoQualityPrediction;;Given anonymized information on thousands of photo albums, predict whether a human evaluator would mark them as 'good'.;['custom metric'];200;Photo Quality Prediction;Featured prediction Competition
2015-07-02 01:59:00;The taxi industry is evolving rapidly. New competitors and technologies are changing the way traditional taxi services do business. While this evolution has created new efficiencies, it has also created new problems.  One major shift is the widespread adoption of electronic dispatch systems that have replaced the VHF-radio dispatch systems of times past. These mobile data terminals are installed in each vehicle and typically provide information on GPS localization and taximeter state. Electronic dispatch systems make it easy to see where a taxi has been, but not necessarily where it is going. In most cases, taxi drivers operating with an electronic dispatch system do not indicate the final destination of their current ride.  Another recent change is the switch from broadcast-based (one to many) radio messages for service dispatching to unicast-based (one to one) messages. With unicast-messages, the dispatcher needs to correctly identify which taxi they should dispatch to a pick up location. Since taxis using electronic dispatch systems do not usually enter their drop off location, it is extremely difficult for dispatchers to know which taxi to contact.  To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict the final destination of a taxi while it is in service. Particularly during periods of high demand, there is often a taxi whose current ride will end near or exactly at a requested pick up location from a new rider. If a dispatcher knew approximately where their taxi drivers would be ending their current rides, they would be able to identify which taxi to assign to each pickup request. The spatial trajectory of an occupied taxi could provide some hints as to where it is going. Similarly, given the taxi id, it might be possible to predict its final destination based on the regularity of pre-hired services. In a significant number of taxi rides (approximately 25%), the taxi has been called through the taxi call-center, and the passenger’s telephone id can be used to narrow the destination prediction based on historical ride data connected to their telephone id. In this challenge, we ask you to build a predictive framework that is able to infer the final destination of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the final trip's destination (WGS84 coordinates). This is the first of two data science challenges that share the same dataset. The Taxi Service Trip Time competition predicts the total time of taxi rides. This competition is affiliated with the organization of ECML/PKDD 2015.;https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;;Predict the destination of taxi trips based on initial partial trajectories;['tabular data', 'ahd@{type}'];381;ECML/PKDD 15: Taxi Trajectory Prediction (I);Research prediction Competition
2015-07-02 01:59:00;This is the second of two data science challenges that share the same dataset. The Taxi Service Trajectory competition predicts the final destination of taxi trips.   To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict how long a driver will have his taxi occupied. If a dispatcher knew approximately when a taxi driver would be ending their current ride, they would be better able to identify which driver to assign to each pickup request.  In this challenge, we ask you to build a predictive framework that is able to infer the trip time of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the travel time of a particular taxi trip. This competition is affiliated with the organization of ECML/PKDD 2015.;https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;;Predict the total travel time of taxi trips based on their initial partial trajectories;['tabular data', 'rmsle'];345;ECML/PKDD 15: Taxi Trip Time Prediction (II);Research prediction Competition
2020-01-22 00:59:00;Who do you think hates traffic more - humans or self-driving cars? The position of nearby automobiles is a key question for autonomous vehicles ― and it's at the heart of our newest challenge.  Self-driving cars have come a long way in recent years, but they're still not flawless. Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles’ ability to accurately perceive objects in traffic. Baidu's Robotics and Autonomous Driving Lab (RAL), along with Peking University, hopes to close the gap once and for all with this challenge. They’re providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images, based on industry-grade CAD car models. Your challenge: develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environment. Succeed and you'll help improve computer vision. That, in turn, will bring autonomous vehicles a big step closer to widespread adoption, so they can help reduce the environmental impact of our growing societies.  Please cite the following paper when using the dataset: ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving @inproceedings{song2019apollocar3d,   title={Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving},   author={Song, Xibin and Wang, Peng and Zhou, Dingfu and Zhu, Rui and Guan, Chenye and Dai, Yuchao and Su, Hao and Li, Hongdong and Yang, Ruigang},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   pages={5452--5462},   year={2019} };https://www.kaggle.com/c/pku-autonomous-driving;Peking University;Can you predict vehicle angle in different settings?;['image data', 'computer vision', 'custom metric'];864;Peking University/Baidu - Autonomous Driving;Featured prediction Competition
2017-07-21 01:59:00;Every minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively. Planet, designer and builder of the world’s largest constellation of Earth-imaging satellites, will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter resolution. While considerable research has been devoted to tracking changes in forests, it typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250 meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest degradation dominate. Furthermore, these existing methods generally cannot differentiate between human causes of forest loss and natural causes. Higher resolution imagery has already been shown to be exceptionally good at this, but robust methods have not yet been developed for Planet imagery.  In this competition, Planet and its Brazilian partner SCCON are challenging Kagglers to label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond. To dig into/explore more Planet data, sign up for a free account. And if you're interested in building applications on Planet data, check out our Application Developer Program. Getting Started   Review the data page, which includes detailed information about the labels and the labeling process. Download a subsample of the data to get familiar with how it looks. Explore the subsample on Kernels. We’ve created a notebook for you to get started.;https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;Planet;Use satellite data to track the human footprint in the Amazon rainforest;['image data', 'forestry', 'meanfscorebeta'];938;Planet: Understanding the Amazon from Space;Featured prediction Competition
2020-05-27 01:59:00;"Problem Statement Misdiagnosis of the many diseases impacting agricultural crops can lead to misuse of chemicals leading to the emergence of resistant pathogen strains, increased input costs, and more outbreaks with significant economic loss and environmental impacts. Current disease diagnosis based on human scouting is time-consuming and expensive, and although computer-vision based models have the promise to increase efficiency, the great variance in symptoms due to age of infected tissues, genetic variations, and light conditions within trees decreases the accuracy of detection.  Specific Objectives Objectives of ‘Plant Pathology Challenge’ are to train a model using images of training dataset to 1) Accurately classify a given image from testing dataset into different diseased category or a healthy leaf; 2) Accurately distinguish between many diseases, sometimes more than one on a single leaf; 3) Deal with rare classes and novel symptoms; 4) Address depth perception—angle, light, shade, physiological age of the leaf; and 5) Incorporate expert knowledge in identification, annotation, quantification, and guiding computer vision to search for relevant features during learning.  Resources If you use the dataset for your project, please cite the preprint https://arxiv.org/abs/2004.11958 Acknowledgments We acknowledge financial support from Cornell Initiative for Digital Agriculture (CIDA) and special thanks to Zach Guillian for help with data collection.    Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.";https://www.kaggle.com/c/plant-pathology-2020-fgvc7;Fine-Grained Visual Categorization 7;Identify the category of foliar diseases in apple trees;['image data', 'agriculture', 'mcauc'];1,317;Plant Pathology 2020 - FGVC7;Research prediction Competition
2018-03-13 00:59:00;Can you differentiate a weed from a crop seedling? The ability to do so effectively can mean better crop yields and better stewardship of the environment. The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.  We're hosting this dataset as a Kaggle competition in order to give it wider exposure, to give the community an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas. Acknowledgments We extend our appreciation to the Aarhus University Department of Engineering Signal Processing Group for hosting the original data.  Citation A Public Image Database for Benchmark of Plant Seedling Classification Algorithms;https://www.kaggle.com/c/plant-seedlings-classification;Kaggle;Determine the species of a seedling from an image;['image data', 'multiclass classification', 'plants', 'meanfscore'];834;Plant Seedlings Classification;Playground prediction Competition
2018-12-18 00:59:00;Help some of the world's leading astronomers grasp the deepest properties of the universe. The human eye has been the arbiter for the classification of astronomical sources in the night sky for hundreds of years. But a new facility -- the Large Synoptic Survey Telescope (LSST) -- is about to revolutionize the field, discovering 10 to 100 times more astronomical sources that vary in the night sky than we've ever known. Some of these sources will be completely unprecedented! The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) asks Kagglers to help prepare to classify the data from this new survey. Competitors will classify astronomical sources that vary with time into different classes, scaling from a small training set to a very large test set of the type the LSST will discover.  More background information is available here.  Acknowledgements  PLAsTiCC is funded through LSST Corporation Grant Award # 2017-03 and administered by the University of Toronto.  Financial support for LSST comes from the National Science Foundation (NSF) through Cooperative Agreement No. 1258333, the Department of Energy (DOE) Office of Science under Contract No. DE-AC02-76SF00515, and private funding raised by the LSST Corporation. The NSF-funded LSST Project Office for construction was established as an operating center under management of the Association of Universities for Research in Astronomy (AURA).  The DOE-funded effort to build the LSST camera is managed by the SLAC National Accelerator Laboratory (SLAC). The National Science Foundation (NSF) is an independent federal agency created by Congress in 1950 to promote the progress of science. NSF supports basic research and people to create knowledge that transforms the future.        Photo Credit: M. Park/Inigo Films/LSST/AURA/NSF;https://www.kaggle.com/c/PLAsTiCC-2018;LSST Project;Can you help make sense of the Universe?;['tabular data', 'astronomy', 'weightedmulticlassloss'];1,094;PLAsTiCC Astronomical Classification;Featured prediction Competition
2015-06-02 01:59:00;"Your friend bailed last minute on poker night? Before giving up on a much-needed evening of bad bluffs and quarter buy ins, light a cigar and get familiar with the rules of the game. Each record in this competition consists of five playing cards and an attribute representing the poker hand. You are asked to predict the best hand you can play based on the cards you've been dealt.   The order of cards is important, which means there are 480 possible Royal Flush hands instead of just four. Identify those, and the other 311,875,200 possible hands correctly, and you’re in the money! ""Isn't this easy? I know two-of-a-kind when I see it"", you might rightfully wonder. And you'd be right.  The intent of this challenge is automatic rules induction, i.e. to learn the rules using machine learning, without hand coding heuristics. Pretend you are in a foreign land, have never played the game before, are given a history of thousands of games, and are asked to come up with the rules. It is potentially difficult to discover rules that can correctly classify poker hands, yet it is trivial for a human to validate the rules objectively. Remember, your algorithm will need to find rules that are general enough to be broadly useful, without being so broad that they end up being occasionally wrong. We suggest reading the paper by Cattral et al. for more background on the topic. Playground competitions are an opportunity to build and stretch your machine learning muscles. Pull up a chair to the data science poker table and ante up. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was created by Robert Cattral and Franz Oppacher. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science";https://www.kaggle.com/c/poker-rule-induction;Kaggle;Determine the poker hand of five playing cards;['multiclass classification', 'tabular data', 'card games', 'categorizationaccuracy'];207;Poker Rule Induction;Playground prediction Competition
2017-11-30 00:59:00;Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years. Porto Seguro, one of Brazil’s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones. In this competition, you’re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year. While Porto Seguro has used machine learning for the past 20 years, they’re looking to Kaggle’s machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers.;https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;Porto Seguro;Predict if a driver will file an insurance claim next year.;['binary classification', 'tabular data', 'normalizedgini'];5,163;Porto Seguro’s Safe Driver Prediction;Featured prediction Competition
2012-11-04 00:59:00;"This competition is now complete. Congratulations to the  winners!  Millions of programmers use Stack Overflow to get high quality answers to their programming questions every day.  We take quality very seriously, and have evolved an effective culture of moderation to safe-guard it. With more than six thousand new questions asked on Stack Overflow every weekday we're looking to add more sophisticated software solutions to our moderation toolbox. Closing Questions Currently about 6% of all new questions end up ""closed"".  Questions can be closed as off topic, not constructive, not a real question, or  too localized.  More in depth descriptions of each reason can be found in the Stack Overflow FAQ.  The exact duplicate close reason has been excluded from this contest, since it depends on previous questions. Your goal is to build a classifier that predicts whether or not a question will be closed given the question as submitted, along with the reason that the question was closed.  Additional  data about the user at question creation time is also available.";https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow;Stack Overflow;Predict which new questions asked on Stack Overflow will be closed;['custom metric'];161;Predict Closed Questions on Stack Overflow;Featured prediction Competition
2015-06-18 01:59:00;West Nile virus is most commonly spread to humans through infected mosquitos. Around 20% of people who become infected with the virus develop symptoms ranging from a persistent fever, to serious neurological illnesses that can result in death.  In 2002, the first human cases of West Nile virus were reported in Chicago. By 2004 the City of Chicago and the Chicago Department of Public Health (CDPH) had established a comprehensive surveillance and control program that is still in effect today. Every week from late spring through the fall, mosquitos in traps across the city are tested for the virus. The results of these tests influence when and where the city will spray airborne pesticides to control adult mosquito populations. Given weather, location, testing, and spraying data, this competition asks you to predict when and where different species of mosquitos will test positive for West Nile virus. A more accurate method of predicting outbreaks of West Nile virus in mosquitos will help the City of Chicago and CPHD more efficiently and effectively allocate resources towards preventing transmission of this potentially deadly virus.  We've jump-started your analysis with some visualizations and starter code in R and Python on Kaggle Scripts. No data download or local environment setup needed!  Acknowledgements  This competition is sponsored by the Robert Wood Johnson Foundation. Data is provided by the Chicago Department of Public Health.;https://www.kaggle.com/c/predict-west-nile-virus;;Predict West Nile virus in mosquitos across the city of Chicago;['binary classification', 'tabular data', 'auc'];1,304;West Nile Virus Prediction;Featured prediction Competition
2013-04-14 14:00:00;Data Science London and the UK Windows Azure Users Group in partnership with Microsoft and Peerindex, announce the Influencers in Social Networks competition as part of  The Big Data Hackathon.   The dataset, provided by Peerindex, comprises a standard, pair-wise preference learning task. Each datapoint describes two individuals, A and B. For each person, 11 p The binary label represents a human judgement about which one of the two individuals is more influential. A label '1' means A is more influential than B. 0 means B is more influential than A. The goal of the challenge is to train a machine learning model which, for pairs of individuals, predicts the human judgement on who is more influential with high accuracy. Labels for the dataset have been collected by PeerIndex using an application similar to the one described in this post. A python script computing a sample benchmark solution is available here:  Competition begins: Saturday, Apr 13, 1pm BST (12 noon UTC)  This competition awards 25% the  ranking points of a standard competition, but does not count towards tiers.;https://www.kaggle.com/c/predict-who-is-more-influential-in-a-social-network;;Predict which people are influential in a social network;['auc'];132;Influencers in Social Networks;Featured prediction Competition
2012-09-08 01:59:00;"Splunk Innovation Prize now OPEN »   Announced at the GigaOM Structure Conference, powered by Splunk, and using data from WordPress.com, this competition is about predicting which people will ""like"" which blog posts from across 90k active blogs  on WordPress.com.  WordPress.com  hosts about half of the 74 million WordPress sites in the world (over 16% of all domains on the web). The winning solutions may be used by WordPress.com  in a recommendation engine, but winning solutions must be open-sourced, so they could be used by anyone to solve a similar problem using similar data in a similar domain. Competition winners will be announced in September at  GigaOM Mobilize. About Splunk: Splunk Inc. provides the engine for machine data™. Splunk software enables organizations to monitor, search, analyze, visualize and act on massive streams of real-time and historical  machine data.  Splunk has donated access to a Splunk server containing the entire WordPress dataset for you to explore, visualize and experiment. When you accept the rules for the competition, you will automatically be sent a personal login to the Splunk  server. There is also a 5K companion competition to the predictive modeling challenge. The Splunk Innovation Prize will be awarded for the most innovative use of Splunk for data science (using the competition dataset). Never used Splunk before?  Here are some tutorials to get you started.   Getting started videos   Search Reference guide (explains the  search language)   About Gigaom: GigaOM is one of the most credible and insightful voices at the intersection of business and technology, with an online audience of more than 5.5 million monthly unique visitors, industry-leading  events, and a pioneering research service and digital community, GigaOM Pro, which provides expert analysis on emerging technology markets.";https://www.kaggle.com/c/predict-wordpress-likes;;Predict which blog posts someone will like.;['custom metric'];75;GigaOM WordPress Challenge: Splunk Innovation Prospect;Prospect prediction Competition
2013-03-27 04:59:00;Prizes will be given in the form of cash grants, with no strings attached. Prizes include:  A grand prize of $10,000 for the team judged to have the most impactful use of the data.;https://www.kaggle.com/c/predicting-parkinson-s-disease-progression-with-smartphone-data;;Can we objectively measure the symptoms of Parkinson’s disease with a smartphone? We have the data to find out!;['rmse'];;Predicting Parkinson's Disease Progression with Smartphone Data;Research prediction Competition
2016-09-20 01:59:00;Like most companies, Red Hat is able to gather a great deal of information over time about the behavior of individuals who interact with them. They’re in search of better methods of using this behavioral data to predict which individuals they should approach—and even when and how to approach them. In this competition, Kagglers are challenged to create a classification algorithm that accurately identifies which customers have the most potential business value for Red Hat based on their characteristics and activities. With an improved prediction model in place, Red Hat will be able to more efficiently prioritize resources to generate more business and better serve their customers.;https://www.kaggle.com/c/predicting-red-hat-business-value;;Classify customer potential;['business', 'tabular data', 'auc'];2,260;Predicting Red Hat Business Value;Featured prediction Competition
2020-07-23 01:59:00;With more than 1 million new diagnoses reported every year, prostate cancer (PCa) is the second most common cancer among males worldwide that results in more than 350,000 deaths annually. The key to decreasing mortality is developing more precise diagnostics. Diagnosis of PCa is based on the grading of prostate tissue biopsies. These tissue samples are examined by a pathologist and scored according to the Gleason grading system. In this challenge, you will develop models for detecting PCa on images of prostate tissue samples, and estimate severity of the disease using the most extensive multi-center dataset on Gleason grading yet available. The grading process consists of finding and classifying cancer tissue into so-called Gleason patterns (3, 4, or 5) based on the architectural growth patterns of the tumor (Fig. 1). After the biopsy is assigned a Gleason score, it is converted into an ISUP grade on a 1-5 scale. The Gleason grading system is the most important prognostic marker for PCa, and the ISUP grade has a crucial role when deciding how a patient should be treated. There is both a risk of missing cancers and a large risk of overgrading resulting in unnecessary treatment. However, the system suffers from significant inter-observer variability between pathologists, limiting its usefulness for individual patients. This variability in ratings could lead to unnecessary treatment, or worse, missing a severe diagnosis.  Automated deep learning systems have shown some promise in accurately grading PCa. Recent research, including two studies independently conducted by the groups hosting this challenge, have shown that these systems can achieve pathologist-level performance. However, these systems/results were not tested with multi-center datasets at scale.     Your work here will improve on these efforts using the most extensive multi-center dataset on Gleason grading yet. The training set consists of around 11,000 whole-slide images of digitized H&E-stained biopsies originating from two centers. This is the largest public whole-slide image dataset available, roughly 8 times the size of the CAMELYON17 challenge, one of the largest digital pathology datasets and best known challenges in the field. Furthermore, in contrast to previous challenges, we are making full diagnostic biopsy images available. Using a sizable multi-center test set, graded by expert uro-pathologists, we will evaluate challenge submissions on their applicability to improve this critical diagnostic function.   Figure 1: An illustration of the Gleason grading process for an example biopsy containing prostate cancer. The most common (blue outline, Gleason pattern 3) and second most common (red outline, Gleason pattern 4) cancer growth patterns present in the biopsy dictate the Gleason score (3+4 for this biopsy), which in turn is converted into an ISUP grade (2 for this biopsy) following guidelines of the International Society of Urological Pathology. Biopsies not containing cancer are represented by an ISUP grade of 0 in this challenge. Radboud University Medical Center and Karolinska Institute have teamed up to organize this competition in collaboration with colleagues from Tampere University. The Computational Pathology Group (CPG) of the Radboud University Medical Center is a research group that develops computer algorithms to aid clinicians. Karolinska Institute’s Department of Medical Epidemiology and Biostatistics (MEB) includes an interdisciplinary research group to improve the diagnostics and treatment of prostate cancer. Together, they hope to further their existing research to make a significant impact on the healthcare of prostate cancer patients. Challenge organizer team: Wouter Bulten, Geert Litjens, Hans Pinckaers, Peter Ström, Martin Eklund, Lars Egevad, Henrik Grönberg, Kimmo Kartasalo, Pekka Ruusuvuori, Tomi Häkkinen, Sohier Dane, Maggie Demkin.  Sponsors The PANDA workshop at MICCAI 2020 is sponsored by ContextVision, Ibex and Google.     Using the data outside of the competition Interested in using the PANDA dataset outside of the competition? Please read this forum post for the latest information on the embargo and the challenge paper.;https://www.kaggle.com/c/prostate-cancer-grade-assessment;PANDA Challenge;Prostate cancer diagnosis using the Gleason grading system;['image data', 'medicine', 'quadraticweightedkappa'];1,010;Prostate cANcer graDe Assessment (PANDA) Challenge;Featured Code Competition
2016-02-16 00:59:00;Picture this. You are a data scientist in a start-up culture with the potential to have a very large impact on the business. Oh, and you are backed up by a company with 140 years' business experience. Curious? Great! You are the kind of person we are looking for. Prudential, one of the largest issuers of life insurance in the USA, is hiring passionate data scientists to join a newly-formed Data Science group solving complex challenges and identifying opportunities. The results have been impressive so far but we want more.  The Challenge In a one-click shopping world with on-demand everything, the life insurance application process is antiquated. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams, a process that takes an average of 30 days. The result? People are turned off. That’s why only 40% of U.S. households own individual life insurance. Prudential wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries. By developing a predictive model that accurately classifies risk using a more automated approach, you can greatly impact public perception of the industry. The results will help Prudential better understand the predictive power of the data points in the existing assessment, enabling us to significantly streamline the process.;https://www.kaggle.com/c/prudential-life-insurance-assessment;;Can you make buying life insurance easier?;['tabular data', 'quadraticweightedkappa'];2,610;Prudential Life Insurance Assessment;Featured prediction Competition
2019-01-31 00:59:00;"So, where we droppin' boys and girls? Battle Royale-style video games have taken the world by storm. 100 players are dropped onto an island empty-handed and must explore, scavenge, and eliminate other players until only one is left standing, all while the play zone continues to shrink.  PlayerUnknown's BattleGrounds (PUBG) has enjoyed massive popularity. With over 50 million copies sold, it's the fifth best selling game of all time, and has millions of active monthly players.   The team at PUBG has made official game data available for the public to explore and scavenge outside of ""The Blue Circle."" This competition is not an official or affiliated PUBG site - Kaggle collected data made possible through the PUBG Developer API. You are given over 65,000 games' worth of anonymized player data, split into training and testing sets, and asked to predict final placement from final in-game stats and initial player ratings.  What's the best strategy to win in PUBG? Should you sit in one spot and hide your way into victory, or do you need to be the top shot? Let's let the data do the talking!";https://www.kaggle.com/c/pubg-finish-placement-prediction;Kaggle;Can you predict the battle royale finish of PUBG Players?;['video games', 'tabular data', 'mae'];1,534;PUBG Finish Placement Prediction (Kernels Only);Playground Code Competition
2015-08-05 01:59:00;"This competition is now complete. Congratulations to the  winners!  Millions of programmers use Stack Overflow to get high quality answers to their programming questions every day.  We take quality very seriously, and have evolved an effective culture of moderation to safe-guard it. With more than six thousand new questions asked on Stack Overflow every weekday we're looking to add more sophisticated software solutions to our moderation toolbox. Closing Questions Currently about 6% of all new questions end up ""closed"".  Questions can be closed as off topic, not constructive, not a real question, or  too localized.  More in depth descriptions of each reason can be found in the Stack Overflow FAQ.  The exact duplicate close reason has been excluded from this contest, since it depends on previous questions. Your goal is to build a classifier that predicts whether or not a question will be closed given the question as submitted, along with the reason that the question was closed.  Additional data about the user at question creation time is also available.";https://www.kaggle.com/c/pycon-2015-tutorial-predict-closed-questions-on-stack-overflow;;Predict which new questions asked on Stack Overflow will be closed;['custom metric'];;Predict Closed Questions on Stack Overflow;Playground prediction Competition
2018-12-05 00:59:00;"""Quick, Draw!"" was released as an experimental game to educate the public in a playful way about how AI works. The game prompts users to draw an image depicting a certain category, such as ”banana,” “table,” etc. The game generated more than 1B drawings, of which a subset was publicly released as the basis for this competition’s training set. That subset contains 50M drawings encompassing 340 label categories. Sounds fun, right? Here's the challenge: since the training data comes from the game itself, drawings can be incomplete or may not match the label. You’ll need to build a recognizer that can effectively learn from this noisy data and perform well on a manually-labeled test set from a different distribution. Your task is to build a better classifier for the existing Quick, Draw! dataset. By advancing models on this dataset, Kagglers can improve pattern recognition solutions more broadly. This will have an immediate impact on handwriting recognition and its robust applications in areas including OCR (Optical Character Recognition), ASR (Automatic Speech Recognition) & NLP (Natural Language Processing).";https://www.kaggle.com/c/quickdraw-doodle-recognition;Google Research;How accurately can you identify a doodle?;['image data', 'map@{k}'];1,316;Quick, Draw! Doodle Recognition Challenge;Featured prediction Competition
2019-02-14 00:09:00;An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world. Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers. In this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content. Here's your chance to combat online trolls at scale. Help Quora uphold their policy of “Be Nice, Be Respectful” and continue to be a place for sharing and growing the world’s knowledge. Important Note Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output. Please read the Kernels FAQ and the data page very carefully to fully understand how this is designed.;https://www.kaggle.com/c/quora-insincere-questions-classification;Quora;Detect toxic content to improve online conversations;['text data', 'binary classification', 'custom metric'];4,037;Quora Insincere Questions Classification;Featured Code Competition
2017-06-07 01:59:00;Where else but Quora can a physicist help a chef with a math problem and get cooking tips in return? Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world. Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term. Currently, Quora uses a Random Forest model to identify duplicate questions. In this competition, Kagglers are challenged to tackle this natural language processing problem by applying advanced techniques to classify whether question pairs are duplicates or not. Doing so will make it easier to find high quality answers to questions resulting in an improved experience for Quora writers, seekers, and readers.;https://www.kaggle.com/c/quora-question-pairs;Quora;Can you identify question pairs that have the same intent?;['internet', 'text data', 'tabular data', 'linguistics', 'logloss'];3,304;Quora Question Pairs;Featured prediction Competition
2011-02-08 10:00:00;There are many obvious ways you can try to improve on our example model. For example, you might try:Using nearest neighbor methods over both packages and users.Building a metric of maintainer quality and adding this predictor to the example logistic model.Incorporating out-degree information from the graphs we're providing. The example model only uses in-degree information.Using regularization to deal with the considerable correlation in the default predictors.;https://www.kaggle.com/c/R;;The aim of this competition is to develop a recommendation engine for R libraries (or packages). (R is opensource statistics software.);['auc'];57;R Package Recommendation Engine;Featured prediction Competition
2012-09-19 01:59:00;Many organizations prospect for loyal supporters and donors by sending direct mail appeals. This is an effective way to build a large base, but can be very expensive and have a low efficiency. Eliminating likely non-donors is the key to running an efficient  Prospecting program and ultimately to pursuing the mission of the organization. Help us help these organizations to target the best prospective donors and fund organizational goals!  Please note: This competition has rules that we have not used previously restricting what kinds of models are acceptable. Please see the rules page for more information.;https://www.kaggle.com/c/Raising-Money-to-Fund-an-Organizational-Mission;;Help worthy organizations more efficiently target and recruit loyal donors to support their causes.;['averageamongtopp'];27;Raising Money to Fund an Organizational Mission;Featured prediction Competition
2015-06-02 01:59:00;"Get started on this competition through Kaggle Scripts In machine learning, it is often said there are no free lunches. How wrong we were. This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness. ""I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,"" says one hopeful poster. What about making an algorithm?  Kaggle is hosting this competition for the machine learning community to use for fun and practice. This data was collected and graciously shared by Althoff et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work: Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014.";https://www.kaggle.com/c/random-acts-of-pizza;Kaggle;Predicting altruism through free pizza;['internet', 'text data', 'binary classification', 'auc'];462;Random Acts of Pizza;Playground prediction Competition
2014-04-02 01:59:00;Quality pseudorandom number generation forms the bedrock on which all of computing is built. From cryptography to financial markets to particle physics, it is our trust in random numbers that props up the modern economy and allows technology to march forth, unabated.  Machine learning is incredibly powerful at recognizing statistical patterns. For this competition, we challenge you to apply your machine learning skills to predict a set of random numbers. To date, there are no known methods for predicting this set of numbers. Some experts have said it is too random, that it can't be done within current limitations of computing power. We believe the creativity of the Kaggle community will triumph over the cynical doubts of the naysayers. If there is one thing the working data scientist can do, it is extract insights from a sea of randomness. Before asking questions in the forums, we recommend reading the authoritative source in this field: A Million Random Digits with 100,000 Normal Deviates, RAND et. al. We also provide this helpful widget to cross validate your submissions:;https://www.kaggle.com/c/random-number-grand-challenge;;Decode a sequence of pseudorandom numbers;['mae'];205;The Random Number Grand Challenge;Featured prediction Competition
2019-08-09 01:59:00;"Do you have your father’s nose?  Blood relatives often share facial features. Now researchers at Northeastern University want to improve their algorithm for facial image classification to bridge the gap between research and other familial markers like DNA results. That will be your challenge in this new Kaggle competition. An automatic kinship classifier has been in the works at Northeastern since 2010. Yet this technology remains largely unseen in practice for a couple of reasons: 1. Existing image databases for kinship recognition tasks aren't large enough to capture and reflect the true data distributions of the families of the world. 2. Many hidden factors affect familial facial relationships, so a more discriminant model is needed than the computer vision algorithms used most often for higher-level categorizations (e.g. facial recognition or object classification). In this competition, you’ll help researchers build a more complex model by determining if two people are blood-related based solely on images of their faces. If you think you can get it ""on the nose,"" this competition is for you.  The SMILE Lab at Northeastern focuses on the frontier research of applied machine learning, social media analytics, human-computer interaction, and high-level image and video understanding. Their research is driven by the explosion of diverse multimedia from the Internet, including both personal and publicly-available photos and videos. They start by treating fundamental theory from learning algorithms as the soul of machine intelligence and arm it with visual perception.";https://www.kaggle.com/c/recognizing-faces-in-the-wild;Northeastern SMILE Lab;Can you determine if two individuals are related?;['image data', 'psychology', 'auc'];528;Northeastern SMILE Lab - Recognizing Faces in the Wild;Playground prediction Competition
2018-02-07 00:59:00;Running a thriving local restaurant isn't always as charming as first impressions appear. There are often all sorts of unexpected troubles popping up that could hurt business. One common predicament is that restaurants need to know how many customers to expect each day to effectively purchase ingredients and schedule staff members. This forecast isn't easy to make because many unpredictable factors affect restaurant attendance, like weather and local competition. It's even harder for newer restaurants with little historical data. Recruit Holdings has unique access to key datasets that could make automated future customer prediction possible. Specifically, Recruit Holdings owns Hot Pepper Gourmet (a restaurant review service), AirREGI (a restaurant point of sales service), and Restaurant Board (reservation log management software). In this competition, you're challenged to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be much more efficient and allow them to focus on creating an enjoyable dining experience for their customers.;https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;Recruit Holdings;Predict how many future visitors a restaurant will receive;['rmsle'];2,157;Recruit Restaurant Visitor Forecasting;Featured prediction Competition
2019-09-27 01:59:00;The cost of some drugs and medical treatments has risen so high in recent years that many patients are having to go without. You can help with a classification project that could make researchers more efficient. One of the more surprising reasons behind the cost is how long it takes to bring new treatments to market. Despite improvements in technology and science, research and development continues to lag. In fact, finding new treatments takes, on average, more than 10 years and costs hundreds of millions of dollars.  Recursion Pharmaceuticals, creators of the industry’s largest dataset of biological images, generated entirely in-house, believes AI has the potential to dramatically improve and expedite the drug discovery process. More specifically, your efforts could help them understand how drugs interact with human cells.  This competition will have you disentangling experimental noise from real biological signals. Your entry will classify images of cells under one of 1,108 different genetic perturbations. You can help eliminate the noise introduced by technical execution and environmental variation between experiments. If successful, you could dramatically improve the industry’s ability to model cellular images according to their relevant biology. In turn, applying AI could greatly decrease the cost of treatments, and ensure these treatments get to patients faster. This competition is a part of the NeurIPS 2019 competition track. Winners will be invited to contribute their solutions towards the workshop presentation. Acknowledgments Thank you to the following sponsors & supporters of this competition:     Google Cloud: Google Cloud is widely recognized as a global leader in delivering a secure, open and intelligent enterprise cloud platform. Our technology is built on Google’s private network and is the product of nearly 20 years of innovation in security, network architecture, collaboration, artificial intelligence and open source software. We offer a simply engineered set of tools and unparalleled technology across Google Cloud Platform and G Suite that help bring people, insights and ideas together. Customers across more than 150 countries trust Google Cloud to modernize their computing environment for today’s digital world.   DoiT: You have the cloud and we have your back. For nearly a decade, we’ve been helping businesses build and scale cloud solutions with our world-class cloud engineering support. We help our customers with technical support and consulting on building and operating complex large-scale distributed systems, developing better machine learning models and setting up big data solutions using Google Cloud, Amazon AWS and Microsoft Azure.   NVIDIA: NVIDIA’s (NASDAQ: NVDA) invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined modern computer graphics and revolutionized parallel computing. More recently, GPU deep learning ignited modern AI — the next era of computing — with the GPU acting as the brain of computers, robots and self-driving cars that can perceive and understand the world. More information at http://nvidianews.nvidia.com.   Lambda: Lambda provides Deep Learning workstations, servers, and GPU cloud services. Lambda Deep Learning infrastructure is used by the world's leading AI research & development organizations including Apple, Microsoft, MIT, Stanford, and the US Government. To learn more, visit www.lambdalabs.com.;https://www.kaggle.com/c/recursion-cellular-image-classification;Recursion Pharmaceuticals;CellSignal: Disentangling biological signal from experimental noise in cellular images;['classification', 'image data', 'biology', 'research', 'categorizationaccuracy'];866;Recursion Cellular Image Classification;Research prediction Competition
2019-02-13 00:59:00;Most flight-related fatalities stem from a loss of “airplane state awareness.” That is, ineffective attention management on the part of pilots who may be distracted, sleepy or in other dangerous cognitive states. Your challenge is to build a model to detect troubling events from aircrew’s physiological data. You'll use data acquired from actual pilots in test situations, and your models should be able to run calculations in real time to monitor the cognitive states of pilots. With your help, pilots could then be alerted when they enter a troubling state, preventing accidents and saving lives. Reducing aircraft fatalities is just one of the complex problems that  Booz Allen Hamilton has been solving for business, government, and military leaders for over 100 years. Through devotion, candor, courage, and character, they produce original solutions where there are no roadmaps. Now you can help them find answers, save lives, and change the world.;https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;Booz Allen Hamilton;Can you tell when a pilot is heading for trouble?;['multiclassloss'];178;Reducing Commercial Aviation Fatalities;Playground prediction Competition
2015-05-05 01:59:00;With over 1,200 quick service restaurants across the globe, TFI is the company behind some of the world's most well-known brands: Burger King, Sbarro, Popeyes, Usta Donerci, and Arby’s. They employ over 20,000 people in Europe and Asia and make significant daily investments in developing new restaurant sites. Right now, deciding when and where to open new restaurants is largely a subjective process based on the personal judgement and experience of development teams. This subjective data is difficult to accurately extrapolate across geographies and cultures.  New restaurant sites take large investments of time and capital to get up and running. When the wrong location for a restaurant brand is chosen, the site closes within 18 months and operating losses are incurred.  Finding a mathematical model to increase the effectiveness of investments in new restaurant sites would allow TFI to invest more in other important business areas, like sustainability, innovation, and training for new employees. Using demographic, real estate, and commercial data, this competition challenges you to predict the annual restaurant sales of 100,000 regional locations. TFI would love to hire an expert Kaggler like you to head up their growing data science team in Istanbul or Shanghai. You'd be tackling problems like the one featured in this competition on a global scale. See the job description here >>;https://www.kaggle.com/c/restaurant-revenue-prediction;;Predict annual restaurant sales based on objective measurements;['regression', 'tabular data', 'rmse'];2,257;Restaurant Revenue Prediction;Featured prediction Competition
2014-06-05 01:59:00;Improve credit risk models by predicting the probability of default on a consumer credit product in the next 18 months. More accurate credit risk evaluations allow issuers of credit to be able to responsibly extend and manage credit lines for their The goal of this contest is to make the most accura  Enter Now! This competition is only open to Masters-level participants who meet the eligibility criteria. Visit the Enter the Competition page to view the eligibility criteria and request entrance.;https://www.kaggle.com/c/risky-business;;Predict the risk of customer credit default;['custom metric'];44;Risky Business;Masters prediction Competition
2015-12-15 00:59:00;Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.;https://www.kaggle.com/c/rossmann-store-sales;;Forecast sales using store, promotion, and competitor data;['tabular data', 'rootmeansquarepercentageerror'];3,298;Rossmann Store Sales;Featured prediction Competition
2019-11-14 00:59:00;Intracranial hemorrhage, bleeding that occurs inside the cranium, is a serious health problem requiring rapid and often intensive medical treatment. For example, intracranial hemorrhages account for approximately 10% of strokes in the U.S., where stroke is the fifth-leading cause of death. Identifying the location and type of any hemorrhage present is a critical step in treating the patient.  Diagnosis requires an urgent procedure. When a patient shows acute neurological symptoms such as severe headache or loss of consciousness, highly trained specialists review medical images of the patient’s cranium to look for the presence, location and type of hemorrhage. The process is complicated and often time consuming.  In this competition, your challenge is to build an algorithm to detect acute intracranial hemorrhage and its subtypes.  You’ll develop your solution using a rich image dataset provided by the Radiological Society of North America (RSNA®) in collaboration with members of the American Society of Neuroradiology and MD.ai.  If successful, you’ll help the medical community identify the presence, location and type of hemorrhage in order to quickly and effectively treat affected patients. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from December 1-6, 2019. Collaborators Four research institutions provided large volumes of de-identified CT studies that were assembled to create the challenge dataset: Stanford University, Thomas Jefferson University, Unity Health Toronto and Universidade Federal de São Paulo (UNIFESP), The American Society of Neuroradiology (ASNR) organized a cadre of more than 60 volunteers to label over 25,000 exams for the challenge dataset. ASNR is the world’s leading organization for the future of neuroradiology representing more than 5,300 radiologists, researchers, interventionalists, and imaging scientists. MD.ai provided tooling and support for the data annotation process.  The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for AI to assist in detection and classification of hemorrhages in order to prioritize and expedite their clinical work. A full set of acknowledgments can be found on this page.;https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection;Radiological Society of North America;Identify acute intracranial hemorrhage and its subtypes;['image data', 'weightedmeancolumnwiselogloss'];1,345;RSNA Intracranial Hemorrhage Detection;Featured prediction Competition
2018-11-01 00:59:00;"In this competition, you’re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here’s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA®) has reached out to Kaggle’s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge.  The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018. Acknowledgements Thank you to the National Institutes of Health Clinical Center for publicly providing the Chest X-Ray dataset [5].  NIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community Original source files and documents  Also, a big thank you to the competition organizers! References  Rui P, Kang K. National Ambulatory Medical Care Survey: 2015 Emergency Department Summary Tables.  Table 27.  Available from: www.cdc.gov/nchs/data/nhamcs/webtables/2015edwebtables.pdf Deaths: Final Data for 2015.  Supplemental Tables. Tables I-21, I-22.  Available from: www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr6606tables.pdf Franquet T.  Imaging of community-acquired pneumonia.  J Thorac Imaging 2018 (epub ahead of print).  PMID 30036297 Kelly B.  The Chest Radiograph. Ulster Med J 2012;81(3):143-148 Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf";https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;Radiological Society of North America;Can you build an algorithm that automatically detects potential pneumonia cases?;['image data', 'medicine', 'custom metric'];1,499;RSNA Pneumonia Detection Challenge;Featured prediction Competition
2020-10-27 00:59:00;If every breath is strained and painful, it could be a serious and potentially life-threatening condition. A pulmonary embolism (PE) is caused by an artery blockage in the lung. It is time consuming to confirm a PE and prone to overdiagnosis. Machine learning could help to more accurately identify PE cases, which would make management and treatment more effective for patients. Currently, CT pulmonary angiography (CTPA), is the most common type of medical imaging to evaluate patients with suspected PE. These CT scans consist of hundreds of images that require detailed review to identify clots within the pulmonary arteries. As the use of imaging continues to grow, constraints of radiologists’ time may contribute to delayed diagnosis. The Radiological Society of North America (RSNA®) has teamed up with the Society of Thoracic Radiology (STR) to help improve the use of machine learning in the diagnosis of PE. In this competition, you’ll detect and classify PE cases. In particular, you'll use chest CTPA images (grouped together as studies) and your data science skills to enable more accurate identification of PE. If successful, you'll help reduce human delays and errors in detection and treatment. With 60,000-100,000 PE deaths annually in the United States, it is among the most fatal cardiovascular diseases. Timely and accurate diagnosis will help these patients receive better care and may also improve outcomes.  This is a Code Competition. Refer to Code Requirements for details.  Acknowledgments The Radiological Society of North America (RSNA®) is an international society of radiologists, medical physicists, and other medical professionals with more than 53,400 members worldwide.  RSNA hosts the world’s premier radiology forum and publishes two top peer-reviewed journals: Radiology, the highest-impact scientific journal in the field, and RadioGraphics, the only journal dedicated to continuing education in radiology.  The Society of Thoracic Radiology (STR) was founded in 1982.  The STR is dedicated to advancing cardiothoracic imaging in clinical application, education, and research in radiology and allied disciplines.  Continuing professional development opportunities provided by the STR include educational and scientific meetings, mentorship programs, grant support and award opportunities, our society journal, Journal of Thoracic Imaging, and global collaboration activities. A full set of acknowledgments can be found on this page.;https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;Radiological Society of North America;Classify Pulmonary Embolism cases in chest CT scans;['health', 'image data', 'weightedmeancolumnwiselogloss'];784;RSNA STR Pulmonary Embolism Detection;Featured Code Competition
2011-02-13 23:00:00;Forecasting travel times helps improve road safety and efficiency. Accurate predictions help commuters make informed decisions about when to travel and on what routes. This helps to lower intensity on problem arterials by encouraging motorists to use underutilised parts of the grid, and where possible, by having them select alternative times and modes of travel.  This competition requires participants to predict travel time on Sydney's M4 freeway from past travel time observations. In addition to better informing network managers and Australian motorists, insights from the competition will improve the general efficiency of the road transport system in Sydney and increase functionality on the government's live traffic website.  Participants in this competition are required to forecast the travel time on the M4 freeway for 15 mins, 30 mins, 45 mins, one hour, 90mins, two hours, six hours, 12 hours, 18 hours and 24 hours ahead. The NSW Roads and Traffic Authority has made 2 years' worth of historical data on road use between 2008 and 2010  available for this competition.  $10,000 is being offered for the winning model.The competition is being hosted by Australia's NSW Roads and Traffic Authority and is being sponsored by the NSW Department of Premier and Cabinet.;https://www.kaggle.com/c/RTA;;This competition requires participants to predict travel time on Sydney's M4 freeway from past travel time observations.;['rmse'];354;RTA Freeway Travel Time Prediction;Featured prediction Competition
2013-02-04 01:00:00;This is a private, invitation-only competition. The relevant information is provided only to contestants.;https://www.kaggle.com/c/RxVolumePrediction;;Predict future prescription volume;['mcap@k'];12;Prescription Volume Prediction;Masters prediction Competition
2020-01-17 00:59:00;"Santa was thrilled with the Kaggle community for minimizing his workshop costs! He had heard rumors that Kagglers were adept at cracking holiday challenges, but, wow, even Santa was surprised at this one. Unfortunately, the North Pole accountants were less pleased. It turns out, the accountants didn't like being one-upped by machine learning experts on the internet. To complicate matters, they've decided to allow an additional 1,000 families attend the workshop. And they've also ""fine tuned"" their accounting formula to try and trip up those fancy solvers some people have at their disposal. Of course, we know that nothing trips up the Kaggle community! (Well, except for maybe over-fitting. But fortunately, that doesn't apply here!) So this is a bonus Santa competition for those who want an additional challenge and the opportunity to continue to improve their optimization skills. Since Santa used up all his budget on accounting fees, this is strictly a Playground competition, with the chance to win some coveted Kaggle Swag. Have fun, and Happy Holidays from the Kaggle Team! Attribution Banner/Listing Photo by Helloquence on Unsplash";https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;Kaggle;Oh what fun it is to revise . . .;['optimization', 'holidays and cultural events', 'custom metric'];106;Santa 2019: Revenge of the Accountants;Playground prediction Competition
2018-01-13 00:59:00;‘Tis the night before Christmas year: two thousand seventeen. Santa’s grown grouchy, borderline mean. What used to be simple for Old St. Nick, is now too puzzling, it’s making him sick! See, Santa always knew, deep down in his gut,  what toy each kid wanted–no ifs, ands, or buts. But fierce population growth, more twins, and toy innovation, has left too complex a problem, in dire need of optimization. “Don’t worry, Mr. Santa”, said an Elf named McMaggle, “I have a solution! Have you heard of Kaggle?” As she explained Kaggle in-depth, Santa’s doubt began turning, he became a believer in the magic of...machine learning. So, Santa’s team needs YOU more than ever this year, to solve this painful problem and save Christmas cheer. The Challenge In this playground competition, you’re challenged to build a toy matching algorithm that maximizes happiness by pairing kids with toys they want. In the dataset, each kid has 10 preferences for their gift (from 1000) and Santa has 1000 preferred kids for every gift available. What makes this extra difficult is that 0.4% of the kids are twins, and by their parents’ request, require the same gift.;https://www.kaggle.com/c/santa-gift-matching;Kaggle;Down through the chimney with lots of toys...;['custom metric'];428;Santa Gift Matching Challenge;Featured prediction Competition
2020-01-16 00:59:00;Hammers ring, are you listenin’ In the shop, toys are glistenin’ Should they see the sights? There might be a fight… Walkin’ ‘round the Workshop Wonderland  Families said, they want to see it Santa said, he’d guarantee it They pick a date But they may have to wait Walkin’  ‘round the Workshop Wonderland We told Santa that he was a madman He just wants to make sure they all smile He’ll say “Are you flexible?“, They’ll say “Yeah man, But can you help us make it worth our while?” “Give them food, or sweater the more they wait, the gifts get better” Please help us rank Or we’ll break the bank! Walkin’ ’round the Workshop Wonderland Santa has exciting news! For 100 days before Christmas, he opened up tours to his workshop. Because demand was so strong, and because Santa wanted to make things as fair as possible, he let each of the 5,000 families that will visit the workshop choose a list of dates they'd like to attend the workshop. Now that all the families have sent Santa their preferences, he's realized it's impossible for everyone to get their top picks, so he's decided to provide extra perks for families that don't get their preferences. In addition, Santa's accounting department has told him that, depending on how families are scheduled, there may be some unexpected and hefty costs incurred. Santa needs the help of the Kaggle community to optimize which day each family is assigned to attend the workshop in order to minimize any extra expenses that would cut into next years toy budget! Can you help Santa out? Attribution Banner/Listing Photo by Nathan Lemon on Unsplash Description Photo by Markus Spiske on Unsplash;https://www.kaggle.com/c/santa-workshop-tour-2019;Kaggle;In the notebook we can build a model, and pretend that it will optimize...;['optimization', 'holidays and cultural events', 'custom metric'];1,620;Santa's Workshop Tour 2019;Featured prediction Competition
2016-05-03 01:59:00;From frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving. Santander Bank is asking Kagglers to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late. In this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.;https://www.kaggle.com/c/santander-customer-satisfaction;Banco Santander;Which customers are happy customers?;['binary classification', 'banking', 'tabular data', 'auc'];5,115;Santander Customer Satisfaction;Featured prediction Competition
2019-04-11 01:59:00;At Santander  our mission is to help people and businesses prosper.  We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.  Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure  we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan? In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.;https://www.kaggle.com/c/santander-customer-transaction-prediction;Banco Santander;Can you identify who will make a transaction?;['binary classification', 'banking', 'tabular data', 'auc'];8,802;Santander Customer Transaction Prediction;Featured prediction Competition
2016-12-22 00:59:00;Ready to make a downpayment on your first house? Or looking to leverage the equity in the home you have? To support needs for a range of financial decisions, Santander Bank offers a lending hand to their customers through personalized product recommendations.  Under their current system, a small number of Santander’s customers receive many recommendations while many others rarely see any resulting in an uneven customer experience. In their second competition, Santander is challenging Kagglers to predict which products their existing customers will use in the next month based on their past behavior and that of similar customers. With a more effective recommendation system in place, Santander can better meet the individual needs of all customers and ensure their satisfaction no matter where they are in life. Disclaimer: This data set does not include any real Santander Spain's customer, and thus it is not representative of Spain's customer base.;https://www.kaggle.com/c/santander-product-recommendation;Banco Santander;Can you pair products with people?;['multiclass classification', 'banking', 'tabular data', 'map@{k}'];1,779;Santander Product Recommendation;Featured prediction Competition
2018-08-21 01:59:00;According to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception.  The digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner… and often before they´ve even realized they need the service.  In their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction.  This means anticipating customer needs in a more concrete, but also simple and personal way.  With so many choices for financial services, this need is greater now than ever before.  In this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.;https://www.kaggle.com/c/santander-value-prediction-challenge;Banco Santander;Predict the value of transactions for potential customers.;['finance', 'banking', 'rmsle'];4,477;Santander Value Prediction Challenge;Featured prediction Competition
2016-01-09 00:59:00;"Fork this script and get started on the problem The North Pole is in an uproar over news that Santa's magic sleigh has been stolen. Able to carry all the world's presents in one trip, it was considered crucial to successfully delivering holiday goodies across the globe in one night. Unwilling to cancel Christmas, Santa is determined to deliver toys to all the good girls and boys using his day-to-day, magic-less sleigh. With so little time to pull off this plan, Santa is once again counting on Kagglers to help. Given the sleigh's antiquated, weight-limited specifications, your challenge is to optimize the routes and loads Santa will take to and from the North Pole. And don't forget about Dasher, Dancer, Prancer, and Vixen; Santa is adamant that the best solutions will minimize the toll of this hectic night on his reindeer friends.  Acknowledgements This competition is brought to you by FICO.";https://www.kaggle.com/c/santas-stolen-sleigh;;♫ Alarm bells ring, are you listening? Santa's sleigh has gone missing ♫;['tabular data', 'custom metric'];1,126;Santa's Stolen Sleigh;Featured prediction Competition
2017-01-31 00:59:00;All was well in Santa's workshop. The gifts were made, the route was planned, the naughty and nice list complete. Santa thought this would finally be the year he didn't need Kaggle's help with his combinatorial conundrums. At last, the Claus family could take the elves and reindeer on that well deserved vacation to the South Pole. Then, with just days until the big night, Santa received an email from a panicked database admin elf. Attached was a server log with the six least jolly words a jolly old St. Nick could read: ALTER TABLE GiftsDROP COLUMN Weight One of the North Pole elf interns had mistakenly deleted the weights for all of the inventory in the workshop! Santa didn't have a backup (remember, this is a guy who makes a list and checks it twice) and, without knowing each present's weight, he didn't know how he would safely pack his many gift bags. Gifts were already on their way to the sleigh packing facility and there wasn't time to re-weigh all the presents. It was once again necessary to summon the holiday talents of Kaggle's elite. Can you help Santa fill his multiple bags with sets of uncertain gifts? Save the season by turning Santa's uncertain probabilities into presents for good little boys and girls.;https://www.kaggle.com/c/santas-uncertain-bags;;♫ Bells are ringing, children singing, all is merry and bright. Santa's elves made a big mistake, now he needs your help tonight ♫;['tabular data', 'custom metric'];692;Santa's Uncertain Bags;Playground prediction Competition
2017-06-30 01:59:00;Housing costs demand a significant investment from both consumers and developers. And when it comes to planning a budget—whether personal or corporate—the last thing anyone needs is uncertainty about one of their biggets expenses. Sberbank, Russia’s oldest and largest bank, helps their customers by making predictions about realty prices so renters, developers, and lenders are more confident when they sign a lease or purchase a building. Although the housing market is relatively stable in Russia, the country’s volatile economy makes forecasting prices as a function of apartment characteristics a unique challenge. Complex interactions between housing features such as number of bedrooms and location are enough to make pricing predictions complicated. Adding an unstable economy to the mix means Sberbank and their customers need more than simple regression models in their arsenal. In this competition, Sberbank is challenging Kagglers to develop algorithms which use a broad spectrum of features to predict realty prices. Competitors will rely on a rich dataset that includes housing data and macroeconomic patterns. An accurate forecasting model will allow Sberbank to provide more certainty to their customers in an uncertain economy.;https://www.kaggle.com/c/sberbank-russian-housing-market;Sberbank;Can you predict realty price fluctuations in Russia’s volatile economy?;['regression', 'banking', 'tabular data', 'housing', 'rmsle'];3,274;Sberbank Russian Housing Market;Featured prediction Competition
2016-03-15 00:59:00;We all have a heart. Although we often take it for granted, it's our heart that gives us the moments in life to imagine, create, and discover. Yet cardiovascular disease threatens to take away these moments. Each day, 1,500 people in the U.S. alone are diagnosed with heart failure—but together, we can help. We can use data science to transform how we diagnose heart disease. By putting data science to work in the cardiology field, we can empower doctors to help more people live longer lives and spend more time with those that they love. Declining cardiac function is a key indicator of heart disease. Doctors determine cardiac function by measuring end-systolic and end-diastolic volumes (i.e., the size of one chamber of the heart at the beginning and middle of each heartbeat), which are then used to derive the ejection fraction (EF). EF is the percentage of blood ejected from the left ventricle with each heartbeat. Both the volumes and the ejection fraction are predictive of heart disease. While a number of technologies can measure volumes or EF, Magnetic Resonance Imaging (MRI) is considered the gold standard test to accurately assess the heart's squeezing ability.  The challenge with using MRI to measure cardiac volumes and derive ejection fraction, however, is that the process is manual and slow. A skilled cardiologist must analyze MRI scans to determine EF. The process can take up to 20 minutes to complete—time the cardiologist could be spending with his or her patients. Making this measurement process more efficient will enhance doctors' ability to diagnose heart conditions early, and carries broad implications for advancing the science of heart disease treatment. The 2015 Data Science Bowl challenges you to create an algorithm to automatically measure end-systolic and end-diastolic volumes in cardiac MRIs. You will examine MRI images from more than 1,000 patients. This data set was compiled by the National Institutes of Health and Children's National Medical Center and is an order of magnitude larger than any cardiac MRI data set released previously. With it comes the opportunity for the data science community to take action to transform how we diagnose heart disease. This is not an easy task, but together we can push the limits of what's possible. We can give people the opportunity to spend more time with the ones they love, for longer than ever before. Acknowledgments The Data Science Bowl is presented by:  The National Heart, Lung, and Blood Institute (NHLBI) provided the MRI images for this competition. Special thanks to NHLBI Intramural Investigators Dr. Michael Hansen and Dr. Andrew Arai. Additional support for the Data Science Bowl was provided by NVIDIA:;https://www.kaggle.com/c/second-annual-data-science-bowl;Booz Allen Hamilton;Transforming How We Diagnose Heart Disease;['image data', 'healthcare', 'crps'];192;Second Annual Data Science Bowl;Featured prediction Competition
2013-11-28 00:59:00;"This competition is the successor to the See Click Predict Fix Hackathon. The purpose of both competitions is to quantify and predict how people will react to a specific 311 issue. What makes an issue urgent? What do citizens really care about? How much does location matter? Being able to predict the most pressing 311 topics will allow governments to focus their efforts on fixing the most important problems. The data set for the competitions contains several hundred thousand 311 issues from four cities. For those who are more interested in using the data for visualization or ""non-predictive"" data mining, we have added a $500 visualization prize. You may submit as many entries as you wish via the Visualization page. If you're plotting issues on maps, displaying the text in some meaningful way, or making any other creative use of the data, save it and post it! About 311 311 is a mechanism by which citizens can express their desire to solve a problem the city or government by submitting a description of what needs to be done, fixed, or changed. In effect, this provides a high degree of transparency between government and its constituents. Once an issue has been established, citizens can vote and make comments on the issue so that government officials have some degree of awareness about what is the most important issue to address.  Sponsors The meeting space has been provided by Microsoft.  Prize money is graciously offered by our sponsors:  On the citizen side, SeeClickFix leverages crowdsourcing to help both maintain the flow of incoming requests but show the public how effective you can be. When anyone in the community can report or comment on any issue, the entire group has a better perspective on what's happening--and how to fix it effectively. For governments, SeeClickFix acts as a completely-customizable CRM that plugs into your existing request management tools. From types of service requests to managing different watch areas, SeeClickFix helps better m  A public policy entrepreneur and open innovation expert David advises numerous governments on open government and open data and works with leading non-profits and businesses on strategy, open innovation and community management. In addition to his work, David is an affiliate with the Berkman Centre for Internet and Society at Harvard where he is looking at issues surrounding the politics of data. You can find David's writing on open innovation, public policy, public sector renewal and open source systems at his blog, or at TechPresident. In addition to his writing, David is frequently invited to speak on open government, policy making, negotiation and strategy to executives, policymakers, and students. You can read a background on how this challenge came to be here.";https://www.kaggle.com/c/see-click-predict-fix;;Predict which 311 issues are most important to citizens;['rmsle'];530;See Click Predict Fix;Featured prediction Competition
2014-08-20 01:59:00;For individuals with drug-resistant epilepsy, responsive neurostimulation systems hold promise for augmenting current therapies and transforming epilepsy care. Of the more than two million Americans who suffer from recurrent, spontaneous epileptic seizures, 500,000 continue to experience seizures despite multiple attempts to control the seizures with medication. For these patients responsive neurostimulation represents a possible therapy capable of aborting seizures before they affect a patient's normal activities.   In order for a responsive neurostimulation device to successfully stop seizures, a seizure must be detected and electrical stimulation applied as early as possible. A seizure that builds and generalizes beyond its area of origin will be very difficult to abort via neurostimulation. Current seizure detection algorithms in commercial responsive neurostimulati In addition, physicians and researchers working in epilepsy must often review large quantities of continuous EEG data to identify seizures, which in some patients may be quite subtle. Automated algorithms to detect seizures in large EEG datasets with low false positive and false negative rates would greatly assist clinical care and basic research. The Competition: Intracranial EEG was recorded from dogs with naturally occurring epilepsy using an ambulatory monitoring system. EEG was sampled from 16 electrodes at 400 Hz, and recorded voltages were referenced to the group average.   In addition, datasets from patients with epilepsy undergoing intracranial EEG monitoring to identify a region of brain that can be resected to prevent future seizures are included in the contest. These datasets have varying numbers of electrodes and are sampled at 500 Hz or 5000 Hz, with recorded voltages referenced to an electrode outside the brain.     Acknowledgements This competition is sponsored by the National Institues of Health (NINDS), and the American Epilepsy Society.;https://www.kaggle.com/c/seizure-detection;;Detect seizures in intracranial EEG recordings;['mcauc'];200;UPenn and Mayo Clinic's Seizure Detection Challenge;Research prediction Competition
2014-11-18 00:59:00;Seizure forecasting systems hold promise for improving the quality of life for patients with epilepsy. Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective -- and even after surgical removal of epilepsy-causing brain tissue, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for EEG-based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects. There is emerging evidence that the temporal dynamics of brain activity can be classified into 4 states: Interictal (between seizures, or baseline), Preictal (prior to seizure), Ictal (seizure), and Post-ictal (after seizures). Seizure forecasting requires the ability to reliably identify a preictal state that can be differentiated from the interictal, ictal, and postictal state. The primary challenge in seizure forecasting is differentiating between the preictal and interictal states. The goal of the competition is to demonstrate the existence and accurate classification of the preictal brain state in dogs and humans with naturally occurring epilepsy. The Competition Intracranial EEG was recorded from dogs with naturally occurring epilepsy using an ambulatory monitoring system. EEG was sampled from 16 electrodes at 400 Hz, and recorded voltages were referenced to the group average. These are long duration recordings, spanning multiple months up to a year and recording up to a hundred seizures in some dogs.  In addition, datasets from patients with epilepsy undergoing intracranial EEG monitoring to identify a region of brain that can be resected to prevent future seizures are included in the contest. These datasets have varying numbers of electrodes and are sampled at 5000 Hz, with recorded voltages referenced to an electrode outside the brain. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity. Seizures are known to cluster, or occur in groups. Patients who typically have seizure clusters receive little benefit from forecasting follow-on seizures. For this contest only lead seizures, defined here as seizures occurring four hours or more after another seizure, are included in the training and testing data sets. In order to avoid any potential contamination between interictal, preictal, and post-ictal EEG signals interictal segments in the canine training and test data were restricted to be at least one week before or after any seizure. In the human data, where the entire monitoring session may last less than one week, interictal data segments were restricted to be at least four hours before or after any seizure. Interictal data segments were chosen at random within these restrictions for both canine and human subjects. Participants are invited to visit the NIH-sponsored International Epilepsy Electrophysiology portal (http://ieeg.org) to review and download annotated interictal and preictal data from other patients and animal subjects. Using ieeg.org data for additional algorithm training is permitted. Acknowledgements This competition is sponsored by the National Institutes of Health (NINDS), the Epilepsy Foundation, and the American Epilepsy Society.    References   Howbert JJ, Patterson EE, Stead SM, Brinkmann B, Vasoli V, Crepeau D, Vite CH, Sturges B, Ruedebusch V, Mavoori J, Leyde K, Sheffield WD, Litt B, Worrell GA (2014) Forecasting seizures in dogs with naturally occurring epilepsy. PLoS One 9(1):e81920.   Cook MJ, O'Brien TJ, Berkovic SF, Murphy M, Morokoff A, Fabinyi G, D'Souza W, Yerra R, Archer J, Litewka L, Hosking S, Lightfoot P, Ruedebusch V, Sheffield WD, Snyder D, Leyde K, Himes D (2013) Prediction of seizure likelihood with a long-term, implanted seizure advisory system in patients with drug-resistant epilepsy: a first-in-man study. LANCET NEUROL 12:563-571.   Park Y, Luo L, Parhi KK, Netoff T (2011) Seizure prediction with spectral power of EEG using cost-sensitive support vector machines. Epilepsia 52:1761-1770.   Davis KA, Sturges BK, Vite CH, Ruedebusch V, Worrell G, Gardner AB, Leyde K, Sheffield WD, Litt B (2011) A novel implanted device to wirelessly record and analyze continuous intracranial canine EEG. Epilepsy Res 96:116-122.   Andrzejak RG, Chicharro D, Elger CE, Mormann F (2009) Seizure prediction: Any better than chance? Clin Neurophysiol.   Snyder DE, Echauz J, Grimes DB, Litt B (2008) The statistics of a practical seizure warning system. J Neural Eng 5: 392–401.   Mormann F, Andrzejak RG, Elger CE, Lehnertz K (2007) Seizure prediction: the long and winding road. Brain 130: 314–333.   Haut S, Shinnar S, Moshe SL, O'Dell C, Legatt AD. (1999) The association between seizure clustering and status epilepticus in patients with intractable complex partial seizures. Epilepsia 40:1832–1834.;https://www.kaggle.com/c/seizure-prediction;;Predict seizures in intracranial EEG recordings;['auc'];504;American Epilepsy Society Seizure Prediction Challenge;Research prediction Competition
2011-10-18 01:59:59;"There's recently been a lot of work done in unsupervised feature learning for classification, with great advances made by approaches such as deep belief nets, graphical models, and transfer learning. Meanwhile, there are a ton of older methods that also  work well, including matrix factorization, random projections, and clustering methods. The purpose of this competition is to find out which of these methods work the best, on relatively large-scale high dimensional learning tasks.   The Short Version In this task, you'll do the following:  Learn a feature representation of at most 100 features, using a small amount of labeled data and a large amount of unlabeled data.  The orginial data is very sparse. Transform training and test data using your learned feature representation. Train a standard linear classifier on the transformed training data. Measure the AUC of the classifier on transformed test data.  We have public data to be used for the leaderboard evaluations, and a separate private data set used for final evaluation through a special submission process.  We have scripts to simplify the training and evaluation; how you do the feature transformation  is up to you.   The Long Version The task that we're evaluating on is a binary-class classification problem, drawn from web classification.  (The data has been cleaned heavily and anonymized.)  The data itself is sparse, high dimensional data with about a million features.  A few features  have non-zero values in many or even all examples; most features have non-zero values in very few examples. Your task is to transform the data from a high dimensional space to a lower dimensional space of at most 100 features.  The goal is to make this new feature space so rich and informative that it allows a new classifier to be trained with the best possible  predictive performance.  Any method of producing a condensed representation is fair game: deep learning graphical models, transfer learning, supervised learning, semi-supervised learning, matrix factorization, random projection, clustering, feature selection,  or anything else you can invent. In addition to a small amount of labeled data, you will also be given a large amount of unlabeled data.  Both the labeled and the unlabeled data can be used to learn good ways to transform the feature space. The final evaluation will be done by using your method to transform training and test sets whose labels are not known.  These transformed data sets will be sent to tne organizers, who will apply the hidden labels and use these new data sets to train and  test a standard supervised classifier.  The data set that produces the best classification performance on test data (using AUC as the evaluation measure) will be declared the winner.  We also provide versions of our evaluation scripts to be used on public  versions of our private evaluation data sets, and these results can be used to update the leaderboard.  Please see the ""Evaluation"" page for full details. Although there is a modest cash prize, the main goal of this competition is to encourage research and share ideas.  The results of this competition will be included in a paper submitted to the 2011 NIPS workshop on deep learning and unsupervised feature learning.  Contestants will be acknowledged by name in this paper for noteworthy performance, including results that do especially  well or which are especially interesting.";https://www.kaggle.com/c/SemiSupervisedFeatureLearning;;There's been a lot of recent work done in unsupervised feature learning for classification and there are a ton of older methods that also work well. The purpose of this competition is to find out which of these methods work best on relatively large-scale high dimensional learning tasks.;['auc'];26;Semi-Supervised Feature Learning;Featured prediction Competition
2015-03-01 00:59:00;"""There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side."" The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.  Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of Socher et al [2]. We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper: http://nlp.stanford.edu/sentiment/ There you will find have source code, a live demo, and even an online interface to help train the model. [1] Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124. [2] Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).";https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews;Kaggle;Classify the sentiment of sentences from the Rotten Tomatoes dataset;['text data', 'multiclass classification', 'categorizationaccuracy'];860;Sentiment Analysis on Movie Reviews;Playground prediction Competition
2019-10-25 01:59:00;Steel is one of the most important building materials of modern times. Steel buildings are resistant to natural and man-made wear which has made the material ubiquitous around the world. To help make production of steel more efficient, this competition will help identify defects.  Severstal is leading the charge in efficient steel mining and production. They believe the future of metallurgy requires development across the economic, ecological, and social aspects of the industry—and they take corporate responsibility seriously. The company recently created the country’s largest industrial data lake, with petabytes of data that were previously discarded. Severstal is now looking to machine learning to improve automation, increase efficiency, and maintain high quality in their production. The production process of flat sheet steel is especially delicate. From heating and rolling, to drying and cutting, several machines touch flat steel by the time it’s ready to ship. Today, Severstal uses images from high frequency cameras to power a defect detection algorithm.  In this competition, you’ll help engineers improve the algorithm by localizing and classifying surface defects on a steel sheet. If successful, you’ll help keep manufacturing standards for steel high and enable Severstal to continue their innovation, leading to a stronger, more efficient world all around us.;https://www.kaggle.com/c/severstal-steel-defect-detection;Severstal;Can you detect and classify defects in steel?;['image data', 'manufacturing', 'dice'];2,431;Severstal: Steel Defect Detection;Featured Code Competition
2016-06-07 01:59:00;From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz. Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay. From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given time and location, you must predict the category of crime that occurred. We're also encouraging you to explore the dataset visually. What can we learn about the city through visualizations like this Top Crimes Map? The top most up-voted scripts from this competition will receive official Kaggle swag as prizes.   Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset is brought to you by SF OpenData, the central clearinghouse for data published by the City and County of San Francisco.;https://www.kaggle.com/c/sf-crime;Kaggle;Predict the category of crimes that occurred in the city by the bay;['multiclass classification', 'crime', 'tabular data', 'multiclassloss'];2,331;San Francisco Crime Classification;Playground prediction Competition
2016-08-01 01:59:00;Every year, approximately 7.6 million companion animals end up in US shelters. Many animals are given up as unwanted by their owners, while others are picked up after getting lost or taken out of cruelty situations. Many of these animals find forever families to take them home, but just as many are not so lucky. 2.7 million dogs and cats are euthanized in the US every year.  Using a dataset of intake information including breed, color, sex, and age from the Austin Animal Center, we're asking Kagglers to predict the outcome for each animal. We also believe this dataset can help us understand trends in animal outcomes. These insights could help shelters focus their energy on specific animals who need a little extra help finding a new home. We encourage you to publish your insights on Scripts so they are publicly accessible. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for data science practice and social good. The dataset is brought to you by Austin Animal Center. Shelter animal statistics were taken from the ASPCA. Glamour shots of Kaggle's shelter pets are pictured above. From left to right: Shelby, Bailey, Hazel, Daisy, and Yeti.;https://www.kaggle.com/c/shelter-animal-outcomes;Kaggle;Help improve outcomes for shelter animals;['multiclass classification', 'tabular data', 'animals', 'multiclassloss'];1,599;Shelter Animal Outcomes;Playground prediction Competition
2019-09-05 01:59:00;Imagine suddenly gasping for air, helplessly breathless for no apparent reason. Could it be a collapsed lung? In the future, your entry in this competition could predict the answer. Pneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifying—it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event. Pneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more confident diagnosis for non-radiologists. The Society for Imaging Informatics in Medicine (SIIM) is the leading healthcare organization for those interested in the current and future use of informatics in medical imaging. Their mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community. Today, they need your help. In this competition, you’ll develop a model to classify (and if present, segment) pneumothorax from a set of chest radiographic images. If successful, you could aid in the early recognition of pneumothoraces and save lives.  If you’re up for the challenge, take a deep breath, and get started now. Note: As specified on the Data Page, the dataset must be retrieved from Cloud Healthcare. Review this tutorial (or in pdf format) for instructions on how to do so. Acknowledgments SIIM Machine Learning Committee Co-Chairs, Steven G. Langer, PhD, CIIP and George Shih, MD, MS for tirelessly leading this effort and making the challenge possible in such a short period of time.  SIIM Machine Learning Committee Members for their dedication in annotating the dataset, helping to define the most useful metrics and running tests to prepare the challenge for launch.  SIIM Hackathon Committee, especially Mohannad Hussain, for their crucial technical support with data conversion.    American College of Radiology (ACR), @RadiologyACR: For Co-hosting the challenge and Co-sponsoring  the Prizes    Society of Thoracic Radiology (STR), @thoracicrad: For their unparalleled expertise in adjudicating the dataset    MD.ai: For providing the annotation tool and helping with the first layer of annotations;https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation;Society for Imaging Informatics in Medicine (SIIM);Identify Pneumothorax disease in chest x-rays;['image data', 'custom metric'];1,475;SIIM-ACR Pneumothorax Segmentation;Featured prediction Competition
2020-08-18 01:59:00;Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection—potentially aided by data science—can make treatment more effective. Currently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or “ugly ducklings” that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account “contextual” images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work. As the leading healthcare organization for informatics in medical imaging, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. SIIM is joined by the International Skin Imaging Collaboration (ISIC), an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions. In this competition, you’ll identify melanoma in images of skin lesions. In particular, you’ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists. Melanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people.;https://www.kaggle.com/c/siim-isic-melanoma-classification;SIIM & ISIC;Identify melanoma in lesion images;['image data', 'mcauc'];3,314;SIIM-ISIC Melanoma Classification;Featured prediction Competition
2011-01-11 23:00:00;•    Look at popular inbound nodes•    Investigate local networks•    Investigate mutuality of edges•    Start reading http://en.wikipedia.org/wiki/Directed_graph •    Also check http://www.sigkdd.org/kddcup/index.php?section=2003&method=info •    Social Networks http://www.sciencedirect.com/science/journal/03788733;https://www.kaggle.com/c/socialNetwork;;This competition requires participants to predict edges in an online social network. The winner will receive free registration and the opportunity to present their solution at IJCNN 2011.;['auc'];117;IJCNN Social Network Challenge;Featured prediction Competition
2018-02-09 00:59:00;Finding footage of a crime caught on tape is an investigator's dream. But even with crystal clear, damning evidence, one critical question always remains–is the footage real? Today, one way to help authenticate footage is to identify the camera that the image was taken with. Forgeries often require splicing together content from two different cameras. But, unfortunately, the most common way to do this now is using image metadata, which can be easily falsified itself. This problem is actively studied by several researchers around the world. Many machine learning solutions have been proposed in the past: least-squares estimates of a camera's color demosaicing filters as classification features, co-occurrences of pixel value prediction errors as features that are passed to sophisticated ensemble classifiers, and using CNNs to learn camera model identification features. However, this is a problem yet to be sufficiently solved. For this competition, the IEEE Signal Processing Society is challenging you to build an algorithm that  identifies which camera model captured an image by using traces intrinsically left in the image. Helping to solve this problem would have a big impact on the verification of evidence used in criminal and civil trials and even news reporting.;https://www.kaggle.com/c/sp-society-camera-model-identification;IEEE Signal Processing Society;Identify from which camera an image was taken;['image data', 'weightedcategorizationaccuracy'];581;IEEE's Signal Processing Society - Camera Model Identification;Featured prediction Competition
2017-12-16 00:59:00;"As I scurried across the candlelit chamber, manuscripts in hand, I thought I'd made it. Nothing would be able to hurt me anymore. Little did I know there was one last fright lurking around the corner. DING! My phone pinged me with a disturbing notification. It was Will, the scariest of Kaggle moderators, sharing news of another data leak.  ""ph’nglui mglw’nafh Cthulhu R’lyeh wgah’nagl fhtagn!"" I cried as I clumsily dropped my crate of unbound, spooky books. Pages scattered across the chamber floor. How will I ever figure out how to put them back together according to the authors who wrote them? Or are they lost, forevermore? Wait, I thought... I know, machine learning! In this year's Halloween playground competition, you're challenged to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft. We're encouraging you (with cash prizes!) to share your insights in the competition's discussion forum and code in Kernels. We've designated prizes to reward authors of kernels and discussion threads that are particularly valuable to the community. Click the ""Prizes"" tab on this overview page to learn more. Getting Started New to Kernels or working with natural language data? We've put together some starter kernels in Python and R to help you hit the ground running.";https://www.kaggle.com/c/spooky-author-identification;Kaggle;Share code and discuss insights to identify horror authors from their writings;['multiclass classification', 'literature', 'linguistics', 'multiclassloss'];1,243;Spooky Author Identification;Playground prediction Competition
2015-10-20 01:59:00;Springleaf puts the humanity back into lending by offering their customers personal and auto loans that help them take control of their lives and their finances. Direct mail is one important way Springleaf's team can connect with customers whom may be in need of a loan.  Direct offers provide huge value to customers who need them, and are a fundamental part of Springleaf's marketing strategy. In order to improve their targeted efforts, Springleaf must be sure they are focusing on the customers who are likely to respond and be good candidates for their services. Using a large set of anonymized features, Springleaf is asking you to predict which customers will respond to a direct mail offer. You are challenged to construct new meta-variables and employ feature-selection methods to approach this dauntingly wide dataset.;https://www.kaggle.com/c/springleaf-marketing-response;;Determine whether to send a direct mail piece to a customer;['binary classification', 'tabular data', 'marketing', 'auc'];2,221;Springleaf Marketing Response;Featured prediction Competition
2020-10-07 01:59:00;Winning the fight against the COVID-19 pandemic will require an effective vaccine that can be equitably and widely distributed. Building upon decades of research has allowed scientists to accelerate the search for a vaccine against COVID-19, but every day that goes by without a vaccine has enormous costs for the world nonetheless. We need new, fresh ideas from all corners of the world. Could online gaming and crowdsourcing help solve a worldwide pandemic? Pairing scientific and crowdsourced intelligence could help computational biochemists make measurable progress.  mRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19,  but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA). Conventional vaccines (like your seasonal flu shots) are packaged in disposable syringes and shipped under refrigeration around the world, but that is not currently possible for mRNA vaccines.  Researchers have observed that RNA molecules have the tendency to spontaneously degrade. This is a serious limitation--a single cut can render the mRNA vaccine useless. Currently, little is known on the details of where in the backbone of a given RNA is most prone to being affected. Without this knowledge, current mRNA vaccines against COVID-19 must be prepared and shipped under intense refrigeration, and are unlikely to reach more than a tiny fraction of human beings on the planet unless they can be stabilized.  The Eterna community, led by Professor Rhiju Das, a computational biochemist at Stanford’s School of Medicine, brings together scientists and gamers to solve puzzles and invent medicine. Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles. The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules. The Eterna community has previously unlocked new scientific principles, made new diagnostics against deadly diseases, and engaged the world’s most potent intellectual resources for the betterment of the public. The Eterna community has advanced biotechnology through its contribution in over 20 publications, including advances in RNA biotechnology. In this competition, we are looking to leverage the data science expertise of the Kaggle community to develop models and design rules for RNA degradation. Your model will predict likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position. We will then score your models on a  second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines. These final test sequences are currently being synthesized and experimentally characterized at Stanford University in parallel to your modeling efforts -- Nature will score your models!  Improving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve.  Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19. The problem we are trying to solve has eluded academic labs, industry R&D groups, and supercomputers, and so we are turning to you. To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.;https://www.kaggle.com/c/stanford-covid-vaccine;Stanford University;Urgent need to bring the COVID-19 vaccine to mass production;['covid19', 'biology', 'public health', 'biotechnology', 'custom metric'];1,636;OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction;Research prediction Competition
2016-08-02 01:59:00;We've all been there: a light turns green and the car in front of you doesn't budge. Or, a previously unremarkable vehicle suddenly slows and starts swerving from side-to-side. When you pass the offending driver, what do you expect to see? You certainly aren't surprised when you spot a driver who is texting, seemingly enraptured by social media, or in a lively hand-held conversation on their phone.  According to the CDC motor vehicle safety division, one in five car accidents is caused by a distracted driver. Sadly, this translates to 425,000 people injured and 3,000 people killed by distracted driving every year. State Farm hopes to improve these alarming statistics, and better insure their customers, by testing whether dashboard cameras can automatically detect drivers engaging in distracted behaviors. Given a dataset of 2D dashboard camera images, State Farm is challenging Kagglers to classify each driver's behavior. Are they driving attentively, wearing their seatbelt, or taking a selfie with their friends in the backseat?;https://www.kaggle.com/c/state-farm-distracted-driver-detection;;Can computer vision spot distracted drivers?;['image data', 'automobiles and vehicles', 'multiclassloss'];1,438;State Farm Distracted Driver Detection;Featured prediction Competition
2018-01-24 00:59:00;Drifting icebergs present threats to navigation and activities in areas such as offshore of the East Coast of Canada.  Currently, many institutions and companies use aerial reconnaissance and shore-based support to monitor environmental conditions and assess risks from icebergs.  However, in remote areas with particularly harsh weather, these methods are not feasible, and the only viable monitoring option is via satellite. Statoil, an international energy company operating worldwide, has worked closely with companies like C-CORE. C-CORE have been using satellite data for over 30 years and have built a computer vision based surveillance system. To keep operations safe and efficient, Statoil is interested in getting a fresh new perspective on how to use machine learning to more accurately detect and discriminate against threatening icebergs as early as possible. In this competition, you’re challenged to build an algorithm that automatically identifies if a remotely sensed target is a ship or iceberg. Improvements made will help drive the costs down for maintaining safe working conditions.;https://www.kaggle.com/c/statoil-iceberg-classifier-challenge;Statoil;Ship or iceberg, can you decide from space?;['image data', 'binary classification', 'weather and climate', 'logloss'];3,341;Statoil/C-CORE Iceberg Classifier Challenge;Featured prediction Competition
2011-03-09 14:00:00;Driving while distracted, fatigued or drowsy may lead to accidents. Activities that divert the driver's attention from the road ahead, such as engaging in a conversation with other passengers in the car, making or receiving phone calls, sending or receiving text messages, eating while driving or events outside the car may cause driver distraction. Fatigue and drowsiness can result from driving long hours or from lack of sleep.  The objective of this challenge is to design a detector/classifier that will detect whether the driver is alert or not alert, employing any combination of vehicular, environmental and driver physiological data that are acquired while driving.The winner receives free registration to the 2011 International Joint Conference on Neural Networks (San Jose, California July 31 - August 5, 2011), which is valued at $950. The winner will also be invited to present their solution at the conference.;https://www.kaggle.com/c/stayalert;;Driving while not alert can be deadly. The objective is to design a classifier that will detect whether the driver is alert or not alert, employing data that are acquired while driving.;['auc'];176;Stay Alert! The Ford Challenge;Featured prediction Competition
2017-01-07 01:00:00;This competition is designed to help you get started with Julia. If you are looking for a good programming language for data science, or if you are already accustomed to one language, we encourage you to also try Julia. Julia is a relatively new language for technical computing that attempts to combine the strengths of other popular programming languages.   Here we introduce two tutorials to highlight some of Julia's features. The first is focused on the basics of the language. In the second, a complete implementation of the K Nearest Neighbor algorithm is presented, highlighting features such as parallelization and speed.  Both tutorials show that it is easy to write code in Julia, due to its intuitive syntax and design. The tutorials also describe some basics of image processing and some concepts of machine learning such as cross validation. After reviewing them, we hope you will be motivated to write your own machine learning algorithms in Julia.  This tutorial focuses on the task of identifying characters from Google Street View images. It differs from traditional character recognition because the data set contains different character fonts and the background is not the same for all images.  Acknowledgements The data was taken from the Chars74K dataset, which consists of images of characters selected from Google Street View images. We ask that you cite the following reference in any publication resulting from your work: T. E. de Campos, B. R. Babu and M. Varma, Character recognition in natural images, Proceedings of the International Conference on Computer Vision Theory and Applications (VISAPP), Lisbon, Portugal, February 2009. This tutorial was developed by Luis Tandalla during his summer 2014 internship at Kaggle.;https://www.kaggle.com/c/street-view-getting-started-with-julia;Kaggle;Use Julia to identify characters from Google Street View images;['image data', 'categorizationaccuracy'];56;First Steps With Julia;Getting Started prediction Competition
2013-11-01 00:59:59;"StumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as ""ephemeral"" or ""evergreen"". The ratings we get from our community give us strong signals that a page may no longer be relevant - but what if we could make this distinction ahead of time? A high quality prediction of ""ephemeral"" or ""evergreen"" would greatly improve a recommendation system like ours. Many people know evergreen content when they see it, but can an algorithm make the same determination without human intuition? Your mission is to build a classifier which will evaluate a large set of URLs and label them as either evergreen or ephemeral. Can you out-class(ify) StumbleUpon? As an added incentive to the prize, a strong performance in this competition may lead to a career-launching internship at one of the best places to work in San Francisco.";https://www.kaggle.com/c/stumbleupon;;Build a classifier to categorize webpages as evergreen or non-evergreen;['internet', 'text data', 'tabular data', 'auc'];624;StumbleUpon Evergreen Classification Challenge;Featured prediction Competition
2018-05-08 01:59:00;Fraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest mobile market in the world and therefore suffers from huge volumes of fradulent traffic. TalkingData, China’s largest independent big data service platform, covers over 70% of active mobile devices nationwide. They handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user’s click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist. While successful, they want to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. In their 2nd competition with Kaggle, you’re challenged to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support your modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!;https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection;TalkingData;Can you detect fraudulent click traffic for mobile app ads?;['auc'];3,946;TalkingData AdTracking Fraud Detection Challenge;Featured prediction Competition
2016-09-06 01:59:00;Nothing is more comforting than being greeted by your favorite drink just as you walk through the door of the corner café. While a thoughtful barista knows you take a macchiato every Wednesday morning at 8:15, it’s much more difficult in a digital space for your preferred brands to personalize your experience. TalkingData, China’s largest third-party mobile data platform, understands that everyday choices and behaviors paint a picture of who we are and what we value. Currently, TalkingData is seeking to leverage behavioral data from more than 70% of the 500 million mobile devices active daily in China to help its clients better understand and interact with their audiences. In this competition, Kagglers are challenged to build a model predicting users’ demographic characteristics based on their app usage, geolocation, and mobile device properties. Doing so will help millions of developers and brand advertisers around the world pursue data-driven marketing efforts which are relevant to their users and catered to their preferences. Acknowledgements;https://www.kaggle.com/c/talkingdata-mobile-user-demographics;TalkingData;Get to know millions of mobile device users;['multiclass classification', 'tabular data', 'demographics', 'mobile and wireless', 'multiclassloss'];1,680;TalkingData Mobile User Demographics;Featured prediction Competition
2016-03-01 00:59:00;In their first recruiting competition, Telstra is challenging Kagglers to predict the severity of service disruptions on their network. Using a dataset of features from their service logs, you're tasked with predicting if a disruption is a momentary glitch or a total interruption of connectivity. Telstra is on a journey to enhance the customer experience - ensuring everyone in the company is putting customers first. In terms of its expansive network, this means continuously advancing how it predicts the scope and timing of service disruptions. Telstra wants to see how you would help it drive customer advocacy by developing a more advanced predictive model for service disruptions and to help it better serve its customers. This challenge was crafted as a simulation of the type of problem you might tackle as a member of the team at Telstra. Kagglers who stand out will be considered for data science roles in Telstra's Big Data team in Telstra’s absolute discretion. Highly-ranked participants will combine technical expertise and intuition in data science problems with a keen business sense and an effortless ability to work with technical and non-technical staff to turn data into real changes that impact customers. Highly-ranked participants will be considered by Telstra for interviews for employment, based on their work in the Competition and ability to meet the selection criteria for any suitable open job vacancy in Melbourne and Sydney, Australia. Participation in this Competition is not a recruitment process and Kaggle does not provide Telstra with recruitment services.;https://www.kaggle.com/c/telstra-recruiting-network;;Predict service faults on Australia's largest telecommunications network;['internet', 'multiclass classification', 'tabular data', 'multiclassloss'];971;Telstra Network Disruptions;Recruitment prediction Competition
2018-01-17 00:59:00;We might be on the verge of too many screens. It seems like everyday, new versions of common objects are “re-invented” with built-in wifi and bright touchscreens. A promising antidote to our screen addiction are voice interfaces.  But, for independent makers and entrepreneurs, it’s hard to build a simple speech detector using free, open data and code. Many voice recognition datasets require preprocessing before a neural network model can be built on them. To help with this, TensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people. In this competition, you're challenged to use the Speech Commands Dataset to build an algorithm that understands simple spoken commands. By improving the recognition accuracy of open-sourced voice interface tools, we can improve product effectiveness and their accessibility.;https://www.kaggle.com/c/tensorflow-speech-recognition-challenge;Google Brain;Can you build an algorithm that understands simple speech commands?;['categorizationaccuracy'];1,314;TensorFlow Speech Recognition Challenge;Featured prediction Competition
2020-01-23 00:59:00;“Why is the sky blue?” This is a question an open-domain question answering (QA) system should be able to respond to. QA systems emulate how people look for information by reading the web to return answers to common questions. Machine learning can be used to improve the accuracy of these answers. Existing natural language models have been focused on extracting answers from a short paragraph rather than reading an entire page of content for proper context. As a result, the responses can be complicated or lengthy. A good answer will be both succinct and relevant. In this competition, your goal is to predict short and long answer responses to real questions about Wikipedia articles. The dataset is provided by Google's Natural Questions, but contains its own unique private test set. A visualization of examples shows long and—where available—short answers. In addition to prizes for the top teams, there is a special set of awards for using TensorFlow 2.0 APIs.  If successful, this challenge will help spur the development of more effective and robust QA systems. About TensorFlow TensorFlow is an open source platform for machine learning. With TensorFlow 2.0, tf.keras is the preferred high-level API for TensorFlow, to make model building easier and more intuitive. You may use the tf.keras built-in compile()/fit() methods, or write your own custom training loops. See the Effective TensorFlow 2.0 guide and the tf.keras guide for more details. TensorFlow 2.0 was recently released and this competition is to challenge Kagglers to use TensorFlow 2.0’s APIs focused on usability, and easier, more intuitive development, to make advancements on Question Answering.;https://www.kaggle.com/c/tensorflow2-question-answering;TensorFlow;Identify the answers to real user questions about Wikipedia page content;['text data', 'text mining', 'custom metric'];1,233;TensorFlow 2.0 Question Answering;Featured Code Competition
2017-11-21 00:59:00;"As many of us can attest, learning another language is tough. Picking up on nuances like slang, dates and times, and local expressions, can often be a distinguishing factor between proficiency and fluency. This challenge is even more difficult for a machine. Many speech and language applications, including text-to-speech synthesis (TTS) and automatic speech recognition (ASR), require text to be converted from written expressions into appropriate ""spoken"" forms. This is a process known as text normalization, and helps convert 12:47 to ""twelve forty-seven"" and $3.16 into ""three dollars, sixteen cents.""  However, one of the biggest challenges when developing a TTS or ASR system for a new language is to develop and test the grammar for all these rules, a task that requires quite a bit of linguistic sophistication and native speaker intuition. A     baby     giraffe     is     6ft    six feet tall     and     weighs     150lb    one hundred fifty pounds .    sil  In this competition, you are challenged to automate the process of developing text normalization grammars via machine learning. This track will focus on English, while a separate will focus on Russian here: Russian Text Normalization Challenge About the sponsor Google's Text Normalization Research Group conducts research and creates tools for the detection, normalization and denormalization of non-standard words such as abbreviations, numbers or currency expressions; and semiotic classes -- text tokens and token sequences that represent particular entities that are semantically constrained, such as measure phrases, addresses or dates. Applications of this work include text-to-speech synthesis, automatic speech recognition, and information extraction/retrieval.";https://www.kaggle.com/c/text-normalization-challenge-english-language;Google;Convert English text from written expressions into spoken forms;['text data', 'linguistics', 'languages', 'categorizationaccuracy'];260;Text Normalization Challenge - English Language;Research prediction Competition
2017-11-21 00:59:00;"As many of us can attest, learning another language is tough. Picking up on nuances like slang, dates and times, and local expressions, can often be a distinguishing factor between proficiency and fluency. This challenge is even more difficult for a machine. Many speech and language applications, including text-to-speech synthesis (TTS) and automatic speech recognition (ASR), require text to be converted from written expressions into appropriate ""spoken"" forms. This is a process known as text normalization, and helps convert 12:47 to ""twelve forty-seven"" and $3.16 into ""three dollars, sixteen cents.""  However, one of the biggest challenges when developing a TTS or ASR system for a new language is to develop and test the grammar for all these rules, a task that requires quite a bit of linguistic sophistication and native speaker intuition. Проверено         12 февраля 2013    двенадцатого февраля две тысячи тринадцатого года ,    sil Архивировано     из     первоисточника     15 февраля 2013    февраля две тысячи тринадцатого года .    sil  In this competition, you are challenged to automate the process of developing text normalization grammars via machine learning. This track will focus on Russian, while a separate will focus on English here: English Text Normalization Challenge About the sponsor Google's Text Normalization Research Group conducts research and creates tools for the detection, normalization and denormalization of non-standard words such as abbreviations, numbers or currency expressions; and semiotic classes -- text tokens and token sequences that represent particular entities that are semantically constrained, such as measure phrases, addresses or dates. Applications of this work include text-to-speech synthesis, automatic speech recognition, and information extraction/retrieval.";https://www.kaggle.com/c/text-normalization-challenge-russian-language;Google;Convert Russian text from written expressions into spoken forms;['text data', 'linguistics', 'languages', 'categorizationaccuracy'];162;Text Normalization Challenge - Russian Language;Research prediction Competition
2018-10-20 01:59:00;Several areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface. But unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers. To create the most accurate seismic images and 3D renderings, TGS (the world’s leading geoscience data company) is hoping Kaggle’s machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not.;https://www.kaggle.com/c/tgs-salt-identification-challenge;TGS;Segment salt deposits beneath the Earth's surface;['image data', 'geology', 'custom metric'];3,229;TGS Salt Identification Challenge;Featured prediction Competition
2016-02-14 00:59:00;"The Allen Institute for Artificial Intelligence (AI2) is working to improve humanity through fundamental advances in artificial intelligence. One critical but challenging problem in AI is to demonstrate the ability to consistently understand and correctly answer general questions about the world.  The Aristo project at AI2 is focused on building such a system. One way Aristo ""learns"" is by extracting facts from various sources and processing them into a structured knowledge base. When taking an exam, questions are parsed and processed along with any accompanying diagrams to determine a strategy for answering. Aristo then uses entailment, statistical analysis, and inference methods to select a final answer. While Aristo's abilities have improved significantly in the last two years, it still doesn't have perfect, reliable methods of gathering knowledge, understanding questions, or reasoning through answers. Using a dataset of multiple choice question and answers from a standardized 8th grade science exam, AI2 is challenging you to create a model that gets to the head of the class.";https://www.kaggle.com/c/the-allen-ai-science-challenge;Allen Institute for Artificial Intelligence;Is your model smarter than an 8th grader?;['text data', 'multiclass classification', 'tabular data', 'artificial intelligence', 'categorizationaccuracy'];170;The Allen AI Science Challenge;Featured prediction Competition
2014-05-06 01:59:00;(Please note: this competition is only open to students of https://www.edx.org/course/mitx/mitx-15-071x-analytics-edge-1416) What predicts happiness? In this competition, you'll be using data from Show of Hands, an informal polling platform for use on mobile devices and the web, to see what aspects and characteristics of people's lives predict happiness. Show of Hands has been downloaded over 300,000 times across Apple and Android app stores, and users have cast more than 75 million votes. In this problem, we'll use data from thousands of users and one hundred different questions to see which responses predict happiness.  Acknowledgements This competition is brought to you by 15.071x, edX, and Show of Hands.;https://www.kaggle.com/c/the-analytics-edge-mit-15-071x;Kaggle;Learn what predicts happiness by using informal polling questions.;['auc'];1,684;The Analytics Edge (15.071x);Playground prediction Competition
2013-06-18 01:59:59;We're aware the competition deadline is tight, but wanted to give Kagglers the chance to work on this interesting problem. The results will be presented at the Workshop on Machine Learning for Bioacoustics at ICML 2013. Acknowledgements http://sabiod.org;https://www.kaggle.com/c/the-icml-2013-bird-challenge;;Identify bird species from continuous audio recordings;['auc'];76;The ICML 2013 Bird Challenge;Research prediction Competition
2013-06-18 01:59:59;"This competition complements the previously held Marinexplore Whale Detection Challenge, in which Cornell University provided data from a ship monitoring application termed ""Auto Buoy"", or AB Monitoring System. In the Marinexplore challenge we received solutions that exceeded 98% accuracy and will ultimately advance the process of automatically classifying North Atlantic Right Whales using the AB Monitoring Platform. Since the results from the previous challenge proved so successful, we decided to extend the goals and consider applications that involve running algorithms on archival data recorded using portable hydrophone assemblies, otherwise referred to as Marine Autonomous Recording Unit (or MARU’s). Since Cornell and its partners have been using the MARU for over a decade, a sizable collection of data has been accumulated. This data spans several ocean basins and covers a variety of marine mammal species. Solutions to this challenge will be ported to a High Performance Computing (HPC) platform, being developed in part through funding provided by the Office of Naval Research (ONR grant N000141210585, Dugan, Clark, LeCun and Van Parijs). Together, Cornell will combine algorithms, HPC technologies and its data archives to explore data using highly accurate measuring tools. We encourage participants who developed prior solutions (through the collaboration with Marinexplore) to test them on this data. Workshop on Machine Learning for Bioacoustics";https://www.kaggle.com/c/the-icml-2013-whale-challenge-right-whale-redux;;Develop recognition solutions to detect and classify right whales for BIG data mining and exploration studies;['auc'];129;The ICML 2013 Whale Challenge - Right Whale Redux;Research prediction Competition
2017-04-13 01:59:00;Nearly half of the world depends on seafood for their main source of protein. In the Western and Central Pacific, where 60% of the world’s tuna is caught, illegal, unreported, and unregulated fishing practices are threatening marine ecosystems, global seafood supplies and local livelihoods. The Nature Conservancy is working with local, regional and global partners to preserve this fishery for the future.  Currently, the Conservancy is looking to the future by using cameras to dramatically scale the monitoring of fishing activities to fill critical science and compliance monitoring data gaps. Although these electronic monitoring systems work well and are ready for wider deployment, the amount of raw data produced is cumbersome and expensive to process manually. The Conservancy is inviting the Kaggle community to develop algorithms to automatically detect and classify species of tunas, sharks and more that fishing boats catch, which will accelerate the video review process. Faster review and more reliable data will enable countries to reallocate human capital to management and enforcement activities which will have a positive impact on conservation and our planet. Machine learning has the ability to transform what we know about our oceans and how we manage them. You can be part of the solution. Resources You can learn more about this competition and The Nature Conservancy in the video below.;https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring;The Nature Conservancy;Can you detect and classify species of fish?;['image data', 'multiclass classification', 'multiclassloss'];2,293;The Nature Conservancy Fisheries Monitoring;Featured prediction Competition
2013-09-29 03:00:00;">> View the San Francisco venue live << Competition ends: Save the date! You're invited. We are organizing not one but two exciting competitions: a 24-hour hackathon followed by a deeper, two-month dive. The purpose of both competitions is to quantify and predict how people will react to a specific 311 issue. What makes an issue urgent? What do citizens really care about? How much does location matter? Being able to predict the most pressing 311 topics will allow governments to focus their efforts on fixing the most important problems. The data set for both competitions contains several hundred thousand 311 issues from four cities. For those in the Bay Area: on the evening of September 27 to the evening of September 28, we will convene at Microsoft San Francisco (835 Market St Ste 700, San Francisco, CA), in the heart of the shopping district.  At that place and time, we will release the password to the competition data.  Please register on the meetup page if you plan to attend. Come see, predict, and fix by working with actual city data on real city problems.  Those outside the Bay Area can still participate in the Hackathon, and also in the ensuing longer competition. For those who are more interested in using the data for visualization or ""non-predictive"" data mining, we have added a $500 visualization prize to both the hackathon and the longer competition.  When the competition opens, you may submit as many entries as you wish via the Visualization page.  If you're plotting issues on maps, displaying the text in some meaningful way, or making any other creative use of the data, save it and post it! Note: the data will be uploaded prior to the hackathon   The data will be encrypted and the key released to participants at the start of the hackathon. Because this is a live event and delays are possible, the start time is approximate. Please be sure you have a program capable of handling encrypted .7z files. About 311 311 is a mechanism by which citizens can express their desire to solve a problem the city or government by submitting a description of what needs to be done, fixed, or changed. In effect, this provides a high degree of transparency between government and its constituents. Once an issue has been established, citizens can vote and make comments on the issue so that government officials have some degree of awareness about what is the most important issue to address.  Sponsors The meeting space has been provided by Microsoft.  Prize money is graciously offered by our sponsors:  On the citizen side, SeeClickFix leverages crowdsourcing to help both maintain the flow of incoming requests but show the public how effective you can be. When anyone in the community can report or comment on any issue, the entire group has a better perspective on what's happening--and how to fix it effectively. For governments, SeeClickFix acts as a completely-customizable CRM that plugs into your existing request management tools. From types of service requests to managing different watch areas, SeeClickFix helps better m  A public policy entrepreneur and open innovation expert David advises numerous governments on open government and open data and works with leading non-profits and businesses on strategy, open innovation and community management. In addition to his work, David is an affiliate with the Berkman Centre for Internet and Society at Harvard where he is looking at issues surrounding the politics of data. You can find David's writing on open innovation, public policy, public sector renewal and open source systems at his blog, or at TechPresident. In addition to his writing, David is frequently invited to speak on open government, policy making, negotiation and strategy to executives, policymakers, and students. You can read a background on how this challenge came to be here.";https://www.kaggle.com/c/the-seeclickfix-311-challenge;;Predict which 311 issues are most important to citizens;['rmsle'];80;See Click Predict Fix - Hackathon;Featured prediction Competition
2016-01-27 00:59:00;Do you laugh (and then get down to work) in the face of terabytes of noisy, non-stationary data? Winton Capital is looking for data scientists who excel at finding the hidden signal in the proverbial haystack, and who are excited by creating novel statistical modelling and data mining techniques.  In this recruiting competition, Winton challenges you to take on the very difficult task of predicting the future (stock returns). Given historical stock performance and a host of masked features, can you predict intra and end of day returns without being deceived by all the noise?  Research scientists at Winton have crafted this competition to be challenging and fun for the community while providing a taste of the types of problems they work on everyday. They're excited to connect with Kagglers who bring a unique background and creative approach to the competition. Winton is offering cash prizes to winning teams as a reward for their work, but the intent of the competition is not commercial. The intellectual property you create remains your own and will be evaluated in the context of suitability for employment.   For more on the culture at Winton, check out the About Winton page or their careers page.;https://www.kaggle.com/c/the-winton-stock-market-challenge;;Join a multi-disciplinary team of research scientists;['finance', 'tabular data', 'wmae'];829;The Winton Stock Market Challenge;Featured prediction Competition
2019-05-31 01:59:00;"We're going to make you an offer you can't refuse: a Kaggle competition!  In a world… where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's ""You had me at 'Hello.'"" For others, the trailer falls short of expectations and you think ""What we have here is a failure to communicate."" In this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries.  You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release. Join in, ""make our day"", and then ""you've got to ask yourself one question: 'Do I feel lucky?'""";https://www.kaggle.com/c/tmdb-box-office-prediction;Kaggle;Can you predict a movie's worldwide box office revenue?;['movies and tv shows', 'tabular data', 'rmsle'];1,398;TMDB Box Office Prediction;Playground prediction Competition
2010-09-20 01:00:00;Tourism is one of the most rapidly growing global industries and tourism forecasting is becoming an increasingly important activity in planning and managing the industry. The time series in this competition have already been studied in detail in a paper by Athanasopoulos, Hyndman, Song and Wu (2010) (to be published in the International Journal of Forecasting). For part one of this contest, entrants must submit forecasts of the next four yearly observations. Forecasts will be tested against the actual future observations for each series.Results for part one will be evaluated on the basis of the average MASE across all series.Note, this is just part one of the competition. Part two will require entrants to forecast monthly and quarterly time series. The overall result will be calculated as the average MASE across all time series and across all frequencies. The overall winner will collect $AUD500 and will be invited to contribute a discussion paper to the International Journal of Forecasting describing their methodology and giving their results, provided either the monthly results are better than 1.38, the quarterly results are better than 1.43 or the yearly results are better than 2.28. These thresholds are the best performing methods in the analysis of these data described in Athanasopoulos et al (2010).  In other words, the winner has to beat the best results in this paper for at least one of the three sets of series. It will also be necessary that the winner be able to describe their method clearly, in sufficient detail to enable replication and in a form suitable for the International Journal of Forecasting. The paper would appear in the April 2011 issue of the IJF.Update: The team Theta Benchmark (see the leaderboard), uses the best method from Athanasopoulos, Hyndman, Song and Wu (2010).;https://www.kaggle.com/c/tourism1;;Part one requires competitors to predict 518 tourism-related time series. The winner of this competition will be invited to contribute a discussion paper to the International Journal of Forecasting.;['custom metric'];55;Tourism Forecasting Part One;Featured prediction Competition
2010-11-22 00:00:00;Tourism is one of the most rapidly growing global industries and tourism  forecasting is becoming an increasingly important activity in planning  and managing the industry. The time series in this competition have  already been studied in detail in a paper by Athanasopoulos, Hyndman, Song and Wu (2010)  (to be published in the International Journal of Forecasting). For part two of this contest, entrants must submit forecasts of the next 24 monthly and 8 quarterly  observations for 793 time series. Forecasts will be evaluated against the actual future  observations for each series.Results for part two will be evaluated on the basis of the average MASE across all series. The overall  result will be calculated as the average MASE across all time series (from part one and part two of this competition).The overall winner will collect $AUD500  and will be invited to contribute a discussion paper to the  International Journal of Forecasting describing their methodology and  giving their results, provided either the monthly results are better  than 1.38, the quarterly results are better than 1.43 or the yearly  results are better than 2.28. These thresholds are the best performing  methods in the analysis of these data described in Athanasopoulos et al  (2010).  In other words, the winner has to beat the best results in this  paper for at least one of the three sets of series (note that 21 teams outdid the yearly threshold in part one of this competition). It will also be  necessary that the winner be able to describe their method clearly, in  sufficient detail to enable replication and in a form suitable for the  International Journal of Forecasting. The paper would appear in the  April 2011 issue of the IJF.Update 22 October: a correction was made to the data file. Please ensure that you build your models using tourismdata2revision2.csv.;https://www.kaggle.com/c/tourism2;;Part two requires competitors to predict 793 tourism-related time series. The winner of this competition will be invited to contribute a discussion paper to the International Journal of Forecasting.;['custom metric'];42;Tourism Forecasting Part Two;Featured prediction Competition
2018-08-14 01:59:00;To explore what our universe is made of, scientists at CERN are colliding protons, essentially recreating mini big bangs, and meticulously observing these collisions with intricate silicon detectors. While orchestrating the collisions and observations is already a massive scientific accomplishment, analyzing the enormous amounts of data produced from the experiments is becoming an overwhelming challenge. Event rates have already reached hundreds of millions of collisions per second, meaning physicists must sift through tens of petabytes of data per year. And, as the resolution of detectors improve, ever better software is needed for real-time pre-processing and filtering of the most promising events, producing even more data. To help address this problem, a team of Machine Learning experts and physics scientists working at CERN (the world largest high energy physics laboratory),  has partnered with Kaggle and prestigious sponsors to answer the question: can machine learning assist high energy physics in discovering and characterizing new particles? Specifically, in this competition, you’re challenged to build an algorithm that quickly reconstructs particle tracks from 3D points left in the silicon detectors. This challenge consists of two phases:   The Accuracy phase has run on Kaggle from May to 13th August 2018 (Winners to be announced by end September). Here we’ll be focusing on the highest score, irrespective of the evaluation time. This phase is an official IEEE WCCI competition (Rio de Janeiro, Jul 2018).    The Throughput phase will run on Codalab starting in September 2018. Participants will submit their software which is evaluated by the platform. Incentive is on the throughput (or speed) of the evaluation while reaching a good score. This phase is an official NIPS competition (Montreal, Dec 2018).   All the necessary information for the Accuracy phase is available here on Kaggle site. The overall TrackML challenge web site is there.;https://www.kaggle.com/c/trackml-particle-identification;CERN;High Energy Physics particle tracking in CERN detectors;['tabular data', 'physics', 'custom metric'];648;TrackML Particle Tracking Challenge;Featured prediction Competition
2014-11-11 00:59:00;In the late 90's, Yann LeCun's team pioneered the successful application of machine learning to optical character recognition. 25 years later, machine learning continues to be an invaluable tool for text processing downstream from the OCR process.  Tradeshift has created a dataset with thousands of documents, representing millions of words. In each document, several bounding boxes containing text are selected. For each piece of text, many features are extracted and certain labels are assigned.  In this competition, participants are asked to create and open source an algorithm that correctly predicts the probability that a piece of text belongs to a given class.;https://www.kaggle.com/c/tradeshift-text-classification;;Classify text blocks in documents;['logloss'];374;Tradeshift Text Classification;Featured prediction Competition
2017-03-26 00:59:00;"What does physics have in common with biology, cooking, cryptography, diy, robotics, and travel? If you answered ""all pursuits are governed by the immutable laws of physics"" we'll begrudgingly give you partial credit. If you answered ""all were chosen randomly by a scheming Kaggle employee for a twisted transfer learning competition"", congratulations, we accept your answer and mark the question as solved. In this competition, we provide the titles, text, and tags of Stack Exchange questions from six different sites. We then ask for tag predictions on unseen physics questions. Solving this problem via a standard machine approach might involve training an algorithm on a corpus of related text. Here, you are challenged to train on material from outside the field. Can an algorithm learn appropriate physics tags from ""extreme-tourism Antarctica""? Let's find out. Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from the Stack Exchange data dump.";https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;;Predict tags from models trained on unrelated topics;['text data', 'multiclass classification', 'tabular data', 'meanfscore'];380;Transfer Learning on Stack Exchange Tags;Playground prediction Competition
2019-01-11 00:59:00;Rudolph the red-nosed reindeer Had some very tired hooves But he had a job to finish Could he do it with the shortest moves? All of the other reindeer Used to laugh and mock his code They always said poor Rudolph Couldn't handle the workload Then one foggy Christmas Eve Santa came to say I see you've taken number theory Please make this night a bit less dreary? Then how the reindeer loved him and each enrolled in an AI degree Rudolph the red-nosed reindeer We get to go to bed early! Rudolph has always believed in working smarter, not harder. And what better way to earn the respect of Comet and Blitzen than showing the initiative to improve Santa's annual route for delivering toys on Christmas Eve? This year,  Rudolph believes he can motivate the overworked Reindeer team by wisely choosing the order in which they visit the houses on Santa's list. The houses in prime cities always leave carrots for the Reindeers alongside the usual cookies and milk. These carrots are just the sustenance the Reindeers need to keep pace. In fact, Rudolph has found that if the Reindeer team doesn't originate from a prime city exactly every 10th step, it takes the 10% longer than it normally would to make their next destination! Can you help Rudolph solve the Traveling Santa problem subject to his carrot constraint? His team--and Santa--are counting on you! Attributions: Reindeer Photo: Norman Tsui Stocking Photo:  Wesley Tingey;https://www.kaggle.com/c/traveling-santa-2018-prime-paths;Kaggle;But does your code recall, the most efficient route of all?;['optimization', 'custom metric'];1,871;Traveling Santa 2018 - Prime Paths;Featured prediction Competition
2013-01-19 01:00:00;This competition will launch at midnight UTC on Saturday, December 15. Santa Claus was excited to learn about the Kaggle competition platform, and wanted to use it for a slightly different purpose. Rather than a predictive modeling problem, he has an optimization problem for you: a very, very important optimization problem. Santa needs help choosing the route he takes when delivering presents around the globe. Every year, Santa has to visit every boy and girl on his list.  It's a tough challenge, and Santa admits he scored a B- on his combinatorical optimization final. He's  hoping you can develop algorithms that will solve his problem year after year. Santa asked that we give you one particular instance of his TSP (Traveling Santa Problem). However, Santa's dilemma isn't quite the same as the Traveling Salesman Problem with which you may be familiar. Santa likes to see new terrain every year--don't ask,  it's a reindeer thing--and doesn't want his route to be predictable.  You're looking for shortest-distance paths through a set of chimneys, but instead of providing one path, Santa asks you to provide two disjoint paths. If one of your paths contains an edge from A to B, your other path must not contain  an edge from A to B or from B to A (either order still counts as using that edge). Your score is the larger of the two distances.  Santa asks competition winners to publish and open source the algorithms they use (for his future use, of course). Rudolph was very adament about minimizing his workload. Trust us, you don't want to be on Rudolph's bad side. Important note about prizes: We believe that Kaggle's public leaderboard is very important for both the fun of the competition and achieving great results, and we want to provide an incentive for everyone to submit to the public leaderboard  all along the way (even though you can easily determine your submission's score all by yourself). So the competition will have two sets of prizes, one based on the scores at the end of the competition, and one based on the scores at  the end of a randomly chosen day (UTC) between December 23 and January 17.  The day will not be revealed (or even chosen) until after the competition ends. (The competition will end at the end of the day UTC on January 18.)   Attributions: Data generation and lots of help framing the problem (including coming up with this TSP variant):  Robert Bosch of Oberlin College Math Department Santa photo: AurélienS Sleigh photo: Creative Tools Globe: William Cook;https://www.kaggle.com/c/traveling-santa-problem;;Solve ye olde traveling salesman problem to help Santa Claus deliver his presents;['custom metric'];353;Traveling Santa Problem;Featured prediction Competition
2020-06-03 13:00:00;"LAUNCHED   This competition was launched and opened for submissions on May 27th 2020. Submissions will close in 1 week at 11:00 AM UTC on June 3rd 2020. The public leaderboard is based on the TREC-COVID Round 2 dataset. The private leaderboard will be based on the Round 3 dataset, which will be evaluated after the competition closes. Review the Data page for more details.  Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presents a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge will identify answers for some of today's questions while building infrastructure to improve tomorrow's search systems. Kaggle first teamed up with the Allen Institute for AI in the launch of the COVID-19 Open Research Dataset (CORD-19). TREC-COVID builds on the CORD-19 Challenge by using the same document set, a collection of biomedical literature articles that has been updated on a weekly rolling basis.  This is the 3rd Round of the TREC-COVID Challenge. Prior runs were hosted directly on the TREC-COVID Site. For this round, you have the option to submit on Kaggle or directly to the TREC-COVID platform. The organizers have added 5 additional COVID-related topics to the 35 topics from the first two rounds, for a total of 40 topics. You will create a retrieval system that returns ranked lists of documents from CORD-19 for (a) each of these additional Round 3 topics (""runs"") and as well as (b) residual rankings on the completed Round 1 & 2 topics, i.e., for any documents not judged in the CORD-19 dataset (not previously included as a ranked document).  The eligible population of documents for Round 3 is anything included in the CORD-19 release up to Round 3's launch date, last updated on May 19th 2020. Following the close of Round 3, NIST will gather the collective set of participants' runs, to include those participants submitting directly through TREC-COVID. The organizers will then assess some reasonable subset of these submissions for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, will then be used to score the submitted runs. It is important to understand that not all documents will be assessed, and thus the private leaderboard score will be based on partial document assessment. With your help, the final document and topic sets together with the cumulative relevance judgments will comprise a COVID test collection. The incremental nature of the collection will support research on search systems for dynamic environments. Acknowledgments The Text REtrieval Conference (TREC) was founded in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. The TREC-COVID Challenge is being organized by the Allen Institute for Artificial Intelligence (AI2), the National Institute of Standards and Technology (NIST), the National Library of Medicine (NLM), Oregon Health and Science University (OHSU), and the University of Texas Health Science Center at Houston (UTHealth). See the NIST press release for more information.";https://www.kaggle.com/c/trec-covid-information-retrieval;TREC-COVID Organizers;Build a pandemic document retrieval system;['nlp', 'covid19', 'text data', 'text mining', 'ndcg@{k}'];19;TREC-COVID Information Retrieval;Research prediction Competition
2020-06-30 01:59:00;Human brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.  In this competition, you will predict multiple assessments plus age from multimodal brain MRI features. You will be working from existing results from other data scientists, doing the important work of validating the utility of multimodal features in a normative population of unaffected subjects. Due to the complexity of the brain and differences between scanners, generalized approaches will be essential to effectively propel multimodal neuroimaging research forward. The Tri-Institutional Georgia State University/Georgia Institute of Technology/Emory University Center for Translational Research in Neuroimaging and Data Science (TReNDS) leverages advanced brain imaging to promote research into brain health. The organization is focused on developing, applying and sharing advanced analytic approaches and neuroinformatics tools. Among its software projects are the GIFT and FIT neuroimaging toolboxes, the COINS data management system, and the COINSTAC toolkit for federated learning, all aimed at supporting data scientists and other neuroimaging researchers. Making the leap from research to clinical application is particularly difficult in brain health. In order to translate to clinical settings, research findings have to be reproduced consistently and validated in out-of-sample instances. The problem is particularly well-suited for data science, but current approaches typically do not generalize well. With this large dataset and competition, your efforts could directly address an important area of brain research. Acknowledgments;https://www.kaggle.com/c/trends-assessment-prediction;GSU/TReNDS;Multiscanner normative age and assessments prediction with brain function, structure, and connectivity;['computer vision', 'neuroscience', 'wmae'];1,047;TReNDS Neuroimaging;Research prediction Competition
2020-06-17 01:59:00;"""My ridiculous dog is amazing."" [sentiment: positive] With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds.  But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment. Help build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools? In this competition we've extracted support phrases from Figure Eight's Data for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.";https://www.kaggle.com/c/tweet-sentiment-extraction;Kaggle;Extract support phrases for sentiment labels;['internet', 'text data', 'custom metric'];2,227;Tweet Sentiment Extraction;Featured Code Competition
2012-06-30 01:59:59;The aim of this competition is to determine the best models to predict the personality traits of Machiavellianism, Narcissism, Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism based on Twitter usage and linguistic inquiry. The organizers provide all interested participants an anonymised dataset of users self assessed personality scores (based checklists developed by Prof Del Paulhus at the University of British Columbia and Prof Sam Gosling and the University of Texas) together  with 337 variables derived from functions of Twitter information and lingusitc analysis. The best performing model(s) will be formally cited in a future paper and any presentations.   The intention of this research is to seperate fact from fiction and examine just what can be predicted by social media use and how this information might be used, both for good and bad. As an organization, the Online Privacy Foundation works to raise awareness  of online privacy issues and empower people to make informed choices about what they do online. We hope you'll support our mission and take part in this competition.    (;https://www.kaggle.com/c/twitter-personality-prediction;;Identify the best performing model(s) to predict personality traits based on Twitter usage;['mcap'];89;Personality Prediction Based on Twitter Stream;Research prediction Competition
2012-06-30 01:59:59;The  aim of the competition is to determine to what degree it's possible to predict people with a sufficiently high degree of Psychopathy based on Twitter usage and Linguistic Inquiry. The organizers provide all interested participants an anonymised dataset of users self assessed psychopathy scores together with 337 variables derived from functions of Twitter information, useage and lingusitc analysis. Psychopathy scores are based on a  checklist developed by Professor Del Paulhus at the University of British Columbia. The model should aim to identify people scoring high in Psychopathy, for the purpose of this competition, defined as 2 SD's above a mean of 1.98. This accounts for roughly 3% of the entire sample and therefore the challenge with this dataset  is developing a model to work with a highly imbalanced dataset.  The best performing model(s) will be formally cited in a future paper/papers. The authors of the winning model may also be invited to attend future conferences to discuss their model.   The intention of this research is to seperate fact from fiction and examine just what can be predicted by social media use and how this information might be used, both for good and bad. As an organization, the Online Privacy Foundation works to raise awareness  of online privacy issues and empower people to make informed choices about what they do online. We hope you'll support our mission and take part in this competition.;https://www.kaggle.com/c/twitter-psychopathy-prediction;;Identify people who have a high degree of Psychopathy based on Twitter usage.;['mcap'];111;Psychopathy Prediction Based on Twitter Usage;Research prediction Competition
2017-04-26 01:59:00;Finding the perfect place to call your new home should be more than browsing through endless listings. RentHop makes apartment search smarter by using data to sort rental listings by quality. But while looking for the perfect apartment is difficult enough, structuring and making sense of all available real estate data programmatically is even harder. Two Sigma and RentHop, a portfolio company of Two Sigma Ventures, invite Kagglers to unleash their creative engines to uncover business value in this unique recruiting competition.   Two Sigma invites you to apply your talents in this recruiting competition featuring rental listing data from RentHop. Kagglers will predict the number of inquiries a new listing receives based on the listing’s creation date and other features. Doing so will help RentHop better handle fraud control, identify potential listing quality issues, and allow owners and agents to better understand renters’ needs and preferences. Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. This challenge is an opportunity for competitors to gain a sneak peek into Two Sigma's data science work outside of finance. Acknowledgments This competition is co-hosted by Two Sigma and RentHop (a portfolio company of Two Sigma Ventures, which is a division of Two Sigma Investments) to encourage creativity in using real world data to solve everyday problems.;https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;Two Sigma;How much interest will a new rental listing on RentHop receive?;['text data', 'multiclass classification', 'tabular data', 'housing', 'multiclassloss'];2,480;Two Sigma Connect: Rental Listing Inquiries;Recruitment prediction Competition
2017-03-02 00:59:00;"How can we use the world’s tools and intelligence to forecast economic outcomes that can never be entirely predictable? This question is at the core of countless economic activities around the world – including at Two Sigma Investments, who has been applying technology and systematic strategies to financial trading since 2001. For over 15 years, Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. Through this exclusive partnership, Two Sigma is excited to explore what untapped value Kaggle's diverse data science community can discover in the financial markets. Economic opportunity depends on the ability to deliver singularly accurate forecasts in a world of uncertainty. By accurately predicting financial movements, Kagglers will learn about scientifically-driven approaches to unlocking significant predictive capability. Two Sigma is excited to find predictive value and gain a better understanding of the skills offered by the global data science crowd. What is a Code Competition? Welcome to Kaggle's very first Code Competition! In contrast to our traditional competitions, where competitors submit only prediction outputs, participants in Code Competitions will submit their code via Kaggle Kernels. All kernels are private by default in Code Competitions. You can build your models in Kernels by running them on a training set and, once you're ready to submit your code, your model's performance will be evaluated against the test set and your score and public leaderboard position revealed. As with our traditional competitions, we still maintain a private leaderboard test set, which your code is also evaluated against for final scoring, but is not revealed until the competition closes. Since Code Competitions are brand new, we ask for your patience if you encounter bugs or frustrating platform quirks. Please report any issues you find in the forums and we'll do our best to respond. Who owns my code? You do. Even though you are submitting code, the intellectual property exchange here works similarly to a standard prediction competition, whereby prize winners have the option to grant a non-exclusive license in exchange for a prize. There is a new addition to the terms for Code Competitions: Kaggle and the competition host reserve a right to review submissions ""for purposes related to evaluation and scoring in this Competition, including but not limited to the assessment of potential cheating behavior."" Please refer to the official competition rules for full details. Getting Started  Review the data page for details about the data and the evaluation metric. You may download the train set for local training. Take a look at the tutorial covering the new code submission process under the submission instructions tab. You'll find step-by-step instructions, some helpful pointers, plus details on environment constraints. Get feedback on your benchmark code and share exploratory analyses with the community by making any of your kernels public. Improve your score!  Note: there is no cost of entry for participation.";https://www.kaggle.com/c/two-sigma-financial-modeling;Two Sigma;Can you uncover predictive value in an uncertain world?;['finance', 'custom metric'];2,066;Two Sigma Financial Modeling Challenge;Featured Code Competition
2019-08-06 01:59:00;August 2019 Update: this competition is closed and is no longer accepting submissions.  The data has been removed from this competition and is not available for use. Thanks for participating! Can we use the content of news analytics to predict stock price performance? The ubiquity of data today enables investors at any scale to make better investment decisions. The challenge is ingesting and interpreting the data to determine which data is useful, finding the signal in this sea of information. Two Sigma  is passionate about this challenge and is excited to share it with the Kaggle community. As a scientifically driven investment manager, Two Sigma has been applying technology and data science to financial forecasts for over 17 years. Their pioneering advances in big data, AI, and machine learning have pushed the investment industry forward. Now, they're eager to engage with Kagglers in this continuing pursuit of innovation. By analyzing news data to predict stock prices, Kagglers have a unique opportunity to advance the state of research in understanding the predictive power of the news. This power, if harnessed, could help predict financial outcomes and generate significant economic impact all over the world.  Data for this competition comes from the following sources:  Market data provided by Intrinio. News data provided by Thomson Reuters. Copyright Thomson Reuters, 2017. All Rights Reserved. Use, duplication, or sale of this service, or data contained herein, except as described in the Competition Rules, is strictly prohibited.    The THOMSON REUTERS Kinesis Logo and THOMSON REUTERS are trademarks of Thomson Reuters and its affiliated companies in the United States and other countries and used herein under license.;https://www.kaggle.com/c/two-sigma-financial-news;Two Sigma;Use news analytics to predict stock price performance;['finance', 'news', 'currencies and foreign exchange', 'custom metric'];2,927;Two Sigma: Using News to Predict Stock Movements;Featured Code Competition
2016-08-19 01:59:00;Even the bravest patient cringes at the mention of a surgical procedure. Surgery inevitably brings discomfort, and oftentimes involves significant post-surgical pain. Currently, patient pain is frequently managed through the use of narcotics that bring a bevy of unwanted side effects. This competition's sponsor is working to improve pain management through the use of indwelling catheters that block or mitigate pain at the source. Pain management catheters reduce dependence on narcotics and speed up patient recovery. Accurately identifying nerve structures in ultrasound images is a critical step in effectively inserting a patient’s pain management catheter. In this competition, Kagglers are challenged to build a model that can identify nerve structures in a dataset of ultrasound images of the neck. Doing so would improve catheter placement and contribute to a more pain free future.;https://www.kaggle.com/c/ultrasound-nerve-segmentation;;Identify nerve structures in ultrasound images of the neck;['image data', 'healthcare', 'dice'];922;Ultrasound Nerve Segmentation;Featured prediction Competition
2019-11-19 00:59:00;Climate change has been at the top of our minds and on the forefront of important political decision-making for many years. We hope you can use this competition’s dataset to help demystify an important climatic variable. Scientists, like those at Max Planck Institute for Meteorology, are leading the charge with new research on the world’s ever-changing atmosphere and they need your help to better understand the clouds. Shallow clouds play a huge role in determining the Earth's climate. They’re also difficult to understand and to represent in climate models. By classifying different types of cloud organization, researchers at Max Planck hope to improve our physical understanding of these clouds, which in turn will help us build better climate models. There are many ways in which clouds can organize, but the boundaries between different forms of organization are murky. This makes it challenging to build traditional rule-based algorithms to separate cloud features. The human eye, however, is really good at detecting features—such as clouds that resemble flowers. In this challenge, you will build a model to classify cloud organization patterns from satellite images. If successful, you’ll help scientists to better understand how clouds will shape our future climate. This research will guide the development of next-generation models which could reduce uncertainties in climate projections. Help us remove the haze from climate models and bring clarity to cloud identification. For more information on the scientific background and how the labels were created see the following paper.;https://www.kaggle.com/c/understanding_cloud_organization;Max Planck Institute for Meteorology;Can you classify cloud structures from satellites?;['image data', 'atmospheric science', 'dice'];1,538;Understanding Clouds from Satellite Images;Research prediction Competition
2011-02-20 23:00:00;Around the world, the pool of funds available for research grants is steadily shrinking (in a relative sense). In Australia, success rates have fallen to 20-25 per cent, meaning that most academics are spending valuable time making applications that end up being rejected. With this problem in mind, the University of Melbourne is hosting a competition to predict the success of grant applications. The winning model will be used by the university to predict which grant applications are likely to be successful, so that less time is wasted on applications that are unlikely to succeed. The university hopes the competition will also shed some light on what factors are important in determining whether an application will succeed. The university has provided a dataset containing 249 features, including variables that represent the size of the grant, the general area of study and de-identified information on the investigators who are applying for the grant. Participants train their models on 8,707 grant applications made between 2004 and 2008. They then make predictions on a further 2,176 applications made in 2009 and the first half of 2010.The winner of this competition will receive US$5,000. To be eligible for the prize, the winning method must be implementable by the University of Melbourne.;https://www.kaggle.com/c/unimelb;;This task requires participants to predict the outcome of grant applications for the University of Melbourne.;['auc'];203;Predict Grant Applications;Featured prediction Competition
2012-11-11 01:00:00;"Note: The prediction phase of this competition has ended. Please join the visualization competition which ends on Nov. 11, 2012. -- This challenge is to develop a statistical model to predict census mail return rates at the Census block group level of geography. The Census Bureau will use this model for planning purposes for the decennial census and for demographic sample surveys. The model-based estimates of predicted mail return will be publicly released in a later version of the Census ""planning database"" containing updated demographic data. Participants are encouraged to develop and evaluate different statistical approaches to proposing the best predictive model for geographic units. The intent is to improve our current predictive analytics. Please note also that as described in the rules, only US citizens and residents are eligible for prizes.";https://www.kaggle.com/c/us-census-challenge;US Census Bureau;Predict census mail return rates.;['nwmae'];243;U.S. Census Return Rate Challenge;Featured prediction Competition
2013-01-20 08:59:00;Winning Analysis and Visualization will receive cash awards for the following ranked entries:  1st: $3,500 2nd:$1,000 3rd: $ 500  All submissions will be evaluated by the Visual.ly team and qualified visualizers will be given admission into the Visual.ly Marketplace for future work.;https://www.kaggle.com/c/visualize-the-state-of-education-in-colorado;;Using 3 years of school grading data supplied by the Colorado Department of Education and R-Squared Research, visually uncover trends in the Colorado public school system.;['rmse'];;Visualize the State of Public Education in Colorado;Research prediction Competition
2019-03-22 00:59:00;Medium voltage overhead power lines run for hundreds of miles to supply power to cities. These great distances make it expensive to manually inspect the lines for damage that doesn't immediately lead to a power outage, such as a tree branch hitting the line or a flaw in the insulator. These modes of damage lead to a phenomenon known as partial discharge — an electrical discharge which does not bridge the electrodes between an insulation system completely. Partial discharges slowly damage the power line, so left unrepaired they will eventually lead to a power outage or start a fire.  Your challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the ENET Centre at VŠB. Effective classifiers using this data will make it possible to continuously monitor power lines for faults. ENET Centre researches and develops renewable energy resources with the goal of reducing or eliminating harmful environmental impacts. Their efforts focus on developing technology solutions around transportation and processing of energy raw materials. By developing a solution to detect partial discharge you’ll help reduce  maintenance costs, and prevent power outages.;https://www.kaggle.com/c/vsb-power-line-fault-detection;Enet Centre, VSB - T.U. of Ostrava;Can you detect faults in above-ground electrical lines?;['binary classification', 'tabular data', 'signal processing', 'matthewscorrelationcoefficient'];1,451;VSB Power Line Fault Detection;Featured prediction Competition
2015-05-26 01:59:00;Walmart operates 11,450 stores in 27 countries, managing inventory across varying climates and cultures. Extreme weather events, like hurricanes, blizzards, and floods, can have a huge impact on sales at the store and product level.  In their second Kaggle recruiting competition, Walmart challenges participants to accurately predict the sales of 111 potentially weather-sensitive products (like umbrellas, bread, and milk) around the time of major weather events at 45 of their retail locations.  Intuitively, we may expect an uptick in the sales of umbrellas before a big thunderstorm, but it's difficult for replenishment managers to correctly predict the level of inventory needed to avoid being out-of-stock or overstock during and after that storm. Walmart relies on a variety of vendor tools to predict sales around extreme weather events, but it's an ad-hoc and time-consuming process that lacks a systematic measure of effectiveness.  Helping Walmart better predict sales of weather-sensitive products will keep valued customers out of the rain. It could also earn you a position at one of the most data-driven retailers in the world!  Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.;https://www.kaggle.com/c/walmart-recruiting-sales-in-stormy-weather;Walmart;Predict how sales of weather-sensitive products are affected by snow and rain;['regression', 'tabular data', 'rmsle'];484;Walmart Recruiting II: Sales in Stormy Weather;Recruitment prediction Competition
2014-05-06 01:59:00;"One challenge of modeling retail data is the need to make decisions based on limited history. If Christmas comes but once a year, so does the chance to see how strategic decisions impacted the bottom line.  In this recruiting competition, job-seekers are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains many departments, and participants must project the sales for each department in each store. To add to the challenge, selected holiday markdown events are included in the dataset. These markdowns are known to affect sales, but it is challenging to predict which departments are affected and the extent of the impact. Want to work in a great environment with some of the world's largest data sets? This is a chance to display your modeling mettle to the Walmart hiring teams. This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Walmart, check the box ""Allow host to contact me"" when you make your first entry.  You must compete as an individual in recruiting competitions. You may only use the provided data to make your predictions.";https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;Walmart;Use historical markdown data to predict store sales;['wmae'];688;Walmart Recruiting - Store Sales Forecasting;Recruitment prediction Competition
2015-12-28 00:59:00;"Walmart uses both art and science to continually make progress on their core mission of better understanding and serving their customers. One way Walmart is able to improve customers' shopping experiences is by segmenting their store visits into different trip types.   Whether they're on a last minute run for new puppy supplies or leisurely making their way through a weekly grocery list, classifying trip types enables Walmart to create the best shopping experience for every customer. Currently, Walmart's trip types are created from a combination of existing customer insights (""art"") and purchase history data (""science""). In their third recruiting competition, Walmart is challenging Kagglers to focus on the (data) science and classify customer trips using only a transactional dataset of the items they've purchased. Improving the science behind trip type classification will help Walmart refine their segmentation process. Walmart is hosting this competition to connect with data scientists who break the mold.";https://www.kaggle.com/c/walmart-recruiting-trip-type-classification;Walmart;Use market basket analysis to classify shopping trips;['multiclass classification', 'tabular data', 'multiclassloss'];1,043;Walmart Recruiting: Trip Type Classification;Recruitment prediction Competition
2017-11-16 00:59:00;This competition focuses on the problem of forecasting the future values of multiple time series, as it has always been one of the most challenging problems in the field. More specifically, we aim the competition at testing state-of-the-art methods designed by the participants, on the problem of forecasting future web traffic for approximately 145,000 Wikipedia articles.  Sequential or temporal observations emerge in many key real-world problems, ranging from biological data, financial markets, weather forecasting, to audio and video processing. The field of time series encapsulates many different problems, ranging from analysis and inference to classification and forecast. What can you do to help predict future views?  This competition will run as two stages and involves prediction of actual future events. There will be a training stage during which the leaderboard is based on historical data, followed by a stage where participants are scored on real future events.   You have complete freedom in how to produce your forecasts: e.g. use of univariate vs multi-variate models, use of metadata (article identifier), hierarchical time series modeling (for different types of traffic), data augmentation (e.g. using Google Trends data to extend the dataset), anomaly and outlier detection and cleaning, different strategies for missing value imputation, and many more types of approaches.   We thank Google Inc. and Voleon for sponsorship of this competition, and Oren Anava and Vitaly Kuznetsov for organizing it. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/web-traffic-time-series-forecasting;Google;Forecast future traffic to Wikipedia pages;['internet', 'tabular data', 'smape'];1,095;Web Traffic Time Series Forecasting;Research prediction Competition
2018-07-10 01:59:00;After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food. To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized. In this competition, you’re challenged to build an algorithm to identifying whale species in images. You’ll analyze Happy Whale’s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you’ll help to open rich fields of understanding for marine mammal population dynamics around the globe. We'd like to thank Happy Whale  for providing this data and problem. Happy Whale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.;https://www.kaggle.com/c/whale-categorization-playground;Kaggle;Can you identify a whale by the picture of its fluke?;['image data', 'animals', 'map@{k}'];528;Humpback Whale Identification Challenge;Playground prediction Competition
2013-04-08 02:00:00;Read the summary of the competition for a quick overview of the impact of the results. We depend on shipping industry's uninterrupted ability to transport goods across long distances. Navigation technologies combine accurate position and environmental data to calculate optimal transport routes. Accounting for and reducing the impact of commercial shipping on the ocean’s environment, while achieving commercial sustainability, is of increasing importance, especially as it relates to the influence of cumulative noise “footprints” on the great whales.    Illustration of ships navigating safely around the habitat of whales. Right whales make a half-dozen types of sounds, but the characteristic up-call is the one identified by the auto-detection buoys.  Right whale up-call Marinexplore and Cornell researchers challenge YOU to beat the existing whale detection algorithm identifying the right whale calls. This will advance ship routing decisions in the region. [For details on the buoy network see a paper published by Acoustical Society of America.] Read the summary of the competition for a quick overview of the impact of the results.;https://www.kaggle.com/c/whale-detection-challenge;;Create an algorithm to detect North Atlantic right whale calls from audio recordings, prevent collisions with shipping traffic;['auc'];245;The Marinexplore and Cornell University Whale Detection Challenge;Featured prediction Competition
2012-03-01 00:59:59;"When studying for a test, you want to know how well you're going to do.  More specifically, you want to know what areas you need to study more.  In order to help students answer this question, we are attempting to predict their probability of answering questions  correctly.  The data in this competition comes from students studying for three tests: the GMAT, SAT, and ACT. You are attempting to predict, for each question attempted in the test set, whether the student will answer the question correctly.  To succeed, you will need to improve on the state-of-the-art in student evaluation.  While the questions included labels  indicating their specified test area, there may be structure which helps better organize the areas of knowledge involved in each question.  In the short term, this will help students figure out what areas they are weak in; but ultimately, this will help create  tests to better measure what a student actually knows. The prize pool is $5,000 ($3,000 for first, $1,500 for second and $500 for third), with entries judged using Capped Binomial Deviance.";https://www.kaggle.com/c/WhatDoYouKnow;;Improve the state of the art in student evaluation by predicting whether a student will answer the next test question correctly.;['custom metric'];238;What Do You Know?;Featured prediction Competition
2015-12-21 00:59:00;Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight? If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see. Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients.  Acknowledgements We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.;https://www.kaggle.com/c/whats-cooking;Kaggle;Use recipe ingredients to categorize the cuisine;['text data', 'food', 'multiclass classification', 'categorizationaccuracy'];1,387;What's Cooking?;Playground prediction Competition
2018-09-25 01:59:00;Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight? If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see. Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients.  Acknowledgements We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.;https://www.kaggle.com/c/whats-cooking-kernels-only;Kaggle;Use recipe ingredients to categorize the cuisine;['text data', 'food', 'multiclass classification', 'categorizationaccuracy'];523;What's Cooking? (Kernels Only);Playground Code Competition
2011-04-10 14:00:00;The dataset used for the competition is a sub-set of a large dataset that has been collected in Qatar University thanks to several volunteers with different backgrounds. The collection of data has been done under the supervision of Dr. Somaya Al-Maadeed.The dataset has been digitized by the research assistant Wael Al-Ayoubi using an HP officejet 6500A Plus scanner. It has been cropped and preprocessed by Dr. Abdelâali Hassaïne.Using this dataset is free of charge for academic purposes (please mention the competition article that will be published in ICDAR2011 to cite this dataset). For industrials purposes, please contact Dr. Somaya Al-Maadeed.This competition is supervised by all of Dr. Somaya Al-Maadeed, Prof. Jihad Mohamad Alja’am, Prof. Ali Mohamed Jaoua from Qatar University and Prof. Ahmed Bouridane from Northumbria University.;https://www.kaggle.com/c/WIC2011;;This competition require participants to develop an algorithm to identify who wrote which documents. The winner will be honored at a special session of the ICDAR 2011 conference.;['mae'];30;ICDAR 2011 - Arabic Writer Identification;Featured prediction Competition
2011-09-21 01:59:59;This competition challenges data-mining experts to build a predictive model that predicts the number of edits an editor will make in the five months after the end date of the training dataset. The dataset is randomly sampled from the English Wikipedia dataset  from the period January 2001 - August 2010. The objective of this competition is to quantitively understand what factors determine editing behavior. We hope to be able to answer questions, using these predictive models, why people stop editing or increase their pace of editing. Contestants are expected to build a predictive model that can be reused by the Wikimedia Foundation to forecast long term trends in the number of edits that we can expect.;https://www.kaggle.com/c/wikichallenge;;This competition challenges data-mining experts to build a predictive model that predicts the number of edits an editor will make five months from the end date of the training dataset.;['rmsle'];90;Wikipedia's Participation Challenge;Featured prediction Competition
2014-07-16 01:59:00;In the past, gathering information was paramount only for top-tier companies. In the information age, mining and categorization of relevant information is necessary for all companies. Media monitoring - the activity of monitoring the output of the print, online and broadcast media - allows every company to search a wide range of media, from printed media to internet publications, and be informed on their area of expertise and remain competitive.  This is a multi-label classification competition for articles coming from Greek printed media. Raw data comes from the scanning of print media, article segmentation, and optical character segmentation, and therefore is quite noisy. Each article is examined by a human annotator and categorized to one or more of the topics being monitored. Topics range from specific persons, products, and companies that can be easily categorized based on keywords, to more general semantic concepts, such as environment or economy. Building multi-label classifiers for the automated annotation of articles into topics can support the work of human annotators by suggesting a list of all topics by order of relevance, or even automate the annotation process for media and/or categories that are easier to predict. This saves valuable time and allows a media monitoring company to expand the portfolio of media being monitored.   Organizers The competition is organized by media monitoring solutions company DataScouting, media monitoring services company ENIMEROSI and the Deparment of Informatics of the Aristotle University of Thessaloniki. It is the challenge accompanying the 15th International Conference on Web Information System Engineering (WISE 2014) that will be held in Thessaloniki, Greece on 12-14 October 2014.      ARISTOTLE    UNIVERSITY OF     THESSALONIKI;https://www.kaggle.com/c/wise-2014;;Multi-label classification of printed media articles to topics;['meanfscore'];120;Greek Media Monitoring Multilabel Classification (WISE 2014);Research prediction Competition
2018-04-02 01:59:00;Google Cloud and NCAA® have teamed up to bring you this year’s version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness® during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.   In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results. This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.;https://www.kaggle.com/c/womens-machine-learning-competition-2018;Google Cloud;Apply machine learning to NCAA® March Madness®;['logloss'];505;Google Cloud & NCAA® ML Competition 2018-Women's;Featured prediction Competition
2019-04-08 07:10:00;As a result of the continued collaboration between Google Cloud and the NCAA®, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.   In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.  As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!  This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.;https://www.kaggle.com/c/womens-machine-learning-competition-2019;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['sports', 'basketball', 'logloss'];500;Google Cloud & NCAA® ML Competition 2019-Women's;Featured prediction Competition
2015-07-01 01:59:00;"In this tutorial competition, we dig a little ""deeper"" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis. Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another Kaggle competition for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem. Deep learning has been in the news a lot over the past few years, even making it to the front page of the New York Times. These machine learning techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a drug discovery task, and cat and dog image recognition. Tutorial Overview This tutorial will help you get started with Word2Vec for natural language processing. It has two goals:  Basic Natural Language Processing: Part 1 of this tutorial is intended for beginners and covers basic natural language processing techniques, which are needed for later parts of the tutorial. Deep Learning for Text Understanding: In Parts 2 and 3, we delve into how to train a model using Word2Vec and how to use the resulting word vectors for sentiment analysis. Since deep learning is a rapidly evolving field, large amounts of the work has not yet been published, or exists only as academic papers. Part 3 of the tutorial is more exploratory than prescriptive -- we experiment with several ways of using Word2Vec rather than giving you a recipe for using the output. To achieve these goals, we rely on an IMDB sentiment analysis data set, which has 100,000 multi-paragraph movie reviews, both positive and negative.  Acknowledgements This dataset was collected in association with the following publication: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). ""Learning Word Vectors for Sentiment Analysis."" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011). (link) Please email the author of that paper if you use the data for any research applications. The tutorial was developed by Angela Chapman during her summer 2014 internship at Kaggle.";https://www.kaggle.com/c/word2vec-nlp-tutorial;Kaggle;Use Google's Word2Vec for movie reviews;['movies and tv shows', 'text data', 'binary classification', 'auc'];577;Bag of Words Meets Bags of Popcorn;Getting Started prediction Competition
2010-06-11 15:29:00;The investment banksThe JP Morgan analysishttp://www.scribd.com/full/31537878?access_key=key-2hriexvwd0a39fqmw9ecThe UBS analysishttp://www.ubs.com/2/e/medlib/wmr/IGWM_spez_2010_en.pdf (predictions on PDF page 37)also available as a podcast or in other languageshttp://www.ubs.com/1/e/bank_for_banks/news/topical_stories/edition_10.htmlGoldman Sachs http://www2.goldmansachs.com/ideas/global-economic-outlook/the-world-cup-2010-doc.pdf (predictions on PDF page 71) Danske Bank http://www.scribd.com/doc/32112498/Danske-Bank-World-Cup-2010-Special Other analysis FiveThirtyEighthttp://www.fivethirtyeight.com/2009/12/world-cup-2010-advancement.htmlNorwegian Computing Center http://vm.nr.no/indexEng.htmlhttp://www.rogerkaufmann.ch/dsaINTe_r.htmhttp://worldcup.bayesialab.com/http://sportbrains.com/worldCup.html;https://www.kaggle.com/c/worldcup2010;;Quants at Goldman Sachs and JP Morgan have modeled the likely outcomes of the 2010 World Cup. Can you do better?;['custom metric'];;World Cup 2010 - Take on the Quants;Featured prediction Competition
2010-06-11 15:28:00;We are also running a World Cup 2010 - Take on the Quants Challenge. The Confidence Challenge requires competitors to predict how far each country will progress through the World Cup and then assign a level of confidence to each prediction. A competitor's score is weighted by their level of confidence. There are more details on the submission instructions page and the evaluation page. The competition closes just before the first game kicks off on June 11th. What is your incentive to enter?The winner of this challenge wins $USD100 to bet on the winner of the FIFA Golden Ball award.;https://www.kaggle.com/c/worldcupconf;;The Confidence Challenge requires competitors to assign a level of confidence to their World Cup predictions.;['custom metric'];63;World Cup 2010 - Confidence Challenge;Featured prediction Competition
2014-01-11 00:59:00;The Personalized Web Search Challenge provides a unique opportunity to consolidate and scrutinize the work from industrial labs on personalizing web search using user-logged search behavior context. It provides a fully anonymized dataset shared by Yandex, which has anonymized user ids, queries, query terms, urls, url domains and clicks. This Challenge and the shared dataset will enable a whole new set of researchers to study the problem of personalizing web search experience. The Personalized Web Search Challenge is a part of series of contests organized by Yandex over many years. This year’s event is the eighth since 2004. In previous years, participants tried to learn to rank documents, predict traffic jams, find similar images, predict relevance of documents using search logs and detect search engine switchings in search sessions. The Challenge is intended as a logical follow-up to the previous two challenges. We ask participants to re-rank URLs of each SERP returned by the search engine according to the personal preferences of the users. In other words, participants need to personalize search using the long-term (user history based) and short-term (session-based) user context. The evaluation relies on a variant of a dwell-time based model of personal relevance and is data-driven, as it is presently accepted in the state-of the-art research on personalized search. The Challenge is a part of the Web Search Click Data workshop (WSCD 2014) and the reports of the best teams are welcome to be presented at this workshop, to be held at WSDM 2014 conference, on 28th February in New York, USA. The workshop is organized by Pavel Serdyukov (Yandex), Georges Dupret (Yahoo!) and Nick Craswell (Microsoft Research/Bing).;https://www.kaggle.com/c/yandex-personalized-web-search-challenge;;Re-rank web documents using personal preferences;['ndcg@{k}'];194;Personalized Web Search Challenge;Featured prediction Competition
2013-07-01 01:59:00;"Here at Yelp, we really love the quality of our data. We're grateful that so many of our users take the time to write such great reviews. We track 3 community-powered metrics of review quality: Useful, Funny, Cool. Over time, a good review will accumulate lots of votes in these categories from the community. However, another extremely important quality feature is the freshness of a review. What if we didn't have to wait for the community to vote on the best reviews to know which ones are high quality? The goal of this competition is to estimate the number of Useful votes a review will receive. Yelp isn't only looking for the answer to this question; we're looking for an engineer that can solve this problem and push their code to production. The prize is a fast track through the recruiting process -- straight to an interview and the opportunity to show Yelp Engineers just what you've got. For more information about the exciting opportunities at Yelp, check out  http://www.yelp.com/careers!   This competition counts towards rankings & achievements.";https://www.kaggle.com/c/yelp-recruiting;;"How many ""useful"" votes will a Yelp review receive? Show off your skills to land an interview for a position on a Yelp data mining team!";['rmsle'];350;Yelp Recruiting Competition;Recruitment prediction Competition
2013-09-01 01:59:59;"We are pleased to announce the 2013 Recommender Systems Challenge associated with ACM RecSys 2013.  Known as RecSysChallenge 2013, this is a LBS contest organized by Yelp. The theme of this year’s competition is personalized business recommendations for Yelp users. We provide a detailed snapshot of Yelp data: over 10,000 businesses, 8,000 check-in sites, 40,000 users, and 200,000 reviews from the Phoenix, AZ metropolitan area. Contest At the heart of any recommender system is an algorithm to predict ratings.  Contestants are asked to predict the users’ future ratings of businesses. Participants will create a model to predict the rating a user would assign to a business. Models will be graded on accuracy using the root mean squared error metric. See the Evaluation page for more details. The competition starts on May 3, 2013 and ends on August 31, 2013. Submissions to the workshop must be submitted by September 8, 2013. A total of $500 in prize money will be presented to the winners. Top ranking teams will also be recognized at the RecSys banquet. Workshop After the competition closes, we will also hold a full-day workshop, co-located with the ACM RecSys 2013 conference, to discuss interesting approaches to the competition problem as well as lessons learned. We invite all contest participants to present their approach to building business recommendations. Contributions may focus on any individual steps such as feature extraction or model building; or describe an end-to-end system to predict business ratings.   Workshop participation is independent of the contest: researchers are welcome to participate in either (or, hopefully, both!).   Looking for last year's challenge?  Check out  RecSys Challenge 2012.";https://www.kaggle.com/c/yelp-recsys-2013;;RecSys Challenge 2013: Yelp business rating prediction;['rmse'];158;RecSys2013: Yelp Business Rating Prediction;Research prediction Competition
2016-04-13 01:59:00;"Does your favorite Ethiopian restaurant take reservations? Will a first date at that authentic looking bistro break your wallet? Is the diner down the street a good call for breakfast? Restaurant labels help Yelp users quickly answer questions like these, narrowing down their results to only restaurants that fit their nuanced needs. In this competition, Yelp is challenging Kagglers to build a model that automatically tags restaurants with multiple labels using a dataset of user-submitted photos. Currently, restaurant labels are manually selected by Yelp users when they submit a review. Selecting the labels is optional, leaving some restaurants un- or only partially-categorized.  In an age of food selfies and photo-centric social storytelling, it may be no surprise to hear that Yelp's users upload an enormous amount of photos every day alongside their written reviews. Can you turn their pictures into (less than a thousand) words?  Yelp isn’t only looking for your best model; we’re looking for data mining engineers that can help us use our data in novel ways while pushing code to production. The prize for this competition is a fast track through the recruiting process and an opportunity to show our data mining teams just what you’ve got! For more information about exciting opportunities at Yelp, check out the Jobs at Yelp competition page and Yelp's own careers page.";https://www.kaggle.com/c/yelp-restaurant-photo-classification;;Predict attribute labels for restaurants using user-submitted photos;['internet', 'image data', 'food', 'meanfscore'];355;Yelp Restaurant Photo Classification;Recruitment prediction Competition
2017-06-03 01:59:00;Video captures a cross-section of our society. And major advances in analyzing and understanding video have the potential to touch all aspects of life from learning and communication to entertainment and play. In this competition, Google is inviting the Kaggle community to join efforts to accelerate research in large-scale video understanding, while giving participants access to the Google Cloud Machine Learning Engine. Today, one of the greatest obstacles to rapid improvements in video understanding research has been the lack of large-scale, labeled datasets open to the public. For example, the availability of large, labeled datasets such as ImageNet has enabled continued breakthroughs in machine learning and machine perception. To that end, Google’s recent release of the YouTube-8M (YT-8M) dataset represents a significant step in this direction. Making this resource open to everyone from students and industry professionals is expected to kickstart innovation in areas such as representation learning and video modeling architectures. In this competition, you are challenged to develop classification algorithms which accurately assign video-level labels using the new and improved YT-8M V2 dataset. The dataset was created from over 7 million YouTube videos (450,000 hours of video) and includes video labels from a vocabulary of 4716 classes (3.4 labels/video on average).  It also comes with pre-extracted audio & visual features from every second of video (3.2B feature vectors in total). By taking part, Kagglers will not only play a pivotal role in setting state-of-the-art benchmarks, but also improve search and organization of video archives.  Getting Started  Review the data page for special instructions on how to access the competition's data. It will be hosted on Google Cloud. Participants have the option to download the data to work locally or work within the Google Cloud ML beta Platform. Review the tutorial on Getting Started with Google Cloud, and try the starter code.  Sign up for a Google Cloud ML Platform free trial account. The free trial account includes $300 in credits!   We've also provided a subsample of the data to explore on Kernels. Take a look at this Python notebook and create your own. Don't forget to review the prize eligibility details, which includes requirements for code open-sourcing and a paper submission.  Because Cloud ML is currently a beta product, Google welcomes the opportunity to hear your feedback about using the tool. Please share your questions and thoughts on the competition's forums. Additional resources specific to the YT-8M dataset and Google Cloud ML can be found here. Acknowledgements Google Cloud Machine Learning, Competition Sponsor Google Cloud Machine Learning is a managed service that enables you to easily build machine learning models, that work on any type of data, of any size. Create your model with the powerful TensorFlow framework that powers many Google products, from GooglePhotos to Google Cloud Speech. Build models of any size with our managed scalable infrastructure. Your trained model is immediately available for use with our global prediction platform that can support thousands of users and TBs of data. The service is integrated with Google Cloud Dataflow for pre-processing, allowing you to access data from Google Cloud Storage, Google BigQuery, and others.;https://www.kaggle.com/c/youtube8m;Google Cloud;Can you produce the best video tag predictions?;['internet', 'image data', 'custom metric'];655;Google Cloud & YouTube-8M Video Understanding Challenge;Featured prediction Competition
2018-08-07 01:59:00;The world is generating and consuming an enormous amount of video content. Currently on YouTube, people watch over 1 billion hours of video every single day. To spur advances in analyzing and understanding video,  Google AI has publicly released a large-scale video dataset that consists of millions of YouTube video features and associated labels from a diverse vocabulary of 3,700+ visual entities called the YouTube-8M Dataset. Last year, we successfully hosted Google Cloud & YouTube-8M Video Understanding Challenge, with 742 participating teams with 946 individual competitors from 60 countries. This competition is the second Kaggle competition based on YouTube 8M dataset, and is focused on learning video representation under budget constraints.  For a lot of video tasks where there are a large number of classes, like recommending new videos or automatic video classification, compact models need to meet memory and computational requirements. This is true even if working in cloud computational environments. Also, compact models make it possible to have limited-memory or catalog indexes on devices in order to do personalized and privacy-preserving computation on user’s personal mobile phones. In this competition, you’re challenged to produce a compact video classification model. Your model size must not exceed 1 GB (this is strictly enforced, through model upload). We encourage participants to train a model that most efficiently uses this budget, rather than ensembles of lots of models.  This competition is being hosted by Google AI (previously known as Google Research) as a part of the European Conference on Computer Vision (ECCV) 2018 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.;https://www.kaggle.com/c/youtube8m-2018;Google Research;Can you create a constrained-size model to predict video labels?;['video data', 'custom metric'];312;The 2nd YouTube-8M Video Understanding Challenge;Featured prediction Competition
2019-10-12 01:59:00;"Imagine being able to search for the moment in any video where an adorable kitten sneezes, even though the uploader didn’t title or describe the video with such descriptive metadata. Now, apply that same concept to videos that cover important or special events like a baby’s first steps or a game-winning goal -- and now we have the ability to quickly find and share special video moments. This technology is called temporal concept localization within video and Google Research can use your help to advance the state of the art in this area.    An example of the detected action ""blowing out candles"" In most web searches, video retrieval and ranking is performed by matching query terms to metadata and other video-level signals. However, we know that videos can contain an array of topics that aren’t always characterized by the uploader, and many of these miss localizations to brief but important moments within the video. Temporal localization can enable applications such as  improved video search (including search within video), video summarization and highlight extraction, action moment detection, improved video content safety, and many others. In previous years, participants worked on advancements in video-level annotations, building both unconstrained and constrained models. In this third challenge based on the YouTube 8M dataset, Kagglers will localize video-level labels to the precise time in the video where the label actually appears, and do this at an unprecedented scale. To put it another way: at what point in the video does the cat sneeze?   If successful, your new machine learning models will significantly improve video understanding for all, by not only identifying the topics relevant to a video, but also pinpointing where in the video they appear. This competition is being hosted by Google Research as a part of the International Conference on Computer Vision (ICCV) 2019 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.";https://www.kaggle.com/c/youtube8m-2019;Google Research;Temporal localization of topics within video;['video data', 'custom metric'];283;The 3rd YouTube-8M Video Understanding Challenge;Research prediction Competition
2018-01-10 16:59:00;Zillow’s Zestimate home valuation has shaken up the U.S. real estate industry since first released 11 years ago. A home is often the largest and most expensive purchase a person makes in his or her lifetime. Ensuring homeowners have a trusted way to monitor this asset is incredibly important. The Zestimate was created to give consumers as much information as possible about homes and the housing market, marking the first time consumers had access to this type of home value information at no cost. “Zestimates” are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property.   And, by continually improving the median margin of error (from 14% at the onset to 5% today), Zillow has since become established as one of the largest, most trusted marketplaces for real estate information in the U.S. and a leading example of impactful machine learning. Zillow Prize, a competition with a one million dollar grand prize, is challenging the data science community to help push the accuracy of the Zestimate even further. Winning algorithms stand to impact the home values of  110M homes across the U.S. In this million-dollar competition, participants will develop an algorithm that makes predictions about the future sale prices of homes. The contest is structured into two rounds, the qualifying round which opens May 24, 2017 and the private round for the 100 top qualifying teams that opens  on Feb 1st, 2018. In the qualifying round, you’ll be building a model to improve the Zestimate residual error. In the final round, you’ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition. Because real estate transaction data is public information, there will be a three-month sales tracking period after each competition round closes where your predictions will be evaluated against the actual sale prices of the homes. The final leaderboard won’t be revealed until the close of the sales tracking period.;https://www.kaggle.com/c/zillow-prize-1;Zillow;Can you improve the algorithm that changed the world of real estate?;['real estate', 'housing', 'custom metric'];3,775;Zillow Prize: Zillow’s Home Value Prediction (Zestimate);Featured prediction Competition
