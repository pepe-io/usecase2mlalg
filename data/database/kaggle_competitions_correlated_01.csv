date_closed;description;link;organisation;subtitle;tags;teams;teams_score;title;type
2019-01-17 00:59:00;This isn't your classic decoder ring puzzle found in a cereal box. There's a twist. Welcome to the Ciphertext Challenge! In this competition, we've encrypted parts of a well-known dataset -- the 20 Newsgroups dataset -- with several simple, classic ciphers.  This dataset is commonly used as a multi-class and NLP sample set, noted for its small size, varied nature, and the first-hand look it offers into the deep existential horrors of the 90s-era internet. With 20 fairly distinct classes and lots of clues, it allows for a wide variety of successful approaches. We've made the problem a little harder to solve. Fabulous Kaggle swag will go to the top competitors - the highest-scoring teams (which might be the first to crack the code!), and the most popular kernel.  Note that this is a short competition, so use your submissions wisely. * = Note: It is possible to apply a number of techniques using ONLY the ciphertext. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice.  You can view and download the unencrypted dataset from Jason Rennie's homepage.  In the words of the host: The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. If you use the dataset in a scientific publication, please reference (at a minimum) the above website. Photo by U.S. Air Force photo/Don Branum;https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge;Kaggle;V8g{9827$A${?^*?}$$v7*.yig$w9.8};['nlp', 'puzzles', 'multiclass classification', 'exploratory data analysis', 'beginner', 'classification', 'data visualization', 'gpu'];142.0;0.525;20 Newsgroups Ciphertext Challenge;Playground prediction Competition
2019-11-13 00:59:00;Self-driving technology presents a rare opportunity to improve the quality of life in many of our communities. Avoidable collisions, single-occupant commuters, and vehicle emissions are choking cities, while infrastructure strains under rapid urban growth. Autonomous vehicles are expected to redefine transportation and unlock a myriad of societal, environmental, and economic benefits. You can apply your data analysis skills in this competition to advance the state of self-driving technology. Lyft, whose mission is to improve people’s lives with the world’s best transportation, is investing in the future of self-driving vehicles. Level 5, their self-driving division, is working on a fleet of autonomous vehicles, and currently has a team of 450+ across Palo Alto, London, and Munich working to build a leading self-driving system (they’re hiring!). Their goal is to democratize access to self-driving technology for hundreds of millions of Lyft passengers. From a technical standpoint, however,  the bar to unlock technical research and development on higher-level autonomy functions like perception, prediction, and planning is extremely high. This implies technical R&D on self-driving cars has traditionally been inaccessible to the broader research community. This dataset aims to democratize access to such data, and foster innovation in higher-level autonomy functions for everyone, everywhere. By conducting a competition, we hope to encourage the research community to focus on hard problems in this space—namely, 3D object detection over semantic maps.  In this competition, you will build and optimize algorithms based on a large-scale dataset. This dataset features the raw sensor camera inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a restricted geographic area.  If successful, you’ll make a significant contribution towards stimulating further development in autonomous vehicles and empowering communities around the world.;https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;Lyft;Can you advance the state of the art in 3D object detection?;['deep learning', 'data cleaning', 'exploratory data analysis', 'automobiles and vehicles', 'neural networks', 'image data', 'beginner', 'data visualization', 'gpu'];547.0;0.612;Lyft 3D Object Detection for Autonomous Vehicles;Featured prediction Competition
2020-05-28 01:59:00;Can a computer learn complex, abstract tasks from just a few examples? Current machine learning techniques are data-hungry and brittle—they can only make sense of patterns they've seen before. Using current methods, an algorithm can gain new skills by exposure to large amounts of data, but cognitive abilities that could broadly generalize to many tasks remain elusive. This makes it very challenging to create systems that can handle the variability and unpredictability of the real world, such as domestic robots or self-driving cars. However, alternative approaches, like inductive programming, offer the potential for more human-like abstraction and reasoning. The Abstraction and Reasoning Corpus (ARC) provides a benchmark to measure AI skill-acquisition on unknown tasks, with the constraint that only a handful of demonstrations are shown to learn a complex task. It provides a glimpse of a future where AI could quickly learn to solve new problems on its own. The Kaggle Abstraction and Reasoning Challenge invites you to try your hand at bringing this future into the present! This competition is hosted by François Chollet, creator of the Keras neural networks library. Chollet’s paper on measuring intelligence provides the context and motivation behind the ARC benchmark. In this competition, you’ll create an AI that can solve reasoning tasks it has never seen before. Each ARC task contains 3-5 pairs of train inputs and outputs, and a test input for which you need to predict the corresponding output with the pattern learned from the train examples. If successful, you’ll help bring computers closer to human cognition and you'll open the door to completely new AI applications!;https://www.kaggle.com/c/abstraction-and-reasoning-challenge;Abstraction and Reasoning Corpus;Create an AI capable of solving reasoning tasks it has never seen before;['exploratory data analysis', 'feature engineering', 'beginner', 'data visualization', 'gpu'];914.0;0.64;Abstraction and Reasoning Challenge;Research Code Competition
2019-07-09 01:59:00;To assess the impact of climate change on Earth's flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the VIGIA project, which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery. This is a kernels-only competition, meaning you must submit predictions using Kaggle Kernels. Read the basics here. Acknowledgments Kaggle is hosting this competition for the machine learning community to use for fun and practice. The original version of this data can be found here, with details in the following paper: Efren López-Jiménez, Juan Irving Vasquez-Gomez, Miguel Angel Sanchez-Acevedo, Juan Carlos Herrera-Lozada, Abril Valeria Uriarte-Arcia, Columnar Cactus Recognition in Aerial Images using a Deep Learning Approach. Ecological Informatics. 2019. Acknowledgements to Consejo Nacional de Ciencia y Tecnología. Project cátedra 1507. Instituto Politècnico Nacional. Universidad de la Cañada. Contributors: Eduardo Armas Garca, Rafael Cano Martnez and Luis Cresencio Mota Carrera. J.I. Vasquez-Gomez, JC. Herrera Lozada. Abril Uriarte, Miguel Sanchez.;https://www.kaggle.com/c/aerial-cactus-identification;Kaggle;Determine whether an image contains a columnar cactus;['deep learning', 'exploratory data analysis', 'pca', 'beginner', 'classification', 'gpu', 'transfer learning', 'cnn'];1225.0;0.656;Aerial Cactus Identification;Playground Code Competition
2016-02-12 00:59:00;"Instead of waking to overlooked ""Do not disturb"" signs, Airbnb travelers find themselves rising with the birds in a whimsical treehouse, having their morning coffee on the deck of a houseboat, or cooking a shared regional breakfast with their hosts. New users on Airbnb can book a place to stay in 34,000+ cities across 190+ countries. By accurately predicting where a new user will book their first travel experience, Airbnb can share more personalized content with their community, decrease the average time to first booking, and better forecast demand. In this recruiting competition, Airbnb challenges you to predict in which country a new user will make his or her first booking. Kagglers who impress with their answer (and an explanation of how they got there) will be considered for an interview for the opportunity to join Airbnb's Data Science and Analytics team.  Wondering if you're a good fit? Check out this article on how Airbnb scaled data science to all sides of their organization, and visit their careers page for more on Airbnb's mission to create a world that inspires human connection.";https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;Airbnb;Where will a new guest book their first travel experience?;[];1458.0;0.665;Airbnb New User Bookings;Recruitment prediction Competition
2018-11-15 00:59:00;Airbus is excited to challenge Kagglers to build a model that detects all ships in satellite images as quickly as possible. Can you find them even in imagery with clouds or haze?  Here’s the backstory: Shipping traffic is growing fast.  More ships increase the chances of infractions at sea like environmentally devastating ship accidents, piracy, illegal fishing, drug trafficking, and illegal cargo movement. This has compelled many organizations, from environmental protection agencies to insurance companies and national government authorities, to have a closer watch over the open seas.   Airbus offers comprehensive maritime monitoring services by building a meaningful solution for wide coverage, fine details, intensive monitoring, premium reactivity and interpretation response. Combining its proprietary-data with highly-trained analysts, they help to support the maritime industry to increase knowledge, anticipate threats, trigger alerts, and improve efficiency at sea. A lot of work has been done over the last 10 years to automatically extract objects from satellite images with significative results but no effective operational effects.  Now Airbus is turning to Kagglers to increase the accuracy and  speed of automatic ship detection. Algorithm Speed Prize: After the Kaggle challenge is complete,  competitors may submit their model via a private Kaggle kernel for a speed evaluation based upon the inference time on over 40.000  images chips (typical size of a full satellite image) to win a special algorithm speed prize.   If you're interested to explore more Airbus data, you are welcomed to check out the OneAtlas Sandbox. And for more insights on our Maritime Surveillance capabilities, have a look at Airbus Intelligence page.;https://www.kaggle.com/c/airbus-ship-detection;Airbus;Find ships on satellite images as quickly as possible;['deep learning', 'binary classification', 'neural networks', 'image data', 'beginner', 'classification', 'gpu', 'transfer learning', 'cnn'];882.0;0.639;Airbus Ship Detection Challenge;Featured prediction Competition
2020-07-21 01:59:00;"That file you downloaded may contain hidden messages that aren’t part of its regular contents. The same technology employed for digital watermarking is also misused by crime rings. Law enforcement must now use steganalysis to detect these messages as part of their investigations. Machine learning is an important tool in the discovery of this secret data.  Current methods produce unreliable results, raising false alarms. One reason for inaccuracy is the many different devices and processing combinations. Yet, detection models are trained on a homogeneous dataset. To increase accuracy, researchers must put data hidden within digital images “into the wild” (hence the name ALASKA) to mimic real world conditions. In the competition, you’ll create an efficient and reliable method to detect secret data hidden within innocuous-seeming digital images. Rather than limiting the data source, these images have been acquired with as many as 50 different cameras (from smartphone to full-format high end) and processed in different fashions. Successful entries will include robust detection algorithms with minimal false positives. The IEEE WIFS (Workshop on Information Forensics and Security) is eager to make this happen again, as a follow up to the ALASKA#1 Challenge. WIFS is an annual event where researchers gather to discuss emerging challenges, exchange fresh ideas, and share state-of-the-art results and technical expertise in the areas of information security and forensics. WIFS has teamed up with Troyes University of Technology, CRIStAL Lab, Lille University, and CNRS to enable more accurate steganalysis.  Law enforcement officers need better methods to combat criminals using hidden messages. The data science community and other researchers can help with better automated detection. More accurate methods could help catch criminals whose communications are hidden in plain sight.  The challenge is organized by Rémi COGRANNE (UTT), Patrick BAS (CRIStAL / CNRS) and Quentin Giboulot (UTT) ; in addition to Kaggle, we have been greatly helped by the following sponsors:";https://www.kaggle.com/c/alaska2-image-steganalysis;Troyes University of Technology;Detect secret data hidden within digital images;['deep learning', 'tpu', 'neural networks', 'feature engineering', 'beginner', 'gpu', 'ensembling', 'cnn'];1095.0;0.65;ALASKA2 Image Steganalysis;Research prediction Competition
2016-12-13 00:59:00;When you’ve been devastated by a serious car accident, your focus is on the things that matter the most: family, friends, and other loved ones. Pushing paper with your insurance agent is the last place you want your time or mental energy spent. This is why Allstate, a personal insurer in the United States, is continually seeking fresh ideas to improve their claims service for the over 16 million households they protect.  Allstate is currently developing automated methods of predicting the cost, and hence severity, of claims. In this recruitment challenge, Kagglers are invited to show off their creativity and flex their technical chops by creating an algorithm which accurately predicts claims severity. Aspiring competitors will demonstrate insight into better ways to predict claims severity for the chance to be part of Allstate’s efforts to ensure a worry-free customer experience. New to Kaggle? This competition is a recruiting competition, your chance to get a foot in the door with the hiring team at Allstate.;https://www.kaggle.com/c/allstate-claims-severity;Allstate Insurance;How severe is an insurance claim?;['model comparison'];3045.0;0.7;Allstate Claims Severity;Recruitment prediction Competition
2013-08-01 01:59:00;When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. This access may allow an employee to read/manipulate resources through various applications or web portals. It is assumed that employees fulfilling the functions of a given role will access the same or similar resources. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work (e.g. not able to log into a reporting portal). A knowledgeable supervisor then takes time to manually grant the needed access in order to overcome access obstacles. As employees move throughout a company, this access discovery/recovery cycle wastes a nontrivial amount of time and money. There is a considerable amount of data regarding an employee’s role within an organization and the resources to which they have access. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access. Objective The objective of this competition is to build a model, learned using historical data, that will determine an employee's access needs, such that manual access transactions (grants and revokes) are minimized as the employee's attributes change over time. The model will take an employee's role information and a resource code and will return whether or not access should be granted. Partners This competition is hosted in collaboration with the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2013);https://www.kaggle.com/c/amazon-employee-access-challenge;;Predict an employee's access needs, given his/her job role;['deep learning', 'feature engineering', 'multiclass classification', 'beginner', 'classification', 'gpu', 'gradient boosting', 'data analytics'];1686.0;0.672;Amazon.com - Employee Access Challenge;Featured prediction Competition
2019-09-08 01:59:00;"Imagine being able to detect blindness before it happened. Millions of people suffer from diabetic retinopathy, the leading cause of blindness among working aged adults. Aravind Eye Hospital in India hopes to detect and prevent this disease among people living in rural areas where medical screening is difficult to conduct. Successful entries in this competition will improve the hospital’s ability to identify potential patients. Further, the solutions will be spread to other Ophthalmologists through the 4th Asia Pacific Tele-Ophthalmology Society (APTOS) Symposium Currently, Aravind technicians travel to these rural areas to capture images and then rely on highly trained doctors to review the images and provide diagnosis. Their goal is to scale their efforts through technology; to gain the ability to automatically screen images for disease and provide information on how severe the condition may be. In this synchronous Kernels-only competition, you'll build a machine learning model to speed up disease detection. You’ll work with thousands of images collected in rural areas to help identify diabetic retinopathy automatically. If successful, you will not only help to prevent lifelong blindness, but these models may be used to detect other sorts of diseases in the future, like glaucoma and macular degeneration.   Get started today!";https://www.kaggle.com/c/aptos2019-blindness-detection;Asia Pacific Tele-Ophthalmology Society (APTOS);Detect diabetic retinopathy to stop blindness before it's too late;['gpu'];2931.0;0.698;APTOS 2019 Blindness Detection;Featured Code Competition
2012-05-01 01:59:59;"The William and Flora Hewlett Foundation (Hewlett) is sponsoring the Automated Student Assessment Prize (ASAP).  Hewlett is appealing to data scientists and machine learning specialists to help solve an important social problem.  We need fast, effective and affordable solutions for automated grading of student-written essays. Hewlett is sponsoring the following prizes:  $60,000:  1 place $30,000:  2 place $10,000:  3 place  You are provided access to hand scored essays, so that you can build, train and test scoring engines against a wide field of competitors.  Your success depends upon how closely you can deliver scores to those of human expert graders.  While we believe that these financial incentives are important, we also intend to introduce top performers both to leading vendors in the industry and/or an established base of interested buyers.  Hewlett is opening the field of automated student assessment to you.  We want to induce a breakthrough that is both personally satisfying and game-changing for improving public education. Today, state departments of education are developing new forms of testing and grading methods, to assess the new common core standards.  In this environment the need for more sophisticated and affordable options is vital.  For example, we know that essays are an important expression of academic achievement, but they are expensive and time consuming for states to grade them by hand.  So, we are frequently limited to multiple-choice standardized tests.  We believe that automated scoring systems can yield fast, effective and affordable solutions that would allow states to introduce essays and other sophisticated testing tools.  We believe that you can help us pave the way towards a breakthrough.  ASAP is designed to achieve the following goals:  Challenge developers of automated student assessment systems to demonstrate their current capabilities. Compare the efficacy and cost of automated scoring to that of human graders. Reveal product capabilities to state departments of education and other key decision makers interested in adopting them.  The graded essays are selected according to specific data characteristics.  On average, each essay is approximately 150 to 550 words in length.  Some are more dependent upon source materials than others.  This range of essay type is provided so that we can better understand the strengths of your solution.  It is our intent to showcase quality and reliability, based on how well you can match expert human graders for each essay. You will be provided with training data for each essay prompt.  The number of training essays does vary.  For example, the lowest amount of training data is 1,190 essays, randomly selected from a total of 1,982.  The data will contain ASCII formatted text for each essay followed by one or more human scores, and (where necessary) a final resolved human score.  Where it is relevant, you are provided with more than one human score, so that you may evaluate the reliability of the human scorers, but - keep in mind - that you will be predicting to the resolved score.  Also, please note that most essays are scored using a holistic scoring rubric.  However, one data set uses a trait scoring rubric.  The variability is intended to test the limits of your scoring engine’s capabilities. Following a period of 3 months to build and/or train your engine, you will be provided with test data that will contain new essays, randomly selected for blind evaluation.  However, you will notice that the rater and resolved score columns will be blank.  You will be asked to supply, based on your engine's predictions for each essay, your score in the resolved score column and then submit your new data set on this site. As part of the file that you will submit with your predictive scores, you will be asked to submit additional information.  We would like to understand both the time and capital that you’ve spent developing your engine, the profile of your team (or you as an individual if you are working alone) and the projected cost to implement your solution on a larger scale, along with any known limitations.  Basically, you will have the opportunity to present your case for who you are, why your model is commercially viable and to what extent you can use your model to satisfy the interests of potential buyers.  This other information will not be used to determine any prize rewards, and it is optional.  But, if you provide it, it will be used to evaluate whether or not your model should be presented to state departments of education and others who stand to benefit from your work. Also, please note that it is our intention to stage other follow-on ASAP phases in the months ahead.  We are starting with graded essays and will follow with new data:  Phase 1: Demonstration for long-form constructed response (essays); Phase 2: Demonstration for short-form constructed response (short answers); Phase 3: Demonstration for symbolic mathematical/logic reasoning (charts/graphs).  In every instance, we seek to drive innovation for new solutions to automated student assessment.  We hope that you will enjoy this process.  May the best model win!";https://www.kaggle.com/c/asap-aes;;Develop an automated scoring algorithm for student-written essays.;[];153.0;0.53;The Hewlett Foundation: Automated Essay Scoring;Featured prediction Competition
2019-12-20 00:59:00;Q: How much does it cost to cool a skyscraper in the summer? A: A lot! And not just in dollars, but in environmental impact. Thankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That’s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don’t work with different building types. In this competition, you’ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies. About the Host  Founded in 1894, ASHRAE serves to advance the arts and sciences of heating, ventilation, air conditioning refrigeration and their allied fields. ASHRAE members represent building system design and industrial process professionals around the world. With over 54,000 members serving in 132 countries, ASHRAE supports research, standards writing, publishing and continuing education - shaping tomorrow’s built environment today. Banner photo by Federico Beccari on Unsplash;https://www.kaggle.com/c/ashrae-energy-prediction;ASHRAE;How much energy will a building consume?;['data cleaning', 'feature engineering', 'exploratory data analysis', 'utility script', 'regression', 'beginner', 'data visualization', 'ensembling', 'gradient boosting'];3614.0;0.707;ASHRAE - Great Energy Predictor III;Featured prediction Competition
2015-02-10 00:59:00;In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.  For this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.;https://www.kaggle.com/c/avazu-ctr-prediction;;Predict whether a mobile ad will be clicked;[];1602.0;0.669;Click-Through Rate Prediction;Featured prediction Competition
2018-06-28 01:59:00;When selling used goods online, a combination of tiny, nuanced details in a product description can make a big difference in drumming up interest. Details like:   And, even with an optimized product listing, demand for a product may simply not exist–frustrating sellers who may have over-invested in marketing. Avito, Russia’s largest classified advertisements website, is deeply familiar with this problem. Sellers on their platform sometimes feel frustrated with both too little demand (indicating something is wrong with the product or the product listing) or too much demand (indicating a hot item with a good description was underpriced). In their fourth Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive.;https://www.kaggle.com/c/avito-demand-prediction;Avito;Predict demand for an online classified ad;['nlp', 'exploratory data analysis', 'feature engineering', 'beginner', 'data visualization', 'gpu', 'text data'];1871.0;0.677;Avito Demand Prediction Challenge;Featured prediction Competition
2020-03-17 00:59:00;Challenge and dataset summary available at https://arxiv.org/abs/2010.00170 Bengali is the 5th most spoken language in the world with hundreds of million of speakers. It’s the official language of Bangladesh and the second most spoken language in India. Considering its reach, there’s significant business and educational interest in developing AI that can optically recognize images of the language handwritten. This challenge hopes to improve on approaches to Bengali recognition.  Optical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to English’s 250 graphemic units). Bangladesh-based non-profit Bengali.AI is focused on helping to solve this problem. They build and release crowdsourced, metadata-rich datasets and open source them through research competitions. Through this work, Bengali.AI hopes to democratize and accelerate research in Bengali language technologies and to promote machine learning education. For this competition, you’re given the image of a handwritten Bengali grapheme and are challenged to separately classify three constituent elements in the image: grapheme root, vowel diacritics, and consonant diacritics. By participating in the competition, you’ll hopefully accelerate Bengali handwritten optical character recognition research and help enable the digitalization of educational resources. Moreover, the methods introduced in the competition will also empower cousin languages in the Indian subcontinent. Acknowledgements:    Apurba: Apurba is the exclusive sponsor of Bengali.AI for this competition. Apurba Technologies Inc. is founded by a group of technology veterans who have been working at the cutting edge of software development in Silicon Valley for many years. Apart from its many ventures, Apurba is a pioneer in Bengali NLP research today and is accelerating AI research in Bangladesh through its contributions.     Intelligent Machines Limited: Intelligent Machines Limited is the technical partner of Bengali.AI for this competition and is providing compute support to Bangladeshi students. IML is an Artificial Intelligence and Advanced Analytics startup offering customized solutions to businesses in Bangladesh. IML believes in the strength of Bangladeshi talented resources and in the possibility of a far greater and developed Bangladesh in the coming days.;https://www.kaggle.com/c/bengaliai-cv19;Bengali.AI;Classify the components of handwritten Bengali;['beginner', 'computer vision', 'exploratory data analysis', 'data visualization'];2059.0;0.682;Bengali.AI Handwritten Grapheme Classification;Research Code Competition
2019-12-13 00:59:00;We’ve all been there: Stuck at a traffic light, only to be given mere seconds to pass through an intersection, behind a parade of other commuters. Imagine if you could help city planners and governments anticipate traffic hot spots ahead of time and reduce the stop-and-go stress of millions of commuters like you. Geotab provides a wide variety of aggregate datasets gathered from commercial vehicle telematics devices. Harnessing the insights from this data has the power to improve safety, optimize operations, and identify opportunities for infrastructure challenges. The dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.  This competition is being hosted in partnership with BigQuery, a data warehouse for manipulating, joining, and querying large scale tabular datasets. BigQuery also offers BigQuery ML, an easy way for users to create and run machine learning models to generate predictions through a SQL query interface. Kaggle recently released a BigQuery integration within our kernels notebook environment, and this starter kernel gives you a great starting point for how to use BQ & BQML. You’re encouraged to use your data savvy, resourcefulness & intuition to find and join in additional external datasets that will increase your models’ predictive power. Alright, stop waiting and get started! Acknowledgments  A big thanks to Geotab for providing the dataset for this competition! Geotab is advancing security, connecting commercial vehicles to the internet and providing web-based analytics to help customers better manage their fleets. Geotab’s open platform and Marketplace, offering hundreds of third-party solution options, allows both small and large businesses to automate operations by integrating vehicle data with their other data assets. As an IoT hub, the in-vehicle device provides additional functionality through IOX Add-Ons. Processing billions of data points a day, Geotab leverages data analytics and machine learning to help customers improve productivity, optimize fleets through the reduction of fuel consumption, enhance driver safety, and achieve strong compliance to regulatory changes. Geotab’s products are represented and sold worldwide through Authorized Geotab Resellers. To learn more, please visit www.geotab.com and follow us @GEOTAB and on LinkedIn.;https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;Google BigQuery;Can you predict wait times at major city intersections?;['deep learning', 'data cleaning', 'feature engineering', 'exploratory data analysis', 'beginner', 'classification', 'data visualization', 'geospatial analysis'];436.0;0.598;BigQuery-Geotab Intersection Congestion;Playground prediction Competition
2015-05-30 01:59:00;Get started on this competition through Kaggle Scripts Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world. The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.  Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Hadi Fanaee Tork using data from Capital Bikeshare. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Fanaee-T, Hadi, and Gama, Joao, Event labeling combining ensemble detectors and background knowledge, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.;https://www.kaggle.com/c/bike-sharing-demand;Kaggle;Forecast use of a city bikeshare system;['data cleaning', 'exploratory data analysis', 'feature engineering', 'random forest', 'regression', 'beginner', 'data visualization'];3242.0;0.703;Bike Sharing Demand;Playground prediction Competition
2012-06-16 01:59:59;"The objective of the competition is to help us build as good a model as possible so that we can, as optimally as this data allows, relate molecular information, to an actual biological response. We have shared the data in the comma separated values (CSV) format. Each row in this data set represents a molecule. The first column contains experimental data describing an actual biological response; the molecule was seen to elicit this response (1),  or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are calculated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The descriptor matrix has  been normalized.";https://www.kaggle.com/c/bioresponse;;Predict a biological response of molecules from their chemical properties;['beginner'];698.0;0.626;Predicting a Biological Response;Featured prediction Competition
2020-09-16 01:59:00;Do you hear the birds chirping outside your window? Over 10,000 bird species occur in the world, and they can be found in nearly every environment, from untouched rainforests to suburbs and even cities. Birds play an essential role in nature. They are high up in the food chain and integrate changes occurring at lower levels. As such, birds are excellent indicators of deteriorating habitat quality and environmental pollution. However, it is often easier to hear birds than see them. With proper sound detection and classification, researchers could automatically intuit factors about an area’s quality of life based on a changing bird population.  There are already many projects underway to extensively monitor birds by continuously recording natural soundscapes over long periods. However, as many living and nonliving things make noise, the analysis of these datasets is often done manually by domain experts. These analyses are painstakingly slow, and results are often incomplete. Data science may be able to assist, so researchers have turned to large crowdsourced databases of focal recordings of birds to train AI models. Unfortunately, there is a domain mismatch between the training data (short recording of individual birds) and the soundscape recordings (long recordings with often multiple species calling at the same time) used in monitoring applications. This is one of the reasons why the performance of the currently used AI models has been subpar.  To unlock the full potential of these extensive and information-rich sound archives, researchers need good machine listeners to reliably extract as much information as possible to aid data-driven conservation. The Cornell Lab of Ornithology’s Center for Conservation Bioacoustics (CCB)’s mission is to collect and interpret sounds in nature. The CCB develops innovative conservation technologies to inspire and inform the conservation of wildlife and habitats globally. By partnering with the data science community, the CCB hopes to further its mission and improve the accuracy of soundscape analyses. In this competition, you will identify a wide variety of bird vocalizations in soundscape recordings. Due to the complexity of the recordings, they contain weak labels. There might be anthropogenic sounds (e.g., airplane overflights) or other bird and non-bird (e.g., chipmunk) calls in the background, with a particular labeled bird species in the foreground. Bring your new ideas to build effective detectors and classifiers for analyzing complex soundscape recordings! If successful, your work will help researchers better understand changes in habitat quality, levels of pollution, and the effectiveness of restoration efforts. Reliable machine listeners would also allow conservationists to deploy more recording units worldwide and would enable data-driven conservation at a scale not yet possible. The eventual conservation outcomes could greatly improve the quality of life for many living organisms—birds and human beings included.;https://www.kaggle.com/c/birdsong-recognition;Cornell Lab of Ornithology;Build tools for bird population monitoring;['audio data', 'feature engineering', 'data visualization', 'exploratory data analysis'];1390.0;0.662;Cornell Birdcall Identification;Research Code Competition
2013-04-18 01:59:00;There is a $10,000 prize pool for this competition, with prizes awarded to the top 3 places:  1st place: $6,500  2nd place: $2,500  3rd place: $1,000;https://www.kaggle.com/c/bluebook-for-bulldozers;;"Predict the auction sale price for a piece of heavy equipment to create a ""blue book"" for bulldozers.";['feature engineering', 'random forest', 'time series analysis', 'regression', 'beginner', 'classification', 'gpu'];474.0;0.603;Blue Book for Bulldozers;Featured prediction Competition
2016-04-19 01:59:00;As a global specialist in personal insurance, BNP Paribas Cardif serves 90 million clients in 36 countries across Europe, Asia and Latin America. In a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers.  In this challenge, BNP Paribas Cardif is providing an anonymized database with two categories of claims:     Kagglers are challenged to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore provide a better service to its customers.;https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;;Can you accelerate BNP Paribas Cardif's claims management process?;[];2920.0;0.698;BNP Paribas Cardif Claims Management;Featured prediction Competition
2016-11-12 00:59:00;A good chocolate soufflé is decadent, delicious, and delicate. But, it's a challenge to prepare. When you pull a disappointingly deflated dessert out of the oven, you instinctively retrace your steps to identify at what point you went wrong. Bosch, one of the world's leading manufacturing companies, has an imperative to ensure that the recipes for the production of its advanced mechanical components are of the highest quality and safety standards. Part of doing so is closely monitoring its parts as they progress through the manufacturing processes.  Because Bosch records data at every step along its assembly lines, they have the ability to apply advanced analytics to improve these manufacturing processes. However, the intricacies of the data and complexities of the production line pose problems for current methods. In this competition, Bosch is challenging Kagglers to predict internal failures using thousands of measurements and tests made for each component along the assembly line. This would enable Bosch to bring quality products at lower costs to the end user.;https://www.kaggle.com/c/bosch-production-line-performance;Bosch;Reduce manufacturing failures;[];1370.0;0.662;Bosch Production Line Performance;Featured prediction Competition
2019-04-12 01:59:00;CareerCon 2019 is upon us! CareerCon is a digital event all about landing your first data science job — and registration is now open! Ahead of the event, we have a fun competition to get you started. See below for a unique challenge and opportunity to share your resume with select CareerCon sponsors.  ___________________________________ The Competition Robots are smart… by design. To fully understand and properly navigate a task, however, they need input about their environment. In this competition, you’ll help robots recognize the floor surface they’re standing on using data collected from Inertial Measurement Units (IMU sensors). We’ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity. Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won’t fall down on the job.  Special thanks for making this competition possible: The data for this competition has been collected by Heikki Huttunen and Francesco Lomio from the Department of Signal Processing and Damoon Mohamadi, Kaan Celikbilek, Pedram Ghazi and Reza Ghabcheloo from the Department of Automation and Mechanical   Engineering both from Tampere University, Finland. We at Kaggle would like thank them all for kindly donating the data that has made this competition possible!;https://www.kaggle.com/c/career-con-2019;Kaggle;Compete to get your resume in front of our sponsors;['deep learning', 'data cleaning', 'feature engineering', 'exploratory data analysis', 'multiclass classification', 'signal processing', 'beginner', 'optimization', 'classification', 'data visualization', 'gpu', 'lstm'];1449.0;0.664;CareerCon 2019 - Help Navigate Robots;Recruitment prediction Competition
2017-09-28 01:59:00;As with any big purchase, full information and transparency are key. While most everyone describes buying a used car as frustrating, it’s just as annoying to sell one, especially online. Shoppers want to know everything about the car but they must rely on often blurry pictures and little information, keeping used car sales a largely inefficient, local industry. Carvana, a successful online used car startup, has seen opportunity to build long term trust with consumers and streamline the online buying process. An interesting part of their innovation is a custom rotating photo studio that automatically captures and processes 16 standard images of each vehicle in their inventory. While Carvana takes high quality photos,  bright reflections and cars with similar colors as the background cause automation errors, which requires a skilled photo editor to change.  In this competition, you’re challenged to develop an algorithm that automatically removes the photo studio background. This will allow Carvana to superimpose cars on a variety of backgrounds. You’ll be analyzing a dataset of photos, covering different vehicles with a wide variety of year, make, and model combinations.;https://www.kaggle.com/c/carvana-image-masking-challenge;Carvana;Automatically identify the boundaries of the car in an image;['advanced', 'intermediate', 'computer vision'];735.0;0.628;Carvana Image Masking Challenge;Featured prediction Competition
2019-12-10 00:59:00;Is there a cat in your dat? A common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured. Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:  binary features low- and high-cardinality nominal features low- and high-cardinality  ordinal features (potentially) cyclical features  This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community. If you're not sure how to get started, you can check out the Categorical Variables  section of Kaggle's Intermediate Machine Learning course.  Have Fun!;https://www.kaggle.com/c/cat-in-the-dat;Kaggle;Binary classification, with every feature a categorical;['data cleaning', 'exploratory data analysis', 'feature engineering', 'categorical data', 'utility script', 'beginner', 'classification', 'data visualization', 'gpu'];1342.0;0.661;Categorical Feature Encoding Challenge;Playground prediction Competition
2020-04-01 01:59:00;Can you find more cat in your dat? We loved the participation and engagement with the first Cat in the Dat competition. Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:  binary features low- and high-cardinality nominal features low- and high-cardinality  ordinal features (potentially) cyclical features  This follow-up competition offers an even more challenging dataset so that you can continue to build your skills with the common machine learning task of encoding categorical variables.  This challenge adds the additional complexity of feature interactions, as well as missing data. This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community. If you're not sure how to get started, you can check out the Categorical Variables  section of Kaggle's Intermediate Machine Learning course.  Have Fun!;https://www.kaggle.com/c/cat-in-the-dat-ii;Kaggle;Binary classification, with every feature a categorical (and interactions!);['neural networks', 'feature engineering', 'exploratory data analysis', 'naive bayes', 'beginner', 'classification', 'data visualization', 'gpu', 'ensembling', 'logistic regression'];1161.0;0.653;Categorical Feature Encoding Challenge II;Playground prediction Competition
2017-12-15 00:59:00;Rules Update: The CDiscount team has updated their rules to allow for use of this dataset for research and academic purposes only. To access the data, go to rules and accept the terms to download the data. Cdiscount.com generated nearly 3 billion euros last year, making it France’s largest non-food e-commerce company. While the company already sells everything from TVs to trampolines, the list of products is still rapidly growing. By the end of this year, Cdiscount.com will have over 30 million products up for sale. This is up from 10 million products only 2 years ago. Ensuring that so many products are well classified is a challenging task. Currently, Cdiscount.com applies machine learning algorithms to the text description of the products in order to automatically predict their category. As these methods now seem close to their maximum potential, Cdiscount.com believes that the next quantitative improvement will be driven by the application of data science techniques to images. In this challenge you will be building a model that automatically classifies the products based on their images. As a quick tour of Cdiscount.com's website can confirm, one product can have one or several images. The data set Cdiscount.com is making available is unique and characterized by superlative numbers in several ways:  Almost 9 million products: half of the current catalogue More than 15 million images at 180x180 resolution More than 5000 categories: yes this is quite an extreme multi-class classification!;https://www.kaggle.com/c/cdiscount-image-classification-challenge;Cdiscount;Categorize e-commerce photos;[];626.0;0.619;Cdiscount’s Image Classification Challenge;Featured prediction Competition
2013-05-25 01:59:00;Example baseline submissions are available as part of the pylearn2 python package available at https://github.com/lisa-lab/pylearn2 The baseline submissions for this contest are in pylearn2/scripts/icml_2013_wrepl/emotions Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set.;https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;;Learn facial expressions from an image;['keras', 'beginner', 'gpu', 'cnn'];56.0;0.455;Challenges in Representation Learning: Facial Expression Recognition Challenge;Research prediction Competition
2019-08-29 01:59:00;Think you can use your data science smarts to make big predictions at a molecular level? This challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules. Researchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science.   This competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. Winning teams will have an opportunity to partner with this multi-university research program on an academic publication Your Challenge In this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant). Once the competition finishes, CHAMPS would like to invite the top teams to present their work, discuss the details of their models, and work with them to write a joint research publication which discusses an open-source implementation of the solution. About Scalar Coupling Using NMR to gain insight into a molecule’s structure and dynamics depends on the ability to accurately predict so-called “scalar couplings”. These are effectively the magnetic interactions between a pair of atoms. The strength of this magnetic interaction depends on intervening electrons and chemical bonds that make up a molecule’s three-dimensional structure. Using state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows. A fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior.  Ultimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease. Join the CHAMPS Scalar Coupling challenge to apply predictive analytics to chemistry and chemical biology.;https://www.kaggle.com/c/champs-scalar-coupling;CHAMPS (CHemistry And Mathematics in Phase Space);Can you measure the magnetic interactions between a pair of atoms?;['feature engineering', 'data visualization', 'gpu'];2749.0;0.695;Predicting Molecular Properties;Featured prediction Competition
2014-10-19 01:59:00;CIFAR-10  is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Kaggle is hosting a CIFAR-10 leaderboard for the machine learning community to use for fun and practice. You can see how your approach compares to the latest research methods on Rodrigo Benenson's classification results page.  Please cite this technical report if you use this dataset: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.;https://www.kaggle.com/c/cifar-10;Kaggle;Identify the subject of 60,000 labeled images;['deep learning', 'keras', 'exploratory data analysis', 'classification', 'gpu', 'transfer learning', 'cnn'];231.0;0.558;CIFAR-10 - Object Recognition in Images;Playground prediction Competition
2019-04-26 01:59:00;Ciphertext Challenge II: The Challengening! It's baaaaaaack! In our first ciphertext competition, we hunted the wilds of the '90s-era internet. This time around, we're exploring the dark slow-broadband-y wastelands of 2011, with the Movie Review Dataset. In 2011 most of the internet hadn't even been invented yet*, so wow, you're in for a treat. Again, simple classic ciphers have been used to encrypt this dataset. Your mission this time: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Also, there are some new ciphers in play this time, which will involve some meta-puzzling. Enjoy! Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the LB (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best competition-related kernels, in both visualization and cryptanalysis, based on upvotes. Go ahead. Get cracking! * - This is not true. Acknowledgements Maas, A., Daly, R., Pham, P., Huang, D., Ng, A. and Potts, C. (2011). Learning Word Vectors for Sentiment Analysis: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. [online] Portland, Oregon, USA: Association for Computational Linguistics, pp. 142–150. Available here.;https://www.kaggle.com/c/ciphertext-challenge-ii;Kaggle;553398 418126 467884 411 374106 551004 356535 539549 487091 290502 121468 556912 469347 515719 201909 101;[];74.0;0.477;Ciphertext Challenge II;Playground prediction Competition
2019-09-06 01:59:00;"Ciphertext Challenge III: Wherefore Art Thou, Simple Ciphers? We've done the 2010's, the 1990s… now it's time for the 80s. The 1580s!! In this new decryption competition's dataset, we've gone from perfectly respectable sources of electronic horror to a time before computers—heck, before calculus was called ""calculus""!  Shakespeare's plays are encrypted, and we time travelers must un-encrypt them so people can do innovative stage productions with intricate makeup, costumes, and possibly—possibly!—Leonardo DiCaprio. Think about it, folks: Leo.* As in previous ciphertext challenges, simple classic ciphers have been used to encrypt this dataset, along with a slightly less simple surprise that expands our definition of ""classic"" into the modern age. The mission is the same: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Meta-puzzles and difficulty await! Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the leaderboard (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best competition-related kernels, in both visualization and cryptanalysis, based on upvotes.  Last, the coveted ""Phil Prize""—for the team that correctly deduces the form AND key of the final cipher—is up for grabs again. Go ahead. Get cracking! * - Leo! Acknowledgements Many thanks to Kaggler LiamLarson for their excellent Shakespeare dataset.";https://www.kaggle.com/c/ciphertext-challenge-iii;;BRBTvl0LNstxQLyxulCEEq1czSFje0Z6iajczo6ktGmitTE=;['beginner', 'exploratory data analysis'];103.0;0.502;Ciphertext Challenge III;Playground prediction Competition
2014-03-03 00:59:00;"The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state:  Any live cell with fewer than two live neighbours dies, as if by underpopulation. Any live cell with two or three live neighbours lives on to the next generation. Any live cell with more than three live neighbours dies, as if by overpopulation. Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction.  These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics.  As Wikipedia tells it,  Ever since its publication, Conway's Game of Life has attracted much interest, because of the surprising ways in which the patterns can evolve. Life provides an example of emergence and self-organization. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that ""design"" and ""organization"" can spontaneously emerge in the absence of a designer. For example, philosopher and cognitive scientist Daniel Dennett has used the analogue of Conway's Life ""universe"" extensively to illustrate the possible evolution of complex philosophical constructs, such as consciousness and free will, from the relatively simple set of deterministic physical laws governing our own universe.  The emergence of order from simple rules begs an interesting question--what happens if we set time backwards? This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse.  Is the chaotic start of Life predictable from its orderly ends?  We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board. Although some people have examined this problem, it is unknown (at least, to us...) just how difficult this will be.";https://www.kaggle.com/c/conway-s-reverse-game-of-life;Kaggle;Reverse the arrow of time in the Game of Life;['gpu'];141.0;0.524;Conway's Reverse Game of Life;Playground prediction Competition
2020-12-01 00:59:00;"This is a relaunch of a previous competition, Conway's Reverse Game of Life, with the following changes:  The grid size is larger (25 vs. 25) and the grid wraps around from top to bottom and left to right Submissions are solved forward by the appropriate number of steps, so that any correct starting solution will achieve a maximum score. This article contains the stepping function that is used for this competition.  Obligatory Disclaimer: A lot has changed since the original competition was launched 6 years ago. With the change from ""exact starting point"" to  ""any correct starting point"", it is possible to get a perfect score. We just don't know how difficult that will be. Use it as a fun learning experience, and don't spoil it for others by posting perfect solutions! ~~~~~~~~~ The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state:  Overpopulation: if a living cell is surrounded by more than three living cells, it dies. Stasis: if a living cell is surrounded by two or three living cells, it survives. Underpopulation: if a living cell is surrounded by fewer than two living cells, it dies. Reproduction: if a dead cell is surrounded by exactly three cells, it becomes a live cell.  These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics. As Wikipedia states Ever since its publication, Conway's Game of Life has attracted much interest, because of the surprising ways in which the patterns can evolve. Life provides an example of emergence and self-organization. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that ""design"" and ""organization"" can spontaneously emerge in the absence of a designer. For example, philosopher and cognitive scientist Daniel Dennett has used the analogue of Conway's Life ""universe"" extensively to illustrate the possible evolution of complex philosophical constructs, such as consciousness and free will, from the relatively simple set of deterministic physical laws governing our own universe.  The emergence of order from simple rules begs an interesting question—what happens if we set time backwards? This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse. Is the chaotic start of Life predictable from its orderly ends? We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board.  This is a Code Competition. Refer to Code Requirements for details.";https://www.kaggle.com/c/conways-reverse-game-of-life-2020;Kaggle;Reverse the arrow of time in the Game of Life;['deep learning', 'keras', 'feature engineering', 'exploratory data analysis', 'neural networks', 'games', 'beginner', 'gpu', 'pytorch', 'cnn'];188.0;0.544;Conway's Reverse Game of Life 2020;Playground Code Competition
2018-09-20 01:59:00;The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?   Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.   In Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need. While this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines. To improve on PMT, the IDB (the largest source of development financing for Latin America and the Caribbean) has turned to the Kaggle community. They believe that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance. Beyond Costa Rica, many countries face this same problem of inaccurately assessing social need. If Kagglers can generate an improvement, the new algorithm could be implemented in other countries around the world. This is a Kernels-Only Competition, so you must submit your code through Kernels, rather than uploading .csv predictions. You can create private Kernels and even share/edit your work with teammates by adding them as collaborators.;https://www.kaggle.com/c/costa-rican-household-poverty-prediction;Inter-American Development Bank;Can you identify which households have the highest need for social welfare assistance?;['data cleaning', 'feature engineering', 'exploratory data analysis', 'xgboost', 'beginner', 'classification', 'data visualization', 'gpu'];618.0;0.619;Costa Rican Household Poverty Level Prediction;Playground Code Competition
2020-03-26 00:59:00;This week 1 forecasting task is now closed for submissions. Click here to visit the week 2 version, and make a submission there. This is one of the two complementary forecasting tasks to predict COVID-19 spread. This task is based on various regions across the world. To start on a single state-level subcomponent, please see the companion forecasting task for California, USA. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching two companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 by region, the primary goal isn't to produce accurate forecasts. It’s to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-1;Kaggle;Forecast daily COVID-19 spread in regions around world;['decision tree', 'logistic regression', 'exploratory data analysis', 'random forest'];544.0;0.611;COVID19 Global Forecasting (Week 1);Research Code Competition
2020-04-06 05:56:00;This week 2 forecasting task is now closed for submissions. Click here to visit the week 3 version, and make a submission there. This is week 2 of Kaggle's COVID19 forecasting series, following the Week 1 competition. This is the 2nd of at least 4 competitions we plan to launch in this series. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-2;Kaggle;Forecast daily COVID-19 spread in regions around world;[];215.0;0.553;COVID19 Global Forecasting (Week 2);Research Code Competition
2020-04-09 01:59:00;This week 3 forecasting task is now closed for submissions. Click here to visit the week 4 version, and make a submission there. This is week 3 of Kaggle's COVID19 forecasting series, following the Week 2 competition. This is the 3rd of at least 4 competitions we plan to launch in this series. All of the prior discussion forums have been migrated to this competition for continuity. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-3;Kaggle;Forecast daily COVID-19 spread in regions around world;[];452.0;0.6;COVID19 Global Forecasting (Week 3);Research Code Competition
2020-04-16 01:59:00;This is week 4 of Kaggle's COVID-19 forecasting series, following the Week 3 competition. This is the 4th competition we've launched in this series. All of the prior discussion forums have been migrated to this competition for continuity. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 15 and May 14 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-4;Kaggle;Forecast daily COVID-19 spread in regions around world;[];472.0;0.603;COVID19 Global Forecasting (Week 4);Research Code Competition
2020-05-12 01:59:00;This is week 5 of Kaggle's COVID-19 forecasting series, following the Week 4 competition. This competition has some changes from prior weeks - be sure to check the Evaluation and Data pages for more details. All of the prior discussion forums have been migrated to this competition for continuity. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves developing quantile estimates intervals for confirmed cases and fatalities between May 12 and June 7 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-global-forecasting-week-5;Kaggle;Forecast daily COVID-19 spread in regions around world;['covid19', 'beginner', 'data visualization', 'random forest'];173.0;0.539;COVID19 Global Forecasting (Week 5);Research Code Competition
2020-03-26 00:59:00;This is one of the two complementary forecasting tasks to predict COVID-19 spread. This one is based on a single state-level subcomponent in California, USA. Our intent in having this region-specific version is to offer a more manageable starting point for the global forecasting task. To start on the global version, please see the companion forecasting task. Background The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO). The Challenge Kaggle is launching two companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 in California, the primary goal isn't to produce accurate forecasts. It’s to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.  As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.  Companies and Organizations There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. Acknowledgements JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;Kaggle;Forecast daily COVID-19 spread in California, USA;['covid19', 'beginner', 'data visualization'];190.0;0.545;COVID19 Local US-CA Forecasting (Week 1);Research Code Competition
2018-06-12 01:59:00;When you're driving, how important is it to be able to quickly tell the difference between a person vs. a stop sign? It's a hugely important, but typically very simple, distinction that you would make reflexively. Autonomous vehicles are not able to do this quite as effortlessly.  This challenge, hosted by the 2018 CVPR workshop on autonomous driving (WAD), asks you to help give autonomously driven vehicles the same edge. Using an unprecedented dataset, you're asked to segment movable objects, such as cars and pedestrians, at instance level within image frames.  By participating in this competition, you'll be helping to further our understand of the current status of computer vision algorithms in solving environmental perception problems for autonomous driving. This challenge is a truly unique opportunity to work on a tremendously high value and high profile problem. The dataset presented here contains over 10 times more fine-labeled images than the largest public dataset of its type. Acknowledgements This competition is hosted by the 2018 CVPR workshop on autonomous driving (WAD), with dataset and evaluation metric contributed by Baidu Inc.;https://www.kaggle.com/c/cvpr-2018-autonomous-driving;CVPR 2018 WAD;Can you segment each objects within image frames captured by vehicles?;['exploratory data analysis', 'automobiles and vehicles', 'image data', 'data visualization', 'gpu', 'computer vision', 'cnn'];141.0;0.524;CVPR 2018 WAD Video Segmentation Challenge;Research prediction Competition
2017-04-13 01:59:00;In the United States, lung cancer strikes 225,000 people every year, and accounts for $12 billion in health care costs. Early detection is critical to give patients the best chance at recovery and survival. One year ago, the office of the U.S. Vice President spearheaded a bold new initiative, the Cancer Moonshot, to make a decade's worth of progress in cancer prevention, diagnosis, and treatment in just 5 years. In 2017, the Data Science Bowl will be a critical milestone in support of the Cancer Moonshot by convening the data science and medical communities to develop lung cancer detection algorithms. Using a data set of thousands of high-resolution lung scans provided by the National Cancer Institute, participants will develop algorithms that accurately determine when lesions in the lungs are cancerous. This will dramatically reduce the false positive rate that plagues the current detection technology, get patients earlier access to life-saving interventions, and give radiologists more time to spend with their patients. This year, the Data Science Bowl will award $1 million in prizes to those who observe the right patterns, ask the right questions, and in turn, create unprecedented impact around cancer screening care and prevention. The funds for the prize purse will be provided by the Laura and John Arnold Foundation. Visit DataScienceBowl.com to: • Sign up to receive news about the competition• Learn about the history of the Data Science Bowl and past competitions• Read our latest insights on emerging analytics techniques  Acknowledgments The Data Science Bowl is presented by  Competition Sponsors Laura and John Arnold FoundationCancer Imaging Program of the National Cancer InstituteAmerican College of RadiologyAmazon Web ServicesNVIDIA Data Support Providers National Lung Screening TrialThe Cancer Imaging ArchiveDiagnostic Image Analysis Group, Radboud UniversityLahey Hospital & Medical CenterCopenhagen University Hospital Supporting Organizations  Bayes ImpactBlack Data Processng AssociatesCode the ChangeData Community DCDataKindGalvanizeGreat Minds in STEMHortonworksINFORMSLesbians Who TechNSBESociety of Asian Scientists & EngineersSociety of Women EngineersUniversity of Texas Austin, Business Analytics Program,McCombs School of BusinessUS Dept. of Health and Human ServicesUS Food and Drug AdministrationWomen in TechnologyWomen of Cyberjutsu;https://www.kaggle.com/c/data-science-bowl-2017;Booz Allen Hamilton;Can you improve lung cancer detection?;['deep learning', 'exploratory data analysis', 'data visualization', 'advanced', 'computer vision', 'cnn'];1972.0;0.68;Data Science Bowl 2017;Featured prediction Competition
2018-04-17 01:59:00;Spot Nuclei. Speed Cures. Imagine speeding up research for almost every disease, from lung cancer and heart disease to rare disorders. The 2018 Data Science Bowl offers our most ambitious mission yet: create an algorithm to automate nucleus detection. We’ve all seen people suffer from diseases like cancer, heart disease, chronic obstructive pulmonary disease, Alzheimer’s, and diabetes. Many have seen their loved ones pass away. Think how many lives would be transformed if cures came faster. By automating nucleus detection, you could help unlock cures faster—from rare disorders to the common cold. Want a snapshot about the 2018 Data Science Bowl? View this video. Why nuclei? Identifying the cells’ nuclei is the starting point for most analyses because most of the human body’s 30 trillion cells contain a nucleus full of DNA, the genetic code that programs each cell. Identifying nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work. By participating, teams will work to automate the process of identifying nuclei, which will  allow for more efficient drug testing, shortening the 10 years it takes for each new drug to come to market. Check out this video overview to find out more. What will participants do? Teams will create a computer model that can identify a range of nuclei across varied conditions. By observing patterns, asking questions, and building a model, participants will have a chance to push state-of-the-art technology farther. Visit DataScienceBowl.com to:  • Sign up to receive news about the competition • Learn about the history of the Data Science Bowl and past competitions • Read our latest insights on emerging analytics techniques;https://www.kaggle.com/c/data-science-bowl-2018;Booz Allen Hamilton;Find the nuclei in divergent images to advance medical discovery;['deep learning', 'exploratory data analysis', 'neural networks', 'biology', 'image data', 'beginner', 'data visualization', 'computer vision', 'cnn'];3634.0;0.708;2018 Data Science Bowl;Featured prediction Competition
2020-01-23 00:59:00;Illuminate Learning. Ignite Possibilities. Uncover new insights in early childhood education and how media can support learning outcomes. Participate in our fifth annual Data Science Bowl, presented by Booz Allen Hamilton and Kaggle. PBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life.  In this challenge, you’ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes. Data Science Bowl is the world’s largest data science competition focused on social good. Each year, this competition gives Kagglers a chance to use their passion to change the world. Over the last four years, more than 50,000+ Kagglers have submitted over 114,000+ submissions, to improve everything from lung cancer and heart disease detection to ocean health.  For more information on the Data Science Bowl, please visit DataScienceBowl.com Where does the data for the competition come from? The data used in this competition is anonymous, tabular data of interactions with the PBS KIDS Measure Up! app. Select data, such as a user’s in-app assessment score or their path through the game, is collected by the PBS KIDS Measure Up! app, a game-based learning tool.  PBS KIDS is committed to creating a safe and secure environment that family members of all ages can enjoy. The PBS KIDS Measure Up! app does not collect any personally identifying information, such as name or location. All of the data used in the competition is anonymous. To view the full PBS KIDS privacy policy, please visit: pbskids.org/privacy. No one will be able to download the entire data set and the participants do not have access to any personally identifiable information about individual users. The Data Science Bowl and the use of data for this year’s competition has been reviewed to ensure that it meets requirements of applicable child privacy regulations by PRIVO, a leading global industry expert in children’s online privacy. What is the PBS KIDS Measure Up! app? In the PBS KIDS Measure Up! app, children ages 3 to 5 learn early STEM concepts focused on length, width, capacity, and weight while going on an adventure through Treetop City, Magma Peak, and Crystal Caves. Joined by their favorite PBS KIDS characters, children can also collect rewards and unlock digital toys as they play. To learn more about PBS KIDS Measure Up!, please click here. PBS KIDS and the PBS KIDS Logo are registered trademarks of PBS. Used with permission. The contents of PBS KIDS Measure Up! were developed under a grant from the Department of Education. However, those contents do not necessarily represent the policy of the Department of Education, and you should not assume endorsement by the Federal Government. The app is funded by a Ready To Learn grant (PR/AWARD No. U295A150003, CFDA No. 84.295A) provided by the Department of Education to the Corporation for Public Broadcasting.;https://www.kaggle.com/c/data-science-bowl-2019;Booz Allen Hamilton;Uncover the factors to help measure how young children learn;['data cleaning', 'exploratory data analysis', 'feature engineering', 'beginner', 'classification', 'data visualization', 'gradient boosting'];3493.0;0.706;2019 Data Science Bowl;Featured Code Competition
2019-04-24 01:59:00;Welcome In this competition you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition.  CareerVillage.org is a nonprofit that crowdsources career advice for underserved youth. Founded in 2011 in four classrooms in New York City, the platform has now served career advice from 25,000 volunteer professionals to over 3.5M online learners. The platform uses a Q&A style similar to StackOverflow or Quora to provide students with answers to any question about any career.  In this Data Science for Good challenge, CareerVillage.org, in partnership with Google.org, is inviting you to help recommend questions to appropriate volunteers. To support this challenge, CareerVillage.org has supplied five years of data.  Problem Statement The U.S. has almost 500 students for every guidance counselor. Underserved youth lack the network to find their career role models, making CareerVillage.org the only option for millions of young people in America and around the globe with nowhere else to turn.  To date, 25,000 volunteers have created profiles and opted in to receive emails when a career question is a good fit for them. This is where your skills come in. To help students get the advice they need, the team at CareerVillage.org needs to be able to send the right questions to the right volunteers. The notifications sent to volunteers seem to have the greatest impact on how many questions are answered. Your objective: develop a method to recommend relevant questions to the professionals who are most likely to answer them.  Criteria for Measuring Solutions Performance: How well does the solution match professionals to the questions they would be motivated to answer? CareerVillage.org will not be able to live-test every submission, so a strong entry will clearly articulate why it will be effective at motivating answers. Easy to implement: The CareerVillage.org team wants to put the winning submissions to work, quickly.  A good entry will be well documented and easy to test in production. Extensibility: In the future, CareerVillage.org aims to add more data features and to accommodate new objectives. Winning submissions should allow for this and other augmentations to be added in the future.;https://www.kaggle.com/c/data-science-for-good-careervillage;CareerVillage.org;Match career advice questions with professionals in the field;['exploratory data analysis', 'recommender systems', 'beginner', 'data visualization', 'text data', 'text mining'];;0.0;Data Science for Good: CareerVillage.org;Analytics  Competition
2019-06-22 01:59:00;"Data Science for Good: City of Los Angeles Help the City of Los Angeles to structure and analyze its job descriptions The City of Los Angeles faces a big hiring challenge: 1/3 of its 50,000 workers are eligible to retire by July of 2020. The city has partnered with Kaggle to create a competition to improve the job bulletins that will fill all those open positions. Problem Statement The content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.   The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to: (1) identify language that can negatively bias the pool of applicants; (2) improve the diversity and quality of the applicant pool; and/or (3) make it easier to determine which promotions are available to employees in each job class. How to Participate  Accept the Rules   Accept the competition rules.     Make Your Submission   Follow the submission instructions.  WIth your help, Los Angeles will overcome a wave of retirements and fill those jobs with a strong and diverse workforce. Good luck and happy Kaggling! Do you think companies can find better candidates by improving their job postings? We hope to create an open-sourced body of work focused on this topic by hosting another Data Science for Good competition, this time in partnership with the City of Los Angeles.";https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;City of Los Angeles;Help the City of Los Angeles to structure and analyze its job descriptions;['nlp', 'data cleaning', 'exploratory data analysis', 'beginner', 'data visualization', 'gpu', 'text data', 'text mining'];;0.0;Data Science for Good: City of Los Angeles;Analytics  Competition
2015-01-01 00:59:00;"hosting a meetup on Scikit-learn We encourage participants to post code via the ""Tutorials"" link on the left.  Don't worry about accuracy or whether your code is perfect.  The aim here is to explore sklearn by using it.    Its implementation is high quality due to s Meetup Information Thursday, March 7, 2013,  “Learning in Python with scikit-learn"" by Andreas Mueller   ""Parallel and large scale learning with scikit-learn"" by Olivier Grisel   notebook interface How to perform scalable text feature extraction with the Hashing Trick and hyper parameters tuning How to optimize memory usage with memory mapping How to approximate kernel Support Vector Machines for large scale datasets A short introduction to Ensembles with model averaging and Random Forests   by day and a Python machine learning hacker by night. He is interested in applications to Natural Language Processing, Computer Vision and predictive modelling.";https://www.kaggle.com/c/data-science-london-scikit-learn;Kaggle;Scikit-learn is an open-source machine learning library for Python. Give it a try here!;[];190.0;0.545;Data Science London + Scikit-learn;Getting Started prediction Competition
2020-04-24 00:17:00;This competition is closed for submissions. Participants' selected code submissions were re-run by the host on a privately-held test set and the private leaderboard results have been finalized. Late submissions will not be opened, due to an inability to replicate the unique design of this competition. Deepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online. These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rights—especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion. Identifying manipulated media is a technically demanding and rapidly evolving challenge that requires collaborations across the entire tech industry and beyond.  AWS, Facebook, Microsoft, the Partnership on AI’s Media Integrity Steering Committee, and academics have come together to build the Deepfake Detection Challenge (DFDC). The goal of the challenge is to spur researchers around the world to build innovative new technologies that can help detect deepfakes and manipulated media. Challenge participants must submit their code into a black box environment for testing. Participants will have the option to make their submission open or closed when accepting the prize. Open proposals will be eligible for challenge prizes as long as they abide by the open source licensing terms. Closed proposals will be proprietary and not be eligible to accept the prizes. Regardless of which track is chosen, all submissions will be evaluated in the same way. Results will be shown on the leaderboard.  The PAI Steering Committee has emphasized the need to ensure that all technical efforts incorporate attention to how the resulting code and products based on it can be made as accessible and useful as possible to key frontline defenders of information quality such as journalists and civic leaders around the world. The DFDC results will be a contribution to this effort and building a robust response to the emergent threat deepfakes pose globally.;https://www.kaggle.com/c/deepfake-detection-challenge;Deepfake Detection Challenge;Identify videos with facial or voice manipulations;[];2265.0;0.686;Deepfake Detection Challenge;Featured Code Competition
2018-09-25 01:59:00;This competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset.  You are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores. What's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost? This is a great competition to explore different models and improve your skills in forecasting.;https://www.kaggle.com/c/demand-forecasting-kernels-only;Kaggle;Predict 3 months of item sales at different stores;['data cleaning', 'feature engineering', 'exploratory data analysis', 'time series analysis', 'beginner', 'data visualization'];461.0;0.602;Store Item Demand Forecasting Challenge;Playground Code Competition
2015-10-06 01:59:00;Optical Character Recognition (OCR) is the process of getting type or handwritten documents into a digitized format. If you've read a classic novel on a digital reading device or had your doctor pull up old healthcare records via the hospital computer system, you've probably benefited from OCR. OCR makes previously static content editable, searchable, and much easier to share. But, a lot of documents eager for digitization are being held back. Coffee stains, faded sun spots, dog-eared pages, and lot of wrinkles are keeping some printed documents offline and in the past.  This competition challenges you to give these documents a machine learning makeover. Given a dataset of images of scanned text that has seen better days, you're challenged to remove the noise. Improving the ease of document enhancement will help us get that rare mathematics book on our e-reader before the next beach vacation. We've kicked off the fun with a few handy scripts to get you started on the dataset. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was created by RM.J. Castro-Bleda, S. España-Boquera, J. Pastor-Pellicer, F. Zamora-Martinez. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science;https://www.kaggle.com/c/denoising-dirty-documents;Kaggle;Remove noise from printed text;['deep learning', 'image data', 'beginner', 'data visualization', 'gpu', 'cnn'];161.0;0.534;Denoising Dirty Documents;Playground prediction Competition
2015-07-28 01:59:00;Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people.  The US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment. Currently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment. Clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient. The need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible – ideally resulting in models with realistic clinical potential. The winning models will be open sourced to maximize the impact such a model can have on improving DR detection. Acknowledgements This competition is sponsored by the California Healthcare Foundation.  Retinal images were provided by EyePACS, a free platform for retinopathy screening.;https://www.kaggle.com/c/diabetic-retinopathy-detection;;Identify signs of diabetic retinopathy in eye images;['deep learning', 'beginner', 'classification', 'gpu', 'transfer learning'];660.0;0.622;Diabetic Retinopathy Detection;Featured prediction Competition
2018-02-27 00:59:00;Who's a good dog? Who likes ear scratches? Well, it seems those fancy deep neural networks don't have all the answers. However, maybe they can answer that ubiquitous question we all ask when meeting a four-legged stranger: what kind of good pup is that? In this playground competition, you are provided a strictly canine subset of ImageNet in order to practice fine-grained image categorization. How well you can tell your Norfolk Terriers from your Norwich Terriers? With 120 breeds of dogs and a limited number training images per class, you might find the problem more, err, ruff than you anticipated.  Acknowledgments We extend our gratitude to the creators of the Stanford Dogs Dataset for making this competition possible: Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li.;https://www.kaggle.com/c/dog-breed-identification;Kaggle;Determine the breed of a dog in an image;['deep learning', 'data cleaning', 'neural networks', 'animals', 'computer science', 'beginner', 'data visualization', 'gpu', 'transfer learning', 'computer vision'];1282.0;0.658;Dog Breed Identification;Playground prediction Competition
2014-02-02 00:59:00;In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat.  This is easy for humans, dogs, and cats. Your computer will find it a bit more difficult.  Deep Blue beat Kasparov at chess in 1997.Watson beat the brightest trivia minds at Jeopardy in 2011.Can you tell Fido from Mittens in 2013? The Asirra data set Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords. Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface: Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research.  Image recognition attacks While random guessing is the easiest form of attack, various forms of image recognition can allow an attacker to make guesses that are better than random. There is enormous diversity in the photo database (a wide variety of backgrounds, angles, poses, lighting, etc.), making accurate automatic classification difficult. In an informal poll conducted many years ago, computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art. For reference, a 60% classifier improves the guessing probability of a 12-image HIP from 1/4096 to 1/459. State of the art The current literature suggests machine classifiers can score above 80% accuracy on this task [1]. Therfore, Asirra is no longer considered safe from attack.  We have created this contest to benchmark the latest computer vision and deep learning approaches to this problem. Can you crack the CAPTCHA? Can you improve the state of the art? Can you create lasting peace between cats and dogs? Okay, we'll settle for the former.  Acknowledgements We extend our thanks to Microsoft Research for providing the data for this competition.;https://www.kaggle.com/c/dogs-vs-cats;Kaggle;Create an algorithm to distinguish dogs from cats;['deep learning', 'binary classification', 'neural networks', 'image data', 'beginner', 'classification', 'model explainability', 'gpu', 'transfer learning', 'computer vision', 'cnn'];213.0;0.553;Dogs vs. Cats;Playground prediction Competition
2017-03-03 00:59:00;In 2013, we hosted one of our favorite for-fun competitions:  Dogs vs. Cats. Much has since changed in the machine learning landscape, particularly in deep learning and image analysis. Back then, a tensor flow was the diffusion of the creamer in a bored mathematician's cup of coffee. Now, even the cucumber farmers are neural netting their way to a bounty. Much has changed at Kaggle as well. Our online coding environment Kernels didn't exist in 2013, and so it was that we approached sharing by scratching primitive glpyhs on cave walls with sticks and sharp objects. No more. Now, Kernels have taken over as the way to share code on Kaggle. IPython is out and Jupyter Notebook is in. We even have TensorFlow. What more could a data scientist ask for? But seriously, what more? Pull requests welcome.  We are excited to bring back the infamous Dogs vs. Cats classification problem as a playground competition with kernels enabled. Although modern techniques may make light of this once-difficult problem, it is through practice of new techniques on old datasets that we will make light of machine learning's future challenges.;https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition;Kaggle;Distinguish images of dogs from cats;['neural networks', 'animals', 'classification', 'gpu', 'computer vision'];1314.0;0.659;Dogs vs. Cats Redux: Kernels Edition;Playground prediction Competition
2018-04-26 01:59:00;Founded in 2000 by a high school teacher in the Bronx, DonorsChoose.org empowers public school teachers from across the country to request much-needed materials and experiences for their students. At any given time, there are thousands of classroom requests that can be brought to life with a gift of any amount. DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website.  Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they need to solve:  How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as efficiently as possible How to increase the consistency of project vetting across different volunteers to improve the experience for teachers How to focus volunteer time on the applications that need the most assistance  The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval. With an algorithm to pre-screen applications, DonorsChoose.org can auto-approve some applications quickly so that volunteers can spend their time on more nuanced and detailed project ​vetting processes, including doing more to help teachers develop projects that qualify for specific funding opportunities.  Your machine learning algorithm can help more teachers get funded more quickly, and with less cost to DonorsChoose.org, allowing them to channel even more funding directly to classrooms across the country.  Getting Started with Kernels Get familiar with the competition data and the machine learning objective quickly using Kernels. Google's engineering education team has put together a starter tutorial implementing benchmark linear classification model.  Acknowledgments Machine Learning Crash Course was created by Google's engineering education team in partnership with numerous Machine Learning subject matter experts across Google.;https://www.kaggle.com/c/donorschoose-application-screening;DonorsChoose.org;Predict whether teachers' project proposals are accepted;['nlp', 'tabular data', 'binary classification', 'feature engineering', 'exploratory data analysis', 'neural networks', 'xgboost', 'beginner', 'classification', 'data visualization', 'text data', 'gradient boosting', 'text mining'];580.0;0.615;DonorsChoose.org Application Screening;Playground prediction Competition
2018-11-27 00:59:00;Hungry for a new competition? Give thanks for this opportunity to avoid those awkward family political dinner discussions and endless holiday movie marathons over the Thanksgiving break. Spend time with your Kaggle family instead to find the real turkey!  In this competition you are tasked with finding the turkey sound signature from pre-extracted audio features. A simple binary problem, or is it? What does a turkey really sound like? How many sounds are similar? Will you be able to find the turkey or will you go a-fowl?  This is a short, fun, holiday, playground competition. Please, do not ruin the fun for yourself and for everyone by using a model trained on the answers.  Don't be a turkey!;https://www.kaggle.com/c/dont-call-me-turkey;Kaggle;Thanksgiving Edition: Find the turkey in the sound bite;['keras', 'deep learning', 'exploratory data analysis', 'xgboost', 'tensorflow', 'beginner', 'classification', 'data visualization', 'gpu', 'lstm', 'pca'];267.0;0.568;Don't call me turkey!;Playground prediction Competition
2019-05-08 01:59:00;Long ago, in the distant, fragrant mists of time, there was a competition… It was not just any competition. It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples… without overfitting. Data scientists ― including Kaggle's very own Will Cukierski ― competed by the hundreds. Legends were made. (Will took 5th place, and eventually ended up working at Kaggle!) People overfit like crazy. It was a Kaggle-y, data science-y madhouse. So… we're doing it again. Don't Overfit II: The Overfittening This is the next logical step in the evolution of weird competitions. Once again we have 20,000 rows of continuous variables, and a mere handful of training samples. Once again, we challenge you not to overfit. Do your best, model without overfitting, and add, perhaps, to your own legend. In addition to bragging rights, the winner also gets swag.  Enjoy! Acknowledgments We hereby salute the hard work that went into the original competition, created by Phil Brierly. Thank you!;https://www.kaggle.com/c/dont-overfit-ii;Kaggle;A Fistful of Samples;['exploratory data analysis', 'feature engineering', 'random forest', 'model comparison', 'beginner', 'bayesian statistics', 'classification', 'data visualization', 'model explainability', 'gpu'];2330.0;0.687;Don't Overfit! II;Playground prediction Competition
2012-01-06 00:59:59;"One of the biggest challenges of an auto dealership purchasing a used car at an auto auction is the risk of that the vehicle might have serious issues that prevent it from being sold to customers. The auto community calls these unfortunate purchases ""kicks"". Kicked cars often result when there are tampered odometers, mechanical issues the dealer is not able to address, issues with getting the vehicle title from the seller, or some other unforeseen problem. Kick cars can be very costly to dealers after transportation cost, throw-away repair work, and market losses in reselling the vehicle. Modelers who can figure out which cars have a higher risk of being kick can provide real value to dealerships trying to provide the best inventory selection possible to their customers. The challenge of this competition is to predict if the car purchased at the Auction is a Kick (bad buy).";https://www.kaggle.com/c/DontGetKicked;;Predict if a car purchased at auction is a lemon;[];570.0;0.614;Don't Get Kicked!;Featured prediction Competition
2016-06-28 01:59:00;Imagine a world where we can use satellite images to help find better access to clean water, prevent poaching of wildlife, predict storms more efficiently, optimize traffic patterns more readily, and inform human behaviors to mitigate the spread of disease.  Thanks to a marked increase of satellites in orbit, we will be able to capture images – and the information contained within – of nearly every place on Earth, every day by 2017. However, our ability to analyze datasets of these images has not advanced as quickly. Changes from day to day in images of the same location are subtle, can be hard to detect, and are difficult to understand in terms of their significance. In this competition, Draper provides a unique dataset of images taken at the same locations over 5 days. Kagglers are challenged to predict the chronological order of the photos taken at each location. Accurately doing so could uncover approaches that have a global impact on commerce, science, and humanitarian works.;https://www.kaggle.com/c/draper-satellite-image-chronology;;Can you put order to space and time?;[];399.0;0.593;Draper Satellite Image Chronology;Featured prediction Competition
2017-03-08 00:59:00;The proliferation of satellite imagery has given us a radically improved understanding of our planet. It has enabled us to better achieve everything from mobilizing resources during disasters to monitoring effects of global warming. What is often taken for granted is that advancements such as these have relied on labeling features of significance like building footprints and roadways fully by hand or through imperfect semi-automated methods. As these large, complex datasets continue to increase exponentially in number, the Defence Science and Technology Laboratory (Dstl) is seeking novel solutions to alleviate the burden on their image analysts. In this competition, Kagglers are challenged to accurately classify features in overhead imagery. Automating feature labeling will not only help Dstl make smart decisions more quickly around the defense and security of the UK, but also bring innovation to computer vision methodologies applied to satellite imagery.;https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;Defence Science & Technology Laboratory;Can you train an eye in the sky?;[];419.0;0.596;Dstl Satellite Imagery Feature Detection;Featured prediction Competition
2019-02-27 00:59:00;Imagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner! Right now, Elo, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key.  Elo has built machine learning models to understand the most important aspects and preferences in their customers’ lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in. In this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers’ lives and help Elo reduce unwanted campaigns, to create the right experience for customers.;https://www.kaggle.com/c/elo-merchant-category-recommendation;Elo;Help understand customer loyalty;['data cleaning', 'exploratory data analysis', 'feature engineering', 'recommender systems', 'regression', 'data visualization'];4127.0;0.713;Elo Merchant Category Recommendation;Featured prediction Competition
2013-02-21 00:59:00;We (the competition hosts) are excited to sponsor the Event Recommendation Engine Challenge, which asks you to predict what events our users will be interested in based on events they’ve responded to in the past, user demographic information, and what events  they’ve seen and clicked on in our app. The insights you discover from this data, and the algorithms the winners create, will allow us to improve our event recommendation algorithm, a core part of our applications and a key element in improving user experience. This is the first competition launching under the  Kaggle Startup Program!;https://www.kaggle.com/c/event-recommendation-engine-challenge;;Predict what events our users will be interested in based on user actions, event metadata, and demographic information.;[];223.0;0.556;Event Recommendation Engine Challenge;Featured prediction Competition
2016-06-11 01:59:00;Planning your dream vacation, or even a weekend escape, can be an overwhelming affair. With hundreds, even thousands, of hotels to choose from at every destination, it's difficult to know which will suit your personal preferences. Should you go with an old standby with those pillow mints you like, or risk a new hotel with a trendy pool bar?   Expedia wants to take the proverbial rabbit hole out of hotel search by providing personalized hotel recommendations to their users. This is no small task for a site with hundreds of millions of visitors every month! Currently, Expedia uses search parameters to adjust their hotel recommendations, but there aren't enough customer specific data to personalize them for each user. In this competition, Expedia is challenging Kagglers to contextualize customer data and predict the likelihood a user will stay at 100 different hotel groups. The data in this competition is a random selection from Expedia and is not representative of the overall statistics.;https://www.kaggle.com/c/expedia-hotel-recommendations;;Which hotel type will an Expedia customer book?;[];1971.0;0.68;Expedia Hotel Recommendations;Featured prediction Competition
2013-11-05 00:59:00;Expedia is the world’s largest online travel agency (OTA) and powers search results for millions of travel shoppers every day. In this competitive market matching users to hotel inventory is very important since users easily jump from website to website. As such, having the best ranking of hotels (“sort”) for specific users with the best integration of price competitiveness gives an OTA the best chance of winning the sale. For this contest, Expedia has provided a dataset that includes shopping and purchase data as well as information on price competitiveness. The data are organized around a set of “search result impressions”, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website. In addition to impressions from the existing algorithm, the data contain impressions where the hotels were randomly sorted, to avoid the position bias of the existing algorithm. The user response is provided as a click on a hotel or/and a purchase of a hotel room. Appended to impressions are the following: 1) Hotel characteristics2) Location attractiveness of hotels3) User’s aggregate purchase history4) Competitive OTA information Models will be scored via performance on a hold-out set.;https://www.kaggle.com/c/expedia-personalized-sort;;Learning to rank hotels to maximize purchases;['deep learning', 'clustering'];336.0;0.582;Personalize Expedia Hotel Searches - ICDM 2013;Featured prediction Competition
2013-12-21 00:59:00;"Looking for a data science position at Facebook?  After two successful prior Kaggle competitions, Facebook continues their mission to identify the best data scientists and software engineers that Kaggle has to offer. In this third installment, they seek candidates who have experience text mining large amounts of data.  This competition tests your text skills on a large dataset from the Stack Exchange sites.  The task is to predict the tags (a.k.a. keywords, topics, summaries), given only the question text and its title. The dataset contains content from disparate stack exchange sites, containing a mix of both technical and non-technical questions.  Positions are available in Menlo Park, Seattle, New York City, and London; candidates must have, or be eligible to obtain, authorization to work in the US or UK. Please note: you must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions. Crawling stack exchange sites to look up answers is not permitted. Facebook will review the code of the top participants before deciding whether to offer an interview.  This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Facebook, check the box ""Allow host to contact me"" when you make your first entry. Acknowledgements We thank Stack Exchange (and its users) for generously releasing the source dataset through its Creative Commons Data Dumps. All data is licensed under the cc-by-sa license.";https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;Facebook;Identify keywords and tags from millions of text questions;['feature engineering', 'exploratory data analysis', 'multiclass classification', 'multilabel classification', 'beginner', 'classification', 'logistic regression'];366.0;0.588;Facebook Recruiting III - Keyword Extraction;Recruitment prediction Competition
2016-07-07 01:59:00;Ever wonder what it's like to work at Facebook? Facebook and Kaggle are launching a machine learning engineering competition for 2016. Trail blaze your way to the top of the leaderboard to earn an opportunity at interviewing for one of the 10+ open roles as a software engineer, working on world class machine learning problems.  The goal of this competition is to predict which place a person would like to check in to. For the purposes of this competition, Facebook created an artificial world consisting of more than 100,000 places located in a 10 km by 10 km square. For a given set of coordinates, your task is to return a ranked list of the most likely places. Data was fabricated to resemble location signals coming from mobile devices, giving you a flavor of what it takes to work with real data complicated by inaccurate and noisy values. Inconsistent and erroneous location data can disrupt experience for services like Facebook Check In. We highly encourage competitors to be active on Kaggle Scripts. Your work there will be thoughtfully included in the decision making process. Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.;https://www.kaggle.com/c/facebook-v-predicting-check-ins;Facebook;Identify the correct place for check ins;[];1209.0;0.655;Facebook V: Predicting Check Ins;Recruitment prediction Competition
2012-07-11 01:59:59;Please note: You must compete as an  in recruiting competitions.  You may only use the data provided to make your predictions.  Facebook will review the code of the top participants before deciding whether to offer an interview.;https://www.kaggle.com/c/FacebookRecruiting;Facebook;Show them your talent, not just your resume.;[];418.0;0.596;Facebook Recruiting Competition;Recruitment prediction Competition
2017-01-07 01:00:00;The objective of this task is to predict keypoint positions on face images. This can be used as a building block in several applications, such as:  tracking faces in images and video analysing facial expressions detecting dysmorphic facial signs for medical diagnosis biometrics / face recognition  Detecing facial keypoints is a very challenging problem.  Facial features vary greatly from one individual to another, and even for a single individual, there is a large amount of variation due to 3D pose, size, position, viewing angle, and illumination conditions. Computer vision research has come a long way in addressing these difficulties, but there remain many opportunities for improvement. This getting-started competition provides a benchmark data set and an  R tutorial to get you going on analysing face images. Get started with R >> Acknowledgements The data set for this competition was graciously provided by  Dr. Yoshua Bengio of the University of Montreal. James Petterson.;https://www.kaggle.com/c/facial-keypoints-detection;Kaggle;Detect the location of keypoints on face images;['deep learning', 'neural networks', 'regression', 'beginner', 'gpu', 'cnn'];175.0;0.54;Facial Keypoints Detection;Getting Started prediction Competition
2018-01-16 00:59:00;Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming. The problem becomes more complex as retailers add new locations with unique needs, new products, ever transitioning seasonal tastes, and unpredictable product marketing. Corporación Favorita, a large Ecuadorian-based grocery retailer, knows this all too well. They operate hundreds of supermarkets, with over 200,000 different products on their shelves. Corporación Favorita has challenged the Kaggle community to build a model that more accurately forecasts product sales. They currently rely on subjective forecasting methods with very little data to back them up and very little automation to execute plans. They’re excited to see how machine learning could better ensure they please customers by having just enough of the right products at the right time.;https://www.kaggle.com/c/favorita-grocery-sales-forecasting;Corporación Favorita;Can you accurately predict sales for a large grocery chain?;['beginner', 'exploratory data analysis', 'data visualization'];1674.0;0.672;Corporación Favorita Grocery Sales Forecasting;Featured prediction Competition
2018-09-25 01:59:00;The European Organization for Nuclear Research is the world’s largest high energy physics laboratory.  LHCb is an experiment set up to explore what happened after the Big Bang that allowed matter to survive and build the Universe we inhabit today.  The Yandex School of Data Analysis (YSDA) is a free Master’s-level program in Computer Science and Data Analysis, which is offered by Yandex since 2007. The aim of the School is to train specialists in data analysis and information retrieval to be able to solve cutting edge industry problems as well as fundamental research challenges. YSDA is associated member of LHCb since December 2014.  Yandex Data Factory are the Machine Learning and data analytics experts that use data science to improve business’ operations, revenues and profitability. By building upon the real-time personalisation and predictive analytics technology of parent company, Yandex, the fourth largest search engine in the world, Yandex Data Factory helps clients improve their business awareness through the exploitation of their own data. Yandex Data Factory’s proven data science and technology continually analyses, tests, refines and reapplies hundreds of hypotheses to the customers’ datasets to determine the best next course of action. It offers tailored, scalable, SaaS-driven Machine Learning services to a wide variety of data-reliant verticals, such as retail, financial services, travel and telecoms, who wish to use their data for purposes such as improving personalisation, segmentation, churn prevention or fraud detection. Yandex Data Factory was founded in 2014 by Yandex and is headquartered in Amsterdam, operating throughout Europe.  Intel (NASDAQ: INTC) is a world leader in computing innovation. The company designs and builds the essential technologies that serve as the foundation for the world’s computing devices. As a leader in corporate responsibility and sustainability, Intel also manufactures the world’s first commercially available “conflict-free” microprocessors. Additional information about Intel is available at http://newsroom.intel.com and http://blogs.intel.com.  The University of Zurich is one of the leading research universities in Europe and offers the widest range of degree programs in Switzerland. It was founded in 1833 and currently has seven faculties: Philosophy, Human Medicine, Economic Sciences, Law, Mathematics and Natural Sciences, Theology and Veterinary Medicine.   Warwick is one of the UK's leading universities, with an acknowledged reputation for excellence in research and teaching, for innovation, and for links with business and industry.  Institute of Nuclear Physics, Polish Academy of Sciences. Founded in 1955 Institute of Nuclear Physics has become leading Particle Physics research institution and ranked as class A+ by Polish Ministry of Higher Education.  Consistently ranked as one of Russia’s top universities, the Higher School of Economics is a leader in Russian education and one of the preeminent economics and social sciences universities in eastern Europe and Eurasia.;https://www.kaggle.com/c/flavours-of-physics-kernels-only;;Identify a rare decay phenomenon;['binary classification', 'feature engineering', 'beginner', 'classification', 'data visualization', 'physics'];64.0;0.465;Flavours of Physics: Finding τ  →  μμμ (Kernels Only);Playground Code Competition
2020-05-12 01:59:00;Tensor Processing Units (TPUs) are Now Available on Kaggle Tensor Processing Unit (TPU) quotas are now available on Kaggle, at no cost to you! TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try. The latest Tensorflow release (TF 2.1) was focused on TPUs and they’re now supported both through the Keras high-level API and at a lower level, in models using a custom training loop. We can’t wait to see how your solutions are accelerated by TPUs! The Challenge It’s difficult to fathom just how vast and diverse our natural world is. There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish – and astonishingly, over 400,000 different types of flowers. In this competition, you’re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we’re sticking to just over 100 types).  To get started with TPUs:  Read the TPU documentation one-pager Then jump right into the Getting Started Notebook for this competition  Quick note: a TPU is a network-connected accelerator and requires a couple extra lines in your code. Flipping the TPU switch in your notebook will not, by itself, accelerate your code.  Have Questions? Martin Görner, Google Developer Advocate and author of Tensorflow without a PhD will be actively engaged in the competition forum. If you have a question or need help troubleshooting, that’s the best place to find help.;https://www.kaggle.com/c/flower-classification-with-tpus;Google Cloud TPU;Use TPUs to classify 104 types of flowers;['deep learning', 'tpu', 'utility script', 'beginner', 'classification', 'cnn'];848.0;0.636;Flower Classification with TPUs;Playground Code Competition
2018-09-25 01:59:00;Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing. In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from cartographic variables. The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type. This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. This competition originally ran in 2015. We are relaunching it as a kernels-only version here. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science;https://www.kaggle.com/c/forest-cover-type-kernels-only;Kaggle;Use cartographic variables to classify forest categories;['multiclass classification', 'exploratory data analysis', 'forestry', 'feature engineering', 'random forest', 'xgboost', 'beginner', 'classification', 'data visualization', 'gpu', 'ensembling', 'gradient boosting', 'pca'];359.0;0.586;Forest Cover Type (Kernels Only);Playground Code Competition
2015-05-12 01:59:00;Get started on this competition with Kaggle Scripts. No data download or local environment needed! Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing. In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type. This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. Acknowledgements Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science;https://www.kaggle.com/c/forest-cover-type-prediction;Kaggle;Use cartographic variables to classify forest categories;['feature engineering', 'exploratory data analysis', 'python', 'beginner', 'classification'];1692.0;0.672;Forest Cover Type Prediction;Playground prediction Competition
2018-08-01 13:59:00;Some sounds are distinct and instantly recognizable, like a baby’s laugh or the strum of a guitar. Other sounds aren’t clear and are difficult to pinpoint. If you close your eyes, can you tell which of the sounds below is a chainsaw versus a blender? Moreover, we often experience a mix of sounds that create an ambience – like the clamoring of construction, a hum of traffic from outside the door, blended with loud laughter from the room, and the ticking of the clock on your wall. The sound clip below is of a busy food court in the UK. Partly because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. Currently, a lot of manual effort is required for tasks like annotating sound collections and providing captions for non-speech events in audiovisual content. To tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 370,000 Creative Commons Licensed sounds) and Google Research’s Machine Perception Team (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this competition. You’re challenged to build a general-purpose automatic audio tagging system using a dataset of audio files covering a wide range of real-world environments. Sounds in the dataset include things like musical instruments, human sounds, domestic sounds, and animals from Freesound’s library, annotated using a vocabulary of more than 40 labels from Google’s AudioSet ontology. To succeed in this competition your systems will need to be able to recognize an increased number of sound events of very diverse nature, and to leverage subsets of training data featuring annotations of varying reliability (see Data section for more information).;https://www.kaggle.com/c/freesound-audio-tagging;;Can you automatically recognize sounds from a wide range of real-world environments?;['beginner', 'exploratory data analysis', 'data visualization', 'feature engineering'];558.0;0.613;Freesound General-Purpose Audio Tagging Challenge;Research prediction Competition
2019-06-18 00:22:00;One year ago, Freesound and Google’s Machine Perception hosted an audio tagging competition challenging Kagglers to build a general-purpose auto tagging system. This year they’re back and taking the challenge to the next level with multi-label audio tagging, doubled number of audio categories, and a noisier than ever training set. If you like raising your ML game, this challenge is for you.  Here's the background: Some sounds are distinct and instantly recognizable, like a baby’s laugh or the strum of a guitar. Other sounds are difficult to pinpoint. If you close your eyes, could you tell the difference between the sound of a chainsaw and the sound of a blender? Because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. A significant amount of manual effort goes into tasks like annotating sound collections and providing captions for non-speech events in audiovisual content. To tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 400,000 Creative Commons Licensed sounds) and Google Research’s Machine Perception Team  (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this new competition. To win this competition, Kagglers will develop an algorithm to tag audio data automatically using a diverse vocabulary of 80 categories. If successful, your systems could be used for several applications, ranging from automatic labelling of sound collections to the development of systems that automatically tag video content or recognize sound events happening in real time. Ready to raise your game? Join the competition! Note, this competition is similar in nature to this competition  with a new dataset, and multi-class labels.  Organizers  Eduardo Fonseca, MTG-UPF, Barcelona Manoj Plakal, Google's Sound Understanding, New York Frederic Font, MTG-UPF, Barcelona Dan Ellis, Google's Sound Understanding, New York   This is a Kernels-only competition. Refer to Kernels Requirements for details.;https://www.kaggle.com/c/freesound-audio-tagging-2019;Freesound;Automatically recognize sounds and apply tags of varying natures;['deep learning', 'audio data', 'beginner', 'classification', 'gpu', 'lstm', 'cnn'];880.0;0.638;Freesound Audio Tagging 2019;Research Code Competition
2019-02-21 21:04:00;The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.  RStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have. In this competition, you’re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.;https://www.kaggle.com/c/ga-customer-revenue-prediction;RStudio;Predict how much GStore customers will spend;['data cleaning', 'exploratory data analysis', 'feature engineering', 'beginner', 'data visualization', 'finance', 'e-commerce services', 'gradient boosting'];3611.0;0.707;Google Analytics Customer Revenue Prediction;Featured prediction Competition
2019-04-23 01:59:00;Can you help end gender bias in pronoun resolution?   Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding, and the resolution of ambiguous pronouns is a longstanding challenge.   Unfortunately, recent studies have suggested gender bias among state-of-the-art coreference resolvers.  Google AI Language aims to improve gender-fairness in modeling by releasing the Gendered Ambiguous Pronouns (GAP) dataset, containing gender-balanced pronouns (50% of its examples containing feminine pronouns, and 50% containing masculine pronouns).   In this two-stage competition, Kagglers are challenged to build pronoun resolution systems that perform equally well regardless of pronoun gender. Stage two's final evaluation will use a new dataset following the same format. To encourage gender-fair modeling, the ratio of masculine to feminine examples in the official test data will not be known ahead of time.    ----------    Please cite the original paper if you use GAP in your work:   @inproceedings{webster2018gap,   title =     {Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns},   author =    {Webster, Kellie and Recasens, Marta and Axelrod, Vera and Baldridge, Jason},   booktitle = {Transactions of the ACL},   year =      {2018},   pages =     {to appear}, };https://www.kaggle.com/c/gendered-pronoun-resolution;Google Research;Pair pronouns to their correct entities;['nlp', 'neural networks', 'exploratory data analysis', 'data visualization', 'gpu', 'logistic regression'];838.0;0.636;Gendered Pronoun Resolution;Research prediction Competition
2019-08-28 21:23:00;"This competition is closed and no longer accepting submissions. The private leaderboard has been finalized as of 8/28/2019.  Important Warning: This competition has an experimental format and submission style (images as submission). Competitors must use generative methods to create their submission images and are not permitted to make submissions that include any images already classified as dogs or altered versions of such images.  To enforce and prevent cheating, we reserve the right to: (a) Visually inspect all participants' submitted images, (b) review any submitted source code, (c) use these reviews to identify violators or determine winners, and (d) disqualify participants from the competition who are found in violation. This is also specified in the competition's rules  Use your training skills to create images, rather than identify them. You’ll be using GANs, which are at the creative frontier of machine learning. You might think of GANs as robot artists in a sense—able to create eerily lifelike images, and even digital worlds.  ""You might not think that programmers are artists, but programming is an extremely creative profession. It’s logic-based creativity. '' -   John Romero  A generative adversarial network (GAN) is a class of machine learning system invented by Ian Goodfellow in 2014. Two neural networks compete with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set. In this competition, you’ll be training generative models to create images of dogs. Only this time… there’s no ground truth data for you to predict. Here, you’ll submit the images and be scored based on how well those images are classified as dogs from pre-trained neural networks.  Take these images, for example. Can you tell which are real vs. generated?  Trick question; they are all generated! Why dogs? We chose dogs because, well, who doesn’t love looking at photos of adorable pups? Moreover, dogs can be classified into many sub-categories (breed, color, size), making them ideal candidates for image generation. Generative methods (in particular, GANs) are currently used in various places on Kaggle for data augmentation. Their potential is vast; they can learn to mimic any distribution of data across any domain: photographs, drawings, music, and prose. If successful, not only will you help advance the state of the art in generative image creation, but you’ll enable us to create more experiments across a variety of domains in the future.  This is a Kernels-only competition. Refer to Kernels Requirements for details.";https://www.kaggle.com/c/generative-dog-images;Kaggle;Experiment with creating puppy pics;['deep learning', 'gan', 'computer vision', 'gpu'];927.0;0.641;Generative Dog Images;Research Code Competition
2016-12-02 00:59:00;Get out your dowsing rods, electromagnetic sensors, … and gradient boosting machines. Kaggle is haunted and we need your help. After a month of making scientific observations and taking careful measurements, we’ve determined that 900 ghouls, ghosts, and goblins are infesting our halls and frightening our data scientists. When trying garlic, asking politely, and using reverse psychology didn't work, it became clear that machine learning is the only answer to banishing our unwanted guests.  So now the hour has come to put the data we’ve collected in your hands. We’ve managed to identify 371 of the ghastly creatures, but need your help to vanquish the rest. And only an accurate classification algorithm can thwart them. Use bone length measurements, severity of rot, extent of soullessness, and other characteristics to distinguish (and extinguish) the intruders. Are you ghost-busters up for the challenge?;https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;;Can you classify monsters haunting Kaggle?;['exploratory data analysis', 'feature engineering', 'beginner', 'classification', 'data visualization'];763.0;0.631;Ghouls, Goblins, and Ghosts... Boo!;Playground prediction Competition
2011-12-16 00:59:59;Banks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit.  Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years. The goal of this competition is to build a model that borrowers can use to help make the best financial decisions. Historical data are provided on 250,000 borrowers and the prize pool is $5,000 ($3,000 for first, $1,500 for second and $500 for third).;https://www.kaggle.com/c/GiveMeSomeCredit;;Improve on the state of the art in credit scoring by predicting the probability that somebody will experience financial distress in the next two years.;['feature engineering', 'exploratory data analysis', 'data visualization', 'classification'];924.0;0.641;Give Me Some Credit;Featured prediction Competition
2020-08-19 03:59:00;"Open up your pantry and you’re likely to find several wheat products. Indeed, your morning toast or cereal may rely upon this common grain. Its popularity as a food and crop makes wheat widely studied. To get large and accurate data about wheat fields worldwide, plant scientists use image detection of ""wheat heads""—spikes atop the plant containing grain. These images are used to estimate the density and size of wheat heads in different varieties. Farmers can use the data to assess health and maturity when making management decisions in their fields. However, accurate wheat head detection in outdoor field images can be visually challenging. There is often overlap of dense wheat plants, and the wind can blur the photographs. Both make it difficult to identify single heads. Additionally, appearances vary due to maturity, color, genotype, and head orientation. Finally, because wheat is grown worldwide, different varieties, planting densities, patterns, and field conditions must be considered. Models developed for wheat phenotyping need to generalize between different growing environments. Current detection methods involve one- and two-stage detectors (Yolo-V3 and Faster-RCNN), but even when trained with a large dataset, a bias to the training region remains. The Global Wheat Head Dataset is led by nine research institutes from seven countries: the University of Tokyo, Institut national de recherche pour l’agriculture, l’alimentation et l’environnement, Arvalis, ETHZ, University of Saskatchewan, University of Queensland, Nanjing Agricultural University, and Rothamsted Research. These institutions are joined by many in their pursuit of accurate wheat head detection, including the Global Institute for Food Security, DigitAg, Kubota, and Hiphen. In this competition, you’ll detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. You will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China. Wheat is a staple across the globe, which is why this competition must account for different growing conditions. Models developed for wheat phenotyping need to be able to generalize between environments. If successful, researchers can accurately estimate the density and size of wheat heads in different varieties. With improved detection farmers can better assess their crops, ultimately bringing cereal, toast, and other favorite dishes to your table.  This is a Code Competition. Refer to Code Requirements for details.";https://www.kaggle.com/c/global-wheat-detection;University of Saskatchewan;Can you help identify wheat heads using image analysis?;['deep learning', 'beginner', 'computer vision', 'gpu'];2245.0;0.686;Global Wheat Detection;Research Code Competition
2018-08-31 01:59:00;"Introduction Google AI (Google’s AI research arm, tasked with advancing AI for everyone) is challenging you to build an algorithm that detects objects automatically using an absolutely massive training dataset ― one with more varied and complex bounding-box annotations and object classes than ever before. Here's the background. Computers are getting better and better at vision. But in a few critical ways, they still can't match a human’s intuitive perception. For example, what do you see when you look at this photo?  Most of us would answer, “a sandy beach, the ocean, a few people walking, some trees, grass, and buildings…a woman walking her dog right there! Oh yeah, and there is a man holding a plastic cup.” Can a computer provide as precise an image description? Google AI wants to further push the capabilities of computer vision. We hope that providing very large training set will stimulate research into more sophisticated object and relationship detection models that will exceed current state-of-the-art performance. The results of this Challenge will be presented at a workshop at the European Conference on Computer Vision 2018.  Object Detection Track Object detection is a central task in computer vision, with applications ranging across search, robotics, self-driving cars, and many others.  As deep network solutions become deeper and more complex, they are often limited by the amount of training data available. With this in mind, to spur advances in analyzing and understanding images, Google AI has publicly released the Open Images dataset.  Open Images follows the tradition of PASCAL VOC, ImageNet and COCO, now at an unprecedented scale. The Open Images Challenge is based on Open Images dataset. The training set of the Challenge contains:  12M bounding-box annotations for 500 object classes on 1.7M training images Images of complex scenes with several objects–an average of 7 boxes per image Highly varied images that contain brand new objects like “fedora” and “snowman” Class hierarchy that reflects the relationships between classes of Open Images.   In this track of the Challenge, you are asked to build the best performing algorithm for automatically detecting objects.  Please refer to the Open Images Challenge page for additional details on the dataset. In addition to this Object Detection track, the Challenge also includes a Visual Relationship Detection track to detect pairs of objects in particular relations, e.g. ""woman playing guitar,"" ""beer on table,"" ""dog inside car"", ""man holding coffee"", etc. The Visual Relationship Detection track is available here.      Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.";https://www.kaggle.com/c/google-ai-open-images-object-detection-track;Google Research;Detect objects in varied and complex images.;[];454.0;0.601;Google AI Open Images - Object Detection Track;Featured prediction Competition
2020-03-19 16:00:00;Update: this competition has been cancelled on account of the COVID-19 pandemic.  As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.    In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results. As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!  This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.  If you want to extend your analysis then try out our Analytics Competition here;https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['exploratory data analysis', 'feature engineering', 'utility script', 'beginner', 'classification', 'data visualization'];;0.0;Google Cloud & NCAA® ML Competition 2020-NCAAM;Featured prediction Competition
2020-03-20 16:00:00;Update: this competition has been cancelled on account of the COVID-19 pandemic.  As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results. As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!  This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here. If you want to extend your analysis then try out our Analytics Competition here;https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['feature engineering', 'beginner', 'classification', 'data visualization', 'gpu'];;0.0;Google Cloud & NCAA® ML Competition 2020-NCAAW;Featured prediction Competition
2020-02-11 00:59:00;"Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.  Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well…yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.   Unfortunately, it’s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That’s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects. In this competition, you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a ""common-sense"" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get! Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.";https://www.kaggle.com/c/google-quest-challenge;Google;Improving automated understanding of complex question answer content;['nlp', 'data cleaning', 'exploratory data analysis', 'feature engineering', 'beginner', 'data visualization'];1571.0;0.668;Google QUEST Q&A Labeling;Featured Code Competition
2016-08-31 01:59:00;Planning a celebration is a balancing act of preparing just enough food to go around without being stuck eating the same leftovers for the next week. The key is anticipating how many guests will come. Grupo Bimbo must weigh similar considerations as it strives to meet daily consumer demand for fresh bakery products on the shelves of over 1 million stores along its 45,000 routes across Mexico.  Currently, daily inventory calculations are performed by direct delivery sales employees who must single-handedly predict the forces of supply, demand, and hunger based on their personal experiences with each store. With some breads carrying a one week shelf life, the acceptable margin for error is small. In this competition, Grupo Bimbo invites Kagglers to develop a model to accurately forecast inventory demand based on historical sales data. Doing so will make sure consumers of its over 100 bakery products aren’t staring at empty shelves, while also reducing the amount spent on refunds to store owners with surplus product unfit for sale.;https://www.kaggle.com/c/grupo-bimbo-inventory-demand;;Maximize sales and minimize returns of bakery goods;[];1963.0;0.679;Grupo Bimbo Inventory Demand;Featured prediction Competition
2020-09-23 01:59:00;"Ahoy there! There's halite to be had and ships to be deployed! Are you ready to navigate the skies and secure your territory? Halite by Two Sigma (""Halite"") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.  Created by Two Sigma in 2016, more than 15,000 people around the world have participated in a Halite challenge. Players apply advanced algorithms in a dynamic, open source game setting. The strategic depth and immersive, interactive nature of Halite games make each challenge a unique learning environment. Halite IV builds on the core game design of Halite III with a number of key changes that shift the focus of the game towards tighter competition on a smaller board. New game features include regenerating halite, shipyard creation, no more ship movement costs, and stealing halite from other players! So dust off your halite meters and fasten your seatbelts. The fourth season of Halite is about to begin!";https://www.kaggle.com/c/halite;Kaggle;Collect the most halite during your match in space;['deep learning', 'video games', 'simulations', 'reinforcement learning', 'beginner'];1139.0;0.652;Halite by Two Sigma;Featured Simulation Competition
2020-07-28 01:59:00;"Note: Put your heads together to solve programming challenges. Google's coding competition, Hash Code, has just finished for 2020. Use this online qualifier from 2019 to keep your skills sharp for future competitions! As the saying goes, ""a picture is worth a thousand words.""  We agree – photos are an important part of contemporary digital and cultural life. How we experience photos largely depends on the story they’re arranged to tell. The same shots could be a monotonous series of snaps or form a narrative masterpiece. Approximately 2.5 billion people around the world carry a camera – in the form of a smartphone – in their pocket every day. We tend to make good use of it, too, taking more photos than ever (back in 2017, Google Photos announced it was backing up more than 1.2 billion photos and videos per day)! The rise of digital photography creates an interesting challenge: what should we do with all of these photos? In this competition, you will compose a slideshow out of a photo collection. Given a list of photos and the tags associated with each photo, you are challenged to arrange the photos into a slideshow that is as interesting as possible (the evaluation section explains what we mean by “interesting”) Will your slideshow tell a good story or be a major snoozefest?";https://www.kaggle.com/c/hashcode-photo-slideshow;Google;Optimizing a photo album from Hash Code 2019;[];89.0;0.491;Hash Code Archive - Photo Slideshow Optimization;Playground Code Competition
2020-05-27 01:59:00;"The Herbarium 2020 FGVC7 Challenge is to identify vascular plant species from a large, long-tailed collection herbarium specimens provided by the New York Botanical Garden (NYBG). The Herbarium 2020 dataset contains over 1M images representing over 32,000 plant species. This is a dataset with a long tail; there are a minimum of 3 specimens per species, however, some species are represented by more than a hundred specimens. This dataset only contains vascular land plants which includes lycophytes, ferns, gymnosperms, and flowering plants. The extinct forms of lycophytes are the major component of coal deposits, ferns are indicators of ecosystem health, gymnosperms provide major habitats for animals, and flowering plants provide all of our crops, vegetables, and fruits.        The teams with the most accurate models will be contacted, with the intention of using them on the un-named plant collections in the NYBG herbarium collection, and assessed by the NYBG plant specialists. Background The New York Botanical Garden (NYBG) herbarium contains more than 7.8 million plant and fungal specimens. Herbaria are a massive repository of plant diversity data.  These collections not only represent a vast amount of plant diversity, but since herbarium collections include specimens dating back hundreds of years, they provide snapshots of plant diversity through time.  The integrity of the plant is maintained in herbaria as a pressed, dried specimen; a specimen collected nearly two hundred years ago by Darwin looks much the same as one collected a month ago by an NYBG botanist.  All specimens not only maintain their morphological features but also include collection dates and locations, and the name of the person who collected the specimen.  This information, multiplied by millions of plant collections, provides the framework for understanding plant diversity on a massive scale and learning how it has changed over time. About This is an FGVC competition hosted as part of the FGVC7 workshop at CVPR 2020 and sponsored by NYBG. Details of this competition are mirrored on the github page. Please post in the forum or open an issue if you have any questions or problems with the dataset.";https://www.kaggle.com/c/herbarium-2020-fgvc7;Fine-Grained Visual Categorization 7;Identify plant species from herbarium specimens. Data from New York Botanical Garden.;['deep learning', 'beginner', 'classification', 'gpu'];153.0;0.53;Herbarium 2020 - FGVC7;Research prediction Competition
2019-03-31 00:59:00;"In this competition, you must create an algorithm to identify metastatic cancer in small image patches taken from larger digital pathology scans. The data for this competition is a slightly modified version of the PatchCamelyon (PCam) benchmark dataset (the original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates).   PCam is highly interesting for both its size, simplicity to get started on, and approachability. In the authors' words:   [PCam] packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. Models can easily be trained on a single GPU in a couple hours, and achieve competitive scores in the Camelyon16 tasks of tumor detection and whole-slide image diagnosis. Furthermore, the balance between task-difficulty and tractability makes it a prime suspect for fundamental machine learning research on topics as active learning, model uncertainty, and explainability.     Acknowledgements   Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Bas Veeling, with additional input from Babak Ehteshami Bejnordi, Geert Litjens, and Jeroen van der Laak.  You may view and download the official Pcam dataset from  GitHub. The data is provided under the CC0 License, following the license of Camelyon16. If you use PCam in a scientific publication, please reference the following papers: [1] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling. ""Rotation Equivariant CNNs for Digital Pathology"".  arXiv:1806.03962 [2] Ehteshami Bejnordi et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA: The Journal of the American Medical Association, 318(22), 2199–2210. doi:jama.2017.14585 Photo by Ousa Chea";https://www.kaggle.com/c/histopathologic-cancer-detection;Kaggle;Identify metastatic tissue in histopathologic scans of lymph node sections;['deep learning', 'binary classification', 'beginner', 'classification', 'data visualization', 'gpu', 'computer vision', 'cnn'];1157.0;0.653;Histopathologic Cancer Detection;Playground prediction Competition
2018-08-30 01:59:00;Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.  Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities. While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.;https://www.kaggle.com/c/home-credit-default-risk;Home Credit Group;Can you predict how capable each applicant is of repaying a loan?;['deep learning', 'exploratory data analysis', 'feature engineering', 'beginner', 'bayesian statistics', 'classification', 'data visualization', 'finance', 'sampling', 'gradient boosting'];7190.0;0.736;Home Credit Default Risk;Featured prediction Competition
2016-04-26 01:59:00;Shoppers rely on Home Depot’s product authority to find and buy the latest products and to get timely solutions to their home improvement needs. From installing a new ceiling fan to remodeling an entire kitchen, with the click of a mouse or tap of the screen, customers expect the correct results to their queries – quickly. Speed, accuracy and delivering a frictionless customer experience are essential. In this competition, Home Depot is asking Kagglers to help them improve their customers' shopping experience by developing a model that can accurately predict the relevance of search results. Search relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. Currently, human raters evaluate the impact of potential changes to their search algorithms, which is a slow and subjective process. By removing or minimizing human input in search relevance evaluation, Home Depot hopes to increase the number of iterations their team can perform on the current search algorithms.;https://www.kaggle.com/c/home-depot-product-search-relevance;;Predict the relevance of search results on homedepot.com;[];2122.0;0.683;Home Depot Product Search Relevance;Featured prediction Competition
2015-05-16 01:59:00;"For agriculture, it is extremely important to know how much it rained on a particular field. However, rainfall is variable in space and time and it is impossible to have rain gauges everywhere. Therefore, remote sensing instruments such as radar are used to provide wide spatial coverage. Rainfall estimates drawn from remotely sensed observations will never exactly match the measurements that are carried out using rain gauges, due to the inherent characteristics of both sensors. Currently, radar observations are ""corrected"" using nearby gauges and a single estimate of rainfall is provided to users who need to know how much it rained. This competition will explore how to address this problem in a probabilistic manner.  Knowing the full probabilistic spread of rainfall amounts can be very useful to drive hydrological and agronomic models -- much more than a single estimate of rainfall.  Image courtesy of NOAA Unlike a conventional Doppler radar, a polarimetric radar transmits radio wave pulses that have both horizontal and vertical orientations. Because rain drops become flatter as they increase in size and because ice crystals tend to be elongated vertically, whereas liquid droplets tend to be flattened, it is possible to infer the size of rain drops and the type of hydrometeor from the differential reflectivity of the two orientations. In this competition, you are given polarimetric radar values and derived quantities at a location over the period of one hour. You will need to produce a probabilistic distribution of the hourly rain gauge total. More details are on the data page. This competition is sponsored by the Artificial Intelligence Committee of the American Meteorological Society. The Climate Corporation has kindly agreed to sponsor the prizes.";https://www.kaggle.com/c/how-much-did-it-rain;;Predict probabilistic distribution of hourly rain given polarimetric radar measurements;[];319.0;0.579;How Much Did It Rain?;Research prediction Competition
2015-12-08 00:59:00;After incorporating feedback from the Kaggle community, as well as scientific and educational partners, the Artificial Intelligence Committee of the American Meteorological Society is excited to be running a second iteration of the How Much Did It Rain? competition. How Much Did It Rain? II is focused on solving the same core rain measurement prediction problem, but approaches it with a new and improved dataset and evaluation metric. This competition will go even further towards building a useful educational tool for universities, as well as making a meaningful contribution to continued meteorological research. Competition Description Rainfall is highly variable across space and time, making it notoriously tricky to measure. Rain gauges can be an effective measurement tool for a specific location, but it is impossible to have them everywhere. In order to have widespread coverage, data from weather radars is used to estimate rainfall nationwide. Unfortunately, these predictions never exactly match the measurements taken using rain gauges. Recently, in an effort to improve their rainfall predictors, the U.S. National Weather Service upgraded their radar network to be polarimetric. These polarimetric radars are able to provide higher quality data than conventional Doppler radars because they transmit radio wave pulses with both horizontal and vertical orientations.   Dual pulses make it easier to infer the size and type of precipitation because rain drops become flatter as they increase in size, whereas ice crystals tend to be elongated vertically. In this competition, you are given snapshots of polarimetric radar values and asked to predict the hourly rain gauge total. A word of caution: many of the gauge values in the training dataset are implausible (gauges may get clogged, for example). More details are on the data page. Acknowledgements This competition is sponsored by the Artificial Intelligence Committee of the American Meteorological Society. Climate Corporation is providing the prize pool.;https://www.kaggle.com/c/how-much-did-it-rain-ii;;Predict hourly rainfall using data from polarimetric radars;['lstm', 'gpu'];587.0;0.616;How Much Did It Rain? II;Research prediction Competition
2019-01-11 00:59:00;In this competition, Kagglers will develop models capable of classifying mixed patterns of proteins in microscope images. The Human Protein Atlas will use these models to build a tool integrated with their smart-microscopy system to identify a protein's location(s) from a high-throughput image. Proteins are “the doers” in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells. Images visualizing proteins in cells are commonly used for biomedical research, and these cells could hold the key for the next breakthrough in medicine. However, thanks to advances in high-throughput microscopy, these images are generated at a far greater pace than what can be manually evaluated. Therefore, the need is greater than ever for automating biomedical image analysis to accelerate the understanding of human cells and disease.   Nature Methods has indicated interest in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team would like to invite top performing teams to join as co-authors in the writing of this paper. Top performing teams will also be eligible to compete for the special prize. Additional information for both the special prize and co-authoring for Nature Methods will become available through the Discussion posts once the main competition is complete.    Acknowledgements  The Human Protein Atlas is a Sweden-based initiative aimed at mapping all human proteins in cells, tissues and organs. All the data in the knowledge resource is open access to allow anyone to pursue exploration of the human proteome. In a recent publication, the Human Protein Atlas team has demonstrated the promise of both citizen science and artificial intelligence approaches in describing the location of human proteins in images, however current results are yet to approach expert-level annotations (Sullivan et al, Nature Biotechnology, Oct 2018).;https://www.kaggle.com/c/human-protein-atlas-image-classification;Human Protein Atlas;Classify subcellular protein patterns in human cells;['deep learning', 'multiclass classification', 'beginner', 'optimization', 'data visualization', 'clustering', 'gpu', 'advanced', 'transfer learning', 'cnn'];2169.0;0.684;Human Protein Atlas Image Classification;Featured prediction Competition
2019-03-01 00:59:00;After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food. To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized. In this competition, you’re challenged to build an algorithm to identify individual whales in images. You’ll analyze Happywhale’s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you’ll help to open rich fields of understanding for marine mammal population dynamics around the globe. Note, this competition is similar in nature to this competition  with an expanded and updated dataset.  We'd like to thank Happywhale  for providing this data and problem. Happywhale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.;https://www.kaggle.com/c/humpback-whale-identification;Kaggle;Can you identify a whale by its tail?;['deep learning', 'data cleaning', 'multiclass classification', 'exploratory data analysis', 'india', 'classification', 'data visualization', 'gpu', 'computer vision', 'cnn'];2129.0;0.683;Humpback Whale Identification;Featured prediction Competition
2019-10-04 01:59:00;Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren’t thinking about the data science that determined your fate. Embarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. “Press 1 if you really tried to spend $500 on cheddar cheese.” While perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection,  you can get on with your chips without the hassle. IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge. In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions  and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.  If successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives. Acknowledgements:   Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions.  Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry.  Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually.  Header Photo by Tim Evans on Unsplash;https://www.kaggle.com/c/ieee-fraud-detection;IEEE Computational Intelligence Society;Can you detect fraud from customer transactions?;['data cleaning', 'feature engineering', 'exploratory data analysis', 'xgboost', 'beginner', 'classification', 'data visualization', 'gpu', 'finance'];6381.0;0.731;IEEE-CIS Fraud Detection;Research prediction Competition
2018-05-31 01:59:00;As shoppers move online, it would be a dream come true to have products in photos classified automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, royal blue vs turquoise in color. Many of today’s general-purpose recognition machines simply cannot perceive such subtle differences between photos, yet these differences could be important for shopping decisions. Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Wish, and Malong Technologies to challenge the data science community to help push the state of the art in automatic image classification. In this competition, FGVC workshop organizers with Wish and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign attribute labels for fashion images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;;Image classification of fashion products.;['beginner', 'exploratory data analysis', 'data visualization'];212.0;0.553;iMaterialist Challenge (Fashion) at FGVC5;Research prediction Competition
2019-06-11 01:59:00;Designers know what they are creating, but what, and how, do people really wear their products? What combinations of products are people using? In this competition, we challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign segmentations and attribute labels for fashion images.  Visual analysis of clothing is a topic that has received increasing attention in recent years. Being able to recognize apparel products and associated attributes from pictures could enhance the shopping experience for consumers, and increase work efficiency for fashion professionals. We present a new clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.   While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders’ needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 92 related fine-grained attributes. Secondly, a total of 50K clothing images (10K with both segmentation and fine-grained attributes, 40K with apparel instance segmentation) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation. Individuals/Teams with top submissions will be invited to present their work live at the FGVC6 workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) 2019 Checkout the iMaterialist-Fashion Competition Github repo for the specifics of the dataset.  Acknowledgments The iMat-Fashion Challenge 2019 is sponsored by Google AI, CVDF, Samasource and Fashionpedia.;https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;Fine-Grained Visual Categorization 6;Fine-grained segmentation task for fashion and apparel;['gpu'];241.0;0.561;iMaterialist (Fashion) 2019 at FGVC6;Research prediction Competition
2020-05-27 01:59:00;Designers know what they are creating, but what, and how, do people really wear their products? What combinations of products are people using? In this competition, we challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign segmentations and attribute labels for fashion images.  Visual analysis of clothing is a topic that has received increasing attention in recent years. Being able to recognize apparel products and associated attributes from pictures could enhance the shopping experience for consumers, and increase work efficiency for fashion professionals. We present a clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.   While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders’ needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 294 related fine-grained attributes. Secondly, a total of 50K clothing images (with both segmentation masks and fine-grained attributes) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation. Individuals/Teams with top submissions will be invited to present their work live at the FGVC7 workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) 2020. Acknowledgments The iMat-Fashion Challenge 2020 is sponsored by Google AI, CVDF, Fashionpedia and Hearst Magazine.;https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;Fine-Grained Visual Categorization 7;Fine-grained segmentation task for fashion and apparel;['deep learning', 'beginner', 'gpu', 'image data'];56.0;0.455;iMaterialist (Fashion) 2020 at FGVC7;Research prediction Competition
2019-06-11 00:09:06;The Metropolitan Museum of Art in New York, also known as The Met, has a diverse collection of over 1.5M objects of which over 200K have been digitized with imagery. The online cataloguing information is generated by Subject Matter Experts (SME) and includes a wide range of data. These include, but are not limited to: multiple object classifications, artist, title, period, date, medium, culture, size, provenance, geographic location, and other related museum objects within The Met’s collection. While the SME-generated annotations describe the object from an art history perspective, they can also be indirect in describing finer-grained attributes from the museum-goer’s understanding. Adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects.    About This is an FGVCx competition hosted as part of the FGVC6 workshop at CVPR 2019. View the github page for more details.  This is a Kernels-only competition. Refer to Kernels Requirements for details.;https://www.kaggle.com/c/imet-2019-fgvc6;Fine-Grained Visual Categorization 6;Recognize artwork attributes from The Metropolitan Museum of Art;['deep learning', 'exploratory data analysis', 'classification', 'data visualization', 'computer vision'];521.0;0.609;iMet Collection 2019 - FGVC6;Research Code Competition
2020-05-29 01:59:00;The Metropolitan Museum of Art in New York, also known as The Met, has a diverse collection of over 1.5M objects of which over 200K have been digitized with imagery. Can you help find the significant attributes to identify a specific work of art? Help advance this research in this notebook competition.    The online cataloguing information is generated by subject matter experts and includes a wide range of data. These include, but are not limited to: multiple object classifications, artist, title, period, date, medium, culture, size, provenance, geographic location, and other related museum objects within The Met’s collection. While the annotations describe the object from an art history perspective, they can also be indirect in describing finer-grained attributes for the museum-goer’s understanding. Adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects.  This is a Code Competition. Refer to Code Requirements for details.  About This is an FGVCx competition hosted as part of the FGVC7 workshop at CVPR 2020.;https://www.kaggle.com/c/imet-2020-fgvc7;Fine-Grained Visual Categorization 7;Recognize artwork attributes from The Metropolitan Museum of Art;['tpu', 'gpu'];96.0;0.497;iMet Collection 2020 - FGVC7;Research Code Competition
2019-06-11 01:59:00;As part of the FGVC6 workshop at CVPR 2019 we are conducting the iNat Challenge 2019 large scale species classification competition,  sponsored by Microsoft. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features a large number of fine-grained categories. Previous versions of the challenge have focused on classifying large numbers of species. This year features a smaller number of highly similar categories captured in a wide variety of situations, from all over the world. In total, the iNat Challenge 2019 dataset contains 1,010 species, with a combined training and validation set of 268,243 images that have been collected and verified by multiple users from iNaturalist. Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC6 workshop.  Participants who make a submission that beats the sample submission can fill out this form to receive $150 in Google Cloud credits.   Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/inaturalist-2019-fgvc6;Fine-Grained Visual Categorization 6;Fine-grained classification spanning a thousand species;['gpu'];214.0;0.553;iNaturalist 2019 at FGVC6;Research prediction Competition
2018-11-13 00:59:00;"Making products that work for people all over the globe is an important value at Google AI. In the field of classification, this means developing models that work well for regions all over the world.  Today, the dataset a model is trained on greatly dictates the performance of that model. A system trained on a dataset that doesn’t represent a broad range of localities could perform worse on images drawn from geographic regions underrepresented in the training data. Google and the industry at large are working to create more diverse & representative datasets. But it is also important for the field to make progress in understanding how to build models when the data available may not cover all audiences a model is meant to reach. Google AI is challenging Kagglers to develop models that are robust to blind spots that might exist in a data set, and to create image recognition systems that can perform well on test images drawn from different geographic distributions than the ones they were trained on. By finding ways to teach image classifiers to generalize to new geographic and cultural contexts, we hope the community will make even more progress in inclusive machine learning that benefits everyone, everywhere. Note: This competition is run in two stages. Refer to the FAQ for an explanation of how this works & the Timeline for specific dates.  This competition is a part of the NIPS 2018 competition track. Winners will be invited to attend and present their solutions at the workshop.     Shankar et al. ""No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World"" NIPS 2017 Workshop on Machine Learning for the Developing World";https://www.kaggle.com/c/inclusive-images-challenge;Google Research;Stress test image classifiers across new geographic distributions;['gpu'];468.0;0.603;Inclusive Images Challenge;Research prediction Competition
2017-08-15 01:59:00;Whether you shop from meticulously planned grocery lists or let whimsy guide your grazing, our unique food rituals define who we are. Instacart, a grocery ordering and delivery app, aims to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you. Instacart’s data science team plays a big part in providing this delightful shopping experience. Currently they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open sourced this data - see their blog post on 3 Million Instacart Orders, Open Sourced. In this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user’s next order. They’re not only looking for the best model, Instacart’s also looking for machine learning engineers to grow their team. Winners of this competition will receive both a cash prize and a fast track through the recruiting process. For more information about exciting opportunities at Instacart, check out their careers page here or e-mail their recruiting team directly at ml.jobs@instacart.com.;https://www.kaggle.com/c/instacart-market-basket-analysis;Instacart;Which products will an Instacart consumer purchase again?;['xgboost', 'recommender systems', 'classification'];2622.0;0.693;Instacart Market Basket Analysis;Featured prediction Competition
2019-06-21 01:59:00;"Welcome to Instant (well, almost) Gratification! In 2015, Kaggle introduced Kernels as a resource to competition participants. It was a controversial decision to add a code-sharing tool to a competitive coding space. We thought it was important to make Kaggle more than a place where competitions are solved behind closed digital doors. Since then, Kernels has grown from its infancy--essentially a blinking cursor in a docker container--into its teenage years. We now have more compute, longer runtimes, better datasets, GPUs, and an improved interface. We have iterated and tested several Kernels-only (KO) competition formats with a true holdout test set, in particular deploying them when we would have otherwise substituted a two-stage competition. However, the experience of submitting to a Kernels-only competition has typically been asynchronous and imperfect; participants wait many days after a competition has concluded for their selected Kernels to be rerun on the holdout test dataset, the leaderboard updated, and the winners announced. This flow causes heartbreak to participants whose Kernels fail on the unseen test set, leaving them with no way to correct tiny errors that spoil months of hard work. Say Hello to Synchronous KO We're now pleased to announce general support for a synchronous Kernels-only format. When you submit from a Kernel, Kaggle will run the code against both the public test set and private test set in real time. This small-but-substantial tweak improves the experience for participants, the host, and Kaggle:  With a truly withheld test set, we are practicing proper, rigorous machine learning. We will be able to offer more varieties of competitions and intend to run many fewer confusing two-stage competitions. You will be able to see if your code runs successfully on the withheld test set and have the leeway to intervene if it fails. We will run all submissions against the private data, not just selected ones. Participants will get the complete and familiar public/private scores available in a traditional competition. The final leaderboard can be released at the end of the competition, without the delay of rerunning Kernels.  This competition is a low-stakes, trial-run introduction to our new synchronous KO implementation. We want to test that the process goes smoothly and gather feedback on your experiences. While it may feel like a normal KO competition, there are complicated new mechanics in play, such as the selection logic of Kernels that are still running when the deadline passes. Since the competition also presents an authentic machine learning problem, it will also award Kaggle medals and points. Have fun, good luck, and welcome to the world of synchronous Kernels competitions!";https://www.kaggle.com/c/instant-gratification;Kaggle;A synchronous Kernels-only competition;['gpu'];1832.0;0.676;Instant Gratification;Featured Code Competition
2016-11-01 00:59:00;7. You read that correctly. That's the start to a real integer sequence, the powers of primes. Want something easier? How about the next number in 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55? If you answered 89, you may enjoy this challenge. Your computer may find it considerably less enjoyable. The On-Line Encyclopedia of Integer Sequences is a 50+ year effort by mathematicians the world over to catalog sequences of integers. If it has a pattern, it's probably in the OEIS, and probably described with amazing detail. This competition challenges you create a machine learning algorithm capable of guessing the next number in an integer sequence. While this sounds like pattern recognition in its most basic form, a quick look at the data will convince you this is anything but basic! Acknowledgments Kaggle is hosting this competition for the data science community to use for fun and education. We thank the OEIS and its contributors for cataloging this data.;https://www.kaggle.com/c/integer-sequence-learning;Kaggle;1, 2, 3, 4, 5, 7?!;[];284.0;0.572;Integer Sequence Learning;Playground prediction Competition
2017-06-22 01:59:00;Cervical cancer is so easy to prevent if caught in its pre-cancerous stage that every woman should have access to effective, life-saving treatment no matter where they live. Today, women worldwide in low-resource settings are benefiting from programs where cancer is identified and treated in a single visit. However, due in part to lacking expertise in the field, one of the greatest challenges of these cervical cancer screen and treat programs is determining the appropriate method of treatment which can vary depending on patients’ physiological differences.  Especially in rural parts of the world, many women at high risk for cervical cancer are receiving treatment that will not work for them due to the position of their cervix. This is a tragedy: health providers are able to identify high risk patients, but may not have the skills to reliably discern which treatment which will prevent cancer in these women. Even worse, applying the wrong treatment has a high cost. A treatment which works effectively for one woman may obscure future cancerous growth in another woman, greatly increasing health risks. Currently, MobileODT offers a Quality Assurance workflow to support remote supervision which helps healthcare providers make better treatment decisions in rural settings. However, their workflow would be greatly improved given the ability to make real-time determinations about patients’ treatment eligibility based on cervix type. In this competition, Intel is partnering with MobileODT to challenge Kagglers to develop an algorithm which accurately identifies a woman’s cervix type based on images. Doing so will prevent ineffectual treatments and allow healthcare providers to give proper referral for cases that require more advanced treatment.  Competition Partner MobileODT has developed and sells the Enhanced Visual Assessment (EVA) System, a digital toolkit for health care workers of every level to provide expert services to patients, anchored at the point-of-care by an FDA-approved, intelligent, mobile-phone based medical device. Combining the algorithmic power of biomedical optics with the computational capabilities and connectivity of mobile phones, MobileODT's connected, intelligent medical systems can be used everywhere, under nearly any conditions. MobileODT's first product, the FDA approved EVA System for colposcopy, is in use by health providers in 31 hospital systems across the US, and in 22 countries, to better screen and treat women for cervical cancer and to conduct forensic colposcopy.;https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;Intel;Which cancer treatment will be most effective?;[];848.0;0.636;Intel & MobileODT Cervical Cancer Screening;Featured prediction Competition
2017-08-16 01:59:00;Tangles of kudzu overwhelm trees in Georgia while cane toads threaten habitats in over a dozen countries worldwide. These are just two invasive species of many which can have damaging effects on the environment, the economy, and even human health. Despite widespread impact, efforts to track the location and spread of invasive species are so costly that they’re difficult to undertake at scale. Currently, ecosystem and plant distribution monitoring depends on expert knowledge. Trained scientists visit designated areas and take note of the species inhabiting them. Using such a highly qualified workforce is expensive, time inefficient, and insufficient since humans cannot cover large areas when sampling.  Because scientists cannot sample a large quantity of areas, some machine learning algorithms are used in order to predict the presence or absence of invasive species in areas that have not been sampled. The accuracy of this approach is far from optimal, but still contributes to approaches to solving ecological problems. In this playground competition, Kagglers are challenged to develop algorithms to more accurately identify whether images of forests and foliage contain invasive hydrangea or not. Techniques from computer vision alongside other current technologies like aerial imaging can make invasive species monitoring cheaper, faster, and more reliable. Acknowledgments Data providers: Christian Requena Mesa, Thore Engel, Amrita Menon, Emma Bradley.;https://www.kaggle.com/c/invasive-species-monitoring;;Identify images of invasive hydrangea;['deep learning', 'cnn'];512.0;0.608;Invasive Species Monitoring;Playground prediction Competition
2019-06-08 01:59:00;Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to nearby areas we are faced with an interesting probem: how do you classify a species in a new region that you may not have seen in previous training data? In order to tackle this problem, we have prepared a challenge where the training data and test data are from different regions, namely The American Southwest and the American Northwest. The species seen in each region overlap, but are not identical, and the challenge is to classify the test species correctly. To this end, we will allow training on our American Southwest data (from CaltechCameraTraps), on iNaturalist 2017/2018 data, and on simulated data generated from Microsoft AirSim. We have provided a taxonomy file mapping our classes into the iNat taxonomy. This is an FGVCx competition as part of the FGVC6 workshop at CVPR 2019, and is sponsored by Microsoft AI for Earth.  There is a github page for the competition here. Please open an issue if you have questions or problems with the dataset. If you use this dataset in publication, please cite: @article{beery2019iwildcam,  title={The iWildCam 2019 Challenge Dataset},  author={Beery, Sara and Morris, Dan and Perona, Pietro},  journal={arXiv preprint arXiv:1907.07617},  year={2019} }   Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/iwildcam-2019-fgvc6;Fine-Grained Visual Categorization 6;Categorize animals in the wild;['data cleaning', 'exploratory data analysis', 'beginner', 'data visualization', 'gpu'];336.0;0.582;iWildCam 2019 - FGVC6;Playground prediction Competition
2020-05-27 01:59:00;Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automatic species classification in camera trap images. However, as we try to expand the scope of these models we are faced with an interesting problem: how do we train models that perform well on new (unseen during training) camera trap locations? Can we leverage data from other modalities, such as citizen science data and remote sensing data?    In order to tackle this problem, we have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to classify species in the test cameras correctly. To explore multimodal solutions, we allow competitors to train on the following data: (i) our camera trap training set (data provided by WCS), (ii) iNaturalist 2017-2019 data, and (iii) multispectral imagery (from Landsat 8) for each of the camera trap locations. On the competition GitHub page we provide the multispectral data, a taxonomy file mapping our classes into the iNat taxonomy, a subset of iNat data mapped into our class set, and a camera trap detection model (the MegaDetector) along with the corresponding detections. If you use this dataset in publication, please cite: @article{beery2020iwildcam,     title={The iWildCam 2020 Competition Dataset},     author={Beery, Sara and Cole, Elijah and Gjoka, Arvi},     journal={arXiv preprint arXiv:2004.10340},     year={2020} }  This is an FGVCx competition as part of the FGVC7 workshop at CVPR 2020, and is sponsored by Microsoft AI for Earth and Wildlife Insights. There is a GitHub page for the competition here. Please open an issue if you have questions or problems with the dataset.    You can find the iWildCam 2018 Competition here, and the iWildCam 2019 Competition here. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/iwildcam-2020-fgvc7;Fine-Grained Visual Categorization 7;Categorize animals in the wild;['exploratory data analysis', 'beginner', 'classification', 'data visualization', 'gpu'];121.0;0.514;iWildCam 2020 - FGVC7;Research prediction Competition
2020-06-23 01:59:00;"It only takes one toxic comment to sour an online discussion. The Conversation AI team, a research initiative founded by Jigsaw and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. If these toxic contributions can be identified, we could have a safer, more collaborative internet. In the previous 2018 Toxic Comment Classification Challenge, Kagglers built multi-headed models to recognize toxicity and several subtypes of toxicity. In 2019, in the Unintended Bias in Toxicity Classification Challenge, you worked to build toxicity models that operate fairly across a diverse range of conversations. This year, we're taking advantage of Kaggle's new TPU support and challenging you to build multilingual models with English-only training data. Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results ""translate"" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages.  As our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.  To get started with TPUs:  Read the TPU documentation one-pager Then jump right into the Getting Started Notebooks for this competition  Quick note: a TPU is a network-connected accelerator and requires a couple extra lines in your code. Flipping the TPU switch in your notebook will not, by itself, accelerate your code.";https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification;Jigsaw/Conversation AI;Use TPUs to identify toxicity comments across multiple languages;['nlp', 'tpu', 'neural networks', 'tensorflow', 'transfer learning'];1621.0;0.67;Jigsaw Multilingual Toxic Comment Classification;Featured Code Competition
2018-03-21 00:59:00;Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments. The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content). In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful. Disclaimer: the dataset for this competition contains text that may be considered profane, vulgar, or offensive.;https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;Jigsaw/Conversation AI;Identify and classify toxic online comments;['nlp', 'naive bayes', 'beginner', 'linguistics', 'classification', 'logistic regression'];4550.0;0.717;Toxic Comment Classification Challenge;Featured prediction Competition
2019-07-18 21:35:00;"Can you help detect toxic comments ― and minimize unintended model bias? That's your challenge in this competition. The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.  Last year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.  Here’s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. ""gay""), even when those comments were not actually toxic (such as ""I am a gay woman""). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users. In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive. Acknowledgments The Conversation AI team would like to thank Civil Comments for making this dataset available publicly and the Online Hate Index Research Project at D-Lab, University of California, Berkeley, whose labeling survey/instrument informed the dataset labeling. We'd also like to thank everyone who has contributed to Conversation AI's research, especially those who took part in our last competition, the success of which led to the creation of this challenge.    This is a Kernels-only competition. Refer to Kernels Requirements for details.";https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;Jigsaw/Conversation AI;Detect toxicity across a diverse range of conversations;['beginner', 'exploratory data analysis', 'data visualization'];3165.0;0.702;Jigsaw Unintended Bias in Toxicity Classification;Featured Code Competition
2019-12-18 00:59:00;"Bored of MNIST? The goal of this competition is to provide a simple extension to the classic MNIST competition we're all familiar with. Instead of using Arabic numerals, it uses a recently-released dataset of Kannada digits. Kannada is a language spoken predominantly by people of Karnataka in southwestern India. The language has roughly 45 million native speakers and is written using the Kannada script. Wikipedia  This competition uses the same format as the MNIST competition in terms of how the data is structured, but it's different in that it is a synchronous re-run Kernels competition. You write your code in a Kaggle Notebook, and when you submit the results, your code is scored on both the public test set, as well as a private (unseen) test set. Technical Information All details of the dataset curation has been captured in the paper titled: Prabhu, Vinay Uday. ""Kannada-MNIST: A new handwritten digits dataset for the Kannada language."" arXiv preprint arXiv:1908.01242 (2019) The github repo of the author can be found here. On the originally-posted dataset, the author suggests some interesting questions you may be interested in exploring. Please note, although this dataset has been released in full, the purpose of this competition is for practice, not to find the labels to submit a perfect score. In addition to the main dataset, the author also disseminated an additional real world handwritten dataset (with 10k images), termed as the 'Dig-MNIST dataset' that can serve as an out-of-domain test dataset. It was created with the help of volunteers that were non-native users of the language, authored on a smaller sheet and scanned with different scanner settings compared to the main dataset. This 'dig-MNIST' dataset serves as a more difficult test-set (An accuracy of 76.1% was reported in the paper cited above) and achieving ~98+% accuracy on this test dataset would be rather commendable. Acknowledgments Kaggle thanks Vinay Prabhu for providing this interesting dataset for a Playground competition. Image reference: https://www.researchgate.net/figure/speech-for-Kannada-numbers_fig2_313113588";https://www.kaggle.com/c/Kannada-MNIST;Kaggle;MNIST like datatset for Kannada handwritten digits;['deep learning', 'exploratory data analysis', 'india', 'beginner', 'classification', 'data visualization', 'gpu', 'research', 'cnn'];1214.0;0.655;Kannada MNIST;Playground Code Competition
2017-12-18 00:59:00;"The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build an algorithm that predicts whether a subscription user will churn using a donated dataset from KKBOX. WSDM (pronounced ""wisdom"") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models. For a subscription business, accurately predicting churn is critical to long-term success. Even slight variations in churn can drastically affect profits. KKBOX is Asia’s leading music streaming service, holding the world’s most comprehensive Asia-Pop music library with over 30 million tracks. They offer a generous, unlimited version of their service to millions of people, supported by advertising and paid subscriptions. This delicate model is dependent on accurately predicting churn of their paid users. In this competition you’re tasked to build an algorithm that predicts whether a user will churn after their subscription expires. Currently, the company uses survival analysis techniques to determine the residual membership life time for each subscriber. By adopting different methods, KKBOX anticipates they’ll discover new insights to why users leave so they can be proactive in keeping users dancing. Winners will present their findings at the WSDM conference February 6-8, 2018 in Los Angeles, CA.  For more information on the conference, click here.";https://www.kaggle.com/c/kkbox-churn-prediction-challenge;KKBOX;Can you predict when subscribers will churn?;['deep learning', 'exploratory data analysis', 'neural networks', 'beginner', 'classification', 'data visualization'];574.0;0.614;WSDM - KKBox's Churn Prediction Challenge;Research prediction Competition
2017-12-18 00:59:00;"The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build a better music recommendation system using a donated dataset from KKBOX. WSDM (pronounced ""wisdom"") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models. Not many years ago, it was inconceivable that the same person would listen to the Beatles, Vivaldi, and Lady Gaga on their morning commute. But, the glory days of Radio DJs have passed, and musical gatekeepers have been replaced with personalizing algorithms and unlimited streaming services. While the public’s now listening to all kinds of music, algorithms still struggle in key areas. Without enough historical data, how would an algorithm know if listeners will like a new song or a new artist? And, how would it know what songs to recommend brand new users? WSDM has challenged the Kaggle ML community to help solve these problems and build a better music recommendation system. The dataset is from KKBOX, Asia’s leading music streaming service, holding the world’s most comprehensive Asia-Pop music library with over 30 million tracks. They currently use a collaborative filtering based algorithm with matrix factorization and word embedding in their recommendation system but believe new techniques could lead to better results. Winners will present their findings at the conference February 6-8, 2018 in Los Angeles, CA.  For more information on the conference, click here, and don't forget to check out the other KKBox/WSDM competition: KKBox Music Churn Prediction Challenge";https://www.kaggle.com/c/kkbox-music-recommendation-challenge;KKBOX;Can you build the best music recommendation system?;['data cleaning', 'feature engineering', 'classification', 'data visualization', 'gradient boosting'];1081.0;0.649;WSDM - KKBox's Music Recommendation Challenge;Research prediction Competition
2016-06-14 01:59:00;Kobe Bryant marked his retirement from the NBA by scoring 60 points in his final game as a Los Angeles Laker on Wednesday, April 12, 2016. Drafted into the NBA at the age of 17, Kobe earned the sport’s highest accolades throughout his long career. Using 20 years of data on Kobe's swishes and misses, can you predict which shots will find the bottom of the net? This competition is well suited for practicing classification basics, feature engineering, and time series analysis. Practice got Kobe an eight-figure contract and 5 championship rings. What will it get you? Acknowledgements Kaggle is hosting this competition for the data science community to use for fun and education. For more data on Kobe and other NBA greats, visit stats.nba.com.;https://www.kaggle.com/c/kobe-bryant-shot-selection;Kaggle;Which shots did Kobe sink?;['basketball', 'sports', 'data visualization', 'intermediate'];1117.0;0.651;Kobe Bryant Shot Selection;Playground prediction Competition
2019-10-15 01:59:00;Build a model to transcribe ancient Kuzushiji into contemporary Japanese characters Imagine the history contained in a thousand years of books. What stories are in those books? What knowledge can we learn from the world before our time? What was the weather like 500 years ago? What happened when Mt. Fuji erupted? How can one fold 100 cranes using only one piece of paper? The answers to these questions are in those books. Japan has millions of books and over a billion historical documents such as personal letters or diaries preserved nationwide. Most of them cannot be read by the majority of Japanese people living today because they were written in “Kuzushiji”. Even though Kuzushiji, a cursive writing style, had been used in Japan for over a thousand years, there are very few fluent readers of Kuzushiji today (only 0.01% of modern Japanese natives). Due to the lack of available human resources, there has been a great deal of interest in using Machine Learning to automatically recognize these historical texts and transcribe them into modern Japanese characters. Nevertheless, several challenges in Kuzushiji recognition have made the performance of existing systems extremely poor. (More information in About Kuzushiji) This is where you come in. The hosts need help from machine learning experts to transcribe Kuzushiji into contemporary Japanese characters. With your help, Center for Open Data in the Humanities (CODH) will be able to develop better algorithms for Kuzushiji recognition. The model is not only a great contribution to the machine learning community, but also a great help for making millions of documents more accessible and leading to new discoveries in Japanese history and culture.  Hosts Center for Open Data  in the Humanities (CODH)  conducts research and development to enhance access to humanities data using state-of-the-art technology in informatics and statistics. The National Institute of Japanese Literature (NIJL) is an institution which strives to serve researchers in the field of Japanese literature as well as those working in various other humanities, by collecting in one location a vast storage of materials related to Japanese literature gathered from all corners of the country.  The National Institute of Informatics (NII) is Japan's only general academic research institution seeking to create future value in the new discipline of informatics. NII seeks to advance integrated research and development activities in information-related fields, including networking, software, and content. Official Collaborators Mikel Bober-Irizar (anokas) Kaggle Grandmaster and Alex Lamb (MILA. Quebec Artificial Intelligence Institute);https://www.kaggle.com/c/kuzushiji-recognition;ROIS-DS Center for Open Data in the Humanities;Opening the door to a thousand years of Japanese culture;['deep learning', 'dailychallenge', 'data cleaning', 'exploratory data analysis', 'data analytics', 'image data', 'beginner', 'data visualization', 'gpu', 'computer vision', 'cnn'];293.0;0.574;Kuzushiji Recognition;Playground prediction Competition
2019-06-04 01:59:00;Did you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. In this competition, we present the largest worldwide dataset to date, to foster progress in this problem. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 200K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. This is the second edition of this challenge. Compared to the first edition, the new dataset is more comprehensive and diverse. See the Data tab for more in-depth discussion on the new released dataset. This challenge is organized in conjunction with the Landmark Retrieval Challenge. In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We encourage participants to use the training data from the recognition challenge (either from this year’s or last year’s dataset) to develop models which could be useful for the retrieval challenge.;https://www.kaggle.com/c/landmark-recognition-2019;Google;Label famous (and not-so-famous) landmarks in images;['gpu'];281.0;0.571;Google Landmark Recognition 2019;Research prediction Competition
2020-09-30 01:59:00;Welcome to the third Landmark Recognition competition! This year, we have worked to set this up as a code competition and collected a new set of test images. Have you ever gone through your vacation photos and asked yourself: What was the name of that temple I visited in China? or Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. In the previous editions of this challenge (2018 and  2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring. This challenge is organized in conjunction with the Landmark Retrieval Challenge 2020, which was launched June 30, 2020. Both challenges are affiliated with the Instance-Level Recognition workshop in ECCV’20.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/landmark-recognition-2020;Google;Label famous (and not-so-famous) landmarks in images;['data cleaning', 'exploratory data analysis', 'image data', 'beginner', 'classification', 'data visualization', 'gpu', 'pytorch'];736.0;0.629;Google Landmark Recognition 2020;Research Code Competition
2018-05-30 01:59:00;[UPDATE] 2019 challenge launched: https://kaggle.com/c/landmark-recognition-2019 Did you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. In this competition, we present the largest worldwide dataset to date, to foster progress in this problem. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are a total of 15K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. This challenge is organized in conjunction with the Landmark Retrieval Challenge ( https://www.kaggle.com/c/landmark-retrieval-challenge ). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.;https://www.kaggle.com/c/landmark-recognition-challenge;Google;Label famous (and not-so-famous) landmarks in images;['deep learning', 'exploratory data analysis', 'image data', 'beginner', 'data visualization', 'computer vision'];477.0;0.604;Google Landmark Recognition Challenge;Research prediction Competition
2020-08-18 01:59:00;Welcome to the third Landmark Retrieval competition! This year, we have worked to set this up as a code competition and we have completely refreshed the test and index image sets.  Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph. In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the Landmark Recognition Challenge 2020. Both challenges will be discussed at the Instance-Level Recognition workshop in ECCV’20. In the previous editions of this challenge (2018 and  2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a representation learning format: rather than creating a submission file with retrieved images, you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality with mean average precision.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/landmark-retrieval-2020;Google;Given an image, can you find all of the same landmarks in a dataset?;['deep learning', 'multiclass classification', 'image data', 'beginner', 'gpu'];541.0;0.611;Google Landmark Retrieval 2020;Research Code Competition
2018-05-30 01:59:00;[UPDATE] 2019 challenge launched: https://kaggle.com/c/landmark-retrieval-2019 Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph. In this competition, Kagglers are given query images and, for each query, are expected to retrieve all database images containing the same landmarks (if any). The new dataset is the largest worldwide dataset for image retrieval research, comprising more than a million images of 15K unique landmarks. We hope that this release will accelerate progress in this important research problem. This challenge is organized in conjunction with the Landmark Recognition Challenge (https://www.kaggle.com/c/landmark-recognition-challenge). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.;https://www.kaggle.com/c/landmark-retrieval-challenge;Google;Given an image, can you find all of the same landmarks in a dataset?;[];209.0;0.552;Google Landmark Retrieval Challenge;Research prediction Competition
2019-06-04 01:59:00;Forecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: when the event will occur, where it will occur, and how large it will be. In this competition, you will address when the earthquake will take place. Specifically, you’ll predict the time remaining before laboratory earthquakes occur from real-time seismic data.  If this challenge is solved and the physics are ultimately shown to scale from the laboratory to the field, researchers will have the potential to improve earthquake hazard assessments that could save lives and billions of dollars in infrastructure. This challenge is hosted by  Los Alamos National Laboratory which enhances national security by ensuring the safety of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.  Acknowledgments:     Geophysics Group: The competition builds on initial work from Bertrand Rouet-Leduc, Claudia Hulbert, and Paul Johnson. B. Rouet-Leduc prepared the data for the competition.    Department of Geosciences: Data are from experiments performed by  Chas Bolton, Jacques Riviere, Paul Johnson and Prof. Chris Marone.    Department of Physics & Astronomy: This competition stemmed from the DOE Council workshop “Information is in the Noise: Signatures of Evolving Fracture and Fracture Networks” held March 2018 that was organized by Prof. Laura J. Pyrak-Nolte.   Department of Energy Office of Science, Basic Energy Sciences, Chemical Sciences, Geosciences and Biosciences Division: The Geosciences core research.   Photo by Nik Shuliahin on Unsplash;https://www.kaggle.com/c/LANL-Earthquake-Prediction;Los Alamos National Laboratory;Can you predict upcoming laboratory earthquakes?;['data cleaning', 'exploratory data analysis', 'feature engineering', 'signal processing', 'regression', 'data visualization'];4521.0;0.717;LANL Earthquake Prediction;Research prediction Competition
2017-03-01 00:59:00;There are estimated to be nearly half a million species of plant in the world. Classification of species has been historically problematic and often results in duplicate identifications. Automating plant recognition might have many applications, including:       The objective of this playground competition is to use binary leaf images and extracted features, including shape, margin & texture, to accurately identify 99 species of plants. Leaves, due to their volume, prevalence, and unique characteristics, are an effective means of differentiating plant species. They also provide a fun introduction to applying techniques that involve image-based features. As a first step, try building a classifier that uses the provided pre-extracted features. Next, try creating a set of your own features. Finally, examine the errors you're making and see what you can do to improve. Acknowledgments Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from leaf images collected by  James Cope, Thibaut Beghin, Paolo Remagnino, & Sarah Barman of the Royal Botanic Gardens, Kew, UK. Charles Mallah, James Cope, James Orwell. Plant Leaf Classification Using Probabilistic Integration of Shape, Texture and Margin Features. Signal Processing, Pattern Recognition and Applications, in press. 2013. We thank the UCI machine learning repository for hosting the dataset.;https://www.kaggle.com/c/leaf-classification;Kaggle;Can you see the random forest for the leaves?;['random forest', 'naive bayes', 'k-means', 'model comparison', 'beginner', 'intermediate', 'classification', 'data visualization', 'logistic regression', 'pca'];1597.0;0.669;Leaf Classification;Playground prediction Competition
2020-12-01 00:59:00;The Connectivity Map, a project within the Broad Institute of MIT and Harvard,  the Laboratory for Innovation Science at Harvard (LISH), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS), present this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms. What is the Mechanism of Action (MoA) of a drug? And why is it important?  In the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short. How do we determine the MoAs of a new drug?  One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs. In this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells’ responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset. As is customary, the dataset has been split into testing and training subsets. Hence, your task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.     How to evaluate the accuracy of a solution?  Based on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic  loss function applied to each drug-MoA annotation pair. If successful, you’ll help to develop an algorithm to predict a compound’s MoA given its cellular signature, thus helping scientists advance the drug discovery process.      This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/lish-moa;Laboratory for Innovation Science at Harvard;Can you improve the algorithm that classifies drugs based on their biological activity?;['exploratory data analysis', 'biology', 'xgboost', 'genetics', 'multilabel classification', 'beginner', 'gpu'];4373.0;0.716;Mechanisms of Action (MoA) Prediction;Research Code Competition
2020-05-26 01:59:00;Think you can use your data science skills to make big predictions at a submicroscopic level? Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.  When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data. The University of Liverpool’s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you’ll use ion channel data to better model automatic identification methods. If successful, you’ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments. Technology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems. Acknowledgements: This would not be possible without the help of the Biotechnology and Biological Sciences Research Council (BBSRC).;https://www.kaggle.com/c/liverpool-ion-switching;University of Liverpool;Identify the number of channels open at each time point;['data cleaning', 'exploratory data analysis', 'time series analysis', 'beginner', 'data visualization', 'gpu', 'cnn'];2618.0;0.693;University of Liverpool - Ion Switching;Research prediction Competition
2014-03-15 00:59:00;This competition asks you to determine whether a loan will default, as well as the loss incurred if it does default. Unlike traditional finance-based approaches to this problem, where one distinguishes between good or bad counterparties in a binary way, we seek to anticipate and incorporate both the default and the severity of the losses that result. In doing so, we are building a bridge between traditional banking, where we are looking at reducing the consumption of economic capital, to an asset-management perspective, where we optimize on the risk to the financial investor. This competition is sponsored by researchers at Imperial College London.;https://www.kaggle.com/c/loan-default-prediction;;Constructing an optimal portfolio of loans;[];672.0;0.623;Loan Default Prediction - Imperial College London;Research prediction Competition
2020-07-01 01:59:00;Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? If you are interested in estimating the uncertainty distribution of the realized values of the same series, be sure to check out its companion competition How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses.  In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy. The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s. In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy. If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications. Acknowledgements Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.;https://www.kaggle.com/c/m5-forecasting-accuracy;University of Nicosia;Estimate the unit sales of Walmart retail goods;['deep learning', 'exploratory data analysis', 'feature engineering', 'neural networks', 'beginner', 'data visualization', 'gpu', 'lstm'];5558.0;0.726;M5 Forecasting - Accuracy;Featured prediction Competition
2020-07-01 01:59:00;Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the uncertainty distribution of the unit sales of various products sold in the USA by Walmart? This specific competition is the first of its kind, opening up new directions for both academic research and how uncertainty could be assessed and used in organizations. If you are interested in providing point (accuracy) forecasts for the same series, be sure to check out its companion competition. How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses.  In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy. The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s. In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days and to make uncertainty estimates for these forecasts. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy. If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications. Acknowledgements Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.;https://www.kaggle.com/c/m5-forecasting-uncertainty;University of Nicosia;Estimate the uncertainty distribution of Walmart unit sales.;['data cleaning', 'exploratory data analysis', 'beginner', 'clustering', 'gpu'];909.0;0.64;M5 Forecasting - Uncertainty;Featured prediction Competition
2017-04-04 17:00:00;Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our fourth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2017 tournament. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2017 results.;https://www.kaggle.com/c/march-machine-learning-mania-2017;;Predict the 2017 NCAA Basketball Tournament;[];441.0;0.599;March Machine Learning Mania 2017;Playground prediction Competition
2016-12-02 00:59:00;Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective. Even after surgical removal of epilepsy, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for electrical brain activity (EEG) based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.  The Competition Transitioning from the Kaggle contests held on seizure detection and seizure prediction in 2014 that primarily involved long-term electrical brain activity recordings from dogs, the current contest focuses on seizure prediction using long-term electrical brain activity recordings from humans obtained from the world-first clinical trial of the implantable NeuroVista Seizure Advisory System. Human brain activity was recorded in the form of intracranial EEG (iEEG), which involves electrodes positioned on the surface of the cerebral cortex and the recording of electrical signals with an ambulatory monitoring system. These are long duration recordings, spanning multiple months up to multiple years and recording large numbers of seizures in some humans. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity.  Acknowledgments  This competition is sponsored by MathWorks, the National Institutes of Health (NINDS), the American Epilepsy Society and the University of Melbourne, and organised in partnership with the Alliance for Epilepsy Research, the University of Pennsylvania and the Mayo Clinic.       References;https://www.kaggle.com/c/melbourne-university-seizure-prediction;;Predict seizures in long-term human intracranial EEG recordings;[];477.0;0.604;Melbourne University AES/MathWorks/NIH Seizure Prediction;Research prediction Competition
2018-04-03 01:59:00;Google Cloud and NCAA® have teamed up to bring you this year’s version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness® during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.   In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results. This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.;https://www.kaggle.com/c/mens-machine-learning-competition-2018;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['logistic regression', 'exploratory data analysis', 'data visualization'];933.0;0.642;Google Cloud & NCAA® ML Competition 2018-Men's;Featured prediction Competition
2019-04-09 08:00:00;As a result of the continued collaboration between Google Cloud and the NCAA, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.   In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.  As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!  This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.;https://www.kaggle.com/c/mens-machine-learning-competition-2019;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['basketball', 'bayesian statistics', 'exploratory data analysis', 'data visualization'];866.0;0.638;Google Cloud & NCAA® ML Competition 2019-Men's;Featured prediction Competition
2018-02-21 21:03:00;It can be hard to know how much something’s really worth. Small details can mean big differences in pricing. For example, one of these sweaters cost $335 and the other cost $9.99. Can you guess which one’s which?   Product pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs.  Mercari, Japan’s biggest community-powered shopping app, knows this problem deeply. They’d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari's marketplace. In this competition, Mercari’s challenging you to build an algorithm that automatically suggests the right product prices. You’ll be provided user-inputted text descriptions of their products, including details like product category name, brand name, and item condition. Note that, because of the public nature of this data, this competition is a “Kernels Only” competition. In the second stage of the challenge, files will only be available through Kernels and you will not be able to modify your approach in response to new data. Read more details in the data tab and Kernels FAQ page.;https://www.kaggle.com/c/mercari-price-suggestion-challenge;Mercari;Can you automatically suggest product prices to online sellers?;['deep learning', 'nlp', 'exploratory data analysis', 'neural networks', 'beginner', 'data visualization', 'rnn'];2382.0;0.689;Mercari Price Suggestion Challenge;Featured Code Competition
2017-07-11 01:59:00;Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler’s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams. . To ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler’s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world’s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler’s production lines.  In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler’s standards.;https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;Daimler;Can you cut the time a Mercedes-Benz spends on the test bench?;['exploratory data analysis', 'xgboost', 'model comparison', 'beginner', 'data visualization', 'pca'];3831.0;0.71;Mercedes-Benz Greener Manufacturing;Featured prediction Competition
2019-03-14 00:59:00;The malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways.   With more than one billion  enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security. As one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences. Can you help protect more than one billion machines from damage BEFORE it happens? Acknowledgements This competition is hosted by Microsoft, Windows Defender ATP Research, Northeastern University College of Computer and Information Science, and Georgia Tech Institute for Information Security & Privacy.      Microsoft contacts  Rob McCann (Robert.McCann@microsoft.com) Christian Seifert (chriseif@microsoft.com) Susan Higgs (Susan.Higgs@microsoft.com) Matt Duncan (Matthew.Duncan@microsoft.com)      Northeastern University contact  Mansour Ahmadi (m.ahmadi@northeastern.edu)      Georgia Tech contacts  Brendan Saltaformaggio (brendan@ece.gatech.edu) Taesoo Kim (taesoo@gatech.edu);https://www.kaggle.com/c/microsoft-malware-prediction;Microsoft;Can you predict if a machine will soon be hit with malware?;['data cleaning', 'feature engineering', 'exploratory data analysis', 'neural networks', 'classification', 'data visualization', 'gpu', 'gradient boosting'];2426.0;0.689;Microsoft Malware Prediction;Research prediction Competition
2018-09-25 01:59:00;"""There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side."" The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.  Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of Socher et al [2]. We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper: http://nlp.stanford.edu/sentiment/ There you will find have source code, a live demo, and even an online interface to help train the model. [1] Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124. [2] Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).";https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;Kaggle;Classify the sentiment of sentences from the Rotten Tomatoes dataset;['deep learning', 'nlp', 'data cleaning', 'dailychallenge', 'exploratory data analysis', 'beginner', 'classification', 'data visualization', 'gpu', 'transfer learning', 'text mining', 'text data', 'cnn'];410.0;0.595;Movie Review Sentiment Analysis (Kernels Only);Playground Code Competition
2017-10-03 01:59:00;A lot has been said during the past several years about how precision medicine and, more concretely, how genetic testing is going to disrupt the way diseases like cancer are treated. But this is only partially happening due to the huge amount of manual work still required. Memorial Sloan Kettering Cancer Center (MSKCC) launched this competition, accepted by the NIPS 2017 Competition Track,  because we need your help to take personalized medicine to its full potential.  Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers).  Currently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature. For this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists have manually annotated thousands of mutations. We need your help to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/msk-redefining-cancer-treatment;Kaggle;Predict the effect of Genetic Variants to enable Personalized Medicine;['nlp', 'feature engineering', 'beginner', 'linguistics', 'classification', 'intermediate', 'svm'];1386.0;0.662;Personalized Medicine: Redefining Cancer Treatment;Research prediction Competition
2018-09-26 01:59:00;In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations.  While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!  To learn how to handle large datasets with ease and solve this problem using TensorFlow, consider taking the Machine Learning with TensorFlow on Google Cloud Platform specialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To make this easier, head to Coursera.org/NEXTextended to claim this specialization for free for the first month!;https://www.kaggle.com/c/new-york-city-taxi-fare-prediction;Google Cloud;Can you predict a rider's taxi fare?;['deep learning', 'data cleaning', 'exploratory data analysis', 'feature engineering', 'xgboost', 'tensorflow', 'linear regression', 'beginner', 'data visualization', 'gpu', 'ensembling', 'geospatial analysis', 'gradient boosting'];1484.0;0.666;New York City Taxi Fare Prediction;Playground prediction Competition
2020-01-07 00:59:00;"“The running back takes the handoff… he breaks a tackle…spins… and breaks free! One man to beat! Past the 50-yard-line! To the 40! The 30! He! Could! Go! All! The! Way!” But will he?  American football is a complex sport. From the 22 players on the field to specific characteristics that ebb and flow throughout the game, it can be challenging to quantify the value of specific plays and actions within a play.  Fundamentally, the goal of football is for the offense to run (rush) or throw (pass) the ball to gain yards, moving towards, then across, the opposing team’s side of the field in order to score. And the goal of the defense is to prevent the offensive team from scoring.  In the National Football League (NFL), roughly a third of teams’ offensive yardage comes from run plays.. Ball carriers are generally assigned the most credit for these plays, but their teammates (by way of blocking), coach (by way of play call), and the opposing defense also play a critical role. Traditional metrics such as ‘yards per carry’ or ‘total rushing yards’ can be flawed; in this competition, the NFL aims to provide better context into what contributes to a successful run play. As an “armchair quarterback” watching the game, you may think you can predict the result of a play when a ball carrier takes the handoff - but what does the data say? In this competition, you will develop a model to predict how many yards a team will gain on given rushing plays as they happen. You'll be provided game, play, and player-level data, including the position and speed of players as provided in the NFL’s Next Gen Stats data. And the best part - you can see how your model performs from your living room, as the leaderboard will be updated week after week on the current season’s game data as it plays out.  Deeper insight into rushing plays will help teams, media, and fans better understand the skill of players and the strategies of coaches. It will also assist the NFL and its teams evaluate the ball carrier, his teammates, his coach, and the opposing defense, in order to make adjustments as necessary. Additionally, the winning model will be provided to the NFL’s Next Gen Stats group to potentially share with teams. You could help the NFL Network generate models to use during games, or for pre-game/post-game breakdowns.";https://www.kaggle.com/c/nfl-big-data-bowl-2020;The National Football League;How many yards will an NFL player gain after receiving a handoff?;['beginner', 'neural networks'];2038.0;0.681;NFL Big Data Bowl;Featured Code Competition
2020-01-03 00:59:00;"Welcome In this challenge, you're tasked to investigate the relationship between the playing surface and the injury and performance of National Football League (NFL) athletes and to examine factors that may contribute to lower extremity injuries. You'll also notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, the NFL’s annual Super Bowl competition designed to spur innovation in player health, safety and performance. The Challenge In the NFL, 12 stadiums have fields with synthetic turf.  Recent investigations of lower limb injuries among football athletes have indicated significantly higher injury rates on synthetic turf compared with natural turf (Mack et al., 2018; Loughran et al., 2019).  In conjunction with the epidemiologic investigations, biomechanical studies of football cleat-surface interactions have shown that synthetic turf surfaces do not release cleats as readily as natural turf and may contribute to the incidence of non-contact lower limb injuries (Kent et al., 2015).  Given these differences in cleat-turf interactions, it has yet to be determined whether player movement patterns and other measures of player performance differ across playing surfaces and how these may contribute to the incidence of lower limb injury.   Now, the NFL is challenging Kagglers to help them examine the effects that playing on synthetic turf versus natural turf can have on player movements and the factors that may contribute to lower extremity injuries.  NFL player tracking, also known as Next Gen Stats, is the capture of real time location data, speed and acceleration for every player, every play on every inch of the field. As part of this challenge, the NFL has provided full player tracking of on-field position for 250 players over two regular season schedules.  One hundred of the athletes in the study data set sustained one or more injuries during the study period that were identified as a non-contact injury of a type that may have turf interaction as a contributing factor to injury.  The remaining 150 athletes serve as a representative sample of the larger NFL population that did not sustain a non-contact lower-limb injury during the study period.  Details of the surface type and environmental parameters that may influence performance and outcome are also provided.  Your challenge is to characterize any differences in player movement between the playing surfaces and identify specific scenarios (e.g., field surface, weather, position, play type, etc.) that interact with player movement to present an elevated risk of injury.  More details on the entry criteria are available in Evaluation Tab. About The NFL The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country. The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries. For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com  Evaluation";https://www.kaggle.com/c/nfl-playing-surface-analytics;The National Football League;Can you investigate the relationship between the playing surface and the injury and performance of NFL athletes?;['exploratory data analysis', 'sports', 'beginner', 'data visualization', 'gpu'];;0.0;NFL 1st and Future - Analytics;Analytics  Competition
2019-01-10 00:59:00;Welcome In this challenge you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. Instead, this challenge asks you to use data to propose specific rule modifications for the NFL that aim to reduce the occurrence of concussions during punt plays. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, presented by Arrow Electronics – the NFL’s annual Super Bowl competition designed to spur innovation in player health, safety and performance. The Challenge For the 2018 season, the NFL revised their kickoff rules in an effort to reduce the risk of injury during those plays. By examining injury reports, player position and velocity data, and game video, they were able to understand the game-play circumstances that may exacerbate the risk of injury to players.  This comprehensive review showed that over the course of all games during the 2015-2017 seasons, the kickoff represented only six percent of plays but 12 percent of concussions. Players had approximately four times the risk of concussion on returned kickoffs compared to running or passing plays. The changes to the kickoff rule aim to address the components that posed the most risk, like the use of a two-man wedge.  Now, the NFL is challenging Kagglers to help them perform the same examination, this time on punt play rules. They have provided data for all punt plays from the 2016 and 2017 NFL seasons that includes player rosters, on-field position data and video data, including the plays in which a player suffered a concussion.   Your challenge is to propose specific rule modifications (e.g. changes to the initial formation, tackling techniques, blocking rules etc.), supported by data, that may reduce the occurrence of concussions during punt plays. More details on the entry criteria are available in Overview tab > Evaluation.  About The NFL The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country. The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to look at anything and everything to protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries. For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com.  Evaluation;https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;The National Football League;Analyze NFL game data and suggest rules to improve player safety during punt plays;['beginner', 'sports', 'exploratory data analysis', 'data visualization'];;0.0;NFL Punt Analytics Competition;Analytics  Competition
2017-06-28 01:59:00;Steller sea lions in the western Aleutian Islands have declined 94 percent in the last 30 years. The endangered western population, found in the North Pacific, are the focus of conservation efforts which require annual population counts. Specially trained scientists at NOAA Fisheries Alaska Fisheries Science Center conduct these surveys using airplanes and unoccupied aircraft systems to collect aerial images. Having accurate population estimates enables us to better understand factors that may be contributing to lack of recovery of Stellers in this area. Currently, it takes biologists up to four months to count sea lions from the thousands of images NOAA Fisheries collects each year. Once individual counts are conducted, the tallies must be reconciled to confirm their reliability. The results of these counts are time-sensitive. In this competition, Kagglers are invited to develop algorithms which accurately count the number of sea lions in aerial photographs. Automating the annual population count will free up critical resources allowing NOAA Fisheries to focus on ensuring we hear the sea lion’s roar for many years to come. Plus, advancements in computer vision applied to aerial population counts may also greatly benefit other endangered species.  Resources Learn more about research being done to better understand what's going on with the endangered Steller sea lion populations by joining scientists on a research vessel to the western Aleutian Islands in the video below.;https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;NOAA;How many sea lions do you see?;['advanced', 'cnn'];385.0;0.591;NOAA Fisheries Steller Sea Lion Population Count;Featured prediction Competition
2018-02-16 00:59:00;"Innovative materials design is needed to tackle some of the most important health, environmental, energy, social, and economic challenges of this century. In particular, improving the properties of materials that are intrinsically connected to the generation and utilization of energy is crucial if we are to mitigate environmental damage due to a growing global demand. Transparent conductors are an important class of compounds that are both electrically conductive and have a low absorption in the visible range, which are typically competing properties. A combination of both of these characteristics is key for the operation of a variety of technological devices such as photovoltaic cells, light-emitting diodes for flat-panel displays, transistors, sensors, touch screens, and lasers. However, only a small number of compounds are currently known to display both transparency and conductivity suitable enough to be used as transparent conducting materials.  Aluminum (Al), gallium (Ga), indium (In) sesquioxides are some of the most promising transparent conductors because of a combination of both large bandgap energies, which leads to optical transparency over the visible range, and high conductivities. These materials are also chemically stable and relatively inexpensive to produce. Alloying of these binary compounds in ternary or quaternary mixtures could enable the design of a new material at a specific composition with improved properties over what is current possible. These alloys are described by the formula (AlxGayInz)2NO3N; where x, y, and z can vary but are limited by the constraint x+y+z = 1. The total number of atoms in the";https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;Kaggle;Predict the key properties of novel transparent semiconductors;['neural networks', 'feature engineering', 'linear regression', 'beginner', 'data visualization', 'pca'];878.0;0.638;Nomad2018 Predicting Transparent Conductors;Research prediction Competition
2017-09-16 01:59:00;In this competition, Kaggle is challenging you to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables. Longtime Kagglers will recognize that this competition objective is similar to the ECML/PKDD trip time challenge we hosted in 2015. But, this challenge comes with a twist. Instead of awarding prizes to the top finishers on the leaderboard, this playground competition was created to reward collaboration and collective learning.  We are encouraging you (with cash prizes!) to publish additional training data that other participants can use for their predictions. We also have designated bi-weekly and final prizes to reward authors of kernels that are particularly insightful or valuable to the community.;https://www.kaggle.com/c/nyc-taxi-trip-duration;Kaggle;Share code and data to improve ride time predictions;['data cleaning', 'exploratory data analysis', 'feature engineering', 'k-means', 'beginner', 'data visualization', 'advanced', 'geospatial analysis'];1257.0;0.657;New York City Taxi Trip Duration;Playground prediction Competition
2019-10-02 01:59:00;"Introduction Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:  Object detection track for detecting bounding boxes around object instances, relaunched from 2018. Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018. Instance segmentation track for segmenting masks of objects in images, brand new for 2019.  Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding. Instance Segmentation Track In this track of the Challenge, you are asked to provide segmentation masks of objects. This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.   Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.  The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision. We are excited to partner with Open Images for this second year of competitions, including this brand new track!";https://www.kaggle.com/c/open-images-2019-instance-segmentation;Google Research;Outline segmentation masks of objects in images;['beginner', 'data visualization', 'gpu'];193.0;0.546;Open Images 2019 - Instance Segmentation;Research prediction Competition
2019-10-02 01:59:00;Introduction Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:  Object detection track for detecting bounding boxes around object instances, relaunched from 2018. Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018. Instance segmentation track for segmenting masks of objects in images, brand new for 2019.  Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding. Object Detection Track In this track of the Challenge, you are asked to predict a tight bounding box around object instances. The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).   Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.  Please refer to the Open Images 2019 Challenge page for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you. The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision. We are excited to partner with Open Images for this second year of competitions. See link here for last year’s Object Detection competition.;https://www.kaggle.com/c/open-images-2019-object-detection;Google Research;Detect objects in varied and complex images;['deep learning', 'neural networks', 'beginner', 'gpu', 'computer vision', 'cnn'];558.0;0.613;Open Images 2019 - Object Detection;Research prediction Competition
2020-08-14 18:00:00;"Introduction Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year the Open Images  Instance Segmentation competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details.  Participants are also welcome to submit to this playground competition beyond the context of RVC. Instance Segmentation Track In this track of the Challenge, you are asked to provide segmentation masks of objects. This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.   Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.  The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.";https://www.kaggle.com/c/open-images-instance-segmentation-rvc-2020;Google Research;Outline segmentation masks of objects in images;[];18.0;0.357;Open Images Instance Segmentation RVC 2020 edition;Playground prediction Competition
2020-08-14 18:00:00;Introduction Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year the Open Images Object Detection competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details.  Participants are also welcome to submit to this playground competition beyond the context of RVC. Object Detection Track In this track, you are asked to predict a tight bounding box around object instances. The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).   Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license. The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.;https://www.kaggle.com/c/open-images-object-detection-rvc-2020;Google Research;Detect objects in varied and complex images;[];89.0;0.491;Open Images Object Detection RVC 2020 edition;Playground prediction Competition
2020-10-07 01:59:00;Imagine one day, your breathing became consistently labored and shallow. Months later you were finally diagnosed with pulmonary fibrosis, a disorder with no known cause and no known cure, created by scarring of the lungs. If that happened to you, you would want to know your prognosis. That’s where a troubling disease becomes frightening for the patient: outcomes can range from long-term stability to rapid deterioration, but doctors aren’t easily able to tell where an individual may fall on that spectrum. Your help, and data science, may be able to aid in this prediction, which would dramatically help both patients and clinicians.  Current methods make fibrotic lung diseases difficult to treat, even with access to a chest CT scan. In addition, the wide range of varied prognoses create issues organizing clinical trials. Finally, patients suffer extreme anxiety—in addition to fibrosis-related symptoms—from the disease’s opaque path of progression. Open Source Imaging Consortium (OSIC) is a not-for-profit, co-operative effort between academia, industry and philanthropy. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis (IPF), fibrosing interstitial lung diseases (ILDs), and other respiratory diseases, including emphysematous conditions. Its mission is to bring together radiologists, clinicians and computational scientists from around the world to improve imaging-based treatments. In this competition, you’ll predict a patient’s severity of decline in lung function based on a CT scan of their lungs. You’ll determine lung function based on output from a spirometer, which measures the volume of air inhaled and exhaled. The challenge is to use machine learning techniques to make a prediction with the image, metadata, and baseline FVC as input. If successful, patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments.  This is a Code Competition. Refer to Code Requirements for details.;https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression;Open Source Imaging Consortium (OSIC);Predict lung function decline;['data cleaning', 'exploratory data analysis', 'feature engineering', 'image data', 'beginner', 'bayesian statistics', 'data visualization', 'gpu'];2097.0;0.683;OSIC Pulmonary Fibrosis Progression;Featured Code Competition
2015-05-19 01:59:00;Get started on this competition through Kaggle Scripts The Otto Group is one of the world’s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate & Barrel (USA), Otto.de (Germany) and 3 Suisses (France). We are selling millions of products worldwide every day, with several thousand products being added to our product line. A consistent analysis of the performance of our products is crucial. However, due to our diverse global infrastructure, many identical products get classified differently. Therefore, the quality of our product analysis depends heavily on the ability to accurately cluster similar products. The better the classification, the more insights we can generate about our product range.  For this competition, we have provided a dataset with 93 features for more than 200,000 products. The objective is to build a predictive model which is able to distinguish between our main product categories. The winning models will be open sourced.;https://www.kaggle.com/c/otto-group-product-classification-challenge;;Classify products into the correct category;['multiclass classification', 'feature engineering', 'xgboost', 'python', 'gpu'];3505.0;0.706;Otto Group Product Classification Challenge;Featured prediction Competition
2017-01-19 00:59:00;The internet is a stimulating treasure trove of possibility. Every day we stumble on news stories relevant to our communities or experience the serendipity of finding an article covering our next travel destination. Outbrain, the web’s leading content discovery platform, delivers these moments while we surf our favorite sites.  Currently, Outbrain pairs relevant content with curious readers in about 250 billion personalized recommendations every month across many thousands of sites. In this competition, Kagglers are challenged to predict which pieces of content its global base of users are likely to click on. Improving Outbrain’s recommendation algorithm will mean more users uncover stories that satisfy their individual tastes.;https://www.kaggle.com/c/outbrain-click-prediction;Outbrain;Can you predict which recommended content each user will click?;['exploratory data analysis'];978.0;0.644;Outbrain Click Prediction;Featured prediction Competition
2016-11-01 00:59:00;With an original Picasso carrying a 106 million dollar price tag, identifying an authentic work of art from a forgery is a high-stakes industry. While algorithms have gotten good at telling us if a still life is of a basket of apples or a sunflower bouquet, they aren't yet able to tell us with certainty if both paintings are by van Gogh.   In this playground competition, we're challenging Kagglers to examine pairs of paintings and determine if they are by the same artist. This is an excellent opportunity to improve your computer vision skills and engage with a unique dataset of art. From the movement of brushstrokes to the use of light and dark, successful algorithms will likely incorporate many aspects of a painter's unique style.  Resources  neural algorithm How Do We See Art: An Eye-Tracker Study  Acknowledgments Many of the images in this dataset were obtained from wikiart.org. Additional paintings were provided by artists whose contributions will be acknowledged at the close of the competition. This playground competition and its datasets were prepared by Small Yellow Duck (Kiri Nichol). This includes the design of the pairwise-evaluation scheme.;https://www.kaggle.com/c/painter-by-numbers;Kaggle;Does every painter leave a fingerprint?;['gpu'];41.0;0.429;Painter by Numbers;Playground prediction Competition
2017-12-16 00:59:00;While long lines and frantically shuffling luggage into plastic bins isn’t a fun experience, airport security is a critical and necessary requirement for safe travel. No one understands the need for both thorough security screenings and short wait times more than U.S. Transportation Security Administration (TSA). They’re responsible for all U.S. airport security, screening more than two million passengers daily. As part of their Apex Screening at Speed Program, DHS has identified high false alarm rates as creating significant bottlenecks at the airport checkpoints. Whenever TSA’s sensors and algorithms predict a potential threat, TSA staff needs to engage in a secondary, manual screening process that slows everything down. And as the number of travelers increase every year and new threats develop, their prediction algorithms need to continually improve to meet the increased demand. Currently, TSA purchases updated algorithms exclusively from the manufacturers of the scanning equipment used. These algorithms are proprietary, expensive, and often released in long cycles. In this competition, TSA is stepping outside their established procurement process and is challenging the broader data science community to help improve the accuracy of their threat prediction algorithms. Using a dataset of images collected on the latest generation of scanners, participants are challenged to identify the presence of simulated threats under a variety of object types, clothing types, and body types. Even a modest decrease in false alarms will help TSA significantly improve the passenger experience while maintaining high levels of security. This is a two-stage competition. Please read our two-stage FAQs to understand more about what this means. All persons contained in the dataset are volunteers who have agreed to have their images used for this competition. The images may contain sensitive content. We kindly request that you conduct yourself with professionalism, respect, and maturity when working with this data.;https://www.kaggle.com/c/passenger-screening-algorithm-challenge;Department of Homeland Security;Improve the accuracy of the Department of Homeland Security's threat recognition algorithms;['exploratory data analysis', 'computer vision', 'gpu', 'cnn'];518.0;0.609;Passenger Screening Algorithm Challenge;Featured prediction Competition
2019-04-10 03:09:00;Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. If homes can be found for them, many precious lives can be saved — and more happy families created. PetFinder.my has been Malaysia’s leading animal welfare platform since 2008, with a database of more than 150,000 animals. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare. Animal adoption rates are strongly correlated to the metadata associated with their online profiles, such as descriptive text and photo characteristics. As one example, PetFinder is currently experimenting with a simple AI tool called the Cuteness Meter, which ranks how cute a pet is based on qualities present in their photos. In this competition you will be developing algorithms to predict the adoptability of pets - specifically, how quickly is a pet adopted? If successful, they will be adapted into AI tools that will guide shelters and rescuers around the world on improving their pet profiles' appeal, reducing animal suffering and euthanization. Top participants may be invited to collaborate on implementing their solutions into AI tools for assessing and improving pet adoption performance, which will benefit global animal welfare.  Important Note Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output.      Photo by Krista Mangulsone on Unsplash;https://www.kaggle.com/c/petfinder-adoption-prediction;PetFinder.my;How cute is that doggy in the shelter?;['feature engineering', 'exploratory data analysis', 'random forest', 'xgboost', 'regression', 'beginner', 'optimization', 'classification', 'data visualization', 'gpu', 'ensembling', 'svm', 'text mining'];2023.0;0.681;PetFinder.my Adoption Prediction;Featured Code Competition
2020-01-22 00:59:00;Who do you think hates traffic more - humans or self-driving cars? The position of nearby automobiles is a key question for autonomous vehicles ― and it's at the heart of our newest challenge.  Self-driving cars have come a long way in recent years, but they're still not flawless. Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles’ ability to accurately perceive objects in traffic. Baidu's Robotics and Autonomous Driving Lab (RAL), along with Peking University, hopes to close the gap once and for all with this challenge. They’re providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images, based on industry-grade CAD car models. Your challenge: develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environment. Succeed and you'll help improve computer vision. That, in turn, will bring autonomous vehicles a big step closer to widespread adoption, so they can help reduce the environmental impact of our growing societies.  Please cite the following paper when using the dataset: ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving @inproceedings{song2019apollocar3d,   title={Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving},   author={Song, Xibin and Wang, Peng and Zhou, Dingfu and Zhu, Rui and Guan, Chenye and Dai, Yuchao and Su, Hao and Li, Hongdong and Yang, Ruigang},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   pages={5452--5462},   year={2019} };https://www.kaggle.com/c/pku-autonomous-driving;Peking University;Can you predict vehicle angle in different settings?;['exploratory data analysis', 'image data', 'utility script', 'beginner', 'data visualization', 'gpu'];864.0;0.637;Peking University/Baidu - Autonomous Driving;Featured prediction Competition
2017-07-21 01:59:00;Every minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively. Planet, designer and builder of the world’s largest constellation of Earth-imaging satellites, will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter resolution. While considerable research has been devoted to tracking changes in forests, it typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250 meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest degradation dominate. Furthermore, these existing methods generally cannot differentiate between human causes of forest loss and natural causes. Higher resolution imagery has already been shown to be exceptionally good at this, but robust methods have not yet been developed for Planet imagery.  In this competition, Planet and its Brazilian partner SCCON are challenging Kagglers to label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond. To dig into/explore more Planet data, sign up for a free account. And if you're interested in building applications on Planet data, check out our Application Developer Program. Getting Started   Review the data page, which includes detailed information about the labels and the labeling process. Download a subsample of the data to get familiar with how it looks. Explore the subsample on Kernels. We’ve created a notebook for you to get started.;https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;Planet;Use satellite data to track the human footprint in the Amazon rainforest;['neural networks', 'multiclass classification', 'exploratory data analysis', 'gpu', 'geospatial analysis'];938.0;0.642;Planet: Understanding the Amazon from Space;Featured prediction Competition
2020-05-27 01:59:00;"Problem Statement Misdiagnosis of the many diseases impacting agricultural crops can lead to misuse of chemicals leading to the emergence of resistant pathogen strains, increased input costs, and more outbreaks with significant economic loss and environmental impacts. Current disease diagnosis based on human scouting is time-consuming and expensive, and although computer-vision based models have the promise to increase efficiency, the great variance in symptoms due to age of infected tissues, genetic variations, and light conditions within trees decreases the accuracy of detection.  Specific Objectives Objectives of ‘Plant Pathology Challenge’ are to train a model using images of training dataset to 1) Accurately classify a given image from testing dataset into different diseased category or a healthy leaf; 2) Accurately distinguish between many diseases, sometimes more than one on a single leaf; 3) Deal with rare classes and novel symptoms; 4) Address depth perception—angle, light, shade, physiological age of the leaf; and 5) Incorporate expert knowledge in identification, annotation, quantification, and guiding computer vision to search for relevant features during learning.  Resources If you use the dataset for your project, please cite the preprint https://arxiv.org/abs/2004.11958 Acknowledgments We acknowledge financial support from Cornell Initiative for Digital Agriculture (CIDA) and special thanks to Zach Guillian for help with data collection.    Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.";https://www.kaggle.com/c/plant-pathology-2020-fgvc7;Fine-Grained Visual Categorization 7;Identify the category of foliar diseases in apple trees;['deep learning', 'exploratory data analysis', 'beginner', 'classification', 'data visualization', 'gpu', 'ensembling', 'plants', 'computer vision'];1317.0;0.66;Plant Pathology 2020 - FGVC7;Research prediction Competition
2018-03-13 00:59:00;Can you differentiate a weed from a crop seedling? The ability to do so effectively can mean better crop yields and better stewardship of the environment. The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.  We're hosting this dataset as a Kaggle competition in order to give it wider exposure, to give the community an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas. Acknowledgments We extend our appreciation to the Aarhus University Department of Engineering Signal Processing Group for hosting the original data.  Citation A Public Image Database for Benchmark of Plant Seedling Classification Algorithms;https://www.kaggle.com/c/plant-seedlings-classification;Kaggle;Determine the species of a seedling from an image;['deep learning', 'data cleaning', 'neural networks', 'beginner', 'data visualization', 'gpu', 'plants', 'computer vision', 'cnn'];834.0;0.635;Plant Seedlings Classification;Playground prediction Competition
2018-12-18 00:59:00;Help some of the world's leading astronomers grasp the deepest properties of the universe. The human eye has been the arbiter for the classification of astronomical sources in the night sky for hundreds of years. But a new facility -- the Large Synoptic Survey Telescope (LSST) -- is about to revolutionize the field, discovering 10 to 100 times more astronomical sources that vary in the night sky than we've ever known. Some of these sources will be completely unprecedented! The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) asks Kagglers to help prepare to classify the data from this new survey. Competitors will classify astronomical sources that vary with time into different classes, scaling from a small training set to a very large test set of the type the LSST will discover.  More background information is available here.  Acknowledgements  PLAsTiCC is funded through LSST Corporation Grant Award # 2017-03 and administered by the University of Toronto.  Financial support for LSST comes from the National Science Foundation (NSF) through Cooperative Agreement No. 1258333, the Department of Energy (DOE) Office of Science under Contract No. DE-AC02-76SF00515, and private funding raised by the LSST Corporation. The NSF-funded LSST Project Office for construction was established as an operating center under management of the Association of Universities for Research in Astronomy (AURA).  The DOE-funded effort to build the LSST camera is managed by the SLAC National Accelerator Laboratory (SLAC). The National Science Foundation (NSF) is an independent federal agency created by Congress in 1950 to promote the progress of science. NSF supports basic research and people to create knowledge that transforms the future.        Photo Credit: M. Park/Inigo Films/LSST/AURA/NSF;https://www.kaggle.com/c/PLAsTiCC-2018;LSST Project;Can you help make sense of the Universe?;['classification', 'astronomy', 'feature engineering'];1094.0;0.65;PLAsTiCC Astronomical Classification;Featured prediction Competition
2017-11-30 00:59:00;Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years. Porto Seguro, one of Brazil’s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones. In this competition, you’re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year. While Porto Seguro has used machine learning for the past 20 years, they’re looking to Kaggle’s machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers.;https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;Porto Seguro;Predict if a driver will file an insurance claim next year.;['binary classification', 'feature engineering', 'dimensionality reduction', 'beginner', 'classification', 'logistic regression'];5163.0;0.723;Porto Seguro’s Safe Driver Prediction;Featured prediction Competition
2013-04-14 14:00:00;Data Science London and the UK Windows Azure Users Group in partnership with Microsoft and Peerindex, announce the Influencers in Social Networks competition as part of  The Big Data Hackathon.   The dataset, provided by Peerindex, comprises a standard, pair-wise preference learning task. Each datapoint describes two individuals, A and B. For each person, 11 p The binary label represents a human judgement about which one of the two individuals is more influential. A label '1' means A is more influential than B. 0 means B is more influential than A. The goal of the challenge is to train a machine learning model which, for pairs of individuals, predicts the human judgement on who is more influential with high accuracy. Labels for the dataset have been collected by PeerIndex using an application similar to the one described in this post. A python script computing a sample benchmark solution is available here:  Competition begins: Saturday, Apr 13, 1pm BST (12 noon UTC)  This competition awards 25% the  ranking points of a standard competition, but does not count towards tiers.;https://www.kaggle.com/c/predict-who-is-more-influential-in-a-social-network;;Predict which people are influential in a social network;[];132.0;0.52;Influencers in Social Networks;Featured prediction Competition
2016-09-20 01:59:00;Like most companies, Red Hat is able to gather a great deal of information over time about the behavior of individuals who interact with them. They’re in search of better methods of using this behavioral data to predict which individuals they should approach—and even when and how to approach them. In this competition, Kagglers are challenged to create a classification algorithm that accurately identifies which customers have the most potential business value for Red Hat based on their characteristics and activities. With an improved prediction model in place, Red Hat will be able to more efficiently prioritize resources to generate more business and better serve their customers.;https://www.kaggle.com/c/predicting-red-hat-business-value;;Classify customer potential;[];2260.0;0.686;Predicting Red Hat Business Value;Featured prediction Competition
2020-07-23 01:59:00;With more than 1 million new diagnoses reported every year, prostate cancer (PCa) is the second most common cancer among males worldwide that results in more than 350,000 deaths annually. The key to decreasing mortality is developing more precise diagnostics. Diagnosis of PCa is based on the grading of prostate tissue biopsies. These tissue samples are examined by a pathologist and scored according to the Gleason grading system. In this challenge, you will develop models for detecting PCa on images of prostate tissue samples, and estimate severity of the disease using the most extensive multi-center dataset on Gleason grading yet available. The grading process consists of finding and classifying cancer tissue into so-called Gleason patterns (3, 4, or 5) based on the architectural growth patterns of the tumor (Fig. 1). After the biopsy is assigned a Gleason score, it is converted into an ISUP grade on a 1-5 scale. The Gleason grading system is the most important prognostic marker for PCa, and the ISUP grade has a crucial role when deciding how a patient should be treated. There is both a risk of missing cancers and a large risk of overgrading resulting in unnecessary treatment. However, the system suffers from significant inter-observer variability between pathologists, limiting its usefulness for individual patients. This variability in ratings could lead to unnecessary treatment, or worse, missing a severe diagnosis.  Automated deep learning systems have shown some promise in accurately grading PCa. Recent research, including two studies independently conducted by the groups hosting this challenge, have shown that these systems can achieve pathologist-level performance. However, these systems/results were not tested with multi-center datasets at scale.     Your work here will improve on these efforts using the most extensive multi-center dataset on Gleason grading yet. The training set consists of around 11,000 whole-slide images of digitized H&E-stained biopsies originating from two centers. This is the largest public whole-slide image dataset available, roughly 8 times the size of the CAMELYON17 challenge, one of the largest digital pathology datasets and best known challenges in the field. Furthermore, in contrast to previous challenges, we are making full diagnostic biopsy images available. Using a sizable multi-center test set, graded by expert uro-pathologists, we will evaluate challenge submissions on their applicability to improve this critical diagnostic function.   Figure 1: An illustration of the Gleason grading process for an example biopsy containing prostate cancer. The most common (blue outline, Gleason pattern 3) and second most common (red outline, Gleason pattern 4) cancer growth patterns present in the biopsy dictate the Gleason score (3+4 for this biopsy), which in turn is converted into an ISUP grade (2 for this biopsy) following guidelines of the International Society of Urological Pathology. Biopsies not containing cancer are represented by an ISUP grade of 0 in this challenge. Radboud University Medical Center and Karolinska Institute have teamed up to organize this competition in collaboration with colleagues from Tampere University. The Computational Pathology Group (CPG) of the Radboud University Medical Center is a research group that develops computer algorithms to aid clinicians. Karolinska Institute’s Department of Medical Epidemiology and Biostatistics (MEB) includes an interdisciplinary research group to improve the diagnostics and treatment of prostate cancer. Together, they hope to further their existing research to make a significant impact on the healthcare of prostate cancer patients. Challenge organizer team: Wouter Bulten, Geert Litjens, Hans Pinckaers, Peter Ström, Martin Eklund, Lars Egevad, Henrik Grönberg, Kimmo Kartasalo, Pekka Ruusuvuori, Tomi Häkkinen, Sohier Dane, Maggie Demkin.  Sponsors The PANDA workshop at MICCAI 2020 is sponsored by ContextVision, Ibex and Google.     Using the data outside of the competition Interested in using the PANDA dataset outside of the competition? Please read this forum post for the latest information on the embargo and the challenge paper.;https://www.kaggle.com/c/prostate-cancer-grade-assessment;PANDA Challenge;Prostate cancer diagnosis using the Gleason grading system;['data cleaning', 'exploratory data analysis', 'image data', 'beginner', 'data visualization', 'computer vision'];1010.0;0.646;Prostate cANcer graDe Assessment (PANDA) Challenge;Featured Code Competition
2016-02-16 00:59:00;Picture this. You are a data scientist in a start-up culture with the potential to have a very large impact on the business. Oh, and you are backed up by a company with 140 years' business experience. Curious? Great! You are the kind of person we are looking for. Prudential, one of the largest issuers of life insurance in the USA, is hiring passionate data scientists to join a newly-formed Data Science group solving complex challenges and identifying opportunities. The results have been impressive so far but we want more.  The Challenge In a one-click shopping world with on-demand everything, the life insurance application process is antiquated. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams, a process that takes an average of 30 days. The result? People are turned off. That’s why only 40% of U.S. households own individual life insurance. Prudential wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries. By developing a predictive model that accurately classifies risk using a more automated approach, you can greatly impact public perception of the industry. The results will help Prudential better understand the predictive power of the data points in the existing assessment, enabling us to significantly streamline the process.;https://www.kaggle.com/c/prudential-life-insurance-assessment;;Can you make buying life insurance easier?;['multiclass classification'];2610.0;0.693;Prudential Life Insurance Assessment;Featured prediction Competition
2019-01-31 00:59:00;"So, where we droppin' boys and girls? Battle Royale-style video games have taken the world by storm. 100 players are dropped onto an island empty-handed and must explore, scavenge, and eliminate other players until only one is left standing, all while the play zone continues to shrink.  PlayerUnknown's BattleGrounds (PUBG) has enjoyed massive popularity. With over 50 million copies sold, it's the fifth best selling game of all time, and has millions of active monthly players.   The team at PUBG has made official game data available for the public to explore and scavenge outside of ""The Blue Circle."" This competition is not an official or affiliated PUBG site - Kaggle collected data made possible through the PUBG Developer API. You are given over 65,000 games' worth of anonymized player data, split into training and testing sets, and asked to predict final placement from final in-game stats and initial player ratings.  What's the best strategy to win in PUBG? Should you sit in one spot and hide your way into victory, or do you need to be the top shot? Let's let the data do the talking!";https://www.kaggle.com/c/pubg-finish-placement-prediction;Kaggle;Can you predict the battle royale finish of PUBG Players?;['data cleaning', 'exploratory data analysis', 'feature engineering', 'video games', 'outlier analysis', 'xgboost', 'regression', 'beginner', 'data visualization', 'ensembling', 'gradient boosting'];1534.0;0.667;PUBG Finish Placement Prediction (Kernels Only);Playground Code Competition
2018-12-05 00:59:00;"""Quick, Draw!"" was released as an experimental game to educate the public in a playful way about how AI works. The game prompts users to draw an image depicting a certain category, such as ”banana,” “table,” etc. The game generated more than 1B drawings, of which a subset was publicly released as the basis for this competition’s training set. That subset contains 50M drawings encompassing 340 label categories. Sounds fun, right? Here's the challenge: since the training data comes from the game itself, drawings can be incomplete or may not match the label. You’ll need to build a recognizer that can effectively learn from this noisy data and perform well on a manually-labeled test set from a different distribution. Your task is to build a better classifier for the existing Quick, Draw! dataset. By advancing models on this dataset, Kagglers can improve pattern recognition solutions more broadly. This will have an immediate impact on handwriting recognition and its robust applications in areas including OCR (Optical Character Recognition), ASR (Automatic Speech Recognition) & NLP (Natural Language Processing).";https://www.kaggle.com/c/quickdraw-doodle-recognition;Google Research;How accurately can you identify a doodle?;['multiclass classification', 'neural networks', 'image data', 'classification', 'data visualization', 'gpu', 'computer vision'];1316.0;0.66;Quick, Draw! Doodle Recognition Challenge;Featured prediction Competition
2019-02-14 00:09:00;An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world. Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers. In this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content. Here's your chance to combat online trolls at scale. Help Quora uphold their policy of “Be Nice, Be Respectful” and continue to be a place for sharing and growing the world’s knowledge. Important Note Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output. Please read the Kernels FAQ and the data page very carefully to fully understand how this is designed.;https://www.kaggle.com/c/quora-insincere-questions-classification;Quora;Detect toxic content to improve online conversations;['deep learning', 'nlp', 'exploratory data analysis', 'beginner', 'classification', 'data visualization', 'gpu', 'text data', 'text mining'];4037.0;0.712;Quora Insincere Questions Classification;Featured Code Competition
2017-06-07 01:59:00;Where else but Quora can a physicist help a chef with a math problem and get cooking tips in return? Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world. Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term. Currently, Quora uses a Random Forest model to identify duplicate questions. In this competition, Kagglers are challenged to tackle this natural language processing problem by applying advanced techniques to classify whether question pairs are duplicates or not. Doing so will make it easier to find high quality answers to questions resulting in an improved experience for Quora writers, seekers, and readers.;https://www.kaggle.com/c/quora-question-pairs;Quora;Can you identify question pairs that have the same intent?;['nlp', 'deep learning', 'exploratory data analysis', 'feature engineering', 'neural networks', 'xgboost', 'beginner', 'intermediate', 'data visualization', 'cnn'];3304.0;0.703;Quora Question Pairs;Featured prediction Competition
2015-06-02 01:59:00;"Get started on this competition through Kaggle Scripts In machine learning, it is often said there are no free lunches. How wrong we were. This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness. ""I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,"" says one hopeful poster. What about making an algorithm?  Kaggle is hosting this competition for the machine learning community to use for fun and practice. This data was collected and graciously shared by Althoff et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work: Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014.";https://www.kaggle.com/c/random-acts-of-pizza;Kaggle;Predicting altruism through free pizza;[];462.0;0.602;Random Acts of Pizza;Playground prediction Competition
2019-08-09 01:59:00;"Do you have your father’s nose?  Blood relatives often share facial features. Now researchers at Northeastern University want to improve their algorithm for facial image classification to bridge the gap between research and other familial markers like DNA results. That will be your challenge in this new Kaggle competition. An automatic kinship classifier has been in the works at Northeastern since 2010. Yet this technology remains largely unseen in practice for a couple of reasons: 1. Existing image databases for kinship recognition tasks aren't large enough to capture and reflect the true data distributions of the families of the world. 2. Many hidden factors affect familial facial relationships, so a more discriminant model is needed than the computer vision algorithms used most often for higher-level categorizations (e.g. facial recognition or object classification). In this competition, you’ll help researchers build a more complex model by determining if two people are blood-related based solely on images of their faces. If you think you can get it ""on the nose,"" this competition is for you.  The SMILE Lab at Northeastern focuses on the frontier research of applied machine learning, social media analytics, human-computer interaction, and high-level image and video understanding. Their research is driven by the explosion of diverse multimedia from the Internet, including both personal and publicly-available photos and videos. They start by treating fundamental theory from learning algorithms as the soul of machine intelligence and arm it with visual perception.";https://www.kaggle.com/c/recognizing-faces-in-the-wild;Northeastern SMILE Lab;Can you determine if two individuals are related?;['deep learning', 'gpu'];528.0;0.61;Northeastern SMILE Lab - Recognizing Faces in the Wild;Playground prediction Competition
2018-02-07 00:59:00;Running a thriving local restaurant isn't always as charming as first impressions appear. There are often all sorts of unexpected troubles popping up that could hurt business. One common predicament is that restaurants need to know how many customers to expect each day to effectively purchase ingredients and schedule staff members. This forecast isn't easy to make because many unpredictable factors affect restaurant attendance, like weather and local competition. It's even harder for newer restaurants with little historical data. Recruit Holdings has unique access to key datasets that could make automated future customer prediction possible. Specifically, Recruit Holdings owns Hot Pepper Gourmet (a restaurant review service), AirREGI (a restaurant point of sales service), and Restaurant Board (reservation log management software). In this competition, you're challenged to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be much more efficient and allow them to focus on creating an enjoyable dining experience for their customers.;https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;Recruit Holdings;Predict how many future visitors a restaurant will receive;['ensembling', 'neural networks'];2157.0;0.684;Recruit Restaurant Visitor Forecasting;Featured prediction Competition
2019-09-27 01:59:00;The cost of some drugs and medical treatments has risen so high in recent years that many patients are having to go without. You can help with a classification project that could make researchers more efficient. One of the more surprising reasons behind the cost is how long it takes to bring new treatments to market. Despite improvements in technology and science, research and development continues to lag. In fact, finding new treatments takes, on average, more than 10 years and costs hundreds of millions of dollars.  Recursion Pharmaceuticals, creators of the industry’s largest dataset of biological images, generated entirely in-house, believes AI has the potential to dramatically improve and expedite the drug discovery process. More specifically, your efforts could help them understand how drugs interact with human cells.  This competition will have you disentangling experimental noise from real biological signals. Your entry will classify images of cells under one of 1,108 different genetic perturbations. You can help eliminate the noise introduced by technical execution and environmental variation between experiments. If successful, you could dramatically improve the industry’s ability to model cellular images according to their relevant biology. In turn, applying AI could greatly decrease the cost of treatments, and ensure these treatments get to patients faster. This competition is a part of the NeurIPS 2019 competition track. Winners will be invited to contribute their solutions towards the workshop presentation. Acknowledgments Thank you to the following sponsors & supporters of this competition:     Google Cloud: Google Cloud is widely recognized as a global leader in delivering a secure, open and intelligent enterprise cloud platform. Our technology is built on Google’s private network and is the product of nearly 20 years of innovation in security, network architecture, collaboration, artificial intelligence and open source software. We offer a simply engineered set of tools and unparalleled technology across Google Cloud Platform and G Suite that help bring people, insights and ideas together. Customers across more than 150 countries trust Google Cloud to modernize their computing environment for today’s digital world.   DoiT: You have the cloud and we have your back. For nearly a decade, we’ve been helping businesses build and scale cloud solutions with our world-class cloud engineering support. We help our customers with technical support and consulting on building and operating complex large-scale distributed systems, developing better machine learning models and setting up big data solutions using Google Cloud, Amazon AWS and Microsoft Azure.   NVIDIA: NVIDIA’s (NASDAQ: NVDA) invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined modern computer graphics and revolutionized parallel computing. More recently, GPU deep learning ignited modern AI — the next era of computing — with the GPU acting as the brain of computers, robots and self-driving cars that can perceive and understand the world. More information at http://nvidianews.nvidia.com.   Lambda: Lambda provides Deep Learning workstations, servers, and GPU cloud services. Lambda Deep Learning infrastructure is used by the world's leading AI research & development organizations including Apple, Microsoft, MIT, Stanford, and the US Government. To learn more, visit www.lambdalabs.com.;https://www.kaggle.com/c/recursion-cellular-image-classification;Recursion Pharmaceuticals;CellSignal: Disentangling biological signal from experimental noise in cellular images;['deep learning', 'exploratory data analysis', 'beginner', 'classification', 'data visualization', 'gpu'];866.0;0.638;Recursion Cellular Image Classification;Research prediction Competition
2019-02-13 00:59:00;Most flight-related fatalities stem from a loss of “airplane state awareness.” That is, ineffective attention management on the part of pilots who may be distracted, sleepy or in other dangerous cognitive states. Your challenge is to build a model to detect troubling events from aircrew’s physiological data. You'll use data acquired from actual pilots in test situations, and your models should be able to run calculations in real time to monitor the cognitive states of pilots. With your help, pilots could then be alerted when they enter a troubling state, preventing accidents and saving lives. Reducing aircraft fatalities is just one of the complex problems that  Booz Allen Hamilton has been solving for business, government, and military leaders for over 100 years. Through devotion, candor, courage, and character, they produce original solutions where there are no roadmaps. Now you can help them find answers, save lives, and change the world.;https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;Booz Allen Hamilton;Can you tell when a pilot is heading for trouble?;['data cleaning', 'exploratory data analysis', 'feature engineering', 'signal processing', 'india', 'beginner', 'data visualization', 'gpu', 'medicine', 'gradient boosting'];178.0;0.541;Reducing Commercial Aviation Fatalities;Playground prediction Competition
2015-05-05 01:59:00;With over 1,200 quick service restaurants across the globe, TFI is the company behind some of the world's most well-known brands: Burger King, Sbarro, Popeyes, Usta Donerci, and Arby’s. They employ over 20,000 people in Europe and Asia and make significant daily investments in developing new restaurant sites. Right now, deciding when and where to open new restaurants is largely a subjective process based on the personal judgement and experience of development teams. This subjective data is difficult to accurately extrapolate across geographies and cultures.  New restaurant sites take large investments of time and capital to get up and running. When the wrong location for a restaurant brand is chosen, the site closes within 18 months and operating losses are incurred.  Finding a mathematical model to increase the effectiveness of investments in new restaurant sites would allow TFI to invest more in other important business areas, like sustainability, innovation, and training for new employees. Using demographic, real estate, and commercial data, this competition challenges you to predict the annual restaurant sales of 100,000 regional locations. TFI would love to hire an expert Kaggler like you to head up their growing data science team in Istanbul or Shanghai. You'd be tackling problems like the one featured in this competition on a global scale. See the job description here >>;https://www.kaggle.com/c/restaurant-revenue-prediction;;Predict annual restaurant sales based on objective measurements;['feature engineering', 'random forest', 'xgboost', 'lightgbm', 'ensembling'];2257.0;0.686;Restaurant Revenue Prediction;Featured prediction Competition
2015-12-15 00:59:00;Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.;https://www.kaggle.com/c/rossmann-store-sales;;Forecast sales using store, promotion, and competitor data;['gpu'];3298.0;0.703;Rossmann Store Sales;Featured prediction Competition
2019-11-14 00:59:00;Intracranial hemorrhage, bleeding that occurs inside the cranium, is a serious health problem requiring rapid and often intensive medical treatment. For example, intracranial hemorrhages account for approximately 10% of strokes in the U.S., where stroke is the fifth-leading cause of death. Identifying the location and type of any hemorrhage present is a critical step in treating the patient.  Diagnosis requires an urgent procedure. When a patient shows acute neurological symptoms such as severe headache or loss of consciousness, highly trained specialists review medical images of the patient’s cranium to look for the presence, location and type of hemorrhage. The process is complicated and often time consuming.  In this competition, your challenge is to build an algorithm to detect acute intracranial hemorrhage and its subtypes.  You’ll develop your solution using a rich image dataset provided by the Radiological Society of North America (RSNA®) in collaboration with members of the American Society of Neuroradiology and MD.ai.  If successful, you’ll help the medical community identify the presence, location and type of hemorrhage in order to quickly and effectively treat affected patients. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from December 1-6, 2019. Collaborators Four research institutions provided large volumes of de-identified CT studies that were assembled to create the challenge dataset: Stanford University, Thomas Jefferson University, Unity Health Toronto and Universidade Federal de São Paulo (UNIFESP), The American Society of Neuroradiology (ASNR) organized a cadre of more than 60 volunteers to label over 25,000 exams for the challenge dataset. ASNR is the world’s leading organization for the future of neuroradiology representing more than 5,300 radiologists, researchers, interventionalists, and imaging scientists. MD.ai provided tooling and support for the data annotation process.  The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for AI to assist in detection and classification of hemorrhages in order to prioritize and expedite their clinical work. A full set of acknowledgments can be found on this page.;https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection;Radiological Society of North America;Identify acute intracranial hemorrhage and its subtypes;['deep learning', 'data cleaning', 'exploratory data analysis', 'image data', 'beginner', 'classification', 'data visualization', 'gpu', 'computer vision'];1345.0;0.661;RSNA Intracranial Hemorrhage Detection;Featured prediction Competition
2018-11-01 00:59:00;"In this competition, you’re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here’s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA®) has reached out to Kaggle’s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge.  The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018. Acknowledgements Thank you to the National Institutes of Health Clinical Center for publicly providing the Chest X-Ray dataset [5].  NIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community Original source files and documents  Also, a big thank you to the competition organizers! References  Rui P, Kang K. National Ambulatory Medical Care Survey: 2015 Emergency Department Summary Tables.  Table 27.  Available from: www.cdc.gov/nchs/data/nhamcs/webtables/2015edwebtables.pdf Deaths: Final Data for 2015.  Supplemental Tables. Tables I-21, I-22.  Available from: www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr6606tables.pdf Franquet T.  Imaging of community-acquired pneumonia.  J Thorac Imaging 2018 (epub ahead of print).  PMID 30036297 Kelly B.  The Chest Radiograph. Ulster Med J 2012;81(3):143-148 Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf";https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;Radiological Society of North America;Can you build an algorithm that automatically detects potential pneumonia cases?;['deep learning', 'data cleaning', 'exploratory data analysis', 'image data', 'beginner', 'classification', 'data visualization', 'gpu', 'transfer learning', 'medicine', 'cnn'];1499.0;0.666;RSNA Pneumonia Detection Challenge;Featured prediction Competition
2020-10-27 00:59:00;If every breath is strained and painful, it could be a serious and potentially life-threatening condition. A pulmonary embolism (PE) is caused by an artery blockage in the lung. It is time consuming to confirm a PE and prone to overdiagnosis. Machine learning could help to more accurately identify PE cases, which would make management and treatment more effective for patients. Currently, CT pulmonary angiography (CTPA), is the most common type of medical imaging to evaluate patients with suspected PE. These CT scans consist of hundreds of images that require detailed review to identify clots within the pulmonary arteries. As the use of imaging continues to grow, constraints of radiologists’ time may contribute to delayed diagnosis. The Radiological Society of North America (RSNA®) has teamed up with the Society of Thoracic Radiology (STR) to help improve the use of machine learning in the diagnosis of PE. In this competition, you’ll detect and classify PE cases. In particular, you'll use chest CTPA images (grouped together as studies) and your data science skills to enable more accurate identification of PE. If successful, you'll help reduce human delays and errors in detection and treatment. With 60,000-100,000 PE deaths annually in the United States, it is among the most fatal cardiovascular diseases. Timely and accurate diagnosis will help these patients receive better care and may also improve outcomes.  This is a Code Competition. Refer to Code Requirements for details.  Acknowledgments The Radiological Society of North America (RSNA®) is an international society of radiologists, medical physicists, and other medical professionals with more than 53,400 members worldwide.  RSNA hosts the world’s premier radiology forum and publishes two top peer-reviewed journals: Radiology, the highest-impact scientific journal in the field, and RadioGraphics, the only journal dedicated to continuing education in radiology.  The Society of Thoracic Radiology (STR) was founded in 1982.  The STR is dedicated to advancing cardiothoracic imaging in clinical application, education, and research in radiology and allied disciplines.  Continuing professional development opportunities provided by the STR include educational and scientific meetings, mentorship programs, grant support and award opportunities, our society journal, Journal of Thoracic Imaging, and global collaboration activities. A full set of acknowledgments can be found on this page.;https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;Radiological Society of North America;Classify Pulmonary Embolism cases in chest CT scans;['keras', 'exploratory data analysis', 'beginner', 'data visualization', 'gpu', 'transfer learning', 'medicine', 'computer vision'];784.0;0.632;RSNA STR Pulmonary Embolism Detection;Featured Code Competition
2018-01-13 00:59:00;‘Tis the night before Christmas year: two thousand seventeen. Santa’s grown grouchy, borderline mean. What used to be simple for Old St. Nick, is now too puzzling, it’s making him sick! See, Santa always knew, deep down in his gut,  what toy each kid wanted–no ifs, ands, or buts. But fierce population growth, more twins, and toy innovation, has left too complex a problem, in dire need of optimization. “Don’t worry, Mr. Santa”, said an Elf named McMaggle, “I have a solution! Have you heard of Kaggle?” As she explained Kaggle in-depth, Santa’s doubt began turning, he became a believer in the magic of...machine learning. So, Santa’s team needs YOU more than ever this year, to solve this painful problem and save Christmas cheer. The Challenge In this playground competition, you’re challenged to build a toy matching algorithm that maximizes happiness by pairing kids with toys they want. In the dataset, each kid has 10 preferences for their gift (from 1000) and Santa has 1000 preferred kids for every gift available. What makes this extra difficult is that 0.4% of the kids are twins, and by their parents’ request, require the same gift.;https://www.kaggle.com/c/santa-gift-matching;Kaggle;Down through the chimney with lots of toys...;['optimization'];428.0;0.597;Santa Gift Matching Challenge;Featured prediction Competition
2020-01-16 00:59:00;Hammers ring, are you listenin’ In the shop, toys are glistenin’ Should they see the sights? There might be a fight… Walkin’ ‘round the Workshop Wonderland  Families said, they want to see it Santa said, he’d guarantee it They pick a date But they may have to wait Walkin’  ‘round the Workshop Wonderland We told Santa that he was a madman He just wants to make sure they all smile He’ll say “Are you flexible?“, They’ll say “Yeah man, But can you help us make it worth our while?” “Give them food, or sweater the more they wait, the gifts get better” Please help us rank Or we’ll break the bank! Walkin’ ’round the Workshop Wonderland Santa has exciting news! For 100 days before Christmas, he opened up tours to his workshop. Because demand was so strong, and because Santa wanted to make things as fair as possible, he let each of the 5,000 families that will visit the workshop choose a list of dates they'd like to attend the workshop. Now that all the families have sent Santa their preferences, he's realized it's impossible for everyone to get their top picks, so he's decided to provide extra perks for families that don't get their preferences. In addition, Santa's accounting department has told him that, depending on how families are scheduled, there may be some unexpected and hefty costs incurred. Santa needs the help of the Kaggle community to optimize which day each family is assigned to attend the workshop in order to minimize any extra expenses that would cut into next years toy budget! Can you help Santa out? Attribution Banner/Listing Photo by Nathan Lemon on Unsplash Description Photo by Markus Spiske on Unsplash;https://www.kaggle.com/c/santa-workshop-tour-2019;Kaggle;In the notebook we can build a model, and pretend that it will optimize...;['utility script', 'exploratory data analysis', 'data visualization'];1620.0;0.67;Santa's Workshop Tour 2019;Featured prediction Competition
2016-05-03 01:59:00;From frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving. Santander Bank is asking Kagglers to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late. In this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.;https://www.kaggle.com/c/santander-customer-satisfaction;Banco Santander;Which customers are happy customers?;['beginner', 'exploratory data analysis', 'feature engineering'];5115.0;0.722;Santander Customer Satisfaction;Featured prediction Competition
2019-04-11 01:59:00;At Santander  our mission is to help people and businesses prosper.  We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.  Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure  we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan? In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.;https://www.kaggle.com/c/santander-customer-transaction-prediction;Banco Santander;Can you identify who will make a transaction?;['binary classification', 'exploratory data analysis', 'feature engineering', 'naive bayes', 'beginner', 'classification', 'data visualization', 'model explainability', 'gpu'];8802.0;0.744;Santander Customer Transaction Prediction;Featured prediction Competition
2016-12-22 00:59:00;Ready to make a downpayment on your first house? Or looking to leverage the equity in the home you have? To support needs for a range of financial decisions, Santander Bank offers a lending hand to their customers through personalized product recommendations.  Under their current system, a small number of Santander’s customers receive many recommendations while many others rarely see any resulting in an uneven customer experience. In their second competition, Santander is challenging Kagglers to predict which products their existing customers will use in the next month based on their past behavior and that of similar customers. With a more effective recommendation system in place, Santander can better meet the individual needs of all customers and ensure their satisfaction no matter where they are in life. Disclaimer: This data set does not include any real Santander Spain's customer, and thus it is not representative of Spain's customer base.;https://www.kaggle.com/c/santander-product-recommendation;Banco Santander;Can you pair products with people?;[];1779.0;0.675;Santander Product Recommendation;Featured prediction Competition
2018-08-21 01:59:00;According to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception.  The digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner… and often before they´ve even realized they need the service.  In their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction.  This means anticipating customer needs in a more concrete, but also simple and personal way.  With so many choices for financial services, this need is greater now than ever before.  In this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.;https://www.kaggle.com/c/santander-value-prediction-challenge;Banco Santander;Predict the value of transactions for potential customers.;['data cleaning', 'feature engineering', 'exploratory data analysis', 'dimensionality reduction', 'beginner', 'data visualization', 'gpu', 'ensembling'];4477.0;0.717;Santander Value Prediction Challenge;Featured prediction Competition
2016-01-27 00:59:00;Do you laugh (and then get down to work) in the face of terabytes of noisy, non-stationary data? Winton Capital is looking for data scientists who excel at finding the hidden signal in the proverbial haystack, and who are excited by creating novel statistical modelling and data mining techniques.  In this recruiting competition, Winton challenges you to take on the very difficult task of predicting the future (stock returns). Given historical stock performance and a host of masked features, can you predict intra and end of day returns without being deceived by all the noise?  Research scientists at Winton have crafted this competition to be challenging and fun for the community while providing a taste of the types of problems they work on everyday. They're excited to connect with Kagglers who bring a unique background and creative approach to the competition. Winton is offering cash prizes to winning teams as a reward for their work, but the intent of the competition is not commercial. The intellectual property you create remains your own and will be evaluated in the context of suitability for employment.   For more on the culture at Winton, check out the About Winton page or their careers page.;https://www.kaggle.com/c/the-winton-stock-market-challenge;;Join a multi-disciplinary team of research scientists;['xgboost', 'finance'];829.0;0.635;The Winton Stock Market Challenge;Featured prediction Competition
2019-05-31 01:59:00;"We're going to make you an offer you can't refuse: a Kaggle competition!  In a world… where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's ""You had me at 'Hello.'"" For others, the trailer falls short of expectations and you think ""What we have here is a failure to communicate."" In this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries.  You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release. Join in, ""make our day"", and then ""you've got to ask yourself one question: 'Do I feel lucky?'""";https://www.kaggle.com/c/tmdb-box-office-prediction;Kaggle;Can you predict a movie's worldwide box office revenue?;['data cleaning', 'exploratory data analysis', 'feature engineering', 'regression', 'beginner', 'data visualization', 'gpu'];1398.0;0.663;TMDB Box Office Prediction;Playground prediction Competition
2018-08-14 01:59:00;To explore what our universe is made of, scientists at CERN are colliding protons, essentially recreating mini big bangs, and meticulously observing these collisions with intricate silicon detectors. While orchestrating the collisions and observations is already a massive scientific accomplishment, analyzing the enormous amounts of data produced from the experiments is becoming an overwhelming challenge. Event rates have already reached hundreds of millions of collisions per second, meaning physicists must sift through tens of petabytes of data per year. And, as the resolution of detectors improve, ever better software is needed for real-time pre-processing and filtering of the most promising events, producing even more data. To help address this problem, a team of Machine Learning experts and physics scientists working at CERN (the world largest high energy physics laboratory),  has partnered with Kaggle and prestigious sponsors to answer the question: can machine learning assist high energy physics in discovering and characterizing new particles? Specifically, in this competition, you’re challenged to build an algorithm that quickly reconstructs particle tracks from 3D points left in the silicon detectors. This challenge consists of two phases:   The Accuracy phase has run on Kaggle from May to 13th August 2018 (Winners to be announced by end September). Here we’ll be focusing on the highest score, irrespective of the evaluation time. This phase is an official IEEE WCCI competition (Rio de Janeiro, Jul 2018).    The Throughput phase will run on Codalab starting in September 2018. Participants will submit their software which is evaluated by the platform. Incentive is on the throughput (or speed) of the evaluation while reaching a good score. This phase is an official NIPS competition (Montreal, Dec 2018).   All the necessary information for the Accuracy phase is available here on Kaggle site. The overall TrackML challenge web site is there.;https://www.kaggle.com/c/trackml-particle-identification;CERN;High Energy Physics particle tracking in CERN detectors;['exploratory data analysis', 'beginner', 'data visualization', 'clustering', 'physics'];648.0;0.621;TrackML Particle Tracking Challenge;Featured prediction Competition
2017-03-26 00:59:00;"What does physics have in common with biology, cooking, cryptography, diy, robotics, and travel? If you answered ""all pursuits are governed by the immutable laws of physics"" we'll begrudgingly give you partial credit. If you answered ""all were chosen randomly by a scheming Kaggle employee for a twisted transfer learning competition"", congratulations, we accept your answer and mark the question as solved. In this competition, we provide the titles, text, and tags of Stack Exchange questions from six different sites. We then ask for tag predictions on unseen physics questions. Solving this problem via a standard machine approach might involve training an algorithm on a corpus of related text. Here, you are challenged to train on material from outside the field. Can an algorithm learn appropriate physics tags from ""extreme-tourism Antarctica""? Let's find out. Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from the Stack Exchange data dump.";https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;;Predict tags from models trained on unrelated topics;['data cleaning', 'beginner', 'data visualization', 'clustering', 'text data'];380.0;0.59;Transfer Learning on Stack Exchange Tags;Playground prediction Competition
2019-01-11 00:59:00;Rudolph the red-nosed reindeer Had some very tired hooves But he had a job to finish Could he do it with the shortest moves? All of the other reindeer Used to laugh and mock his code They always said poor Rudolph Couldn't handle the workload Then one foggy Christmas Eve Santa came to say I see you've taken number theory Please make this night a bit less dreary? Then how the reindeer loved him and each enrolled in an AI degree Rudolph the red-nosed reindeer We get to go to bed early! Rudolph has always believed in working smarter, not harder. And what better way to earn the respect of Comet and Blitzen than showing the initiative to improve Santa's annual route for delivering toys on Christmas Eve? This year,  Rudolph believes he can motivate the overworked Reindeer team by wisely choosing the order in which they visit the houses on Santa's list. The houses in prime cities always leave carrots for the Reindeers alongside the usual cookies and milk. These carrots are just the sustenance the Reindeers need to keep pace. In fact, Rudolph has found that if the Reindeer team doesn't originate from a prime city exactly every 10th step, it takes the 10% longer than it normally would to make their next destination! Can you help Rudolph solve the Traveling Santa problem subject to his carrot constraint? His team--and Santa--are counting on you! Attributions: Reindeer Photo: Norman Tsui Stocking Photo:  Wesley Tingey;https://www.kaggle.com/c/traveling-santa-2018-prime-paths;Kaggle;But does your code recall, the most efficient route of all?;['beginner', 'optimization', 'data visualization'];1871.0;0.677;Traveling Santa 2018 - Prime Paths;Featured prediction Competition
2020-06-03 13:00:00;"LAUNCHED   This competition was launched and opened for submissions on May 27th 2020. Submissions will close in 1 week at 11:00 AM UTC on June 3rd 2020. The public leaderboard is based on the TREC-COVID Round 2 dataset. The private leaderboard will be based on the Round 3 dataset, which will be evaluated after the competition closes. Review the Data page for more details.  Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presents a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge will identify answers for some of today's questions while building infrastructure to improve tomorrow's search systems. Kaggle first teamed up with the Allen Institute for AI in the launch of the COVID-19 Open Research Dataset (CORD-19). TREC-COVID builds on the CORD-19 Challenge by using the same document set, a collection of biomedical literature articles that has been updated on a weekly rolling basis.  This is the 3rd Round of the TREC-COVID Challenge. Prior runs were hosted directly on the TREC-COVID Site. For this round, you have the option to submit on Kaggle or directly to the TREC-COVID platform. The organizers have added 5 additional COVID-related topics to the 35 topics from the first two rounds, for a total of 40 topics. You will create a retrieval system that returns ranked lists of documents from CORD-19 for (a) each of these additional Round 3 topics (""runs"") and as well as (b) residual rankings on the completed Round 1 & 2 topics, i.e., for any documents not judged in the CORD-19 dataset (not previously included as a ranked document).  The eligible population of documents for Round 3 is anything included in the CORD-19 release up to Round 3's launch date, last updated on May 19th 2020. Following the close of Round 3, NIST will gather the collective set of participants' runs, to include those participants submitting directly through TREC-COVID. The organizers will then assess some reasonable subset of these submissions for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, will then be used to score the submitted runs. It is important to understand that not all documents will be assessed, and thus the private leaderboard score will be based on partial document assessment. With your help, the final document and topic sets together with the cumulative relevance judgments will comprise a COVID test collection. The incremental nature of the collection will support research on search systems for dynamic environments. Acknowledgments The Text REtrieval Conference (TREC) was founded in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. The TREC-COVID Challenge is being organized by the Allen Institute for Artificial Intelligence (AI2), the National Institute of Standards and Technology (NIST), the National Library of Medicine (NLM), Oregon Health and Science University (OHSU), and the University of Texas Health Science Center at Houston (UTHealth). See the NIST press release for more information.";https://www.kaggle.com/c/trec-covid-information-retrieval;TREC-COVID Organizers;Build a pandemic document retrieval system;[];19.0;0.362;TREC-COVID Information Retrieval;Research prediction Competition
2020-06-30 01:59:00;Human brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.  In this competition, you will predict multiple assessments plus age from multimodal brain MRI features. You will be working from existing results from other data scientists, doing the important work of validating the utility of multimodal features in a normative population of unaffected subjects. Due to the complexity of the brain and differences between scanners, generalized approaches will be essential to effectively propel multimodal neuroimaging research forward. The Tri-Institutional Georgia State University/Georgia Institute of Technology/Emory University Center for Translational Research in Neuroimaging and Data Science (TReNDS) leverages advanced brain imaging to promote research into brain health. The organization is focused on developing, applying and sharing advanced analytic approaches and neuroinformatics tools. Among its software projects are the GIFT and FIT neuroimaging toolboxes, the COINS data management system, and the COINSTAC toolkit for federated learning, all aimed at supporting data scientists and other neuroimaging researchers. Making the leap from research to clinical application is particularly difficult in brain health. In order to translate to clinical settings, research findings have to be reproduced consistently and validated in out-of-sample instances. The problem is particularly well-suited for data science, but current approaches typically do not generalize well. With this large dataset and competition, your efforts could directly address an important area of brain research. Acknowledgments;https://www.kaggle.com/c/trends-assessment-prediction;GSU/TReNDS;Multiscanner normative age and assessments prediction with brain function, structure, and connectivity;['deep learning', 'exploratory data analysis', 'feature engineering', 'random forest', 'neural networks', 'beginner', 'data visualization', 'gpu'];1047.0;0.648;TReNDS Neuroimaging;Research prediction Competition
2020-06-17 01:59:00;"""My ridiculous dog is amazing."" [sentiment: positive] With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds.  But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment. Help build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools? In this competition we've extracted support phrases from Figure Eight's Data for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.";https://www.kaggle.com/c/tweet-sentiment-extraction;Kaggle;Extract support phrases for sentiment labels;[];2227.0;0.685;Tweet Sentiment Extraction;Featured Code Competition
2017-04-26 01:59:00;Finding the perfect place to call your new home should be more than browsing through endless listings. RentHop makes apartment search smarter by using data to sort rental listings by quality. But while looking for the perfect apartment is difficult enough, structuring and making sense of all available real estate data programmatically is even harder. Two Sigma and RentHop, a portfolio company of Two Sigma Ventures, invite Kagglers to unleash their creative engines to uncover business value in this unique recruiting competition.   Two Sigma invites you to apply your talents in this recruiting competition featuring rental listing data from RentHop. Kagglers will predict the number of inquiries a new listing receives based on the listing’s creation date and other features. Doing so will help RentHop better handle fraud control, identify potential listing quality issues, and allow owners and agents to better understand renters’ needs and preferences. Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. This challenge is an opportunity for competitors to gain a sneak peek into Two Sigma's data science work outside of finance. Acknowledgments This competition is co-hosted by Two Sigma and RentHop (a portfolio company of Two Sigma Ventures, which is a division of Two Sigma Investments) to encourage creativity in using real world data to solve everyday problems.;https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;Two Sigma;How much interest will a new rental listing on RentHop receive?;[];2480.0;0.69;Two Sigma Connect: Rental Listing Inquiries;Recruitment prediction Competition
2017-03-02 00:59:00;"How can we use the world’s tools and intelligence to forecast economic outcomes that can never be entirely predictable? This question is at the core of countless economic activities around the world – including at Two Sigma Investments, who has been applying technology and systematic strategies to financial trading since 2001. For over 15 years, Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. Through this exclusive partnership, Two Sigma is excited to explore what untapped value Kaggle's diverse data science community can discover in the financial markets. Economic opportunity depends on the ability to deliver singularly accurate forecasts in a world of uncertainty. By accurately predicting financial movements, Kagglers will learn about scientifically-driven approaches to unlocking significant predictive capability. Two Sigma is excited to find predictive value and gain a better understanding of the skills offered by the global data science crowd. What is a Code Competition? Welcome to Kaggle's very first Code Competition! In contrast to our traditional competitions, where competitors submit only prediction outputs, participants in Code Competitions will submit their code via Kaggle Kernels. All kernels are private by default in Code Competitions. You can build your models in Kernels by running them on a training set and, once you're ready to submit your code, your model's performance will be evaluated against the test set and your score and public leaderboard position revealed. As with our traditional competitions, we still maintain a private leaderboard test set, which your code is also evaluated against for final scoring, but is not revealed until the competition closes. Since Code Competitions are brand new, we ask for your patience if you encounter bugs or frustrating platform quirks. Please report any issues you find in the forums and we'll do our best to respond. Who owns my code? You do. Even though you are submitting code, the intellectual property exchange here works similarly to a standard prediction competition, whereby prize winners have the option to grant a non-exclusive license in exchange for a prize. There is a new addition to the terms for Code Competitions: Kaggle and the competition host reserve a right to review submissions ""for purposes related to evaluation and scoring in this Competition, including but not limited to the assessment of potential cheating behavior."" Please refer to the official competition rules for full details. Getting Started  Review the data page for details about the data and the evaluation metric. You may download the train set for local training. Take a look at the tutorial covering the new code submission process under the submission instructions tab. You'll find step-by-step instructions, some helpful pointers, plus details on environment constraints. Get feedback on your benchmark code and share exploratory analyses with the community by making any of your kernels public. Improve your score!  Note: there is no cost of entry for participation.";https://www.kaggle.com/c/two-sigma-financial-modeling;Two Sigma;Can you uncover predictive value in an uncertain world?;[];2066.0;0.682;Two Sigma Financial Modeling Challenge;Featured Code Competition
2019-08-06 01:59:00;August 2019 Update: this competition is closed and is no longer accepting submissions.  The data has been removed from this competition and is not available for use. Thanks for participating! Can we use the content of news analytics to predict stock price performance? The ubiquity of data today enables investors at any scale to make better investment decisions. The challenge is ingesting and interpreting the data to determine which data is useful, finding the signal in this sea of information. Two Sigma  is passionate about this challenge and is excited to share it with the Kaggle community. As a scientifically driven investment manager, Two Sigma has been applying technology and data science to financial forecasts for over 17 years. Their pioneering advances in big data, AI, and machine learning have pushed the investment industry forward. Now, they're eager to engage with Kagglers in this continuing pursuit of innovation. By analyzing news data to predict stock prices, Kagglers have a unique opportunity to advance the state of research in understanding the predictive power of the news. This power, if harnessed, could help predict financial outcomes and generate significant economic impact all over the world.  Data for this competition comes from the following sources:  Market data provided by Intrinio. News data provided by Thomson Reuters. Copyright Thomson Reuters, 2017. All Rights Reserved. Use, duplication, or sale of this service, or data contained herein, except as described in the Competition Rules, is strictly prohibited.    The THOMSON REUTERS Kinesis Logo and THOMSON REUTERS are trademarks of Thomson Reuters and its affiliated companies in the United States and other countries and used herein under license.;https://www.kaggle.com/c/two-sigma-financial-news;Two Sigma;Use news analytics to predict stock price performance;['nlp', 'binary classification', 'exploratory data analysis', 'feature engineering', 'neural networks', 'time series analysis', 'beginner', 'data visualization', 'finance', 'lstm', 'ensembling', 'pca'];2927.0;0.698;Two Sigma: Using News to Predict Stock Movements;Featured Code Competition
2016-08-19 01:59:00;Even the bravest patient cringes at the mention of a surgical procedure. Surgery inevitably brings discomfort, and oftentimes involves significant post-surgical pain. Currently, patient pain is frequently managed through the use of narcotics that bring a bevy of unwanted side effects. This competition's sponsor is working to improve pain management through the use of indwelling catheters that block or mitigate pain at the source. Pain management catheters reduce dependence on narcotics and speed up patient recovery. Accurately identifying nerve structures in ultrasound images is a critical step in effectively inserting a patient’s pain management catheter. In this competition, Kagglers are challenged to build a model that can identify nerve structures in a dataset of ultrasound images of the neck. Doing so would improve catheter placement and contribute to a more pain free future.;https://www.kaggle.com/c/ultrasound-nerve-segmentation;;Identify nerve structures in ultrasound images of the neck;['beginner', 'gpu'];922.0;0.641;Ultrasound Nerve Segmentation;Featured prediction Competition
2019-11-19 00:59:00;Climate change has been at the top of our minds and on the forefront of important political decision-making for many years. We hope you can use this competition’s dataset to help demystify an important climatic variable. Scientists, like those at Max Planck Institute for Meteorology, are leading the charge with new research on the world’s ever-changing atmosphere and they need your help to better understand the clouds. Shallow clouds play a huge role in determining the Earth's climate. They’re also difficult to understand and to represent in climate models. By classifying different types of cloud organization, researchers at Max Planck hope to improve our physical understanding of these clouds, which in turn will help us build better climate models. There are many ways in which clouds can organize, but the boundaries between different forms of organization are murky. This makes it challenging to build traditional rule-based algorithms to separate cloud features. The human eye, however, is really good at detecting features—such as clouds that resemble flowers. In this challenge, you will build a model to classify cloud organization patterns from satellite images. If successful, you’ll help scientists to better understand how clouds will shape our future climate. This research will guide the development of next-generation models which could reduce uncertainties in climate projections. Help us remove the haze from climate models and bring clarity to cloud identification. For more information on the scientific background and how the labels were created see the following paper.;https://www.kaggle.com/c/understanding_cloud_organization;Max Planck Institute for Meteorology;Can you classify cloud structures from satellites?;['deep learning', 'exploratory data analysis', 'classification', 'data visualization', 'gpu', 'computer vision', 'cnn'];1538.0;0.667;Understanding Clouds from Satellite Images;Research prediction Competition
2011-02-20 23:00:00;Around the world, the pool of funds available for research grants is steadily shrinking (in a relative sense). In Australia, success rates have fallen to 20-25 per cent, meaning that most academics are spending valuable time making applications that end up being rejected. With this problem in mind, the University of Melbourne is hosting a competition to predict the success of grant applications. The winning model will be used by the university to predict which grant applications are likely to be successful, so that less time is wasted on applications that are unlikely to succeed. The university hopes the competition will also shed some light on what factors are important in determining whether an application will succeed. The university has provided a dataset containing 249 features, including variables that represent the size of the grant, the general area of study and de-identified information on the investigators who are applying for the grant. Participants train their models on 8,707 grant applications made between 2004 and 2008. They then make predictions on a further 2,176 applications made in 2009 and the first half of 2010.The winner of this competition will receive US$5,000. To be eligible for the prize, the winning method must be implementable by the University of Melbourne.;https://www.kaggle.com/c/unimelb;;This task requires participants to predict the outcome of grant applications for the University of Melbourne.;[];203.0;0.55;Predict Grant Applications;Featured prediction Competition
2019-03-22 00:59:00;Medium voltage overhead power lines run for hundreds of miles to supply power to cities. These great distances make it expensive to manually inspect the lines for damage that doesn't immediately lead to a power outage, such as a tree branch hitting the line or a flaw in the insulator. These modes of damage lead to a phenomenon known as partial discharge — an electrical discharge which does not bridge the electrodes between an insulation system completely. Partial discharges slowly damage the power line, so left unrepaired they will eventually lead to a power outage or start a fire.  Your challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the ENET Centre at VŠB. Effective classifiers using this data will make it possible to continuously monitor power lines for faults. ENET Centre researches and develops renewable energy resources with the goal of reducing or eliminating harmful environmental impacts. Their efforts focus on developing technology solutions around transportation and processing of energy raw materials. By developing a solution to detect partial discharge you’ll help reduce  maintenance costs, and prevent power outages.;https://www.kaggle.com/c/vsb-power-line-fault-detection;Enet Centre, VSB - T.U. of Ostrava;Can you detect faults in above-ground electrical lines?;['deep learning', 'exploratory data analysis', 'feature engineering', 'beginner', 'classification', 'data visualization', 'gpu', 'rnn', 'cnn'];1451.0;0.664;VSB Power Line Fault Detection;Featured prediction Competition
2014-05-06 01:59:00;"One challenge of modeling retail data is the need to make decisions based on limited history. If Christmas comes but once a year, so does the chance to see how strategic decisions impacted the bottom line.  In this recruiting competition, job-seekers are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains many departments, and participants must project the sales for each department in each store. To add to the challenge, selected holiday markdown events are included in the dataset. These markdowns are known to affect sales, but it is challenging to predict which departments are affected and the extent of the impact. Want to work in a great environment with some of the world's largest data sets? This is a chance to display your modeling mettle to the Walmart hiring teams. This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Walmart, check the box ""Allow host to contact me"" when you make your first entry.  You must compete as an individual in recruiting competitions. You may only use the provided data to make your predictions.";https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;Walmart;Use historical markdown data to predict store sales;['regression', 'xgboost', 'beginner', 'random forest'];688.0;0.625;Walmart Recruiting - Store Sales Forecasting;Recruitment prediction Competition
2015-12-28 00:59:00;"Walmart uses both art and science to continually make progress on their core mission of better understanding and serving their customers. One way Walmart is able to improve customers' shopping experiences is by segmenting their store visits into different trip types.   Whether they're on a last minute run for new puppy supplies or leisurely making their way through a weekly grocery list, classifying trip types enables Walmart to create the best shopping experience for every customer. Currently, Walmart's trip types are created from a combination of existing customer insights (""art"") and purchase history data (""science""). In their third recruiting competition, Walmart is challenging Kagglers to focus on the (data) science and classify customer trips using only a transactional dataset of the items they've purchased. Improving the science behind trip type classification will help Walmart refine their segmentation process. Walmart is hosting this competition to connect with data scientists who break the mold.";https://www.kaggle.com/c/walmart-recruiting-trip-type-classification;Walmart;Use market basket analysis to classify shopping trips;[];1043.0;0.647;Walmart Recruiting: Trip Type Classification;Recruitment prediction Competition
2017-11-16 00:59:00;This competition focuses on the problem of forecasting the future values of multiple time series, as it has always been one of the most challenging problems in the field. More specifically, we aim the competition at testing state-of-the-art methods designed by the participants, on the problem of forecasting future web traffic for approximately 145,000 Wikipedia articles.  Sequential or temporal observations emerge in many key real-world problems, ranging from biological data, financial markets, weather forecasting, to audio and video processing. The field of time series encapsulates many different problems, ranging from analysis and inference to classification and forecast. What can you do to help predict future views?  This competition will run as two stages and involves prediction of actual future events. There will be a training stage during which the leaderboard is based on historical data, followed by a stage where participants are scored on real future events.   You have complete freedom in how to produce your forecasts: e.g. use of univariate vs multi-variate models, use of metadata (article identifier), hierarchical time series modeling (for different types of traffic), data augmentation (e.g. using Google Trends data to extend the dataset), anomaly and outlier detection and cleaning, different strategies for missing value imputation, and many more types of approaches.   We thank Google Inc. and Voleon for sponsorship of this competition, and Oren Anava and Vitaly Kuznetsov for organizing it. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.;https://www.kaggle.com/c/web-traffic-time-series-forecasting;Google;Forecast future traffic to Wikipedia pages;['nlp', 'exploratory data analysis', 'neural networks', 'data visualization', 'advanced', 'lstm', 'gradient boosting'];1095.0;0.65;Web Traffic Time Series Forecasting;Research prediction Competition
2018-07-10 01:59:00;After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food. To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized. In this competition, you’re challenged to build an algorithm to identifying whale species in images. You’ll analyze Happy Whale’s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you’ll help to open rich fields of understanding for marine mammal population dynamics around the globe. We'd like to thank Happy Whale  for providing this data and problem. Happy Whale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.;https://www.kaggle.com/c/whale-categorization-playground;Kaggle;Can you identify a whale by the picture of its fluke?;['multiclass classification', 'exploratory data analysis', 'data visualization', 'transfer learning', 'computer vision'];528.0;0.61;Humpback Whale Identification Challenge;Playground prediction Competition
2015-12-21 00:59:00;Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight? If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see. Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients.  Acknowledgements We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.;https://www.kaggle.com/c/whats-cooking;Kaggle;Use recipe ingredients to categorize the cuisine;['nlp', 'multiclass classification', 'feature engineering', 'xgboost', 'recommender systems', 'multilabel classification', 'beginner', 'classification'];1387.0;0.662;What's Cooking?;Playground prediction Competition
2018-09-25 01:59:00;Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight? If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see. Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients.  Acknowledgements We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.;https://www.kaggle.com/c/whats-cooking-kernels-only;Kaggle;Use recipe ingredients to categorize the cuisine;['deep learning', 'exploratory data analysis', 'multiclass classification', 'feature engineering', 'logistic regression', 'data visualization', 'gpu', 'ensembling', 'svm', 'text data'];523.0;0.609;What's Cooking? (Kernels Only);Playground Code Competition
2018-04-02 01:59:00;Google Cloud and NCAA® have teamed up to bring you this year’s version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness® during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.   In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results. This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.;https://www.kaggle.com/c/womens-machine-learning-competition-2018;Google Cloud;Apply machine learning to NCAA® March Madness®;[];505.0;0.607;Google Cloud & NCAA® ML Competition 2018-Women's;Featured prediction Competition
2019-04-08 07:10:00;As a result of the continued collaboration between Google Cloud and the NCAA®, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.   In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.  As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!  This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.;https://www.kaggle.com/c/womens-machine-learning-competition-2019;Google Cloud;Apply Machine Learning to NCAA® March Madness®;['beginner', 'feature engineering', 'gpu', 'random forest'];500.0;0.606;Google Cloud & NCAA® ML Competition 2019-Women's;Featured prediction Competition
2015-07-01 01:59:00;"In this tutorial competition, we dig a little ""deeper"" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis. Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another Kaggle competition for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem. Deep learning has been in the news a lot over the past few years, even making it to the front page of the New York Times. These machine learning techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a drug discovery task, and cat and dog image recognition. Tutorial Overview This tutorial will help you get started with Word2Vec for natural language processing. It has two goals:  Basic Natural Language Processing: Part 1 of this tutorial is intended for beginners and covers basic natural language processing techniques, which are needed for later parts of the tutorial. Deep Learning for Text Understanding: In Parts 2 and 3, we delve into how to train a model using Word2Vec and how to use the resulting word vectors for sentiment analysis. Since deep learning is a rapidly evolving field, large amounts of the work has not yet been published, or exists only as academic papers. Part 3 of the tutorial is more exploratory than prescriptive -- we experiment with several ways of using Word2Vec rather than giving you a recipe for using the output. To achieve these goals, we rely on an IMDB sentiment analysis data set, which has 100,000 multi-paragraph movie reviews, both positive and negative.  Acknowledgements This dataset was collected in association with the following publication: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). ""Learning Word Vectors for Sentiment Analysis."" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011). (link) Please email the author of that paper if you use the data for any research applications. The tutorial was developed by Angela Chapman during her summer 2014 internship at Kaggle.";https://www.kaggle.com/c/word2vec-nlp-tutorial;Kaggle;Use Google's Word2Vec for movie reviews;['nlp', 'deep learning', 'beginner', 'classification', 'gpu'];577.0;0.615;Bag of Words Meets Bags of Popcorn;Getting Started prediction Competition
2016-04-13 01:59:00;"Does your favorite Ethiopian restaurant take reservations? Will a first date at that authentic looking bistro break your wallet? Is the diner down the street a good call for breakfast? Restaurant labels help Yelp users quickly answer questions like these, narrowing down their results to only restaurants that fit their nuanced needs. In this competition, Yelp is challenging Kagglers to build a model that automatically tags restaurants with multiple labels using a dataset of user-submitted photos. Currently, restaurant labels are manually selected by Yelp users when they submit a review. Selecting the labels is optional, leaving some restaurants un- or only partially-categorized.  In an age of food selfies and photo-centric social storytelling, it may be no surprise to hear that Yelp's users upload an enormous amount of photos every day alongside their written reviews. Can you turn their pictures into (less than a thousand) words?  Yelp isn’t only looking for your best model; we’re looking for data mining engineers that can help us use our data in novel ways while pushing code to production. The prize for this competition is a fast track through the recruiting process and an opportunity to show our data mining teams just what you’ve got! For more information about exciting opportunities at Yelp, check out the Jobs at Yelp competition page and Yelp's own careers page.";https://www.kaggle.com/c/yelp-restaurant-photo-classification;;Predict attribute labels for restaurants using user-submitted photos;['exploratory data analysis', 'data visualization'];355.0;0.586;Yelp Restaurant Photo Classification;Recruitment prediction Competition
2017-06-03 01:59:00;Video captures a cross-section of our society. And major advances in analyzing and understanding video have the potential to touch all aspects of life from learning and communication to entertainment and play. In this competition, Google is inviting the Kaggle community to join efforts to accelerate research in large-scale video understanding, while giving participants access to the Google Cloud Machine Learning Engine. Today, one of the greatest obstacles to rapid improvements in video understanding research has been the lack of large-scale, labeled datasets open to the public. For example, the availability of large, labeled datasets such as ImageNet has enabled continued breakthroughs in machine learning and machine perception. To that end, Google’s recent release of the YouTube-8M (YT-8M) dataset represents a significant step in this direction. Making this resource open to everyone from students and industry professionals is expected to kickstart innovation in areas such as representation learning and video modeling architectures. In this competition, you are challenged to develop classification algorithms which accurately assign video-level labels using the new and improved YT-8M V2 dataset. The dataset was created from over 7 million YouTube videos (450,000 hours of video) and includes video labels from a vocabulary of 4716 classes (3.4 labels/video on average).  It also comes with pre-extracted audio & visual features from every second of video (3.2B feature vectors in total). By taking part, Kagglers will not only play a pivotal role in setting state-of-the-art benchmarks, but also improve search and organization of video archives.  Getting Started  Review the data page for special instructions on how to access the competition's data. It will be hosted on Google Cloud. Participants have the option to download the data to work locally or work within the Google Cloud ML beta Platform. Review the tutorial on Getting Started with Google Cloud, and try the starter code.  Sign up for a Google Cloud ML Platform free trial account. The free trial account includes $300 in credits!   We've also provided a subsample of the data to explore on Kernels. Take a look at this Python notebook and create your own. Don't forget to review the prize eligibility details, which includes requirements for code open-sourcing and a paper submission.  Because Cloud ML is currently a beta product, Google welcomes the opportunity to hear your feedback about using the tool. Please share your questions and thoughts on the competition's forums. Additional resources specific to the YT-8M dataset and Google Cloud ML can be found here. Acknowledgements Google Cloud Machine Learning, Competition Sponsor Google Cloud Machine Learning is a managed service that enables you to easily build machine learning models, that work on any type of data, of any size. Create your model with the powerful TensorFlow framework that powers many Google products, from GooglePhotos to Google Cloud Speech. Build models of any size with our managed scalable infrastructure. Your trained model is immediately available for use with our global prediction platform that can support thousands of users and TBs of data. The service is integrated with Google Cloud Dataflow for pre-processing, allowing you to access data from Google Cloud Storage, Google BigQuery, and others.;https://www.kaggle.com/c/youtube8m;Google Cloud;Can you produce the best video tag predictions?;[];655.0;0.622;Google Cloud & YouTube-8M Video Understanding Challenge;Featured prediction Competition
2018-08-07 01:59:00;The world is generating and consuming an enormous amount of video content. Currently on YouTube, people watch over 1 billion hours of video every single day. To spur advances in analyzing and understanding video,  Google AI has publicly released a large-scale video dataset that consists of millions of YouTube video features and associated labels from a diverse vocabulary of 3,700+ visual entities called the YouTube-8M Dataset. Last year, we successfully hosted Google Cloud & YouTube-8M Video Understanding Challenge, with 742 participating teams with 946 individual competitors from 60 countries. This competition is the second Kaggle competition based on YouTube 8M dataset, and is focused on learning video representation under budget constraints.  For a lot of video tasks where there are a large number of classes, like recommending new videos or automatic video classification, compact models need to meet memory and computational requirements. This is true even if working in cloud computational environments. Also, compact models make it possible to have limited-memory or catalog indexes on devices in order to do personalized and privacy-preserving computation on user’s personal mobile phones. In this competition, you’re challenged to produce a compact video classification model. Your model size must not exceed 1 GB (this is strictly enforced, through model upload). We encourage participants to train a model that most efficiently uses this budget, rather than ensembles of lots of models.  This competition is being hosted by Google AI (previously known as Google Research) as a part of the European Conference on Computer Vision (ECCV) 2018 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.;https://www.kaggle.com/c/youtube8m-2018;Google Research;Can you create a constrained-size model to predict video labels?;['deep learning', 'beginner', 'data visualization', 'gpu', 'lstm'];312.0;0.578;The 2nd YouTube-8M Video Understanding Challenge;Featured prediction Competition
2019-10-12 01:59:00;"Imagine being able to search for the moment in any video where an adorable kitten sneezes, even though the uploader didn’t title or describe the video with such descriptive metadata. Now, apply that same concept to videos that cover important or special events like a baby’s first steps or a game-winning goal -- and now we have the ability to quickly find and share special video moments. This technology is called temporal concept localization within video and Google Research can use your help to advance the state of the art in this area.    An example of the detected action ""blowing out candles"" In most web searches, video retrieval and ranking is performed by matching query terms to metadata and other video-level signals. However, we know that videos can contain an array of topics that aren’t always characterized by the uploader, and many of these miss localizations to brief but important moments within the video. Temporal localization can enable applications such as  improved video search (including search within video), video summarization and highlight extraction, action moment detection, improved video content safety, and many others. In previous years, participants worked on advancements in video-level annotations, building both unconstrained and constrained models. In this third challenge based on the YouTube 8M dataset, Kagglers will localize video-level labels to the precise time in the video where the label actually appears, and do this at an unprecedented scale. To put it another way: at what point in the video does the cat sneeze?   If successful, your new machine learning models will significantly improve video understanding for all, by not only identifying the topics relevant to a video, but also pinpointing where in the video they appear. This competition is being hosted by Google Research as a part of the International Conference on Computer Vision (ICCV) 2019 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.";https://www.kaggle.com/c/youtube8m-2019;Google Research;Temporal localization of topics within video;['data visualization'];283.0;0.571;The 3rd YouTube-8M Video Understanding Challenge;Research prediction Competition
2018-01-10 16:59:00;Zillow’s Zestimate home valuation has shaken up the U.S. real estate industry since first released 11 years ago. A home is often the largest and most expensive purchase a person makes in his or her lifetime. Ensuring homeowners have a trusted way to monitor this asset is incredibly important. The Zestimate was created to give consumers as much information as possible about homes and the housing market, marking the first time consumers had access to this type of home value information at no cost. “Zestimates” are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property.   And, by continually improving the median margin of error (from 14% at the onset to 5% today), Zillow has since become established as one of the largest, most trusted marketplaces for real estate information in the U.S. and a leading example of impactful machine learning. Zillow Prize, a competition with a one million dollar grand prize, is challenging the data science community to help push the accuracy of the Zestimate even further. Winning algorithms stand to impact the home values of  110M homes across the U.S. In this million-dollar competition, participants will develop an algorithm that makes predictions about the future sale prices of homes. The contest is structured into two rounds, the qualifying round which opens May 24, 2017 and the private round for the 100 top qualifying teams that opens  on Feb 1st, 2018. In the qualifying round, you’ll be building a model to improve the Zestimate residual error. In the final round, you’ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition. Because real estate transaction data is public information, there will be a three-month sales tracking period after each competition round closes where your predictions will be evaluated against the actual sale prices of the homes. The final leaderboard won’t be revealed until the close of the sales tracking period.;https://www.kaggle.com/c/zillow-prize-1;Zillow;Can you improve the algorithm that changed the world of real estate?;['data cleaning', 'feature engineering', 'exploratory data analysis', 'xgboost', 'beginner', 'intermediate', 'data visualization', 'clustering', 'ensembling'];3775.0;0.709;Zillow Prize: Zillow’s Home Value Prediction (Zestimate);Featured prediction Competition
