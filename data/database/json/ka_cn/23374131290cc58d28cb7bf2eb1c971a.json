{"title": "[PyTorch] BERT + EndpointSpanExtractor + KFold", "description": "2019/03/23 Update:  Inspired by hanxiao/bert-as-service, the hidden states (context vectors) of the second-to-last layer is used instead of the ones from the last layer. Q: Why not the last hidden layer? Why second-to-last? A: The last layer is too closed to the target functions (i.e. masked language model and next sentence prediction) during pre-training, therefore may be biased to those targets. If you question about this argument and want to use the last hidden layer anyway, please feel free to set pooling_layer=-1.", "link": "https://www.kaggle.com/ceshine/pytorch-bert-endpointspanextractor-kfold", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pytorch", "nltk", "spacy"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-03-23 17:29:01", "date_scraped": "2020-12-12 20:09:27", "words": 83, "sentences": 4, "runtime": 0.002, "description_category": "Education & Research", "description_category_score": 0.2285522222518921, "description_category_runtime": 9.598, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.9203987121582031, "description_subcategory_runtime": 14.836, "category": "Education & Research", "category_score": 0.2285522222518921, "subcategory": "Machine Learning", "subcategory_score": 0.9203987121582031, "runtime_cat": 24.435, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.704", "language_code": "en", "language_score": "0.9999967849877025", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "20190323 update inspired hanxiaobertasservice hidden state context vector secondtolast layer used instead one last layer q last hidden layer secondtolast last layer closed target function ie masked language model next sentence prediction pretraining therefore may biased target question argument want use last hidden layer anyway please feel free set pooling_layer1", "tags_descriptive": []}