{"title": "Introduction to Boosting using LGBM(LB: 0.68357)", "description": "In this kernel, I would like to introduce you to boosting and LightGBM. Many of the kernels written for this competition have used light GBM and have produced really good results. These kernels have produced really useful insights into how to use light GBM. In this kernel, I would like deep dive into LightGBM and provide you my insights into how it works. I am writing this kernel to provide more insights into how lgbm works and how we can improve our model. I will be using Asmita's kernel (https://www.kaggle.com/asmitavikas/feature-engineered-0-68310) as a base. (This kernel does not provide any insights into feature engineering. One can refer to Asmita's kernel for that). I will be starting with a few basic questions about boosting and then move into specific one's about LightGBM (especially its hyper parameters) References/useful resources: https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/ https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/ https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/ https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc What is boosting? Boosting combines a set of weak learners to form a strong rule. It is an iterative process. How boosting works? The weak rules are generated by a base learning algorithm. These rules are generated iteratively and combined at the end to form a single strong rule. Boosting gives more importance to observations which are wrongly classified. How is boosting different from bagging? Unlike bagging algorithms which just reduce variance, boosting reduces both bias and variance.", "link": "https://www.kaggle.com/vinnsvinay/introduction-to-boosting-using-lgbm-lb-0-68357", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2017-11-22 02:56:52", "date_scraped": "2020-12-13 12:31:41", "words": 218, "sentences": 14, "sum_nltk": "In this kernel, I would like to introduce you to boosting and LightGBM.\nMany of the kernels written for this competition have used light GBM and have produced really good results.\nThese kernels have produced really useful insights into how to use light GBM.\nIn this kernel, I would like deep dive into LightGBM and provide you my insights into how it works.\nI am writing this kernel to provide more insights into how lgbm works and how we can improve our model.\nI will be using Asmita's kernel (https://www.kaggle.com/asmitavikas/feature-engineered-0-68310) as a base.\n(This kernel does not provide any insights into feature engineering.\nOne can refer to Asmita's kernel for that).\nI will be starting with a few basic questions about boosting and then move into specific one's about LightGBM (especially its hyper parameters) References/useful resources: https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/ https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/ https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/ https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc What is boosting?\nBoosting combines a set of weak learners to form a strong rule.\nHow boosting works?\nThe weak rules are generated by a base learning algorithm.\nThese rules are generated iteratively and combined at the end to form a single strong rule.\nHow is boosting different from bagging?\nUnlike bagging algorithms which just reduce variance, boosting reduces both bias and variance.", "sum_nltk_words": 189, "sum_nltk_runtime": 0.004, "sum_t5": "boosting is an iterative process that combines weak learners to form a strong rule. boosting reduces bias and gives more importance to observations which are wrongly classified. boosting reduces bias and gives more importance to observations which are wrongly classified. boosting is a powerful algorithm that can be used to improve a model. a kernel that can be used to improve a model can be used to improve a model. a kernel that can be used to improve a model can be used to", "sum_t5_words": 84, "sum_t5_runtime": 6.954, "runtime": 0.0, "nltk_category": "Construction & Engineering", "nltk_category_score": 0.44874539971351624, "nltk_category_runtime": 31.367, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.6227201223373413, "nltk_subcategory_runtime": 50.95, "category": "Construction & Engineering", "category_score": 0.44874539971351624, "subcategory": "Machine Learning", "subcategory_score": 0.6227201223373413, "runtime_cat": 82.317, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.756", "language_code": "en", "language_score": "0.9999975108692356", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "kernel would like introduce boosting lightgbm many kernel written competition used light gbm produced really good result kernel produced really useful insight use light gbm kernel would like deep dive lightgbm provide insight work writing kernel provide insight lgbm work improve model using asmitas kernel httpswwwkagglecomasmitavikasfeatureengineered068310 base kernel provide insight feature engineering one refer asmitas kernel starting basic question boosting move specific one lightgbm especially hyper parameter referencesuseful resource httpswwwanalyticsvidhyacomblog201706whichalgorithmtakesthecrownlightgbmvsxgboost httpswwwanalyticsvidhyacomblog201509completeguideboostingmethods httpswwwanalyticsvidhyacomblog201511quickintroductionboostingalgorithmsmachinelearning httpsmediumcompushkarmandothttpsmediumcompushkarmandotwhatislightgbmhowtoimplementithowtofinetunetheparameters60347819b7fc boosting boosting combine set weak learner form strong rule iterative process boosting work weak rule generated base learning algorithm rule generated iteratively combined end form single strong rule boosting give importance observation wrongly classified boosting different bagging unlike bagging algorithm reduce variance boosting reduces bias variance", "tags_descriptive": []}