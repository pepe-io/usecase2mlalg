{"title": "Keras EfficientNet B2 Starter code", "description": "This competition provides an exciting and challenging task of doing multi-label classification on a dataset with well over half a million images. There are multiple very nice notebooks which perform only 2 or 3 epochs with all the training data. In this notebook I will try out and see what the effect is of using more epochs but less steps per epoch. By averaging the predictions made during the last few epochs we should be able to achieve a nice LB score. This also should provide some alternative ways to experiment for the Kagglers that don't have the adequate computing resources available and are dependent on Kaggle Kernels. As model I will be using the EfficientNet B2 model. It should be able to provide highly accurate predictions while still being able to run within the kernel limits. With 9 hours max time for a GPU kernel you have to make some trade-offs ;-) I hope this kernel will be usefull and may'be will provide you with some new and alternative ideas to try out. If you like it..then please upvote it ;-) Any feedback or remarks are appreciated. Lets start by importing all the necessary modules. Note!! This kernel is now updated for Stage2 Training and Test data..altough with less epochs because of the increase in train and test data.", "link": "https://www.kaggle.com/rsmits/keras-efficientnet-b2-starter-code", "tags": ["DL", "Classification"], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-11-15 09:03:22", "date_scraped": "2020-12-13 16:47:30", "words": 220, "sentences": 11, "sum_nltk": "This competition provides an exciting and challenging task of doing multi-label classification on a dataset with well over half a million images.\nThere are multiple very nice notebooks which perform only 2 or 3 epochs with all the training data.\nIn this notebook I will try out and see what the effect is of using more epochs but less steps per epoch.\nBy averaging the predictions made during the last few epochs we should be able to achieve a nice LB score.\nThis also should provide some alternative ways to experiment for the Kagglers that don't have the adequate computing resources available and are dependent on Kaggle Kernels.\nAs model I will be using the EfficientNet B2 model.\nIt should be able to provide highly accurate predictions while still being able to run within the kernel limits.\nWith 9 hours max time for a GPU kernel you have to make some trade-offs ;-) I hope this kernel will be usefull and may'be will provide you with some new and alternative ideas to try out.\nThis kernel is now updated for Stage2 Training and Test data..altough with less epochs because of the increase in train and test data.", "sum_nltk_words": 189, "sum_nltk_runtime": 0.002, "sum_t5": "this competition provides an exciting and challenging task of doing multi-label classification on a dataset with well over half a million images. there are multiple very nice notebooks which perform only 2 or 3 epochs with all the training data. this will be a notebook to see what the effect is of using more epochs but less steps per epoch. this will also provide some alternative ways to experiment for the Kagglers that don't have the adequate computing resources available and are dependent on", "sum_t5_words": 84, "sum_t5_runtime": 5.198, "runtime": 0.0, "nltk_category": "Finance", "nltk_category_score": 0.3324826657772064, "nltk_category_runtime": 16.573, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.835608184337616, "nltk_subcategory_runtime": 26.588, "category": "Finance", "category_score": 0.3324826657772064, "subcategory": "Machine Learning", "subcategory_score": 0.835608184337616, "runtime_cat": 43.161, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.744", "language_code": "en", "language_score": "0.9999954116746713", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "competition provides exciting challenging task multilabel classification dataset well half million image multiple nice notebook perform 2 3 epoch training data notebook try see effect using epoch le step per epoch averaging prediction made last epoch able achieve nice lb score also provide alternative way experiment kagglers dont adequate computing resource available dependent kaggle kernel model using efficientnet b2 model able provide highly accurate prediction still able run within kernel limit 9 hour max time gpu kernel make tradeoff hope kernel usefull maybe provide new alternative idea try like itthen please upvote feedback remark appreciated let start importing necessary module note kernel updated stage2 training test dataaltough le epoch increase train test data", "tags_descriptive": ["Deep Learning (DL)", "Classification"]}