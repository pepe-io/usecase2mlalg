{"title": "Automated Model Tuning", "description": "Introduction: Automated Hyperparameter TuningIn this notebook, we will talk through a complete example of using automated hyperparameter tuning to optimize a machine learning model. In particular, we will use Bayesian Optimization and the Hyperopt library to tune the hyperparameters of a gradient boosting machine. Additional Notebooks If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:  A Gentle Introduction Manual Feature Engineering Part One Manual Feature Engineering Part Two Introduction to Automated Feature Engineering Advanced Automated Feature Engineering Feature Selection Intro to Model Tuning: Grid and Random Search Automated Model Tuning  There are four approaches to tuning the hyperparameters of a machine learning model  Manual: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results. Grid Search: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient! Random search: set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources. Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.  These are listed in general order of least to most efficient. While we already conquered 2 and 3 in this notebook (we didn't even try method 1), we have yet to take on automated hyperparameter tuning. There are a number of methods to do this including genetic programming, Bayesian optimization, and gradient based methods. Here we will focus only on Bayesian optimization, using the Tree Parzen Esimator (don't worry, you don't need to understand this in detail) in the Hyperopt open-source Python library. For a little more background (we'll cover everything you need below), here is an introductory article on Bayesian optimization, and here is an article on automated hyperparameter tuning using Bayesian optimization. Here we'll get right into automated hyperparameter tuning, so for the necessary background on model tuning, refer to this kernel Bayesian Optimization PrimerThe problem with grid and random search is that these are uninformed methods because they do not use the past results from different values of hyperparameters in the objective function (remember the objective function takes in the hyperparameters and returns the model cross validation score). We record the results of the objective function for each set of hyperparameters, but the algorithms do not select the next hyperparameter values from this information. Intuitively, if we have the past results, we should  use them to reason about what hyperparameter values work the best and choose the next values wisely to try and spend more iterations evaluating promising values. Evaluating hyperparameters in the objective function is very time-consuming, and the concept of Bayesian optimization is to limit calls to the evaluation function by choosing the next hyperparameter values based on the previous results. This allows the algorithm to spend more time evaluating promising hyperparameter values and less time in low-scoring regions of the hyperparameter space. For example, consider the image below:  If you were choosing the next number of trees to try for the random forest, where would you concentrate your search? Probably around 100 trees because that is where the lowest errors have tended to occur (imagine this is a problem where we want to minimize the error). In effect, you have just done Bayesian hyperparameter optimization in your head! You formed a probability model of the error as a function of the hyperparameters and then selected the next hyperparameter values by maximizing the probability of a low error. Bayesian optimization works by building a surrogate function (in the form of a probability model) of the objective function P(score|hyperparameters. The surrogate function is much cheaper to evaluate than the objective, so the algorithm chooses the next values to try in the objective based on maximizing a criterion on the surrogate (usually expected improvement), exactly what you would have done with respect to the image above. The surrogate function is based on past evaluation results - pairs of (score, hyperparameter) records - and is continually updated with each objective function evaluation. Bayesian optimization therefore uses Bayesian reasoning: form an initial model (called a prior) and then update it with more evidence. The idea is that as the data accumulates, the surrogate function gets closer and closer to the objective function, and the hyperparameter values that are the best in the surrogate function will also do the best in the objective function. Bayesian optimization methods differ in the algorithm used to build the surrogate function and choose the next hyperparameter values to try. Some of the common choices are Gaussian Process (implemented in Spearmint), Random Forest Regression (in SMAC), and the Tree Parzen Estimator (TPE) in Hyperopt (technical details can be found in this article, although they won't be necessary to use the methods). Four Part of Bayesian OptimizationBayesian hyperparameter optimization requires the same four parts as we implemented in grid and random search:  Objective Function: takes in an input (hyperparameters) and returns a score to minimize or maximize (the cross validation score) Domain space: the range of input values (hyperparameters) to evaluate Optimization Algorithm: the method used to construct the surrogate function and choose the next values to evaluate Results: score, value pairs that the algorithm uses to build the surrogate function  The only differences are that now our objective function will return a score to minimize (this is just convention in the field of optimization), our domain space will be probability distributions rather than a hyperparameter grid, and the optimization algorithm will be an informed method that uses past results to choose the next hyperparameter values to evaluate. HyperoptHyperopt is an open-source Python library the implements Bayesian Optimization using the Tree Parzen Estimator algorithm to construct the surrogate function and select the next hyperparameter values to evaluate in the objective function. There are a number of other libraries such as Spearmint (Guassian process surrogate function) and SMAC (random forest regression surrogate function) sharing the same problem structure. The four parts of an optimization problem that we develop here will apply to all the libraries with only a change in syntax. Morevoer, the optimization methods as applied to the Gradient Boosting Machine will translate to other machine learning models or any problem where we have to minimize a function. Gradient Boosting MachineWe will use the gradient booosting machine (GBM) as our model to tune in the LightGBM library. The GBM is our choice of model because it performs extremely well for these types of problems (as shown on the leaderboard) and because the performance is heavily dependent on the choice of hyperparameter values. For more details of the Gradient Boosting Machine (GBM), check out this high-level blog post, or this in depth technical article. Cross Validation with Early StoppingAs with random and grid search, we will evaluate each set of hyperparameters using 5 fold cross validation on the training data. The GBM model will be trained with early stopping, where estimators are added to the ensemble until the validation score has not decrease for 100 iterations (estimators added). Cross validation and early stopping will be implemented using the LightGBM cv function. We will use 5 folds and 100 early stopping rounds. Dataset and ApproachAs before, we will work with a limited section of the data - 10000 observations for training and 6000 observations for testing. This will allow the optimization within the notebook to finish in a reasonable amount of time. Later in the notebook, I'll present results from 1000 iterations of Bayesian hyperparameter optimization on the reduced dataset and we then will see if these results translate to a full dataset (from this kernel). The functions developed here can be taken and run on any dataset, or used with any machine learning model (just with minor changes in the details) and working with a smaller dataset will allow us to learn all of the concepts. I am currently running 500 iterations of Bayesian hyperparameter optimization on a complete dataset and will make the results available when the search is completed.", "link": "https://www.kaggle.com/willkoehrsen/automated-model-tuning", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pattern", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-07-09 02:20:48", "date_scraped": "2020-12-12 20:47:32", "words": 1392, "sentences": 44, "sum_nltk": "Additional Notebooks If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:  A Gentle Introduction Manual Feature Engineering Part One Manual Feature Engineering Part Two Introduction to Automated Feature Engineering Advanced Automated Feature Engineering Feature Selection Intro to Model Tuning: Grid and Random Search Automated Model Tuning  There are four approaches to tuning the hyperparameters of a machine learning model  Manual: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data.\nHere we'll get right into automated hyperparameter tuning, so for the necessary background on model tuning, refer to this kernel Bayesian Optimization PrimerThe problem with grid and random search is that these are uninformed methods because they do not use the past results from different values of hyperparameters in the objective function (remember the objective function takes in the hyperparameters and returns the model cross validation score).\nHyperoptHyperopt is an open-source Python library the implements Bayesian Optimization using the Tree Parzen Estimator algorithm to construct the surrogate function and select the next hyperparameter values to evaluate in the objective function.", "sum_nltk_words": 193, "sum_nltk_runtime": 0.017, "sum_t5": "this notebook will talk through a complete example of using automated hyperparameter tuning. we will use Bayesian Optimization and the hyperopt library to tune the hyperparameters of a gradient boosting machine. there are four approaches to tuning the hyperparameters of a machine learning model. there are genetic programming, Bayesian optimization, and evolutionary algorithms to conduct a guided search for the best hyperparameters. if you want to learn more about this problem, check out my other work on this problem", "sum_t5_words": 79, "sum_t5_runtime": 6.932, "runtime": 0.014, "nltk_category": "Education & Research", "nltk_category_score": 0.2480977177619934, "nltk_category_runtime": 17.592, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8192151784896851, "nltk_subcategory_runtime": 27.794, "category": "Education & Research", "category_score": 0.2480977177619934, "subcategory": "Machine Learning", "subcategory_score": 0.8192151784896851, "runtime_cat": 45.386, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.792", "language_code": "en", "language_score": "0.999996503905185", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introduction automated hyperparameter tuningin notebook talk complete example using automated hyperparameter tuning optimize machine learning model particular use bayesian optimization hyperopt library tune hyperparameters gradient boosting machine additional notebook havent checked work problem complete list notebook completed far gentle introduction manual feature engineering part one manual feature engineering part two introduction automated feature engineering advanced automated feature engineering feature selection intro model tuning grid random search automated model tuning four approach tuning hyperparameters machine learning model manual select hyperparameters based intuitionexperienceguessing train model hyperparameters score validation data repeat process run patience satisfied result grid search set grid hyperparameter value combination train model score validation data approach every single combination hyperparameters value tried inefficient random search set grid hyperparameter value select random combination train model score number search iteration set based timeresources automated hyperparameter tuning use method gradient descent bayesian optimization evolutionary algorithm conduct guided search best hyperparameters listed general order least efficient already conquered 2 3 notebook didnt even try method 1 yet take automated hyperparameter tuning number method including genetic programming bayesian optimization gradient based method focus bayesian optimization using tree parzen esimator dont worry dont need understand detail hyperopt opensource python library little background well cover everything need introductory article bayesian optimization article automated hyperparameter tuning using bayesian optimization well get right automated hyperparameter tuning necessary background model tuning refer kernel bayesian optimization primerthe problem grid random search uninformed method use past result different value hyperparameters objective function remember objective function take hyperparameters return model cross validation score record result objective function set hyperparameters algorithm select next hyperparameter value information intuitively past result use reason hyperparameter value work best choose next value wisely try spend iteration evaluating promising value evaluating hyperparameters objective function timeconsuming concept bayesian optimization limit call evaluation function choosing next hyperparameter value based previous result allows algorithm spend time evaluating promising hyperparameter value le time lowscoring region hyperparameter space example consider image choosing next number tree try random forest would concentrate search probably around 100 tree lowest error tended occur imagine problem want minimize error effect done bayesian hyperparameter optimization head formed probability model error function hyperparameters selected next hyperparameter value maximizing probability low error bayesian optimization work building surrogate function form probability model objective function pscorehyperparameters surrogate function much cheaper evaluate objective algorithm chooses next value try objective based maximizing criterion surrogate usually expected improvement exactly would done respect image surrogate function based past evaluation result pair score hyperparameter record continually updated objective function evaluation bayesian optimization therefore us bayesian reasoning form initial model called prior update evidence idea data accumulates surrogate function get closer closer objective function hyperparameter value best surrogate function also best objective function bayesian optimization method differ algorithm used build surrogate function choose next hyperparameter value try common choice gaussian process implemented spearmint random forest regression smac tree parzen estimator tpe hyperopt technical detail found article although wont necessary use method four part bayesian optimizationbayesian hyperparameter optimization requires four part implemented grid random search objective function take input hyperparameters return score minimize maximize cross validation score domain space range input value hyperparameters evaluate optimization algorithm method used construct surrogate function choose next value evaluate result score value pair algorithm us build surrogate function difference objective function return score minimize convention field optimization domain space probability distribution rather hyperparameter grid optimization algorithm informed method us past result choose next hyperparameter value evaluate hyperopthyperopt opensource python library implement bayesian optimization using tree parzen estimator algorithm construct surrogate function select next hyperparameter value evaluate objective function number library spearmint guassian process surrogate function smac random forest regression surrogate function sharing problem structure four part optimization problem develop apply library change syntax morevoer optimization method applied gradient boosting machine translate machine learning model problem minimize function gradient boosting machinewe use gradient booosting machine gbm model tune lightgbm library gbm choice model performs extremely well type problem shown leaderboard performance heavily dependent choice hyperparameter value detail gradient boosting machine gbm check highlevel blog post depth technical article cross validation early stoppingas random grid search evaluate set hyperparameters using 5 fold cross validation training data gbm model trained early stopping estimator added ensemble validation score decrease 100 iteration estimator added cross validation early stopping implemented using lightgbm cv function use 5 fold 100 early stopping round dataset approachas work limited section data 10000 observation training 6000 observation testing allow optimization within notebook finish reasonable amount time later notebook ill present result 1000 iteration bayesian hyperparameter optimization reduced dataset see result translate full dataset kernel function developed taken run dataset used machine learning model minor change detail working smaller dataset allow u learn concept currently running 500 iteration bayesian hyperparameter optimization complete dataset make result available search completed", "tags_descriptive": []}