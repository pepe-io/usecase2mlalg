{"title": "Exploratory study on feature selection", "description": "Thank you for opening this script! I have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts. Please upvote this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks. My other exploratory studies can be accessed here : https://www.kaggle.com/sharmasanthosh/kernels  Layout of the documentThe prediction process is divided into two notebooks. This notebook : Covers data statistics, data visualization, and feature selection Part 2 : Covers prediction using various algorithms : https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-of-ml-algorithms  Data statistics Shape Datatypes Description Skew Class distribution  Data Interaction Correlation Scatter plot  Data Visualization Box and density plots Grouping of one hot encoded attributes  Data Cleaning Remove unnecessary columns  Data Preparation Original Delete rows or impute values in case of missing StandardScaler MinMaxScaler Normalizer  Feature selection ExtraTreesClassifier GradientBoostingClassifier RandomForestClassifier XGBClassifier RFE SelectPercentile PCA PCA + SelectPercentile Feature Engineering  Evaluation, prediction, and analysis LDA (Linear algo) LR (Linear algo) KNN (Non-linear algo) CART (Non-linear algo) Naive Bayes (Non-linear algo) SVC (Non-linear algo) Bagged Decision Trees (Bagging) Random Forest (Bagging) Extra Trees (Bagging) AdaBoost (Boosting) Stochastic Gradient Boosting (Boosting) Voting Classifier (Voting) MLP (Deep Learning) XGBoost", "link": "https://www.kaggle.com/sharmasanthosh/exploratory-study-on-feature-selection", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "pattern"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2016-10-13 04:03:02", "date_scraped": "2020-12-12 20:01:43", "words": 227, "sentences": 6, "sum_nltk": "Thank you for opening this script!\nI have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.\nPlease upvote this kernel so that it reaches the top of the chart and is easily locatable by new users.\nThanks.\nMy other exploratory studies can be accessed here : https://www.kaggle.com/sharmasanthosh/kernels  Layout of the documentThe prediction process is divided into two notebooks.\nThis notebook : Covers data statistics, data visualization, and feature selection Part 2 : Covers prediction using various algorithms : https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-of-ml-algorithms  Data statistics Shape Datatypes Description Skew Class distribution  Data Interaction Correlation Scatter plot  Data Visualization Box and density plots Grouping of one hot encoded attributes  Data Cleaning Remove unnecessary columns  Data Preparation Original Delete rows or impute values in case of missing StandardScaler MinMaxScaler Normalizer  Feature selection ExtraTreesClassifier GradientBoostingClassifier RandomForestClassifier XGBClassifier RFE SelectPercentile PCA PCA + SelectPercentile Feature Engineering  Evaluation, prediction, and analysis LDA (Linear algo) LR (Linear algo) KNN (Non-linear algo) CART (Non-linear algo) Naive Bayes (Non-linear algo) SVC (Non-linear algo) Bagged Decision Trees (Bagging) Random Forest (Bagging) Extra Trees (Bagging) AdaBoost (Boosting) Stochastic Gradient Boosting (Boosting) Voting Classifier (Voting) MLP (Deep Learning) XGBoost", "sum_nltk_words": 211, "sum_nltk_runtime": 0.003, "sum_t5": "this notebook : Covers data statistics, data visualization, and feature selection. Part 2 : Covers prediction using various algorithms. sharmasanthosh's other exploratory studies can be accessed here. he has written a kernel that is a good starting point for new machine learning enthusiasts. he hopes to have it available to new users. if you have any suggestions, please upvote this kernel.", "sum_t5_words": 61, "sum_t5_runtime": 6.06, "runtime": 0.005, "nltk_category": "Utilities", "nltk_category_score": 0.37493085861206055, "nltk_category_runtime": 27.367, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8975893259048462, "nltk_subcategory_runtime": 44.089, "category": "Utilities", "category_score": 0.37493085861206055, "subcategory": "Machine Learning", "subcategory_score": 0.8975893259048462, "runtime_cat": 71.456, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.801", "language_code": "en", "language_score": "0.9999966283494148", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "thank opening script made effort document every step involved prediction process notebook act good starting point new kagglers new machine learning enthusiast please upvote kernel reach top chart easily locatable new user comment improve kernel welcome thanks exploratory study accessed httpswwwkagglecomsharmasanthoshkernels layout documentthe prediction process divided two notebook notebook cover data statistic data visualization feature selection part 2 cover prediction using various algorithm httpswwwkagglecomsharmasanthoshforestcovertypepredictionexploratorystudyofmlalgorithms data statistic shape datatypes description skew class distribution data interaction correlation scatter plot data visualization box density plot grouping one hot encoded attribute data cleaning remove unnecessary column data preparation original delete row impute value case missing standardscaler minmaxscaler normalizer feature selection extratreesclassifier gradientboostingclassifier randomforestclassifier xgbclassifier rfe selectpercentile pca pca selectpercentile feature engineering evaluation prediction analysis lda linear algo lr linear algo knn nonlinear algo cart nonlinear algo naive bayes nonlinear algo svc nonlinear algo bagged decision tree bagging random forest bagging extra tree bagging adaboost boosting stochastic gradient boosting boosting voting classifier voting mlp deep learning xgboost", "tags_descriptive": []}