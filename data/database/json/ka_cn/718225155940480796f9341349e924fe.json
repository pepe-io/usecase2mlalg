{"title": "Better predictions: stacking with VotingClassifier", "description": "As you may well know in most Kaggle competitions the winners usually resort to stacking or meta-ensembling, which is a technique involving the combination of several 1st level predictive models to generate a 2nd level model which tends to outperform all of them. This usually happens because the 2nd level model is somewhat able to exploit the strengths of each 1st level model where they perform best, while smoothing the impact of their weaknesses in other parts of the dataset. There are different methods and \"schools of thought\" on how stacking can be performed. If you are interested in this topic, then I suggest you to have a look at this and this to start. Here I will show a simple technique that is known as \"Soft Voting\" and can be implemented with Sklearn VotingClassifier.", "link": "https://www.kaggle.com/den3b81/better-predictions-stacking-with-votingclassifier", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2017-02-23 15:19:42", "date_scraped": "2020-12-13 17:56:15", "words": 135, "sentences": 5, "runtime": 0.0, "description_category": "Real Estate, Rental & Leasing", "description_category_score": 0.1412329077720642, "description_category_runtime": 11.815, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.48083171248435974, "description_subcategory_runtime": 18.715, "category": "Real Estate, Rental & Leasing", "category_score": 0.1412329077720642, "subcategory": "Machine Learning", "subcategory_score": 0.48083171248435974, "runtime_cat": 30.531, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.817", "language_code": "en", "language_score": "0.9999980792886564", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "may well know kaggle competition winner usually resort stacking metaensembling technique involving combination several 1st level predictive model generate 2nd level model tends outperform usually happens 2nd level model somewhat able exploit strength 1st level model perform best smoothing impact weakness part dataset different method school thought stacking performed interested topic suggest look start show simple technique known soft voting implemented sklearn votingclassifier", "tags_descriptive": []}