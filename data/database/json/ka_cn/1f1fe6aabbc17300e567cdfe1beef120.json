{"title": "Gaussian Naive Bayes", "description": "IntroductionIn this kernel, we will apply Bayesian inference on Santander Customer Transaction data, which has a binary target and 200 continuous features. We model the target as unknown Y and the features as observation X. The prior pY(y) reflects our knowledge about the unknown before observation. In this problem, Y is Bernoulli (only two classes) so it can be specified by setting the positive probability, which is usually set as the proportion of the positive class in the data. The likelihood fX|Y(x|y) models the distribution of the observation given that we know the class. The posterior pY|X(y|x) is our updated knowledge about the unknown after observation. The MAP (Maximum A Posteriori) estimator picks the class with the highest posterior probability. For binary classification, it has the same effect as setting a threshold of 0.5 for the positive posterior probability. The LMS (Least Mean Squares) estimator E[Y|X] picks the mean of the posterior distribution. For binary classification, this is just the positive posterior probability pY|X(1|x), which is what we need to submit for the competition. The Bayes rule for this problem is of the form pY|X(y|x)=pY(y)fX|Y(x|y)\u2211y\u2032pY(y\u2032)fX|Y(x|y\u2032) Here X represents a sequence of 200 observations X0,X1,\u2026,X199. We assume that the likelihood distributions are normal and independent. This gives us the Gaussian naive Bayes classifier (Gaussian means normal and naive means independent): pY|X0,X1,\u2026,X199(y|x0,x1,\u2026,x199)=pY(y)\u220f199i=0fXi|Y(xi|y)\u22111y\u2032=0pY(y\u2032)\u220f199i=0fXi|Y(xi|y\u2032) Note that we only require 1 number for the prior and 800 numbers for the likelihood (200 sample means and variances for each of the two classes). \"Fitting\" is just computing those numbers, and \"predicting\" is carried out according to the above formula (although we need to operate on the log scale because multiplying many small numbers poses a problem when our machine has limited precision). It is a very simple and efficient model.", "link": "https://www.kaggle.com/blackblitz/gaussian-naive-bayes", "tags": ["Exploratory Data Analysis", "Naive Bayes", "Classification"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pattern"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-03-02 05:40:00", "date_scraped": "2020-12-13 17:07:48", "words": 294, "sentences": 15, "sum_nltk": "IntroductionIn this kernel, we will apply Bayesian inference on Santander Customer Transaction data, which has a binary target and 200 continuous features.\nWe model the target as unknown Y and the features as observation X.\nThe prior pY(y) reflects our knowledge about the unknown before observation.\nThe likelihood fX|Y(x|y) models the distribution of the observation given that we know the class.\nThe posterior pY|X(y|x) is our updated knowledge about the unknown after observation.\nThe MAP (Maximum A Posteriori) estimator picks the class with the highest posterior probability.\nFor binary classification, it has the same effect as setting a threshold of 0.5 for the positive posterior probability.\nThe LMS (Least Mean Squares) estimator E[Y|X] picks the mean of the posterior distribution.\nFor binary classification, this is just the positive posterior probability pY|X(1|x), which is what we need to submit for the competition.\nWe assume that the likelihood distributions are normal and independent.\nThis gives us the Gaussian naive Bayes classifier (Gaussian means normal and naive means independent): pY|X0,X1,\u2026,X199(y|x0,x1,\u2026,x199)=pY(y)\u220f199i=0fXi|Y(xi|y)\u22111y\u2032=0pY(y\u2032)\u220f199i=0fXi|Y(xi|y\u2032) Note that we only require 1 number for the prior and 800 numbers for the likelihood (200 sample means and variances for each of the two classes).", "sum_nltk_words": 184, "sum_nltk_runtime": 0.003, "sum_t5": "in this kernel, we will apply Bayesian inference on Santander Customer Transaction data. we assume that the likelihood distributions are normal and independent. the prior pY(y) reflects our knowledge about the unknown before observation. the posterior pY|X(y|x) is our updated knowledge about the unknown after observation. the MAP (Maximum A Posteriori) estimator picks the class with the highest posterior probability.", "sum_t5_words": 60, "sum_t5_runtime": 6.288, "runtime": 0.01, "nltk_category": "Finance", "nltk_category_score": 0.05575413629412651, "nltk_category_runtime": 24.014, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8642213940620422, "nltk_subcategory_runtime": 39.19, "category": "Finance", "category_score": 0.05575413629412651, "subcategory": "Machine Learning", "subcategory_score": 0.8642213940620422, "runtime_cat": 63.204, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.77", "language_code": "en", "language_score": "0.9999984679946902", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introductionin kernel apply bayesian inference santander customer transaction data binary target 200 continuous feature model target unknown feature observation x prior pyy reflects knowledge unknown observation problem bernoulli two class specified setting positive probability usually set proportion positive class data likelihood fxyxy model distribution observation given know class posterior pyxyx updated knowledge unknown observation map maximum posteriori estimator pick class highest posterior probability binary classification effect setting threshold 05 positive posterior probability lm least mean square estimator eyx pick mean posterior distribution binary classification positive posterior probability pyx1x need submit competition bayes rule problem form pyxyxpyyfxyxyypyyfxyxy x represents sequence 200 observation x0x1x199 assume likelihood distribution normal independent give u gaussian naive bayes classifier gaussian mean normal naive mean independent pyx0x1x199yx0x1x199pyy199i0fxiyxiy1y0pyy199i0fxiyxiy note require 1 number prior 800 number likelihood 200 sample mean variance two class fitting computing number predicting carried according formula although need operate log scale multiplying many small number pose problem machine limited precision simple efficient model", "tags_descriptive": ["Exploratory Data Analysis", "Naive Bayes", "Classification"]}