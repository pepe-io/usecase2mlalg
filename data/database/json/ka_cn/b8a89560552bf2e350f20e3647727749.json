{"title": "IEEE Fraud: XGBoost with GPU (Fit in 40s)", "description": "About this kernelBefore I get started, I just wanted to say: huge props to Inversion! The official starter kernel is AWESOME; it's so simple, clean, straightforward, and pragmatic. It certainly saved me a lot of time wrangling with data, so that I can directly start tuning my models (real data scientists will call me lazy, but hey I'm an engineer I just want my stuff to work). I noticed two tiny problems with it:  It takes a lot of RAM to run, which means that if you are using a GPU, it might crash as you try to fill missing values. It takes a while to run (roughly 3500 seconds, which is more than an hour; again, I'm a lazy guy and I don't like waiting).  With this kernel, I bring some small changes:  Decrease RAM usage, so that it won't crash when you change it to GPU. I simply changed when we are deleting unused variables. Decrease running time from ~3500s to ~40s (yes, that's almost 90x faster), at the cost of a slight decrease in score. This is done by adding a single argument.  Again, my changes are super minimal (cause Inversion's kernel was already so awesome), but I hope it will save you some time and trouble (so that you can start working on cool stuff). ChangelogV4  Change some wording Prints XGBoost version Add random state to XGB for reproducibility", "link": "https://www.kaggle.com/xhlulu/ieee-fraud-xgboost-with-gpu-fit-in-40s", "tags": ["Classification", "Xgboost"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-07-24 17:32:38", "date_scraped": "2020-12-12 21:40:28", "words": 238, "sentences": 10, "sum_nltk": "About this kernelBefore I get started, I just wanted to say: huge props to Inversion!\nThe official starter kernel is AWESOME; it's so simple, clean, straightforward, and pragmatic.\nIt certainly saved me a lot of time wrangling with data, so that I can directly start tuning my models (real data scientists will call me lazy, but hey I'm an engineer I just want my stuff to work).\nI noticed two tiny problems with it:  It takes a lot of RAM to run, which means that if you are using a GPU, it might crash as you try to fill missing values.\nIt takes a while to run (roughly 3500 seconds, which is more than an hour; again, I'm a lazy guy and I don't like waiting).\nWith this kernel, I bring some small changes:  Decrease RAM usage, so that it won't crash when you change it to GPU.\nDecrease running time from ~3500s to ~40s (yes, that's almost 90x faster), at the cost of a slight decrease in score.\nAgain, my changes are super minimal (cause Inversion's kernel was already so awesome), but I hope it will save you some time and trouble (so that you can start working on cool stuff).", "sum_nltk_words": 197, "sum_nltk_runtime": 0.003, "sum_t5": "inversion's kernel is so simple, clean, straightforward, and pragmatic. it takes a lot of RAM to run, which means that if you are using a GPU, it might crash. if you are using a GPU, it might crash as you try to fill missing values. if you are using a GPU, it might crash as you try to fill missing values. if you are using a GPU, it might crash as you try to fill missing values.", "sum_t5_words": 77, "sum_t5_runtime": 5.021, "runtime": 0.0, "nltk_category": "Construction & Engineering", "nltk_category_score": 0.4025559425354004, "nltk_category_runtime": 18.145, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.5699855089187622, "nltk_subcategory_runtime": 29.317, "category": "Construction & Engineering", "category_score": 0.4025559425354004, "subcategory": "Machine Learning", "subcategory_score": 0.5699855089187622, "runtime_cat": 47.461, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.754", "language_code": "en", "language_score": "0.9999960031546855", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "kernelbefore get started wanted say huge prop inversion official starter kernel awesome simple clean straightforward pragmatic certainly saved lot time wrangling data directly start tuning model real data scientist call lazy hey im engineer want stuff work noticed two tiny problem take lot ram run mean using gpu might crash try fill missing value take run roughly 3500 second hour im lazy guy dont like waiting kernel bring small change decrease ram usage wont crash change gpu simply changed deleting unused variable decrease running time 3500s 40 yes thats almost 90x faster cost slight decrease score done adding single argument change super minimal cause inversion kernel already awesome hope save time trouble start working cool stuff changelogv4 change wording print xgboost version add random state xgb reproducibility", "tags_descriptive": ["Classification", "Xgboost"]}