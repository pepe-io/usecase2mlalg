{"title": "M5: How to get your public LB score & rank", "description": "M5 SetupThis competition is a little different from others. The true labels of the public leaderboard are now revealed. The following is an excerpt from the M5 Competition Guide: After the end of the validation phase, i.e., from June 1, 2020 to 30 June of the same year, the participants will be provided with the actual values of the 28 days of data used for scoring their performance during the validation phase. They will be asked then to re-estimate or adjust (if needed) their forecasting models in order to submit their final forecasts and prediction intervals for the following 28 days, i.e., the data used for the final evaluation of the participants. During this time, there will be no leaderboard, meaning that no feedback will be given to the participants about their score after submitting their forecasts. Thus, although the participants will be free to (re)submit their forecasts any time they wish (a maximum of 5 entries per day), they will not be aware of their absolute, as well as their relative performance. The final ranks of the participants will be made available only at the end of competition, when the test data will be made available. This is done in order for the competition to simulate reality as closely as possible, given that in real life forecasters do not know the future.  So while the public LB on Kaggle will either get infested by scores that use the true labels or Kaggle will freeze the public LB, we now have access to the actual labels and hence can calculate the validation scores (and rank as of 31st May, 2020) at various levels of aggregations ourselves. The weights used in this notebook are the weights for the public LB (validation data). Note that the private LB (evaluation data) uses a different set of weights. A summary of the weights comparison is shared here: https://www.kaggle.com/rohanrao/m5-the-weighing-scale Note that the final private LB ranking will be based on the test data at the end of the competition.", "link": "https://www.kaggle.com/rohanrao/m5-how-to-get-your-public-lb-score-rank", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": [], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-06-17 17:48:52", "date_scraped": "2020-12-13 13:22:57", "words": 335, "sentences": 12, "sum_nltk": "The following is an excerpt from the M5 Competition Guide: After the end of the validation phase, i.e., from June 1, 2020 to 30 June of the same year, the participants will be provided with the actual values of the 28 days of data used for scoring their performance during the validation phase.\nThey will be asked then to re-estimate or adjust (if needed) their forecasting models in order to submit their final forecasts and prediction intervals for the following 28 days, i.e., the data used for the final evaluation of the participants.\nDuring this time, there will be no leaderboard, meaning that no feedback will be given to the participants about their score after submitting their forecasts.\nThus, although the participants will be free to (re)submit their forecasts any time they wish (a maximum of 5 entries per day), they will not be aware of their absolute, as well as their relative performance.\nNote that the private LB (evaluation data) uses a different set of weights.\nA summary of the weights comparison is shared here: https://www.kaggle.com/rohanrao/m5-the-weighing-scale Note that the final private LB ranking will be based on the test data at the end of the competition.", "sum_nltk_words": 192, "sum_nltk_runtime": 0.004, "sum_t5": "the public leaderboard is now revealed. the final ranks of the participants will be made available only at the end of competition. the public LB will either get infested by scores that use the true labels or Kaggle will freeze the public LB. the private LB uses a different set of weights. the final private LB ranking will be based on the test data at the end of the competition.. the competition is open to all participants.", "sum_t5_words": 77, "sum_t5_runtime": 6.193, "runtime": 0.004, "nltk_category": "Finance", "nltk_category_score": 0.37586548924446106, "nltk_category_runtime": 17.864, "nltk_subcategory": "Valuation", "nltk_subcategory_score": 0.6275374293327332, "nltk_subcategory_runtime": 29.074, "category": "Finance", "category_score": 0.37586548924446106, "subcategory": "Valuation", "subcategory_score": 0.6275374293327332, "runtime_cat": 46.939, "programming_language": "Jupyter Notebook", "ml_score": "0.5", "engagement_score": "0.73", "language_code": "en", "language_score": "0.9999969462868132", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "m5 setupthis competition little different others true label public leaderboard revealed following excerpt m5 competition guide end validation phase ie june 1 2020 30 june year participant provided actual value 28 day data used scoring performance validation phase asked reestimate adjust needed forecasting model order submit final forecast prediction interval following 28 day ie data used final evaluation participant time leaderboard meaning feedback given participant score submitting forecast thus although participant free resubmit forecast time wish maximum 5 entry per day aware absolute well relative performance final rank participant made available end competition test data made available done order competition simulate reality closely possible given real life forecaster know future public lb kaggle either get infested score use true label kaggle freeze public lb access actual label hence calculate validation score rank 31st may 2020 various level aggregation weight used notebook weight public lb validation data note private lb evaluation data us different set weight summary weight comparison shared httpswwwkagglecomrohanraom5theweighingscale note final private lb ranking based test data end competition", "tags_descriptive": []}