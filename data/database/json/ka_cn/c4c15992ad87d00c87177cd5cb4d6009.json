{"title": "Support Vector Machine - [0.925]", "description": "Support Vector Machine scores LB 0.925In our previous kernel here, we built 512 linear models using logistic regression in conjuction with the magic feature and scored LB 0.808. In this kernel we will build 512 nonlinear models using support vector machine with polynomial degree=4 kernel and score LB 0.926. Previously we performed feature selection with Lasso, aka LR's L1-penalty. Now we will perform feature selection with sklearn's VarianceThreshold selector (which will select more or less the same features). The success of this kernel demonstrates the nature of \"Instant Gratification\" competition data. It appears that the data is actually 512 datasets combined together. Each dataset has rougly 512 observations. Thus the total training data has 262144 = 512 * 512 observations. Each partial dataset is identified by a unique wheezy-copper-turtle-magic value. This kernel shows that each dataset's target is a nonlinear function of approximately 40 important features (and each dataset uses a different 40 important features). The next thing to investigate is whether there are interactions between the partial datasets that can improve prediction. If that is the case then instead of building 512 separate models, we need to build a single model that allows interactions. (Possibly NN with interesting architecture). Also each model (in this kernel) only uses about 40 features. Within each partial dataset, are the other 215 features really useless, or can we use them to improve prediction? Load Data", "link": "https://www.kaggle.com/cdeotte/support-vector-machine-0-925", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-05-21 00:23:21", "date_scraped": "2020-12-13 11:53:26", "words": 232, "sentences": 15, "sum_nltk": "Support Vector Machine scores LB 0.925In our previous kernel here, we built 512 linear models using logistic regression in conjuction with the magic feature and scored LB 0.808.\nIn this kernel we will build 512 nonlinear models using support vector machine with polynomial degree=4 kernel and score LB 0.926.\nPreviously we performed feature selection with Lasso, aka LR's L1-penalty.\nNow we will perform feature selection with sklearn's VarianceThreshold selector (which will select more or less the same features).\nThe success of this kernel demonstrates the nature of \"Instant Gratification\" competition data.\nIt appears that the data is actually 512 datasets combined together.\nEach dataset has rougly 512 observations.\nThus the total training data has 262144 = 512 * 512 observations.\nEach partial dataset is identified by a unique wheezy-copper-turtle-magic value.\nThis kernel shows that each dataset's target is a nonlinear function of approximately 40 important features (and each dataset uses a different 40 important features).\nThe next thing to investigate is whether there are interactions between the partial datasets that can improve prediction.\nAlso each model (in this kernel) only uses about 40 features.\nWithin each partial dataset, are the other 215 features really useless, or can we use them to improve prediction?\nLoad Data", "sum_nltk_words": 192, "sum_nltk_runtime": 0.003, "sum_t5": "support vector machine scores LB 0.925. this kernel builds 512 nonlinear models using support vector machine. each partial dataset is identified by a unique wheezy-copper-turtle-magic value. each model (in this kernel) only uses about 40 features. a randomized test will be performed to see if there are interactions. a randomized test will be performed to see if there are interactions. a randomized test will be performed to see if there are interactions", "sum_t5_words": 72, "sum_t5_runtime": 6.264, "runtime": 0.005, "nltk_category": "Utilities", "nltk_category_score": 0.32186880707740784, "nltk_category_runtime": 19.472, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8700733780860901, "nltk_subcategory_runtime": 31.217, "category": "Utilities", "category_score": 0.32186880707740784, "subcategory": "Machine Learning", "subcategory_score": 0.8700733780860901, "runtime_cat": 50.69, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.756", "language_code": "en", "language_score": "0.9999951470044299", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "support vector machine score lb 0925in previous kernel built 512 linear model using logistic regression conjuction magic feature scored lb 0808 kernel build 512 nonlinear model using support vector machine polynomial degree4 kernel score lb 0926 previously performed feature selection lasso aka lr l1penalty perform feature selection sklearns variancethreshold selector select le feature success kernel demonstrates nature instant gratification competition data appears data actually 512 datasets combined together dataset rougly 512 observation thus total training data 262144 512 512 observation partial dataset identified unique wheezycopperturtlemagic value kernel show datasets target nonlinear function approximately 40 important feature dataset us different 40 important feature next thing investigate whether interaction partial datasets improve prediction case instead building 512 separate model need build single model allows interaction possibly nn interesting architecture also model kernel us 40 feature within partial dataset 215 feature really useless use improve prediction load data", "tags_descriptive": []}