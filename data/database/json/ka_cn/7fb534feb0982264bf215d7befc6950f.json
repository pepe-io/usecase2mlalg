{"title": "Easily Load train.csv w/o Crash, Save Feather File", "description": "The Problem:The training data, train.csv is a large file (~5 GB) - this can be problematic if you have relatively low RAM (8 GB).The Solution:Set low_memory=True in Pandas' read_csvOn a machine with relatively low RAM, attempting to load the entire file in a pandas DataFrame can lead to failure caused by running out of memory.  One way of fixing this issue is to make use of the low_memory=True argument of read_csv.  With this method, the csv file is processed in chunks requiring lower memory usage, while at the same time reading the csv's contents into a single DataFrame. But the Jupyter kernel still keeps restarting even with low_memory=True....why?The dtypes of the columns of the DataFrame must be specified in read_csv if we wish to set low_memory=True.  This is because not specifying dtypes forces pandas to guess column dtypes - which is a memory-intensive task.  Please see this Stack Overflow answer for a additional explanation: https://stackoverflow.com/a/27232309 The Complete SolutionWe first create a new file called small_train.csv using only the first row of data from train.csv:", "link": "https://www.kaggle.com/kunalkotian/easily-load-train-csv-w-o-crash-save-feather-file", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": [], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2017-11-20 09:58:39", "date_scraped": "2020-12-12 19:41:27", "words": 178, "sentences": 6, "runtime": 0.002, "description_category": "Real Estate, Rental & Leasing", "description_category_score": 0.23801657557487488, "description_category_runtime": 18.311, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.5640372037887573, "description_subcategory_runtime": 29.562, "category": "Real Estate, Rental & Leasing", "category_score": 0.23801657557487488, "subcategory": "Machine Learning", "subcategory_score": 0.5640372037887573, "runtime_cat": 47.874, "programming_language": "Jupyter Notebook", "ml_score": "0.5", "engagement_score": "0.75", "language_code": "en", "language_score": "0.9999989451301575", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "problemthe training data traincsv large file 5 gb problematic relatively low ram 8 gbthe solutionset low_memorytrue panda read_csvon machine relatively low ram attempting load entire file panda dataframe lead failure caused running memory one way fixing issue make use low_memorytrue argument read_csv method csv file processed chunk requiring lower memory usage time reading csvs content single dataframe jupyter kernel still keep restarting even low_memorytruewhythe dtypes column dataframe must specified read_csv wish set low_memorytrue specifying dtypes force panda guess column dtypes memoryintensive task please see stack overflow answer additional explanation httpsstackoverflowcoma27232309 complete solutionwe first create new file called small_traincsv using first row data traincsv", "tags_descriptive": []}