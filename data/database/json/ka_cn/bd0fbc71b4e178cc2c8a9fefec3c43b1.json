{"title": "Object Detection Techniques", "description": "Object Detection : Historical PerspectiveThis notebook is forked and edited from the awesome youtube channel of Siraj Rawal where he demo'd about YOLO v2. It's amazing, but to apreciate the accuracy of object detection,segmentation and labelling of YOLOv2, one must go through the eventful history of progress in this field. This gives an idea on how every major Deep Learning Architecture works int the field of Computer Vision. Once done that, one'll realize that these advance networks are not so blackbox-y anymore and really simple to understand and easy to build upon. Have Fun!! Some Object Detection History (2001-2017)1. The first efficient Face Detector (Viola-Jones Algorithm, 2001) An efficient algorithm for face detection was invented by Paul Viola & Michael Jones  Their demo showed faces being detected in real time on a webcam feed. Was the most stunning demonstration of computer vision and its potential at the time.  Soon, it was implemented in OpenCV & face detection became synonymous with Viola and Jones algorithm. BASIC IDEA:  Took a bunch of faces as data.  Hard-coded the features of a face. Trained an SVM(Classifier) on the featureset of the faces. Used that Classifier to detect faces!       Disadvantage : It was unable to detect faces in other orientation or configurations (tilted,upside down,wearing a mask,etc.)    2. Much more efficient detection technique (Histograms of Oriented Gradients, 2005) Navneet Dalal and Bill Triggs invented \"HOG\" for pedestrian detection Their feature descriptor, Histograms of Oriented Gradients (HOG), significantly outperformed existing algorithms in this task Handcoded features, just like before  For every single pixel, we want to look at the pixels that directly surrounding it:     Goal is, how dark is current pixel compared to surrounding pixels? We will then draw an arrow showing in which direction the image is getting darker:    We repeat that process for every single pixel in the image Every pixel is replaced by an arrow. These arrows are called gradients Gradients show the flow from light to dark across the entire image:    We'll break up the image into small squares of 16x16 pixels each In each square, we\u2019ll count up how many gradients point in each major direction Then we\u2019ll replace that square in the image with the arrow directions that were the strongest. End result? Original image converted into simple representation that captures basic structure of a face in a simple way: Detecting faces means find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:    BASIC IDEA :  For an Image II, analyze each pixel PiPi of II for the relative dark pixels directly surounding it. Then add an arrow pointing in the direction of the flow of darkness relative to PiPi. This process of assigning an oriented gradient to a pixel p by analyzing it's surrounding pixels is performed for every pixel in the image. Assuming HOG(II) as a function that takes an input as an Image I, what it does is replaces every pixel with an arrow. Arrows = Gradients. Gradients show the flow from light to dark across an antire image. Since complex feaatures like eyes may end up giving too many gradients, we need to aggregate the whole HOG(I) in order to make a 'global representation' .  So, we break up the image into squares of 1616x1616 and assign an aggregate gradient G\u2032G\u2032 to each square ,where the aggregate function could be max(All gradients inside the square),min(),etc.      Disadvantage : Despite being good in many applications, it still used hand coded features which failed in a more generalized setting with much noise and distractions in the background.    The Deep Learning Era begins (2012) Convolutional Neural Networks became the gold standard for image classification after Kriszhevsky's CNN's performance during ImageNet   While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.  In classification, there\u2019s generally an image with a single object as the focus and the task is to say what that image is  But when we look at the world around us, we carry out far more complex task  We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another! Can CNNs help us with such complex tasks? Yes.    We can take a classifier like VGGNet or Inception and turn it into an object detector by sliding a small window across the image At each step you run the classifier to get a prediction of what sort of object is inside the current window.  Using a sliding window gives several hundred or thousand predictions for that image, but you only keep the ones the classifier is the most certain about. This approach works but it\u2019s obviously going to be very slow, since you need to run the classifier many times.It's a brute-force-y approach.AND computationaly expensive.  BASIC IDEA :  Take an image II and divide it into nn equal squares(ii). Then, II is a set of smaller images  (i1i1,i2i2,i3i3...inin) Run X a pre-trained image classifier CNN over each square ii. X analyzes each square ii and classifies it into a object class ll with a probability score of \u03b1\u03b1. This operation result into a bunch of labels X(II) = (l1,\u03b11l1,\u03b11),(l2,\u03b12l2,\u03b12) ... (ln,\u03b1nln,\u03b1n)  , where  (l1,\u03b11l1,\u03b11) = X(i1i1). We keep only those labels, which the CNN feels most confident about i.e. the labels with the highest scores. Approach for identifiction is Labeling then Detection.     A better approach, R-CNN  R-CNN creates bounding boxes, or region proposals, using a process called Selective Search  At a high level, Selective Search looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.    Generate a set of proposals for bounding boxes. Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is. Run the box through a linear regression model to output tighter coordinates for the box once the object has been classified.  Some improvements to R-CNNR-CNN: https://arxiv.org/abs/1311.2524 Fast R-CNN: https://arxiv.org/abs/1504.08083 Faster R-CNN: https://arxiv.org/abs/1506.01497 Mask R-CNN: https://arxiv.org/abs/1703.06870  But YOLO takes a different approach What is YOLO?If you aren't motivated enough to know what it is may be this video will get you excited! (https://www.youtube.com/watch?v=VOC3huqHrss)  YOLO takes a completely different approach.  It\u2019s not a traditional classifier that is repurposed to be an object detector.  YOLO actually looks at the image just once (hence its name: You Only Look Once) but in a clever way.  YOLO divides up the image into a grid of 13 by 13 cells:   Each of these cells is responsible for predicting 5 bounding boxes.  A bounding box describes the rectangle that encloses an object. YOLO also outputs a confidence score that tells us how certain it is that the predicted bounding box actually encloses some object. This score doesn\u2019t say anything about what kind of object is in the box, just if the shape of the box is any good.  The predicted bounding boxes may look something like the following (the higher the confidence score, the fatter the box is drawn):   For each bounding box, the cell also predicts a class.  This works just like a classifier: it gives a probability distribution over all the possible classes.  YOLO was trained on the PASCAL VOC dataset, which can detect 20 different classes such as:  bicycle  boat car cat dog person  The confidence score for the bounding box and the class prediction are combined into one final score that tells us the probability that this bounding box contains a specific type of object.  For example, the big fat yellow box on the left is 85% sure it contains the object \u201cdog\u201d:    Since there are 13\u00d713 = 169 grid cells and each cell predicts 5 bounding boxes, we end up with 845 bounding boxes in total.  It turns out that most of these boxes will have very low confidence scores, so we only keep the boxes whose final score is 30% or more (you can change this threshold depending on how accurate you want the detector to be).  The final prediction is then:   From the 845 total bounding boxes we only kept these three because they gave the best results.  But note that even though there were 845 separate predictions, they were all made at the same time \u2014 the neural network just ran once. And that\u2019s why YOLO is so powerful and fast.  The architecture of YOLO is simple, it\u2019s just a convolutional neural network:  This neural network only uses standard layer types: convolution with a 3\u00d73 kernel and max-pooling with a 2\u00d72 kernel. No fancy stuff. There is no fully-connected layer in YOLOv2. The very last convolutional layer has a 1\u00d71 kernel and exists to reduce the data to the shape 13\u00d713\u00d7125. This 13\u00d713 should look familiar: that is the size of the grid that the image gets divided into. So we end up with 125 channels for every grid cell. These 125 numbers contain the data for the bounding boxes and the class predictions. Why 125? Well, each grid cell predicts 5 bounding boxes and a bounding box is described by 25 data elements:  x, y, width, height for the bounding box\u2019s rectangle the confidence score the probability distribution over the classes  Using YOLO is simple: you give it an input image (resized to 416\u00d7416 pixels), it goes through the convolutional network in a single pass, and comes out the other end as a 13\u00d713\u00d7125 tensor describing the bounding boxes for the grid cells. All you need to do then is compute the final scores for the bounding boxes and throw away the ones scoring lower than 30%. Improvements to YOLO v1YoLO v2 vs YoLO v1  Speed (45 frames per second\u200a\u2014\u200abetter than realtime) Network understands generalized object representation (This allowed them to train the network on real world images and predictions on artwork was still fairly accurate). faster version (with smaller architecture)\u200a\u2014\u200a155 frames per sec but is less accurate.  Paper here https://arxiv.org/pdf/1612.08242v1.pdf", "link": "https://www.kaggle.com/infernop/object-detection-techniques", "tags": ["CNN"], "kind": ["Project", "(Notebook)"], "ml_libs": ["pattern"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-03-13 07:13:53", "date_scraped": "2020-12-12 18:38:54", "words": 1774, "sentences": 67, "sum_nltk": "A better approach, R-CNN  R-CNN creates bounding boxes, or region proposals, using a process called Selective Search  At a high level, Selective Search looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.\nYOLO divides up the image into a grid of 13 by 13 cells:   Each of these cells is responsible for predicting 5 bounding boxes.\nYOLO also outputs a confidence score that tells us how certain it is that the predicted bounding box actually encloses some object.\nYOLO was trained on the PASCAL VOC dataset, which can detect 20 different classes such as:  bicycle  boat car cat dog person  The confidence score for the bounding box and the class prediction are combined into one final score that tells us the probability that this bounding box contains a specific type of object.\nWell, each grid cell predicts 5 bounding boxes and a bounding box is described by 25 data elements:  x, y, width, height for the bounding box\u2019s rectangle the confidence score the probability distribution over the classes  Using YOLO is simple: you give it an input image (resized to 416\u00d7416 pixels), it goes through the convolutional network in a single pass, and comes out the other end as a 13\u00d713\u00d7125 tensor describing the bounding boxes for the grid cells.", "sum_nltk_words": 233, "sum_nltk_runtime": 0.02, "sum_t5": "a notebook is forked and edited from the awesome youtube channel of siraj rawal. it gives an idea on how every major Deep Learning Architecture works in the field of Computer Vision. the first efficient face detecting algorithm was invented by Paul Viola & Michael Jones. the most efficient detection technique was \"histograms of oriented gradients\" for pedestrian detection. the first'smart' algorithm was a'smart' algorithm.", "sum_t5_words": 65, "sum_t5_runtime": 6.551, "runtime": 0.011, "nltk_category": "Utilities", "nltk_category_score": 0.17975711822509766, "nltk_category_runtime": 20.935, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.7803246378898621, "nltk_subcategory_runtime": 34.002, "category": "Utilities", "category_score": 0.17975711822509766, "subcategory": "Machine Learning", "subcategory_score": 0.7803246378898621, "runtime_cat": 54.938, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.773", "language_code": "en", "language_score": "0.9999974281267554", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "object detection historical perspectivethis notebook forked edited awesome youtube channel siraj rawal demod yolo v2 amazing apreciate accuracy object detectionsegmentation labelling yolov2 one must go eventful history progress field give idea every major deep learning architecture work int field computer vision done onell realize advance network blackboxy anymore really simple understand easy build upon fun object detection history 200120171 first efficient face detector violajones algorithm 2001 efficient algorithm face detection invented paul viola michael jones demo showed face detected real time webcam feed stunning demonstration computer vision potential time soon implemented opencv face detection became synonymous viola jones algorithm basic idea took bunch face data hardcoded feature face trained svmclassifier featureset face used classifier detect face disadvantage unable detect face orientation configuration tiltedupside downwearing masketc 2 much efficient detection technique histogram oriented gradient 2005 navneet dalal bill triggs invented hog pedestrian detection feature descriptor histogram oriented gradient hog significantly outperformed existing algorithm task handcoded feature like every single pixel want look pixel directly surrounding goal dark current pixel compared surrounding pixel draw arrow showing direction image getting darker repeat process every single pixel image every pixel replaced arrow arrow called gradient gradient show flow light dark across entire image well break image small square 16x16 pixel square well count many gradient point major direction well replace square image arrow direction strongest end result original image converted simple representation capture basic structure face simple way detecting face mean find part image look similar known hog pattern extracted bunch training face basic idea image ii analyze pixel pipi ii relative dark pixel directly surounding add arrow pointing direction flow darkness relative pipi process assigning oriented gradient pixel p analyzing surrounding pixel performed every pixel image assuming hogii function take input image replaces every pixel arrow arrow gradient gradient show flow light dark across antire image since complex feaatures like eye may end giving many gradient need aggregate whole hogi order make global representation break image square 1616x1616 assign aggregate gradient gg square aggregate function could maxall gradient inside squareminetc disadvantage despite good many application still used hand coded feature failed generalized setting much noise distraction background deep learning era begin 2012 convolutional neural network became gold standard image classification kriszhevskys cnns performance imagenet result impressive image classification far simpler complexity diversity true human visual understanding classification there generally image single object focus task say image look world around u carry far complex task see complicated sight multiple overlapping object different background classify different object also identify boundary difference relation one another cnns help u complex task yes take classifier like vggnet inception turn object detector sliding small window across image step run classifier get prediction sort object inside current window using sliding window give several hundred thousand prediction image keep one classifier certain approach work obviously going slow since need run classifier many timesits bruteforcey approachand computationaly expensive basic idea take image ii divide nn equal squaresii ii set smaller image i1i1i2i2i3i3inin run x pretrained image classifier cnn square ii x analyzes square ii classifies object class probability score operation result bunch label xii l11l11l22l22 lnnlnn l11l11 xi1i1 keep label cnn feel confident ie label highest score approach identifiction labeling detection better approach rcnn rcnn creates bounding box region proposal using process called selective search high level selective search look image window different size size try group together adjacent pixel texture color intensity identify object generate set proposal bounding box run image bounding box pretrained alexnet finally svm see object image box run box linear regression model output tighter coordinate box object classified improvement rcnnrcnn httpsarxivorgabs13112524 fast rcnn httpsarxivorgabs150408083 faster rcnn httpsarxivorgabs150601497 mask rcnn httpsarxivorgabs170306870 yolo take different approach yoloif arent motivated enough know may video get excited httpswwwyoutubecomwatchvvoc3huqhrss yolo take completely different approach traditional classifier repurposed object detector yolo actually look image hence name look clever way yolo divide image grid 13 13 cell cell responsible predicting 5 bounding box bounding box describes rectangle encloses object yolo also output confidence score tell u certain predicted bounding box actually encloses object score doesnt say anything kind object box shape box good predicted bounding box may look something like following higher confidence score fatter box drawn bounding box cell also predicts class work like classifier give probability distribution possible class yolo trained pascal voc dataset detect 20 different class bicycle boat car cat dog person confidence score bounding box class prediction combined one final score tell u probability bounding box contains specific type object example big fat yellow box left 85 sure contains object dog since 1313 169 grid cell cell predicts 5 bounding box end 845 bounding box total turn box low confidence score keep box whose final score 30 change threshold depending accurate want detector final prediction 845 total bounding box kept three gave best result note even though 845 separate prediction made time neural network ran thats yolo powerful fast architecture yolo simple convolutional neural network neural network us standard layer type convolution 33 kernel maxpooling 22 kernel fancy stuff fullyconnected layer yolov2 last convolutional layer 11 kernel exists reduce data shape 1313125 1313 look familiar size grid image get divided end 125 channel every grid cell 125 number contain data bounding box class prediction 125 well grid cell predicts 5 bounding box bounding box described 25 data element x width height bounding box rectangle confidence score probability distribution class using yolo simple give input image resized 416416 pixel go convolutional network single pas come end 1313125 tensor describing bounding box grid cell need compute final score bounding box throw away one scoring lower 30 improvement yolo v1yolo v2 v yolo v1 speed 45 frame per second better realtime network understands generalized object representation allowed train network real world image prediction artwork still fairly accurate faster version smaller architecture 155 frame per sec le accurate paper httpsarxivorgpdf161208242v1pdf", "tags_descriptive": ["Convolutional Neural Network (CNN)"]}