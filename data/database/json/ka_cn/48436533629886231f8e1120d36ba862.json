{"title": "TPU On All 300k Images Without Crashing", "description": "About this kernelMost of this notebook is blatantly stolen from https://www.kaggle.com/xhlulu/alaska2-efficientnet-on-tpus I've uploaded some tfrecord datasets. All permutations of the same cover image should be separated into each tfrecord file. So two tfrecord files should never have any image that matches the cover. This should make validation better, because you don't want the model to see the cover in train and then the stega image in validation. The main contribution here is the 6 alaska datasets that have been turned into TFRecords and a cache trick. Because TPUs have limited ram, if you decode the JPEG and then cache it, you will run out of ram because you are caching an uncompressed JPEG, which is massive. Instead, cache the JPEG binary and decode it. TPUs have alot of CPU cores for that job and your main bottleneck is usually file access. Also, the tfrecords stream better than 300k images as separate files. Tweak it yourself and experiment!", "link": "https://www.kaggle.com/hooong/tpu-on-all-300k-images-without-crashing", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-07-03 21:13:23", "date_scraped": "2020-12-12 16:29:24", "words": 157, "sentences": 10, "runtime": 0.002, "description_category": "Media & Publishing", "description_category_score": 0.4177435040473938, "description_category_runtime": 14.953, "description_subcategory": "Valuation", "description_subcategory_score": 0.950211763381958, "description_subcategory_runtime": 23.865, "category": "Media & Publishing", "category_score": 0.4177435040473938, "subcategory": "Valuation", "subcategory_score": 0.950211763381958, "runtime_cat": 38.818, "programming_language": "Jupyter Notebook", "ml_score": "0.7", "engagement_score": "0.672", "language_code": "en", "language_score": "0.9999969297492518", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "kernelmost notebook blatantly stolen httpswwwkagglecomxhlulualaska2efficientnetontpus ive uploaded tfrecord datasets permutation cover image separated tfrecord file two tfrecord file never image match cover make validation better dont want model see cover train stega image validation main contribution 6 alaska datasets turned tfrecords cache trick tpus limited ram decode jpeg cache run ram caching uncompressed jpeg massive instead cache jpeg binary decode tpus alot cpu core job main bottleneck usually file access also tfrecords stream better 300k image separate file tweak experiment", "tags_descriptive": []}