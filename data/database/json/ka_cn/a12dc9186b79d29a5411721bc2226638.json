{"title": "[spaCy] Name Entity Recognition", "description": "\u25bc See the result at bottom directly if you're in a hurry \u25bc Idea and motivationFor case like: 'Is Taiwan a good place to live ?' If we apply 'Taiwan' to word2vec directly, there are 3 cases might happen:  It works!  Taiwan is a very common word, so the word2vec method really captured its meaning in its vector space!  Somehow it works, but not very well.  Taiwan is not a very common word, so word2vec model cannot capture its meaning very well.  It fails.  Either the word is too rare or contains typo in it.   And spaCy comes to me. It has an useful attribute \"ent_type_\" for each token, it tries to estimate the word's entity type in its pre-defined categories based on the sentence's context. For example: testcase = 'Is Taiwan a good place to live ?' doc = nlp(testcase) # apply spacy on the sentense taiwan = doc[1] # word 'Taiwan' is at index 1 print(taiwan.ent_type_)  # yield \"GPE\" which means \"Countries, cities, states.\"    Then we can replace the original sentence. From: 'Is Taiwan a good place to live ?' To  : 'Is country a good place to live ?'    The word \"country\"'s meaning is very likely captured better than the word 'Taiwan'. Although it losses some information, but the dataset becomes less noisy. It is kind of trade off, and I believe that adding a model trained on such dataset can increase ensembling result. Requirements install spaCy and its corpus ( I use its en_core_web_md corpus )  So, you can't run it directly here :(", "link": "https://www.kaggle.com/hubert0527/spacy-name-entity-recognition", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["tensorflow", "nltk", "spacy", "pattern"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2017-06-01 17:25:19", "date_scraped": "2020-12-13 16:22:58", "words": 275, "sentences": 11, "sum_nltk": "\u25bc See the result at bottom directly if you're in a hurry \u25bc Idea and motivationFor case like: 'Is Taiwan a good place to live ?' If we apply 'Taiwan' to word2vec directly, there are 3 cases might happen:  It works!\nTaiwan is a very common word, so the word2vec method really captured its meaning in its vector space!\nTaiwan is not a very common word, so word2vec model cannot capture its meaning very well.\nIt has an useful attribute \"ent_type_\" for each token, it tries to estimate the word's entity type in its pre-defined categories based on the sentence's context.\nFor example: testcase = 'Is Taiwan a good place to live ?' doc = nlp(testcase) # apply spacy on the sentense taiwan = doc[1] # word 'Taiwan' is at index 1 print(taiwan.ent_type_)  # yield \"GPE\" which means \"Countries, cities, states.\"    Then we can replace the original sentence.\nFrom: 'Is Taiwan a good place to live ?' To  : 'Is country a good place to live ?'    The word \"country\"'s meaning is very likely captured better than the word 'Taiwan'.\nIt is kind of trade off, and I believe that adding a model trained on such dataset can increase ensembling result.", "sum_nltk_words": 204, "sum_nltk_runtime": 0.003, "sum_t5": "spaCy has an attribute \"ent_type_\" for each token, it tries to estimate the word's entity type in its pre-defined categories based on the sentence's context. although it loses some information, but the dataset becomes less noisy. spaCy is a very powerful tool for analyzing the meaning of words. it is a very fast and easy way to get a good idea of what a word means.", "sum_t5_words": 66, "sum_t5_runtime": 5.893, "runtime": 0.003, "nltk_category": "Biotechnological & Life Sciences", "nltk_category_score": 0.1009862869977951, "nltk_category_runtime": 20.863, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.3488021790981293, "nltk_subcategory_runtime": 33.948, "category": "Biotechnological & Life Sciences", "category_score": 0.1009862869977951, "subcategory": "Machine Learning", "subcategory_score": 0.3488021790981293, "runtime_cat": 54.81, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.776", "language_code": "en", "language_score": "0.999996459436554", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "see result bottom directly youre hurry idea motivationfor case like taiwan good place live apply taiwan word2vec directly 3 case might happen work taiwan common word word2vec method really captured meaning vector space somehow work well taiwan common word word2vec model cannot capture meaning well fails either word rare contains typo spacy come useful attribute ent_type_ token try estimate word entity type predefined category based sentence context example testcase taiwan good place live doc nlptestcase apply spacy sentense taiwan doc1 word taiwan index 1 printtaiwanent_type_ yield gpe mean country city state replace original sentence taiwan good place live country good place live word country meaning likely captured better word taiwan although loss information dataset becomes le noisy kind trade believe adding model trained dataset increase ensembling result requirement install spacy corpus use en_core_web_md corpus cant run directly", "tags_descriptive": []}