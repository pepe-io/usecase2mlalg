{"title": "Lessons from Toxic : Blending is the new sexy", "description": "The public kernals are filled with Blends of Blends of Blends and maybe someone can blend to 1.00000 too. Ok, Nothing wrong in that. I just thought good solid kernals should not be lost within the chaos. So, I've compiled and categorized a list of resources that have taught something to me. Please add on to this list in the comments below, if I've missed any good kernals. Simple Naive-Bayes: https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline#261316          # Custom SK learn class for Naive Bayes    Tf-IDF and simple Logistic Regression: https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams            # ngram range=(2,6)   -->    can capture a lot of information! https://www.kaggle.com/yekenot/toxic-regression  Wordbatch: https://www.kaggle.com/anttip/wordbatch-1-3-3-fm-ftrl-lb-0-9812                          # FM_FTRL  H20, Word2Vec in R: https://www.kaggle.com/brandenkmurray/h2o-word2vec-starter-toxic-comments  Deep learning: GRU(Gated Recurrent Units): https://www.kaggle.com/yekenot/pooled-gru-fasttext/code  Capsule Net(with GRU): https://www.kaggle.com/chongjiujjin/capsule-net-with-gru https://github.com/Godricly/comment_toxic https://github.com/bojone/Capsule/blob/master/Capsule_Keras.py                   # Base implementations of CapsNet in Keras https://github.com/XifengGuo/CapsNet-Keras    LSTM: https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras                             # Explains basic of LSTM well https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-069                         # Simple baseline https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout                              # Glove pretrained features + LSTM  Attention: https://www.kaggle.com/sermakarevich/hierarchical-attention-network https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043                      # LSTM + Attention layer  Convolution(CNNs): https://www.kaggle.com/yekenot/textcnn-2d-convolution https://www.kaggle.com/sbongo/for-beginners-go-even-deeper-with-char-gram-cnn  Bi Directional GRU CNNs: https://www.kaggle.com/konohayui/bi-gru-cnn-poolings https://www.kaggle.com/eashish/bidirectional-gru-with-convolution  Blending Helpers:What is blending?  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/51058 What all do I need to check before blending?  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/50827            ## Model correlations https://www.kaggle.com/ogrellier/things-you-need-to-be-aware-of-before-stacking https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/49964          ## Class leakage in level 0 models    OOF stackering: https://www.kaggle.com/hhstrand/oof-stacking-regime  Stacker scrpits: https://www.kaggle.com/reppic/lazy-ensembling-algorithm  Covariance shift / Adversarial Validation: ( By how much test is different than train?) https://www.kaggle.com/ogrellier/check-unicode-script-distribution https://www.kaggle.com/ogrellier/adversarial-validation-and-lb-shakeup https://www.kaggle.com/konradb/adversarial-validation  Others:Creating more data:Using translations  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038  Study materials: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/46073#latest-277688 https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/46038  Spell checker: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/51426  Tuning:Tuning DL models :  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/50602  And a company good at Marketing.  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/47172  Good pipeline though. https://github.com/neptune-ml/kaggle-toxic-starter And finally here's my blender:", "link": "https://www.kaggle.com/jagangupta/lessons-from-toxic-blending-is-the-new-sexy", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "h2o"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-03-17 09:02:19", "date_scraped": "2020-12-13 12:20:07", "words": 465, "sentences": 8, "sum_nltk": "I just thought good solid kernals should not be lost within the chaos.\nPlease add on to this list in the comments below, if I've missed any good kernals.\nSimple Naive-Bayes: https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline#261316          # Custom SK learn class for Naive Bayes    Tf-IDF and simple Logistic Regression: https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams            # ngram range=(2,6)   -->    can capture a lot of information!\nhttps://www.kaggle.com/yekenot/toxic-regression  Wordbatch: https://www.kaggle.com/anttip/wordbatch-1-3-3-fm-ftrl-lb-0-9812                          # FM_FTRL  H20, Word2Vec in R: https://www.kaggle.com/brandenkmurray/h2o-word2vec-starter-toxic-comments  Deep learning: GRU(Gated Recurrent Units): https://www.kaggle.com/yekenot/pooled-gru-fasttext/code  Capsule Net(with GRU): https://www.kaggle.com/chongjiujjin/capsule-net-with-gru https://github.com/Godricly/comment_toxic https://github.com/bojone/Capsule/blob/master/Capsule_Keras.py                   # Base implementations of CapsNet in Keras https://github.com/XifengGuo/CapsNet-Keras    LSTM: https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras                             # Explains basic of LSTM well https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-069                         # Simple baseline https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout                              # Glove pretrained features + LSTM  Attention: https://www.kaggle.com/sermakarevich/hierarchical-attention-network https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043                      # LSTM + Attention layer  Convolution(CNNs): https://www.kaggle.com/yekenot/textcnn-2d-convolution https://www.kaggle.com/sbongo/for-beginners-go-even-deeper-with-char-gram-cnn  Bi Directional GRU CNNs: https://www.kaggle.com/konohayui/bi-gru-cnn-poolings https://www.kaggle.com/eashish/bidirectional-gru-with-convolution  Blending Helpers:What is blending?\nhttps://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/51058 What all do I need to check before blending?\nhttps://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/50827            ## Model correlations https://www.kaggle.com/ogrellier/things-you-need-to-be-aware-of-before-stacking https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/49964          ## Class leakage in level 0 models    OOF stackering: https://www.kaggle.com/hhstrand/oof-stacking-regime  Stacker scrpits: https://www.kaggle.com/reppic/lazy-ensembling-algorithm  Covariance shift / Adversarial Validation: ( By how much test is different than train?) https://www.kaggle.com/ogrellier/check-unicode-script-distribution https://www.kaggle.com/ogrellier/adversarial-validation-and-lb-shakeup https://www.kaggle.com/konradb/adversarial-validation  Others:Creating more data:Using translations  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038  Study materials: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/46073#latest-277688 https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/46038  Spell checker: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/51426  Tuning:Tuning DL models :  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/50602  And a company good at Marketing.\nhttps://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/47172  Good pipeline though.\nhttps://github.com/neptune-ml/kaggle-toxic-starter And finally here's my blender:", "sum_nltk_words": 416, "sum_nltk_runtime": 0.005, "sum_t5": "a list of resources that have taught me something is below. if you've missed any good kernals, please add them in the comments below. a custom SK learn class for Naive Bayes Tf-IDF and simple Logistic Regression. a svm-based class for nb-svm-strong-linear-baseline is available here. a svm-based class for nb-svm-strong-line", "sum_t5_words": 50, "sum_t5_runtime": 6.938, "runtime": 0.0, "nltk_category": "Utilities", "nltk_category_score": 0.4330502450466156, "nltk_category_runtime": 83.125, "nltk_subcategory": "Judicial Applied", "nltk_subcategory_score": 0.5512896180152893, "nltk_subcategory_runtime": 139.719, "category": "Utilities", "category_score": 0.4330502450466156, "subcategory": "Judicial Applied", "subcategory_score": 0.5512896180152893, "runtime_cat": 222.845, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.746", "language_code": "en", "language_score": "0.9999985675420389", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "public kernals filled blend blend blend maybe someone blend 100000 ok nothing wrong thought good solid kernals lost within chaos ive compiled categorized list resource taught something please add list comment ive missed good kernals simple naivebayes httpswwwkagglecomjhowardnbsvmstronglinearbaseline httpswwwkagglecomjhowardnbsvmstronglinearbaseline261316 custom sk learn class naive bayes tfidf simple logistic regression httpswwwkagglecomtunguzlogisticregressionwithwordsandcharngrams ngram range26 capture lot information httpswwwkagglecomyekenottoxicregression wordbatch httpswwwkagglecomanttipwordbatch133fmftrllb09812 fm_ftrl h20 word2vec r httpswwwkagglecombrandenkmurrayh2oword2vecstartertoxiccomments deep learning grugated recurrent unit httpswwwkagglecomyekenotpooledgrufasttextcode capsule netwith gru httpswwwkagglecomchongjiujjincapsulenetwithgru httpsgithubcomgodriclycomment_toxic httpsgithubcombojonecapsuleblobmastercapsule_keraspy base implementation capsnet kera httpsgithubcomxifengguocapsnetkeras lstm httpswwwkagglecomsbongoforbeginnerstacklingtoxicusingkeras explains basic lstm well httpswwwkagglecomcvxtzkerasbidirectionallstmbaselinelb0069 simple baseline httpswwwkagglecomjhowardimprovedlstmbaselineglovedropout glove pretrained feature lstm attention httpswwwkagglecomsermakarevichhierarchicalattentionnetwork httpswwwkagglecomqqgeogorkeraslstmattentionglove840blb0043 lstm attention layer convolutioncnns httpswwwkagglecomyekenottextcnn2dconvolution httpswwwkagglecomsbongoforbeginnersgoevendeeperwithchargramcnn bi directional gru cnns httpswwwkagglecomkonohayuibigrucnnpoolings httpswwwkagglecomeashishbidirectionalgruwithconvolution blending helperswhat blending httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion51058 need check blending httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion50827 model correlation httpswwwkagglecomogrellierthingsyouneedtobeawareofbeforestacking httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion49964 class leakage level 0 model oof stackering httpswwwkagglecomhhstrandoofstackingregime stacker scrpits httpswwwkagglecomreppiclazyensemblingalgorithm covariance shift adversarial validation much test different train httpswwwkagglecomogrelliercheckunicodescriptdistribution httpswwwkagglecomogrellieradversarialvalidationandlbshakeup httpswwwkagglecomkonradbadversarialvalidation otherscreating datausing translation httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion48038 study material httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion46073latest277688 httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion46038 spell checker httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion51426 tuningtuning dl model httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion50602 company good marketing httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion47172 good pipeline though httpsgithubcomneptunemlkaggletoxicstarter finally here blender", "tags_descriptive": []}