{"title": "Do Pretrained Embeddings Give You The Extra Edge?", "description": "In this kernel, we shall see if pretrained embeddings like Word2Vec, GLOVE and Fasttext, which are pretrained using billions of words could improve our accuracy score as compared to training our own embedding. We will compare the performance of models using these pretrained embeddings against the baseline model that doesn't use any pretrained embeddings in my previous kernel here.  Perhaps it's a good idea to briefly step in the world of word embeddings and see what's the difference between Word2Vec, GLOVE and Fasttext. Embeddings generally represent geometrical encodings of words based on how frequently appear together in a text corpus. Various implementations of word embeddings described below differs in the way as how they are constructed. Word2Vec The main idea behind it is that you train a model on the context on each word, so similar words will have similar numerical representations. Just like a normal feed-forward densely connected neural network(NN) where you have a set of independent variables and a target dependent variable that you are trying to predict, you first break your sentence into words(tokenize) and create a number of pairs of words, depending on the window size. So one of the combination could be a pair of words such as ('cat','purr'), where cat is the independent variable(X) and 'purr' is the target dependent variable(Y) we are aiming to predict. We feed the 'cat' into the NN through an embedding layer initialized with random weights, and pass it through the softmax layer with ultimate aim of predicting 'purr'. The optimization method such as SGD minimize the loss function \"(target word | context words)\" which seeks to minimize the loss of predicting the target words given the context words. If we do this with enough epochs, the weights in the embedding layer would eventually represent the vocabulary of word vectors, which is the \"coordinates\" of the words in this geometric vector space.  The above example assumes the skip-gram model. For the Continuous bag of words(CBOW), we would basically be predicting a word given the context. GLOVE GLOVE works similarly as Word2Vec. While you can see above that Word2Vec is a \"predictive\" model that predicts context given word, GLOVE learns by constructing a co-occurrence matrix (words X context) that basically count how frequently a word appears in a context. Since it's going to be a gigantic matrix, we factorize this matrix to achieve a lower-dimension representation. There's a lot of details that goes in GLOVE but that's the rough idea. FastText FastText is quite different from the above 2 embeddings. While Word2Vec and GLOVE treats each word as the smallest unit to train on, FastText uses n-gram characters as the smallest unit. For example, the word vector ,\"apple\", could be broken down into separate word vectors units as \"ap\",\"app\",\"ple\". The biggest benefit of using FastText is that it generate better word embeddings for rare words, or even words not seen during training because the n-gram character vectors are shared with other words. This is something that Word2Vec and GLOVE cannot achieve.", "link": "https://www.kaggle.com/sbongo/do-pretrained-embeddings-give-you-the-extra-edge", "tags": ["NLP"], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "vocabulary", "gensim"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-02-01 08:43:47", "date_scraped": "2020-12-13 12:20:06", "words": 503, "sentences": 22, "sum_nltk": "In this kernel, we shall see if pretrained embeddings like Word2Vec, GLOVE and Fasttext, which are pretrained using billions of words could improve our accuracy score as compared to training our own embedding.\nPerhaps it's a good idea to briefly step in the world of word embeddings and see what's the difference between Word2Vec, GLOVE and Fasttext.\nWord2Vec The main idea behind it is that you train a model on the context on each word, so similar words will have similar numerical representations.\nJust like a normal feed-forward densely connected neural network(NN) where you have a set of independent variables and a target dependent variable that you are trying to predict, you first break your sentence into words(tokenize) and create a number of pairs of words, depending on the window size.\nWhile Word2Vec and GLOVE treats each word as the smallest unit to train on, FastText uses n-gram characters as the smallest unit.\nThe biggest benefit of using FastText is that it generate better word embeddings for rare words, or even words not seen during training because the n-gram character vectors are shared with other words.", "sum_nltk_words": 181, "sum_nltk_runtime": 0.006, "sum_t5": "a kernel is a test to see if pretrained embeddings can improve our accuracy. word2vec, GLOVE and fasttext are pretrained using billions of words. a kernel is a test to see if the accuracy of embeddings is improved. a kernel is a test to see if the accuracy of embeddings is improved. a kernel is a test to see if a model can improve its accuracy.", "sum_t5_words": 66, "sum_t5_runtime": 6.769, "runtime": 0.005, "nltk_category": "Construction & Engineering", "nltk_category_score": 0.38677752017974854, "nltk_category_runtime": 17.115, "nltk_subcategory": "Quality", "nltk_subcategory_score": 0.49491173028945923, "nltk_subcategory_runtime": 27.567, "category": "Construction & Engineering", "category_score": 0.38677752017974854, "subcategory": "Quality", "subcategory_score": 0.49491173028945923, "runtime_cat": 44.682, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.8", "language_code": "en", "language_score": "0.9999972214397602", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "kernel shall see pretrained embeddings like word2vec glove fasttext pretrained using billion word could improve accuracy score compared training embedding compare performance model using pretrained embeddings baseline model doesnt use pretrained embeddings previous kernel perhaps good idea briefly step world word embeddings see whats difference word2vec glove fasttext embeddings generally represent geometrical encoding word based frequently appear together text corpus various implementation word embeddings described differs way constructed word2vec main idea behind train model context word similar word similar numerical representation like normal feedforward densely connected neural networknn set independent variable target dependent variable trying predict first break sentence wordstokenize create number pair word depending window size one combination could pair word catpurr cat independent variablex purr target dependent variabley aiming predict feed cat nn embedding layer initialized random weight pas softmax layer ultimate aim predicting purr optimization method sgd minimize loss function target word context word seek minimize loss predicting target word given context word enough epoch weight embedding layer would eventually represent vocabulary word vector coordinate word geometric vector space example assumes skipgram model continuous bag wordscbow would basically predicting word given context glove glove work similarly word2vec see word2vec predictive model predicts context given word glove learns constructing cooccurrence matrix word x context basically count frequently word appears context since going gigantic matrix factorize matrix achieve lowerdimension representation there lot detail go glove thats rough idea fasttext fasttext quite different 2 embeddings word2vec glove treat word smallest unit train fasttext us ngram character smallest unit example word vector apple could broken separate word vector unit apappple biggest benefit using fasttext generate better word embeddings rare word even word seen training ngram character vector shared word something word2vec glove cannot achieve", "tags_descriptive": ["Natural Language Processing (NLP)"]}