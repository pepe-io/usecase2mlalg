{"title": "Flowers TPU: Concise EfficientNet B7", "description": "About this kernelPretty much a fork of the very comprehensive starter kernel. I didn't add much, remove a whole bunch (please check out the official kernel for more info), hide some big scary functions. I mostly just hooked the largest efficientnet laying around and ran it for a couple 'pochs. Please consume this with moderation (only 30h per week!). PS: This notebook costs ~8$ to run ;) Updates V9: Tried warmup by only training softmax layer for 5 epochs before unfreezing all weights. V10: More data augmentations V11: Use LR Scheduler, idea comes from here. V12: Use both training and validation data to train model. V14: Train longer (25 epochs). V15: Back to 20 epochs; Global Max Pooling instead of Average. V16: Roll back to Global Average Pooling V18: Roll back to V13", "link": "https://www.kaggle.com/xhlulu/flowers-tpu-concise-efficientnet-b7", "tags": ["CNN"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-03-10 16:37:38", "date_scraped": "2020-12-12 19:56:26", "words": 133, "sentences": 10, "runtime": 0.006, "description_category": "Finance", "description_category_score": 0.20544151961803436, "description_category_runtime": 13.274, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.3818216919898987, "description_subcategory_runtime": 20.758, "category": "Finance", "category_score": 0.20544151961803436, "subcategory": "Machine Learning", "subcategory_score": 0.3818216919898987, "runtime_cat": 34.032, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.753", "language_code": "en", "language_score": "0.9999950606277561", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "kernelpretty much fork comprehensive starter kernel didnt add much remove whole bunch please check official kernel info hide big scary function mostly hooked largest efficientnet laying around ran couple pochs please consume moderation 30h per week p notebook cost 8 run update v9 tried warmup training softmax layer 5 epoch unfreezing weight v10 data augmentation v11 use lr scheduler idea come v12 use training validation data train model v14 train longer 25 epoch v15 back 20 epoch global max pooling instead average v16 roll back global average pooling v18 roll back v13", "tags_descriptive": ["Convolutional Neural Network (CNN)"]}