{"title": "1D CNN (single model score: 0.14, 0.16 or 0.23)", "description": "Below a discussion of team Waffle convolutions inc.'s implementation of the 1D convolutional model (place 45 on the leaderboard), show our biased comparison to LSTMs, quickly describe its performance and explain some of our 'design' choices. Comparison to LSTMA quick overview of its benefits with respect to the LSTM approach:  Doesn't overfit as much  Much shorter run times, 1 epoch ~ 130 sec on a 1050 GPU, as opposed to 400 sec epochs for a 250 unit LSTM  Intuitively much clearer what is happening (might just be me though)  Seems to perform better (with only magic features, 0.160 single model on lb)   Downsides of the 1D CNN:  Sentences which are 'equal' but have different sequence of words (eg. 'Is bacon the best thing since sliced bread?' or 'Since sliced bread is bacon the best thing?') flunk easily  No concept of word importance (such TFIDF, can be added with a dense layer though)  Our implementation cannot switch question1 with question2 to double the data   However in all fairness, we played around more with the 1D convolutional models than LSTMs, we simply couldn't get the latter to have similar performance. The actual model", "link": "https://www.kaggle.com/rethfro/1d-cnn-single-model-score-0-14-0-16-or-0-23", "tags": ["CNN"], "kind": ["Project", "(Notebook)"], "ml_libs": [], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2017-06-07 12:16:31", "date_scraped": "2020-12-13 16:22:58", "words": 202, "sentences": 4, "sum_nltk": "Below a discussion of team Waffle convolutions inc.'s implementation of the 1D convolutional model (place 45 on the leaderboard), show our biased comparison to LSTMs, quickly describe its performance and explain some of our 'design' choices.\nComparison to LSTMA quick overview of its benefits with respect to the LSTM approach:  Doesn't overfit as much  Much shorter run times, 1 epoch ~ 130 sec on a 1050 GPU, as opposed to 400 sec epochs for a 250 unit LSTM  Intuitively much clearer what is happening (might just be me though)  Seems to perform better (with only magic features, 0.160 single model on lb)   Downsides of the 1D CNN:  Sentences which are 'equal' but have different sequence of words (eg.\n'Is bacon the best thing since sliced bread?' or 'Since sliced bread is bacon the best thing?') flunk easily  No concept of word importance (such TFIDF, can be added with a dense layer though)  Our implementation cannot switch question1 with question2 to double the data   However in all fairness, we played around more with the 1D convolutional models than LSTMs, we simply couldn't get the latter to have similar performance.\nThe actual model", "sum_nltk_words": 199, "sum_nltk_runtime": 0.002, "sum_t5": "team waffle convolutions inc. have compared the 1D convolutional model to LSTMs. the model is a'simple' 1D convolutional model with a'simple''simple' 1D convolutional model. the model is a'simple' 1D model with a'simple' 1D convolutional model. the model is a'simple' 1D model with", "sum_t5_words": 42, "sum_t5_runtime": 5.242, "runtime": 0.002, "nltk_category": "Utilities", "nltk_category_score": 0.1345123052597046, "nltk_category_runtime": 18.71, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8181923031806946, "nltk_subcategory_runtime": 29.761, "category": "Utilities", "category_score": 0.1345123052597046, "subcategory": "Machine Learning", "subcategory_score": 0.8181923031806946, "runtime_cat": 48.472, "programming_language": "Jupyter Notebook", "ml_score": "0.5", "engagement_score": "0.758", "language_code": "en", "language_score": "0.9999969510430996", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "discussion team waffle convolution inc implementation 1d convolutional model place 45 leaderboard show biased comparison lstms quickly describe performance explain design choice comparison lstma quick overview benefit respect lstm approach doesnt overfit much much shorter run time 1 epoch 130 sec 1050 gpu opposed 400 sec epoch 250 unit lstm intuitively much clearer happening might though seems perform better magic feature 0160 single model lb downside 1d cnn sentence equal different sequence word eg bacon best thing since sliced bread since sliced bread bacon best thing flunk easily concept word importance tfidf added dense layer though implementation cannot switch question1 question2 double data however fairness played around 1d convolutional model lstms simply couldnt get latter similar performance actual model", "tags_descriptive": ["Convolutional Neural Network (CNN)"]}