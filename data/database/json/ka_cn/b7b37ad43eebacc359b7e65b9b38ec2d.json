{"title": "Exploratory study of ML algorithms", "description": "<....Work in progress...> Thank you for opening this script! I have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts. Please upvote this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks.  Layout of the documentThe prediction process is divided into two notebooks. Part 1 : Covers data statistics, data visualization, and feature selection : https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-on-feature-selection This notebook : Covers prediction using various algorithms  Data statistics Shape Datatypes Description Skew Class distribution  Data Interaction Correlation Scatter plot  Data Visualization Box and density plots Grouping of one hot encoded attributes  Data Cleaning Remove unnecessary columns  Data Preparation Original Delete rows or impute values in case of missing StandardScaler MinMaxScaler Normalizer  Feature selection ExtraTreesClassifier GradientBoostingClassifier RandomForestClassifier XGBClassifier RFE SelectPercentile PCA PCA + SelectPercentile Feature Engineering  Evaluation, prediction, and analysis LDA (Linear algo) LR (Linear algo) KNN (Non-linear algo) CART (Non-linear algo) Naive Bayes (Non-linear algo) SVC (Non-linear algo) Bagged Decision Trees (Bagging) Random Forest (Bagging) Extra Trees (Bagging) AdaBoost (Boosting) Stochastic Gradient Boosting (Boosting) Voting Classifier (Voting) MLP (Deep Learning) XGBoost", "link": "https://www.kaggle.com/sharmasanthosh/exploratory-study-of-ml-algorithms", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "keras"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2016-10-08 09:12:53", "date_scraped": "2020-12-12 20:01:44", "words": 220, "sentences": 6, "sum_nltk": "<....Work in progress...> Thank you for opening this script!\nI have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.\nPlease upvote this kernel so that it reaches the top of the chart and is easily locatable by new users.\nYour comments on how we can improve this kernel is welcome.\nThanks.\nLayout of the documentThe prediction process is divided into two notebooks.\nPart 1 : Covers data statistics, data visualization, and feature selection : https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-on-feature-selection This notebook : Covers prediction using various algorithms  Data statistics Shape Datatypes Description Skew Class distribution  Data Interaction Correlation Scatter plot  Data Visualization Box and density plots Grouping of one hot encoded attributes  Data Cleaning Remove unnecessary columns  Data Preparation Original Delete rows or impute values in case of missing StandardScaler MinMaxScaler Normalizer  Feature selection ExtraTreesClassifier GradientBoostingClassifier RandomForestClassifier XGBClassifier RFE SelectPercentile PCA PCA + SelectPercentile Feature Engineering  Evaluation, prediction, and analysis LDA (Linear algo) LR (Linear algo) KNN (Non-linear algo) CART (Non-linear algo) Naive Bayes (Non-linear algo) SVC (Non-linear algo) Bagged Decision Trees (Bagging) Random Forest (Bagging) Extra Trees (Bagging) AdaBoost (Boosting) Stochastic Gradient Boosting (Boosting) Voting Classifier (Voting) MLP (Deep Learning) XGBoost", "sum_nltk_words": 213, "sum_nltk_runtime": 0.003, "sum_t5": "a new kernel is being developed to help machine learning enthusiasts. it covers the prediction process using various algorithms. the kernel is currently in beta and is available for download. it is currently in beta and is expected to be released in january. back to mail online home. back to the page you came from.. back to the page you came from. back to the page you came from. back to the page you came from.", "sum_t5_words": 76, "sum_t5_runtime": 5.937, "runtime": 0.003, "nltk_category": "Utilities", "nltk_category_score": 0.4467850923538208, "nltk_category_runtime": 27.012, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9617606997489929, "nltk_subcategory_runtime": 42.796, "category": "Utilities", "category_score": 0.4467850923538208, "subcategory": "Machine Learning", "subcategory_score": 0.9617606997489929, "runtime_cat": 69.809, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.757", "language_code": "en", "language_score": "0.9999981813008142", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "work progress thank opening script made effort document every step involved prediction process notebook act good starting point new kagglers new machine learning enthusiast please upvote kernel reach top chart easily locatable new user comment improve kernel welcome thanks layout documentthe prediction process divided two notebook part 1 cover data statistic data visualization feature selection httpswwwkagglecomsharmasanthoshforestcovertypepredictionexploratorystudyonfeatureselection notebook cover prediction using various algorithm data statistic shape datatypes description skew class distribution data interaction correlation scatter plot data visualization box density plot grouping one hot encoded attribute data cleaning remove unnecessary column data preparation original delete row impute value case missing standardscaler minmaxscaler normalizer feature selection extratreesclassifier gradientboostingclassifier randomforestclassifier xgbclassifier rfe selectpercentile pca pca selectpercentile feature engineering evaluation prediction analysis lda linear algo lr linear algo knn nonlinear algo cart nonlinear algo naive bayes nonlinear algo svc nonlinear algo bagged decision tree bagging random forest bagging extra tree bagging adaboost boosting stochastic gradient boosting boosting voting classifier voting mlp deep learning xgboost", "tags_descriptive": []}