{"title": "Siamese (two pretrained weights) 0.855", "description": "Most voted kernelIn this competition one of the most voted kernels is Siamese (pretrained) 0.822 by See--. Unfortunatelly for me I realized that by the end of the competition. Mostly I was working on porting Dlib metric learning paradigm to this competition.  However by reading original kernel Martin discloses something realy important in the Ensmble section The assembly strategy consist in compute a score matrix (or dimension test by train) that is a linear combination of the standard and bootstrap model. Generation of the submission using the score matrix is unchanged. Trial and error suggest a weight of 0.45 for the standard model and 0.55 for the bootstrap model. The resulting ensemble as an accuracy of 0.78563 using a threshold of 0.92.  So I did that....And used both weights provided using the weighting Martin suggested and I really hit on a good starting point. The rest of  this kernel code is just the same as See-- originally posted. I believe now that this is the starting point of the competion (at least for those working on Siameze networks). Hope that this may help come people with their mergings. Updated TL;DRI am just using the pretrained weights from  @martinpiotte. Thanks to @suicaokhoailang for creating the updated kernel. I think the important steps to improve to 0.9 are:  Get rid of lapjv dependency. It really slows down training/trying different ideas. Load images as RGB (and retrain). I can't find where, but the current first place wrote that it helps by ~0.1.  Interesting: The mpiotte-bootstrap-model only scored 0.697. Though, it was better on the playgroud competition.  TL;DRI tried to refactor @martinpiotte's original kernel here. I changed almost nothing beside commenting out the latter 380 epochs since it can't fit into a kernel. I also generated the new bounding boxes in my kernel here and saved it as a .csv instead of pickle for readability. A few things to point out:  Training more will probably improve your score, maybe as many as 500 epochs. We only train for 20 epochs in this kernel.  You may try to improve your training time by applying this technique (thanks Brian): https://www.kaggle.com/c/humpback-whale-identification/discussion/74402#444476 .  Consider using a pretrained model(s), good for blending.", "link": "https://www.kaggle.com/voglinio/siamese-two-pretrained-weights-0-855", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-02-20 09:59:36", "date_scraped": "2020-12-12 21:28:20", "words": 373, "sentences": 26, "sum_nltk": "Most voted kernelIn this competition one of the most voted kernels is Siamese (pretrained) 0.822 by See--.\nHowever by reading original kernel Martin discloses something realy important in the Ensmble section The assembly strategy consist in compute a score matrix (or dimension test by train) that is a linear combination of the standard and bootstrap model.\nTrial and error suggest a weight of 0.45 for the standard model and 0.55 for the bootstrap model.\nSo I did that....And used both weights provided using the weighting Martin suggested and I really hit on a good starting point.\nHope that this may help come people with their mergings.\nUpdated TL;DRI am just using the pretrained weights from  @martinpiotte.\nI can't find where, but the current first place wrote that it helps by ~0.1.\nInteresting: The mpiotte-bootstrap-model only scored 0.697.\nTL;DRI tried to refactor @martinpiotte's original kernel here.\nI changed almost nothing beside commenting out the latter 380 epochs since it can't fit into a kernel.\nA few things to point out:  Training more will probably improve your score, maybe as many as 500 epochs.\nWe only train for 20 epochs in this kernel.\nConsider using a pretrained model(s), good for blending.", "sum_nltk_words": 189, "sum_nltk_runtime": 0.005, "sum_t5": "one of the most voted kernels is Siamese (pretrained) 0.822 by See--. TL;DRI tried to refactor @martinpiotte's original kernel here. he changed almost nothing except commenting out the latter 380 epochs. he also saved the new bounding boxes in his kernel as a.csv instead of pickle for readability.", "sum_t5_words": 48, "sum_t5_runtime": 6.232, "runtime": 0.003, "nltk_category": "Justice, Law and Regulations", "nltk_category_score": 0.4660651385784149, "nltk_category_runtime": 20.7, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.6711399555206299, "nltk_subcategory_runtime": 32.821, "category": "Justice, Law and Regulations", "category_score": 0.4660651385784149, "subcategory": "Machine Learning", "subcategory_score": 0.6711399555206299, "runtime_cat": 53.521, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.719", "language_code": "en", "language_score": "0.9999978007081822", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "voted kernelin competition one voted kernel siamese pretrained 0822 see unfortunatelly realized end competition mostly working porting dlib metric learning paradigm competition however reading original kernel martin discloses something realy important ensmble section assembly strategy consist compute score matrix dimension test train linear combination standard bootstrap model generation submission using score matrix unchanged trial error suggest weight 045 standard model 055 bootstrap model resulting ensemble accuracy 078563 using threshold 092 thatand used weight provided using weighting martin suggested really hit good starting point rest kernel code see originally posted believe starting point competion least working siameze network hope may help come people merging updated tldri using pretrained weight martinpiotte thanks suicaokhoailang creating updated kernel think important step improve 09 get rid lapjv dependency really slows trainingtrying different idea load image rgb retrain cant find current first place wrote help 01 interesting mpiottebootstrapmodel scored 0697 though better playgroud competition tldri tried refactor martinpiottes original kernel changed almost nothing beside commenting latter 380 epoch since cant fit kernel also generated new bounding box kernel saved csv instead pickle readability thing point training probably improve score maybe many 500 epoch train 20 epoch kernel may try improve training time applying technique thanks brian httpswwwkagglecomchumpbackwhaleidentificationdiscussion74402444476 consider using pretrained model good blending", "tags_descriptive": []}