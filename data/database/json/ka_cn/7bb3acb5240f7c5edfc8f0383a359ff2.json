{"title": "Intro to Model Tuning: Grid and Random Search", "description": "Introduction: Hyperparameter Tuning using Grid and Random SearchIn this notebook, we will explore two methods for hyperparameter tuning a machine learning model. In contrast to model parameters which are learned during training, model hyperparameters are set by the data scientist ahead of training and control implementation aspects of the model. The weights learned during training of a linear regression model are parameters while the number of trees in a random forest is a model hyperparameter because this is set by the data scientist. Hyperparameters can be thought of as model settings. These settings need to be tuned for each problem because the best model hyperparameters for one particular dataset will not be the best across all datasets. The process of hyperparameter tuning (also called hyperparameter optimization) means finding the combination of hyperparameter values for a machine learning model that performs the best - as measured on a validation dataset - for a problem. (Quick Note: a lot of data scientists use the terms parameters and hyperparameters interchangeably to refer to the model settings. While this is technically incorrect, it's pretty common practice and it's usually possible to tell when they are referring to parameters learned during training versus hyperparameters. I'll try to stick to using model hyperparameters or model settings and I'll  point out when I'm talking about a parameter that is learned during training. If you're still confused, this article may help you out!) Additional Notebooks If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:  A Gentle Introduction Manual Feature Engineering Part One Manual Feature Engineering Part Two Introduction to Automated Feature Engineering Advanced Automated Feature Engineering Feature Selection Intro to Model Tuning: Grid and Random Search Automated Model Tuning  There are several approaches to hyperparameter tuning  Manual: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.  Grid Search: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient! Random search: set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources.  Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.  (This Wikipedia Article provides a good high-level overview of tuning options with links for more details) In this notebook, we will implement approaches 2 and 3 for a Gradient Boosting Machine Learning Model. In a future notebook, we will implement automated hyperparameter tuning using Bayesian optimization, specifically the Hyperopt library. If you want to get an idea of how automated hyperparameter tuning is done, check out this article. Model: Gradient Boosting MachineThe Gradient Boosting Machine (GBM) has recently emerged as one of the top machine learning models. The GBM is extremely effective on structured data - where the information is in rows and columns - and medium sized datasets - where there are at most a few million observations. We will focus on this model because it is currently the top performing method for most competitions on Kaggle and because the performance is highly dependent on the hyperparameter choices. The basics you need to know about the GBM are that it is an ensemble method that works by training many individual learners, almost always decision trees. However, unlike in a random forest where the trees are trained in parallel, in a GBM, the trees are trained sequentially with each tree learning from the mistakes of the previous ones. The hundreds or thousands of weak learners are combined to make a single strong ensemble learner with the contributions of each individual learned during training using Gradient Descent (the weights of the individual trees would therefore be a model parameter). The GBM has many hyperparameters to tune that control both the overall ensemble (such as the learning rate) and the individual decision trees (such as the number of leaves in the tree or the maximum depth of the tree). It is difficult to know which combination of hyperparameters will work best based only on theory because there are complex interactions between hyperparameters. Hence the need for hyperparameter tuning: the only way to find the optimal hyperparameter values is to try many different combinations on a dataset! We will use the implementation of the Gradient Boosting Machine in the LightGBM library. This is a much faster (and some say more accurate) implementation than that available in Scikit-Learn. For more details of the Gradient Boosting Machine (GBM), check out this high-level blog post, or this in depth technical article. Getting StartedWith the necessary background out of the way, let's get started. For this notebook, we will work with a subset of the data consisting of 10000 rows. Hyperparameter tuning is extremely computationally expensive and working with the full dataset in a Kaggle Kernel would not be feasible for more than a few search iterations. However, the same ideas that we will implement here can be applied to the full dataset and while this notebook is specifically aimed at the GBM, the methods can be applied for any machine learning model. To \"test\" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. When we do hyperparameter tuning, it's crucial to not tune the hyperparameters on the testing data. We can only use the testing data a single time when we evaluate the final model that has been tuned on the validation data. To actually test our methods from this notebook, we would need to train the best model on all of the training data, make predictions on the actual testing data, and then submit our answers to the competition.", "link": "https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search", "tags": ["Classification"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-07-17 16:03:16", "date_scraped": "2020-12-12 20:47:32", "words": 1005, "sentences": 37, "sum_nltk": "The weights learned during training of a linear regression model are parameters while the number of trees in a random forest is a model hyperparameter because this is set by the data scientist.\nThe process of hyperparameter tuning (also called hyperparameter optimization) means finding the combination of hyperparameter values for a machine learning model that performs the best - as measured on a validation dataset - for a problem.\nIf you're still confused, this article may help you out!) Additional Notebooks If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:  A Gentle Introduction Manual Feature Engineering Part One Manual Feature Engineering Part Two Introduction to Automated Feature Engineering Advanced Automated Feature Engineering Feature Selection Intro to Model Tuning: Grid and Random Search Automated Model Tuning  There are several approaches to hyperparameter tuning  Manual: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data.\nGrid Search: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data.\n(This Wikipedia Article provides a good high-level overview of tuning options with links for more details) In this notebook, we will implement approaches 2 and 3 for a Gradient Boosting Machine Learning Model.", "sum_nltk_words": 219, "sum_nltk_runtime": 0.013, "sum_t5": "model hyperparameters are set by the data scientist ahead of training and control implementation aspects of the model. the weights learned during training of a linear regression model are parameters while the number of trees in a random forest is a model hyperparameter. the process of hyperparameter tuning means finding the combination of hyperparameter values for a machine learning model that performs the best. grid search: set up a grid of hyperparameter values and for each combination, train the model with the hyperparameters,", "sum_t5_words": 83, "sum_t5_runtime": 6.953, "runtime": 0.005, "nltk_category": "Real Estate, Rental & Leasing", "nltk_category_score": 0.3309676945209503, "nltk_category_runtime": 19.256, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9285489320755005, "nltk_subcategory_runtime": 30.728, "category": "Real Estate, Rental & Leasing", "category_score": 0.3309676945209503, "subcategory": "Machine Learning", "subcategory_score": 0.9285489320755005, "runtime_cat": 49.984, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.82", "language_code": "en", "language_score": "0.9999977707179606", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introduction hyperparameter tuning using grid random searchin notebook explore two method hyperparameter tuning machine learning model contrast model parameter learned training model hyperparameters set data scientist ahead training control implementation aspect model weight learned training linear regression model parameter number tree random forest model hyperparameter set data scientist hyperparameters thought model setting setting need tuned problem best model hyperparameters one particular dataset best across datasets process hyperparameter tuning also called hyperparameter optimization mean finding combination hyperparameter value machine learning model performs best measured validation dataset problem quick note lot data scientist use term parameter hyperparameters interchangeably refer model setting technically incorrect pretty common practice usually possible tell referring parameter learned training versus hyperparameters ill try stick using model hyperparameters model setting ill point im talking parameter learned training youre still confused article may help additional notebook havent checked work problem complete list notebook completed far gentle introduction manual feature engineering part one manual feature engineering part two introduction automated feature engineering advanced automated feature engineering feature selection intro model tuning grid random search automated model tuning several approach hyperparameter tuning manual select hyperparameters based intuitionexperienceguessing train model hyperparameters score validation data repeat process run patience satisfied result grid search set grid hyperparameter value combination train model score validation data approach every single combination hyperparameters value tried inefficient random search set grid hyperparameter value select random combination train model score number search iteration set based timeresources automated hyperparameter tuning use method gradient descent bayesian optimization evolutionary algorithm conduct guided search best hyperparameters wikipedia article provides good highlevel overview tuning option link detail notebook implement approach 2 3 gradient boosting machine learning model future notebook implement automated hyperparameter tuning using bayesian optimization specifically hyperopt library want get idea automated hyperparameter tuning done check article model gradient boosting machinethe gradient boosting machine gbm recently emerged one top machine learning model gbm extremely effective structured data information row column medium sized datasets million observation focus model currently top performing method competition kaggle performance highly dependent hyperparameter choice basic need know gbm ensemble method work training many individual learner almost always decision tree however unlike random forest tree trained parallel gbm tree trained sequentially tree learning mistake previous one hundred thousand weak learner combined make single strong ensemble learner contribution individual learned training using gradient descent weight individual tree would therefore model parameter gbm many hyperparameters tune control overall ensemble learning rate individual decision tree number leaf tree maximum depth tree difficult know combination hyperparameters work best based theory complex interaction hyperparameters hence need hyperparameter tuning way find optimal hyperparameter value try many different combination dataset use implementation gradient boosting machine lightgbm library much faster say accurate implementation available scikitlearn detail gradient boosting machine gbm check highlevel blog post depth technical article getting startedwith necessary background way let get started notebook work subset data consisting 10000 row hyperparameter tuning extremely computationally expensive working full dataset kaggle kernel would feasible search iteration however idea implement applied full dataset notebook specifically aimed gbm method applied machine learning model test tuning result save training data 6000 row separate testing set hyperparameter tuning crucial tune hyperparameters testing data use testing data single time evaluate final model tuned validation data actually test method notebook would need train best model training data make prediction actual testing data submit answer competition", "tags_descriptive": ["Classification"]}