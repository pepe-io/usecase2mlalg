{"title": "Lessons learnt from Fast.ai lectures Part - 1", "description": "This Notebook is inspired by fast.ai's Machine Learning course. I really liked the teaching methodology as its completely application oriented, in oppose to other theorotical approaches in academic course. I learnt a lot of practical things,  like how to actually work with categorical features, how to normalize continuous data, how to handle missing values in your dataset, how to make the best use of timestamps in your dataset, best practices while creating trainning, validation and test datasets, what actually random forest do (intuitionally) and realized how easy it is to tuning hyperparameters and many more things. You might have noted that I have used \"how to\" in front of almost all the things that I learnt because this is what I actually learnt: \"how to do something\", instead of plain theory. This notebook is meant for people's who want to learn all these thing but cannnot manage time to watch 1-1:30 hrs long videos (there are 7 lectures in the ML course, in total). I tried my best:  To put in as much stuff as possible. To keep it simple and intuitional. To avoid using fastai library (I personally think it requires you to learn a lot of context).  I have taken the trouble (rather, I enjoyed doing it) of going through the code and tried replicating the codes as normal python functions. So, you can see how the things are actually implemented instead of looking at some random function name, knowing what it does but not having any clue of how its implemented. Note: This notebook is not complete and exhaustive, I have not implemented everything that was covered in the course but I tried my best to make the notebook as end-to-end as possible. Note: This is pretty long notebook, you might want to bookmark it. The notebook will also have a Part-2 covering the rest of the lessons learnt.", "link": "https://www.kaggle.com/ankursingh12/lessons-learnt-from-fast-ai-lectures-part-1", "tags": ["Classification", "Feature Engineering"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-01-15 13:39:56", "date_scraped": "2020-12-12 17:19:45", "words": 315, "sentences": 13, "sum_nltk": "I learnt a lot of practical things,  like how to actually work with categorical features, how to normalize continuous data, how to handle missing values in your dataset, how to make the best use of timestamps in your dataset, best practices while creating trainning, validation and test datasets, what actually random forest do (intuitionally) and realized how easy it is to tuning hyperparameters and many more things.\nYou might have noted that I have used \"how to\" in front of almost all the things that I learnt because this is what I actually learnt: \"how to do something\", instead of plain theory.\nThis notebook is meant for people's who want to learn all these thing but cannnot manage time to watch 1-1:30 hrs long videos (there are 7 lectures in the ML course, in total).\nSo, you can see how the things are actually implemented instead of looking at some random function name, knowing what it does but not having any clue of how its implemented.\nNote: This notebook is not complete and exhaustive, I have not implemented everything that was covered in the course but I tried my best to make the notebook as end-to-end as possible.\nNote: This is pretty long notebook, you might want to bookmark it.", "sum_nltk_words": 206, "sum_nltk_runtime": 0.003, "sum_t5": "this notebook is inspired by fast.ai's Machine Learning course. it is a practical notebook, with 7 lectures in total. the notebook is not complete and exhaustive. it will also have a Part-2 covering the rest of the lessons learnt.. a: \"i have tried replicating the codes as normal python functions\". a: \"i have not implemented everything that was covered in the course\" a: \"i have not implemented everything that was covered in the course\"", "sum_t5_words": 74, "sum_t5_runtime": 6.48, "runtime": 0.002, "nltk_category": "Education & Research", "nltk_category_score": 0.451351523399353, "nltk_category_runtime": 17.934, "nltk_subcategory": "Judicial Applied", "nltk_subcategory_score": 0.6405053734779358, "nltk_subcategory_runtime": 28.74, "category": "Education & Research", "category_score": 0.451351523399353, "subcategory": "Judicial Applied", "subcategory_score": 0.6405053734779358, "runtime_cat": 46.674, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.658", "language_code": "en", "language_score": "0.9999968699792547", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "notebook inspired fastais machine learning course really liked teaching methodology completely application oriented oppose theorotical approach academic course learnt lot practical thing like actually work categorical feature normalize continuous data handle missing value dataset make best use timestamps dataset best practice creating trainning validation test datasets actually random forest intuitionally realized easy tuning hyperparameters many thing might noted used front almost thing learnt actually learnt something instead plain theory notebook meant people want learn thing cannnot manage time watch 1130 hr long video 7 lecture ml course total tried best put much stuff possible keep simple intuitional avoid using fastai library personally think requires learn lot context taken trouble rather enjoyed going code tried replicating code normal python function see thing actually implemented instead looking random function name knowing clue implemented note notebook complete exhaustive implemented everything covered course tried best make notebook endtoend possible note pretty long notebook might want bookmark notebook also part2 covering rest lesson learnt", "tags_descriptive": ["Classification", "Feature Engineering"]}