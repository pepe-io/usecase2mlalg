{"title": "FAT19: MixUp Keras on PreProcessedData LB632", "description": "Introductionsee V56 for the best result of LB632 -- Finally I beat the current best public kernel using Keras :) -- This probably be my last update on this kernel -- If you find this kernel helpful, please upvote Version upto V60 have a silly bug of 'if <-- elif' so that model selection is wrong  This is my effort to do a Keras replication with comparable baseline to the great kernel of @mhiro2 https://www.kaggle.com/mhiro2/simple-2d-cnn-classifier-with-pytorch (and further improved by @peining), which in turns use the excellent pre-processed data of @daisukelab https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data) -- Note that to inference to the private data in stage-2, you have to preprocess data yourself. One change I made in a Keras version, in addition to a simple conv net, we can also use a pre-defined architectures [trained from scratch] MobileNetV2, InceptionV3 and Xception where you can choose in the kernel. Also, many ideas borrow from a nice kernel of @voglinio https://www.kaggle.com/voglinio/keras-2d-model-5-fold-log-specgram-curated-only , I also borrow the SoftMax+BCE loss & TTA ideas from Giba's kernel (BTW, we all know Giba without having to mention his user :). I apologize that my code is not at all clean; some of the pytorch code is still here albeit not used. Major Updates V1 [CV680, LB574] V4 [CV66x, LB576] V5 [] Add image augmentation module V9 [CV679] Add lwlrap TF metric (credit @rio114 : https://www.kaggle.com/rio114/keras-cnn-with-lwlrap-evaluation ) V11 [] Employ list of augmentations mentioned in https://github.com/sainathadapa/kaggle-freesound-audio-tagging/blob/master/approaches_all.md V16 [] Add BCEwithLogits (use only with ACTIVATION = 'linear') V17 add SimpleCNN similar to the pytorch baseline V22 add Curated-Only, Train-augment options V23 add CRNN model V30 LB598 with shallow CNN in 400s, set iteration to 150 V39 LB608 with CoarseDropout Augmentation V40 Simple Snapshot (Checkpoint) Ensemble V52 [CV811, LB616] MixUp+CoarseDropout : credit https://www.kaggle.com/mathormad/resnet50-v2-keras-focal-loss-mix-up  V56 [CV830, LB632] Change Architecture to get the best result  V61 fix silly bugs on model selection", "link": "https://www.kaggle.com/ratthachat/fat19-mixup-keras-on-preprocesseddata-lb632", "tags": ["DL"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow", "pytorch"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-05-19 06:59:35", "date_scraped": "2020-12-12 20:05:18", "words": 310, "sentences": 5, "sum_nltk": "Introductionsee V56 for the best result of LB632 -- Finally I beat the current best public kernel using Keras :) -- This probably be my last update on this kernel -- If you find this kernel helpful, please upvote Version upto V60 have a silly bug of 'if <-- elif' so that model selection is wrong  This is my effort to do a Keras replication with comparable baseline to the great kernel of @mhiro2 https://www.kaggle.com/mhiro2/simple-2d-cnn-classifier-with-pytorch (and further improved by @peining), which in turns use the excellent pre-processed data of @daisukelab https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data) -- Note that to inference to the private data in stage-2, you have to preprocess data yourself.\nMajor Updates V1 [CV680, LB574] V4 [CV66x, LB576] V5 [] Add image augmentation module V9 [CV679] Add lwlrap TF metric (credit @rio114 : https://www.kaggle.com/rio114/keras-cnn-with-lwlrap-evaluation ) V11 [] Employ list of augmentations mentioned in https://github.com/sainathadapa/kaggle-freesound-audio-tagging/blob/master/approaches_all.md V16 [] Add BCEwithLogits (use only with ACTIVATION = 'linear') V17 add SimpleCNN similar to the pytorch baseline V22 add Curated-Only, Train-augment options V23 add CRNN model V30 LB598 with shallow CNN in 400s, set iteration to 150 V39 LB608 with CoarseDropout Augmentation V40 Simple Snapshot (Checkpoint) Ensemble V52 [CV811, LB616] MixUp+CoarseDropout : credit https://www.kaggle.com/mathormad/resnet50-v2-keras-focal-loss-mix-up  V56 [CV830, LB632] Change Architecture to get the best result  V61 fix silly bugs on model selection", "sum_nltk_words": 216, "sum_nltk_runtime": 0.003, "sum_t5": "i've updated the kernel to use a similar baseline to the great kernel of @mhiro2. i've also added a lwlrap metric to the cnn. i've also added a pytorch module to the cnn. i've also added a lwlrap metric to the cnn. i've also added a lwlrap metric to the c", "sum_t5_words": 51, "sum_t5_runtime": 6.956, "runtime": 0.0, "nltk_category": "Accommodation & Food", "nltk_category_score": 0.29402339458465576, "nltk_category_runtime": 33.781, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9839709997177124, "nltk_subcategory_runtime": 54.665, "category": "Accommodation & Food", "category_score": 0.29402339458465576, "subcategory": "Machine Learning", "subcategory_score": 0.9839709997177124, "runtime_cat": 88.446, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.754", "language_code": "en", "language_score": "0.9999954065419364", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introductionsee v56 best result lb632 finally beat current best public kernel using kera probably last update kernel find kernel helpful please upvote version upto v60 silly bug elif model selection wrong effort kera replication comparable baseline great kernel mhiro2 httpswwwkagglecommhiro2simple2dcnnclassifierwithpytorch improved peining turn use excellent preprocessed data daisukelab httpswwwkagglecomdaisukelabcreatingfat2019preprocesseddata note inference private data stage2 preprocess data one change made kera version addition simple conv net also use predefined architecture trained scratch mobilenetv2 inceptionv3 xception choose kernel also many idea borrow nice kernel voglinio httpswwwkagglecomvogliniokeras2dmodel5foldlogspecgramcuratedonly also borrow softmaxbce loss tta idea gibas kernel btw know giba without mention user apologize code clean pytorch code still albeit used major update v1 cv680 lb574 v4 cv66x lb576 v5 add image augmentation module v9 cv679 add lwlrap tf metric credit rio114 httpswwwkagglecomrio114kerascnnwithlwlrapevaluation v11 employ list augmentation mentioned httpsgithubcomsainathadapakagglefreesoundaudiotaggingblobmasterapproaches_allmd v16 add bcewithlogits use activation linear v17 add simplecnn similar pytorch baseline v22 add curatedonly trainaugment option v23 add crnn model v30 lb598 shallow cnn 400s set iteration 150 v39 lb608 coarsedropout augmentation v40 simple snapshot checkpoint ensemble v52 cv811 lb616 mixupcoarsedropout credit httpswwwkagglecommathormadresnet50v2kerasfocallossmixup v56 cv830 lb632 change architecture get best result v61 fix silly bug model selection", "tags_descriptive": ["Deep Learning (DL)"]}