{"title": "2 - Quick study: LGBM, XGB and Catboost [LB: 1.66]", "description": "2.  Quick study: LGBM, XGB and Catboost\u00a0 Hi, and welcome!  In this short kernel, we will: 1) run  baseline versions of LightGBM, XGBoost and Catboost  over the Google Analytics Customer Revenue Prediction Challenge dataset, 2)  present the offline rmse, the running time and the public score of each of them and 3) create a trivial linear ensemble obtaining a 1.6677 score in the public leaderboard. \u00a0   Model Rounds Train RMSE Validation RMSE Train time Public Score     LightGBM 5000 1.505 1.60372  7min 48s 1.6717   XGBoost 2000 1.568 1.64924 54min 54s  1.6946   Catboost 1000 1.52184 1.61231 2min 24s 1.6722   Ensemble -- -- -- -- 1.6677    Result table from Conclusions section. This kernel is strongly based on these previous work:  LGBM (RF) starter [LB: 1.70] - Preprocessing is taken as-is from this awesome kernel by FabienDaniel. LightGBM + XGBoost + Catboost - LGBM, XGBoost and Catboost functions taken and ligerely adapted from this other awesome kernel by Samrat P.  The notebook has the following sections:  Preprocessing Models 2.1. LightGBM 2.2. XGBoost 2.3. Catboost   Ensemble and submissions Conclusions References", "link": "https://www.kaggle.com/julian3833/2-quick-study-lgbm-xgb-and-catboost-lb-1-66", "tags": ["Gradient Boosting"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "catboost", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-10-20 20:16:40", "date_scraped": "2020-12-12 20:08:17", "words": 202, "sentences": 9, "sum_nltk": "2.  Quick study: LGBM, XGB and Catboost\u00a0 Hi, and welcome!\nIn this short kernel, we will: 1) run  baseline versions of LightGBM, XGBoost and Catboost  over the Google Analytics Customer Revenue Prediction Challenge dataset, 2)  present the offline rmse, the running time and the public score of each of them and 3) create a trivial linear ensemble obtaining a 1.6677 score in the public leaderboard.\nModel Rounds Train RMSE Validation RMSE Train time Public Score     LightGBM 5000 1.505 1.60372  7min 48s 1.6717   XGBoost 2000 1.568 1.64924 54min 54s  1.6946   Catboost 1000 1.52184 1.61231 2min 24s 1.6722   Ensemble -- -- -- -- 1.6677    Result table from Conclusions section.\nThis kernel is strongly based on these previous work:  LGBM (RF) starter [LB: 1.70] - Preprocessing is taken as-is from this awesome kernel by FabienDaniel.\nLightGBM + XGBoost + Catboost - LGBM, XGBoost and Catboost functions taken and ligerely adapted from this other awesome kernel by Samrat P.\nThe notebook has the following sections:  Preprocessing Models 2.1.\nLightGBM 2.2.\nXGBoost 2.3.\nCatboost   Ensemble and submissions Conclusions References", "sum_nltk_words": 189, "sum_nltk_runtime": 0.002, "sum_t5": "this kernel is strongly based on these previous work. it will run baseline versions of LightGBM, XGBoost and Catboost. it will present the offline rmse, the running time and the public score of each of them. it will also create a trivial linear ensemble obtaining a 1.6677 score in the public leaderboard. the kernel is strongly based on these previous work.. a.", "sum_t5_words": 62, "sum_t5_runtime": 4.654, "runtime": 0.002, "nltk_category": "Finance", "nltk_category_score": 0.47176462411880493, "nltk_category_runtime": 22.996, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8645455241203308, "nltk_subcategory_runtime": 36.404, "category": "Finance", "category_score": 0.47176462411880493, "subcategory": "Machine Learning", "subcategory_score": 0.8645455241203308, "runtime_cat": 59.4, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.747", "language_code": "en", "language_score": "0.9999957896640734", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "2 quick study lgbm xgb catboost hi welcome short kernel 1 run baseline version lightgbm xgboost catboost google analytics customer revenue prediction challenge dataset 2 present offline rmse running time public score 3 create trivial linear ensemble obtaining 16677 score public leaderboard model round train rmse validation rmse train time public score lightgbm 5000 1505 160372 7min 48s 16717 xgboost 2000 1568 164924 54min 54s 16946 catboost 1000 152184 161231 2min 24 16722 ensemble 16677 result table conclusion section kernel strongly based previous work lgbm rf starter lb 170 preprocessing taken asis awesome kernel fabiendaniel lightgbm xgboost catboost lgbm xgboost catboost function taken ligerely adapted awesome kernel samrat p notebook following section preprocessing model 21 lightgbm 22 xgboost 23 catboost ensemble submission conclusion reference", "tags_descriptive": ["Gradient Boosting"]}