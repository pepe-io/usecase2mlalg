{"title": "Start Here: A Gentle Introduction", "description": "Introduction: Home Credit Default Risk CompetitionThis notebook is intended for those who are new to machine learning competitions or want a gentle introduction to the problem. I purposely avoid jumping into complicated models or joining together lots of data in order to show the basics of how to get started in machine learning! Any comments or suggestions are much appreciated. In this notebook, we will take an initial look at the Home Credit default risk machine learning competition currently hosted on Kaggle. The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:  Supervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features Classification: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)  DataThe data is provided by Home Credit, a service dedicated to provided lines of credit (loans) to the unbanked population. Predicting whether or not a client will repay a loan or have difficulty is a critical business need, and Home Credit is hosting this competition on Kaggle to see what sort of models the machine learning community can develop to help them in this task. There are 7 different sources of data:  application_train/application_test: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating 0: the loan was repaid or 1: the loan was not repaid.  bureau: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits. bureau_balance: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.  previous_application: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.  POS_CASH_BALANCE: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows. credit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows. installments_payment: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.   This diagram shows how all of the data is related:  Moreover, we are provided with the definitions of all the columns (in HomeCredit_columns_description.csv) and an example of the expected submission file. In this notebook, we will stick to using only the main application training and testing data. Although if we want to have any hope of seriously competing, we need to use all the data, for now we will stick to one file which should be more manageable. This will let us establish a baseline that we can then improve upon. With these projects, it's best to build up an understanding of the problem a little at a time rather than diving all the way in and getting completely lost! Metric: ROC AUCOnce we have a grasp of the data (reading through the column descriptions helps immensely), we need to understand the metric by which our submission is judged. In this case, it is a common classification metric known as the Receiver Operating Characteristic Area Under the Curve (ROC AUC, also sometimes called AUROC). The ROC AUC may sound intimidating, but it is relatively straightforward once you can get your head around the two individual concepts. The Reciever Operating Characteristic (ROC) curve graphs the true positive rate versus the false positive rate:  A single line on the graph indicates the curve for a single model, and movement along a line indicates changing the threshold used for classifying a positive instance. The threshold starts at 0 in the upper right to and goes to 1 in the lower left. A curve that is to the left and above another curve indicates a better model. For example, the blue model is better than the red model, which is better than the black diagonal line which indicates a naive random guessing model. The Area Under the Curve (AUC) explains itself by its name! It is simply the area under the ROC curve. (This is the integral of the curve.) This metric is between 0 and 1 with a better model scoring higher. A model that simply guesses at random will have an ROC AUC of 0.5. When we measure a classifier according to the ROC AUC, we do not generation 0 or 1 predictions, but rather a probability between 0 and 1. This may be confusing because we usually like to think in terms of accuracy, but when we get into problems with inbalanced classes (we will see this is the case), accuracy is not the best metric. For example, if I wanted to build a model that could detect terrorists with 99.9999% accuracy, I would simply make a model that predicted every single person was not a terrorist. Clearly, this would not be effective (the recall would be zero) and we use more advanced metrics such as ROC AUC or the F1 score to more accurately reflect the performance of a classifier. A model with a high ROC AUC will also have a high accuracy, but the ROC AUC is a better representation of model performance. Not that we know the background of the data we are using and the metric to maximize, let's get into exploring the data. In this notebook, as mentioned previously, we will stick to the main data sources and simple models which we can build upon in future work. Follow-up Notebooks For those looking to keep working on this problem, I have a series of follow-up notebooks:  Manual Feature Engineering Part One Manual Feature Engineering Part Two Introduction to Automated Feature Engineering Advanced Automated Feature Engineering Feature Selection Intro to Model Tuning: Grid and Random Search Automated Model Tuning Model Tuning Results  I'll add more notebooks as I finish them! Thanks for all the comments!", "link": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "tags": ["Classification", "Exploratory Data Analysis"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pattern", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-08-25 04:00:06", "date_scraped": "2020-12-12 20:47:32", "words": 1116, "sentences": 44, "sum_nltk": "Introduction: Home Credit Default Risk CompetitionThis notebook is intended for those who are new to machine learning competitions or want a gentle introduction to the problem.\nThis is a standard supervised classification task:  Supervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features Classification: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)  DataThe data is provided by Home Credit, a service dedicated to provided lines of credit (loans) to the unbanked population.\nPredicting whether or not a client will repay a loan or have difficulty is a critical business need, and Home Credit is hosting this competition on Kaggle to see what sort of models the machine learning community can develop to help them in this task.\nprevious_application: previous applications for loans at Home Credit of clients who have loans in the application data.\nPOS_CASH_BALANCE: monthly data about previous point of sale or cash loans clients have had with Home Credit.\nIn this case, it is a common classification metric known as the Receiver Operating Characteristic Area Under the Curve (ROC AUC, also sometimes called AUROC).", "sum_nltk_words": 199, "sum_nltk_runtime": 0.012, "sum_t5": "home credit is hosting a machine learning competition on Kaggle. the goal is to predict whether or not an applicant will be able to repay a loan. the notebook is intended for those who are new to machine learning. the goal is to train a model to learn to predict the labels from the features. the notebook is intended for those who are new to machine learning. a san francisco-based company is hosting the competition.", "sum_t5_words": 75, "sum_t5_runtime": 6.238, "runtime": 0.005, "nltk_category": "Finance", "nltk_category_score": 0.5667473673820496, "nltk_category_runtime": 18.124, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9806005954742432, "nltk_subcategory_runtime": 28.024, "category": "Finance", "category_score": 0.5667473673820496, "subcategory": "Machine Learning", "subcategory_score": 0.9806005954742432, "runtime_cat": 46.149, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.85", "language_code": "en", "language_score": "0.9999979515017834", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introduction home credit default risk competitionthis notebook intended new machine learning competition want gentle introduction problem purposely avoid jumping complicated model joining together lot data order show basic get started machine learning comment suggestion much appreciated notebook take initial look home credit default risk machine learning competition currently hosted kaggle objective competition use historical loan application data predict whether applicant able repay loan standard supervised classification task supervised label included training data goal train model learn predict label feature classification label binary variable 0 repay loan time 1 difficulty repaying loan datathe data provided home credit service dedicated provided line credit loan unbanked population predicting whether client repay loan difficulty critical business need home credit hosting competition kaggle see sort model machine learning community develop help task 7 different source data application_trainapplication_test main training testing data information loan application home credit every loan row identified feature sk_id_curr training application data come target indicating 0 loan repaid 1 loan repaid bureau data concerning client previous credit financial institution previous credit row bureau one loan application data multiple previous credit bureau_balance monthly data previous credit bureau row one month previous credit single previous credit multiple row one month credit length previous_application previous application loan home credit client loan application data current loan application data multiple previous loan previous application one row identified feature sk_id_prev pos_cash_balance monthly data previous point sale cash loan client home credit row one month previous point sale cash loan single previous loan many row credit_card_balance monthly data previous credit card client home credit row one month credit card balance single credit card many row installments_payment payment history previous loan home credit one row every made payment one row every missed payment diagram show data related moreover provided definition column homecredit_columns_descriptioncsv example expected submission file notebook stick using main application training testing data although want hope seriously competing need use data stick one file manageable let u establish baseline improve upon project best build understanding problem little time rather diving way getting completely lost metric roc auconce grasp data reading column description help immensely need understand metric submission judged case common classification metric known receiver operating characteristic area curve roc auc also sometimes called auroc roc auc may sound intimidating relatively straightforward get head around two individual concept reciever operating characteristic roc curve graph true positive rate versus false positive rate single line graph indicates curve single model movement along line indicates changing threshold used classifying positive instance threshold start 0 upper right go 1 lower left curve left another curve indicates better model example blue model better red model better black diagonal line indicates naive random guessing model area curve auc explains name simply area roc curve integral curve metric 0 1 better model scoring higher model simply guess random roc auc 05 measure classifier according roc auc generation 0 1 prediction rather probability 0 1 may confusing usually like think term accuracy get problem inbalanced class see case accuracy best metric example wanted build model could detect terrorist 999999 accuracy would simply make model predicted every single person terrorist clearly would effective recall would zero use advanced metric roc auc f1 score accurately reflect performance classifier model high roc auc also high accuracy roc auc better representation model performance know background data using metric maximize let get exploring data notebook mentioned previously stick main data source simple model build upon future work followup notebook looking keep working problem series followup notebook manual feature engineering part one manual feature engineering part two introduction automated feature engineering advanced automated feature engineering feature selection intro model tuning grid random search automated model tuning model tuning result ill add notebook finish thanks comment", "tags_descriptive": ["Classification", "Exploratory Data Analysis"]}