{"title": "Visualizing Kannada MNIST with t-SNE", "description": "Visualizing Kannada MNIST with t-SNEA 3 part series on Dimensionality reduction techniques using the Kannada MNIST dataset  Drawbacks of PCAIn my last kernel titled Visualizing Kannada MNIST with PCA, I demonstrated how we could visualize the Kannada MNIST dataset by reducing the 784 dimensions into 2 thereby, making it easy to be viewed by human eye. We used PCA for reducing the dimensionality of the dataset. PCA essentially tries project the original data onto the directions where variance is maximum. In our case it projcected the data onto 2 Dimensions which could then be easily visualised. However, the visualisations produced by PCA was not able to do such a good job in differentiating all the digits.This is because PCA is a linear projection, which means it can\u2019t capture non-linear dependencies. In this notebook we shall explore another Dimensionality reduction technique called t-SNE and see if it gives us better results as compared to PCA  Part2: t-SNE(T-distributed stochastic neighbour embedding) in Python  Table of Contents What is t-SNE Embeddings] t-SNE under the hood t-SNE with Scikit learn Interactively visualising t-SNE with Bokeh Further Readings  t-SNEt-SNE or T-distributed stochastic neighbour embedding takes a high dimensional data set and reduces it to a low dimensional graph that retains a lot of the original information. It does so by giving each data point a location in a two or three-dimensional map. This technique finds clusters in data thereby making sure that an embedding preserves the meaning in the data.  t-SNE reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. EmbeddingsAn embedding is essentially a low-dimensional space into which a high dimensional vector can be translated. During translation, an embedding preserves the semantic relationship of the inputs by placing similar inputs close together in the embedding space. Let\u2019s try and wrap our head around this concept with examples. Here is a grab from the creators of the Embedding projector, a tool which enables us to visualise high dimensional data easily.  Embeddings from Parul Pandey on Vimeo.t-SNE under the hoodt-SNE, was proposed by Geoffry Hinton\u2019s and Laurens van der Maaten  back in 2008. Their paper titled Visualizing Data using t-SNE is an essential read for soembody trying to understand t-SNE. Here is how t-SNE basically works:  First, a probability distribution is created in a high dimensional space. This means if we pick a point in the dataset, we define the probability of picking another point as a neighbour. Next, a low dimensional space is then created that has the same(or as near as possible) probability distribution as the high Dimensional space.  t-SNE with Scikit learnScikit-learn has an implementation of t-SNE available, and you can check its documentation here.", "link": "https://www.kaggle.com/parulpandey/visualizing-kannada-mnist-with-t-sne", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-05-18 13:29:24", "date_scraped": "2020-12-13 12:26:21", "words": 455, "sentences": 19, "sum_nltk": "Visualizing Kannada MNIST with t-SNEA 3 part series on Dimensionality reduction techniques using the Kannada MNIST dataset  Drawbacks of PCAIn my last kernel titled Visualizing Kannada MNIST with PCA, I demonstrated how we could visualize the Kannada MNIST dataset by reducing the 784 dimensions into 2 thereby, making it easy to be viewed by human eye.\nIn this notebook we shall explore another Dimensionality reduction technique called t-SNE and see if it gives us better results as compared to PCA  Part2: t-SNE(T-distributed stochastic neighbour embedding) in Python  Table of Contents What is t-SNE Embeddings] t-SNE under the hood t-SNE with Scikit learn Interactively visualising t-SNE with Bokeh Further Readings  t-SNEt-SNE or T-distributed stochastic neighbour embedding takes a high dimensional data set and reduces it to a low dimensional graph that retains a lot of the original information.\nEmbeddingsAn embedding is essentially a low-dimensional space into which a high dimensional vector can be translated.\nHere is a grab from the creators of the Embedding projector, a tool which enables us to visualise high dimensional data easily.\nHere is how t-SNE basically works:  First, a probability distribution is created in a high dimensional space.", "sum_nltk_words": 193, "sum_nltk_runtime": 0.005, "sum_t5": "t-SNE is a dimensionality reduction technique that reduces dimensionality. it is a technique that finds clusters in data thereby keeping meaning in the data. t-SNE is a technique that reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. it is a'simple' technique that can be used to reduce dimensionality. it is a linear projection, which means it can\u2019t capture non-linear dependencies.", "sum_t5_words": 65, "sum_t5_runtime": 6.984, "runtime": 0.007, "nltk_category": "Accommodation & Food", "nltk_category_score": 0.20688201487064362, "nltk_category_runtime": 20.662, "nltk_subcategory": "Student", "nltk_subcategory_score": 0.2527768313884735, "nltk_subcategory_runtime": 33.467, "category": "Accommodation & Food", "category_score": 0.20688201487064362, "subcategory": "Student", "subcategory_score": 0.2527768313884735, "runtime_cat": 54.128, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.705", "language_code": "en", "language_score": "0.9999962226162626", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "visualizing kannada mnist tsnea 3 part series dimensionality reduction technique using kannada mnist dataset drawback pcain last kernel titled visualizing kannada mnist pca demonstrated could visualize kannada mnist dataset reducing 784 dimension 2 thereby making easy viewed human eye used pca reducing dimensionality dataset pca essentially try project original data onto direction variance maximum case projcected data onto 2 dimension could easily visualised however visualisation produced pca able good job differentiating digitsthis pca linear projection mean cant capture nonlinear dependency notebook shall explore another dimensionality reduction technique called tsne see give u better result compared pca part2 tsnetdistributed stochastic neighbour embedding python table content tsne embeddings tsne hood tsne scikit learn interactively visualising tsne bokeh reading tsnetsne tdistributed stochastic neighbour embedding take high dimensional data set reduces low dimensional graph retains lot original information giving data point location two threedimensional map technique find cluster data thereby making sure embedding preserve meaning data tsne reduces dimensionality trying keep similar instance close dissimilar instance apart embeddingsan embedding essentially lowdimensional space high dimensional vector translated translation embedding preserve semantic relationship input placing similar input close together embedding space let try wrap head around concept example grab creator embedding projector tool enables u visualise high dimensional data easily embeddings parul pandey vimeotsne hoodtsne proposed geoffry hintons laurens van der maaten back 2008 paper titled visualizing data using tsne essential read soembody trying understand tsne tsne basically work first probability distribution created high dimensional space mean pick point dataset define probability picking another point neighbour next low dimensional space created sameor near possible probability distribution high dimensional space tsne scikit learnscikitlearn implementation tsne available check documentation", "tags_descriptive": []}