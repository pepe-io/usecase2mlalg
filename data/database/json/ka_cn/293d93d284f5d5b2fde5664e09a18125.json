{"title": "CNN for Sentence Classification by Yoon Kim", "description": "Convolutional Neural Networks for Sentence Classification by Yoon Kim in KerasKim's paper was published in 2014 and showed that not only are CNNs great for images but also text. With an extreamly simple architecture Kim outperformed all other models in 4 out of 7 benchmarks at the time. In my first attempt to reproduce a paper on Kaggle, I'm going to try and recreate what he did with Keras. \u00a0Architecture - architecture diagram taken from Kim's paper The architecture Kim used goes like this: Firstly, sentences are represented as vectors of words and those words are converted into (300D) vectors giving us a 2D representation for each sentence. Kim uses a few different approaches for creating these word vectors:  CNN-rand: where embeddings are randomly assigned to each word CNN-static: word2vec is used to provide word embeddings. Unknown words are randomly initialised. These embeddings are kept fixed. CNN-non-static: as above, but the vectors are fine-tuned (ie they can be changed) during training  Kim also tried another approach which I won't attempt to do here.. because quite frankly I don't know how. I might attempt this in the future.  CNN-multichannel: Here Kim uses two sets of word vectors. My understanding is he applies both to the sentence (resulting in two, sentence_length x 300D matricies) and performs convolutions on both (see below). During training he fine tunes one embedding but keeps the other one fixed. He states the hope of this was that it would act as a form of regularization but the results were mixed. I'm not sure if this approach was investigated and improved upon in other papers or not. Hopefully it was, it sounds like the kind of thing that might work.  He then performs convolutions on these 2D representations, but he does it in a way you don't see very often. Convolutions of different window sizes (3, 4 and 5) are performed on the representations directly and then max pooled. This is a little like an inception module in the Goog LeNet, but not something I've really seen anywhere else. Finally after a layout of dropout, an output layer is directly used to do the predictions. And that's it! With one conv layer and some max pooling we can get fantastic results! Why this paper is interestingWe all know LSTMs are the king of sentence classification... but are they really? Recently I tried to classify a million sentences where I work with a (admittedly complicated) LSTM. It was estimated it would take 130 hours to run on the machine available. With something inspired by this model it takes around an hour and we achieved compariable accuracy (these are short sentences which is believe why that works). It's also interesting to see totally different approaches to solving this problem. Update while working on another problem I found myself doing something Kim does in this paper which you don't often see: When randomly initializing words not in word2vec, we obtained slight improvements by sampling each dimension from U[\u2212a, a] where a was chosen such that the randomly initialized vectors have the same variance as the pre-trained ones. It would be interesting to see if employing more sophisticated methods to mirror the distribution of pre-trained vectors in the initialization process gives further improvements.  This is really interesting and something I would like to explore in another kernel. Places this differs from Kim's paperSadly I couldn't find the same data Kim used, but I've managed to find something very similar sounding to the SST1 data (from which I can infer the SST2 data) where it looks like we can achieve simiar results. Kim uses a 300D w2v model trained using cbow. I couldn't find this model, but used a 200D model trained using GloVe. The major impact of this is we use an embedding dimension of 200 here. I also swapped out the optimizer Kim used (AdaDelta) for Adam - AdaDelta took forever to run but it looks like you get the same results as Adam. If you're happy to sit there for around 100 epochs you can try it out. Data prepSo let's get started. Before we do anything else we need to get some data and prep it", "link": "https://www.kaggle.com/hamishdickson/cnn-for-sentence-classification-by-yoon-kim", "tags": ["NLP", "Classification", "CNN"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-11-24 17:05:07", "date_scraped": "2020-12-13 14:14:35", "words": 702, "sentences": 36, "sum_nltk": "Convolutional Neural Networks for Sentence Classification by Yoon Kim in KerasKim's paper was published in 2014 and showed that not only are CNNs great for images but also text.\nArchitecture - architecture diagram taken from Kim's paper The architecture Kim used goes like this: Firstly, sentences are represented as vectors of words and those words are converted into (300D) vectors giving us a 2D representation for each sentence.\nKim uses a few different approaches for creating these word vectors:  CNN-rand: where embeddings are randomly assigned to each word CNN-static: word2vec is used to provide word embeddings.\nCNN-non-static: as above, but the vectors are fine-tuned (ie they can be changed) during training  Kim also tried another approach which I won't attempt to do here..\nCNN-multichannel: Here Kim uses two sets of word vectors.\nConvolutions of different window sizes (3, 4 and 5) are performed on the representations directly and then max pooled.\nUpdate while working on another problem I found myself doing something Kim does in this paper which you don't often see: When randomly initializing words not in word2vec, we obtained slight improvements by sampling each dimension from U[\u2212a, a] where a was chosen such that the randomly initialized vectors have the same variance as the pre-trained ones.\nKim uses a 300D w2v model trained using cbow.", "sum_nltk_words": 212, "sum_nltk_runtime": 0.007, "sum_t5": "a paper by Yoon Kim in 2014 showed that CNNs are great for images and text. he performed convolutions on these 2D representations, but in a way you don't see very often. he then performs convolutions of different window sizes (3, 4 and 5) on the representations directly and then max pooled. he then used a different approach to create the vectors. he then used a different approach to create the vectors.", "sum_t5_words": 72, "sum_t5_runtime": 6.7, "runtime": 0.008, "nltk_category": "Justice, Law and Regulations", "nltk_category_score": 0.29529115557670593, "nltk_category_runtime": 20.494, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9605736136436462, "nltk_subcategory_runtime": 33.521, "category": "Justice, Law and Regulations", "category_score": 0.29529115557670593, "subcategory": "Machine Learning", "subcategory_score": 0.9605736136436462, "runtime_cat": 54.015, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.733", "language_code": "en", "language_score": "0.999997703943937", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "convolutional neural network sentence classification yoon kim keraskims paper published 2014 showed cnns great image also text extreamly simple architecture kim outperformed model 4 7 benchmark time first attempt reproduce paper kaggle im going try recreate kera architecture architecture diagram taken kims paper architecture kim used go like firstly sentence represented vector word word converted 300d vector giving u 2d representation sentence kim us different approach creating word vector cnnrand embeddings randomly assigned word cnnstatic word2vec used provide word embeddings unknown word randomly initialised embeddings kept fixed cnnnonstatic vector finetuned ie changed training kim also tried another approach wont attempt quite frankly dont know might attempt future cnnmultichannel kim us two set word vector understanding applies sentence resulting two sentence_length x 300d matricies performs convolution see training fine tune one embedding keep one fixed state hope would act form regularization result mixed im sure approach investigated improved upon paper hopefully sound like kind thing might work performs convolution 2d representation way dont see often convolution different window size 3 4 5 performed representation directly max pooled little like inception module goog lenet something ive really seen anywhere else finally layout dropout output layer directly used prediction thats one conv layer max pooling get fantastic result paper interestingwe know lstms king sentence classification really recently tried classify million sentence work admittedly complicated lstm estimated would take 130 hour run machine available something inspired model take around hour achieved compariable accuracy short sentence believe work also interesting see totally different approach solving problem update working another problem found something kim paper dont often see randomly initializing word word2vec obtained slight improvement sampling dimension ua chosen randomly initialized vector variance pretrained one would interesting see employing sophisticated method mirror distribution pretrained vector initialization process give improvement really interesting something would like explore another kernel place differs kims papersadly couldnt find data kim used ive managed find something similar sounding sst1 data infer sst2 data look like achieve simiar result kim us 300d w2v model trained using cbow couldnt find model used 200d model trained using glove major impact use embedding dimension 200 also swapped optimizer kim used adadelta adam adadelta took forever run look like get result adam youre happy sit around 100 epoch try data prepso let get started anything else need get data prep", "tags_descriptive": ["Natural Language Processing (NLP)", "Classification", "Convolutional Neural Network (CNN)"]}