{"title": "Introduction to Generative Adversarial Networks", "description": "Imagine if we had access to the true data distribution Pdata(x) we could sample from that distribution in order to generate new samples, however there is no direct way to do this as typically this distribution is complex and high-dimensional. What if we could instead sample from a random noise (e.g. Normal distribution) and then learn to transform that to Pdata(x). Neural networks are a prime candidate to capture functions with high complexity and we can use to to capture this transformation. This is exactly what the do. They train the transformer network or Generator along with another network, called the Discriminator, in a game theoretic way. Going back to our image generation example: The Generator network (G), tries to fool the discriminator in thinking that the generated images are real,meaning that they are taken from Pdata, and The Discriminator network (D), tries to differentiate between real (x\u223cPdata) and fake images. Random noise is fed into the Generator that transforms it into a \"fake image\". The Discriminator is fed both from the training set images (pdata(x)) and the fake images coming from the Generator and it has to tell them apart. The idea behind GAN, is to train both of these networks alternatively to do the best they can in generating and discriminating images. The intuition is that by improving one of these networks, in this game theoretic manner, the other network has to do a better job to win the game, and that in turn improves its performance and this loop continues.", "link": "https://www.kaggle.com/roydatascience/introduction-to-generative-adversarial-networks", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-08-15 23:21:03", "date_scraped": "2020-12-12 20:11:09", "words": 253, "sentences": 11, "sum_nltk": "Imagine if we had access to the true data distribution Pdata(x) we could sample from that distribution in order to generate new samples, however there is no direct way to do this as typically this distribution is complex and high-dimensional.\nWhat if we could instead sample from a random noise (e.g. Normal distribution) and then learn to transform that to Pdata(x).\nNeural networks are a prime candidate to capture functions with high complexity and we can use to to capture this transformation.\nThey train the transformer network or Generator along with another network, called the Discriminator, in a game theoretic way.\nGoing back to our image generation example: The Generator network (G), tries to fool the discriminator in thinking that the generated images are real,meaning that they are taken from Pdata, and The Discriminator network (D), tries to differentiate between real (x\u223cPdata) and fake images.\nRandom noise is fed into the Generator that transforms it into a \"fake image\".\nThe Discriminator is fed both from the training set images (pdata(x)) and the fake images coming from the Generator and it has to tell them apart.\nThe idea behind GAN, is to train both of these networks alternatively to do the best they can in generating and discriminating images.", "sum_nltk_words": 201, "sum_nltk_runtime": 0.003, "sum_t5": "a random distribution is used to generate samples, but this is not possible. a neural network can be used to train networks to do the best they can. the generator network (G) tries to fool the discriminator in thinking that the generated images are real. the discriminator network is fed both from the training set images (pdata(x)) and the fake images coming from the Generator. a neural network can be used to train networks to do the best they can in generating and discriminating images.", "sum_t5_words": 85, "sum_t5_runtime": 5.364, "runtime": 0.0, "nltk_category": "Utilities", "nltk_category_score": 0.31654441356658936, "nltk_category_runtime": 18.287, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9024067521095276, "nltk_subcategory_runtime": 29.754, "category": "Utilities", "category_score": 0.31654441356658936, "subcategory": "Machine Learning", "subcategory_score": 0.9024067521095276, "runtime_cat": 48.041, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.741", "language_code": "en", "language_score": "0.9999990203258535", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "imagine access true data distribution pdatax could sample distribution order generate new sample however direct way typically distribution complex highdimensional could instead sample random noise eg normal distribution learn transform pdatax neural network prime candidate capture function high complexity use capture transformation exactly train transformer network generator along another network called discriminator game theoretic way going back image generation example generator network g try fool discriminator thinking generated image realmeaning taken pdata discriminator network try differentiate real xpdata fake image random noise fed generator transforms fake image discriminator fed training set image pdatax fake image coming generator tell apart idea behind gan train network alternatively best generating discriminating image intuition improving one network game theoretic manner network better job win game turn improves performance loop continues", "tags_descriptive": []}