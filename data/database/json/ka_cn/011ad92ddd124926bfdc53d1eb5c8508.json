{"title": "TensorFlow roBERTa - [0.705]", "description": "TensorFlow roBERTa Starter - LB 0.705This notebook is a TensorFlow template for solving Kaggle's Tweet Sentiment Extraction competition as a question and answer roBERTa formulation. In this notebook, we show how to tokenize the data, create question answer targets, and how to build a custom question answer head for roBERTa in TensorFlow. Note that HuggingFace transformers don't have a TFRobertaForQuestionAnswering so we must make our own from TFRobertaModel. This notebook can achieve LB 0.715 with some modifications. Have fun experimenting! You can also run this code offline and it will save the best model weights during each of the 5 folds of training. Upload those weights to a private Kaggle dataset and attach to this notebook. Then you can run this notebook with the line model.fit() commented out, and this notebook will instead load your offline models. It will use your offline models to predict oof and predict test. Hence this notebook can easily be converted to an inference notebook. An inference notebook is advantageous because it will only take 10 minutes to commit and submit instead of 2 hours. Better to train 2 hours offline separately.", "link": "https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-04-14 04:57:52", "date_scraped": "2020-12-13 17:53:52", "words": 187, "sentences": 11, "runtime": 0.0, "description_category": "Utilities", "description_category_score": 0.06892897933721542, "description_category_runtime": 16.853, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.8637850284576416, "description_subcategory_runtime": 27.399, "category": "Utilities", "category_score": 0.06892897933721542, "subcategory": "Machine Learning", "subcategory_score": 0.8637850284576416, "runtime_cat": 44.252, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.781", "language_code": "en", "language_score": "0.999997032697052", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "tensorflow roberta starter lb 0705this notebook tensorflow template solving kaggles tweet sentiment extraction competition question answer roberta formulation notebook show tokenize data create question answer target build custom question answer head roberta tensorflow note huggingface transformer dont tfrobertaforquestionanswering must make tfrobertamodel notebook achieve lb 0715 modification fun experimenting also run code offline save best model weight 5 fold training upload weight private kaggle dataset attach notebook run notebook line modelfit commented notebook instead load offline model use offline model predict oof predict test hence notebook easily converted inference notebook inference notebook advantageous take 10 minute commit submit instead 2 hour better train 2 hour offline separately", "tags_descriptive": []}