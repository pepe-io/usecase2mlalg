{"title": "Make Chris Deotte's data augmentation faster", "description": "Data Augmentation in batch form and running on GPU/TPU.This notebook implements a batch form of Chris Deotte's great data augmention in Rotation Augmentation GPU/TPU - [0.96+]. It also shows how to perform data augmentation on GPU / TPU directly. * [Important] Although running data augmentation directly on GPU/TPU is faster, it loses the advantage of separating the data processing on CPU and model training on GPU/TPU. * Therefore, we don't encourage the idea of using tf.keras.layers.Layer for data augmentation shown in this kernel. * However, the batch implementation is still beneficial when doing data augmentation on CPU by using tf.data.Dataset. * In order to make the measurement of timing, we measure it with a dummy layer which does the data processing in GPU / TPU, but return dummy tensor. This is to avoid the overhead of sending large tensor back to CPU.", "link": "https://www.kaggle.com/yihdarshieh/make-chris-deotte-s-data-augmentation-faster", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-06-19 14:04:39", "date_scraped": "2020-12-12 19:56:26", "words": 142, "sentences": 7, "runtime": 0.005, "description_category": "Utilities", "description_category_score": 0.31594157218933105, "description_category_runtime": 14.026, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.7913873791694641, "description_subcategory_runtime": 22.456, "category": "Utilities", "category_score": 0.31594157218933105, "subcategory": "Machine Learning", "subcategory_score": 0.7913873791694641, "runtime_cat": 36.483, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.667", "language_code": "en", "language_score": "0.9999947698972786", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "data augmentation batch form running gputputhis notebook implement batch form chris deottes great data augmention rotation augmentation gputpu 096 also show perform data augmentation gpu tpu directly important although running data augmentation directly gputpu faster loses advantage separating data processing cpu model training gputpu therefore dont encourage idea using tfkeraslayerslayer data augmentation shown kernel however batch implementation still beneficial data augmentation cpu using tfdatadataset order make measurement timing measure dummy layer data processing gpu tpu return dummy tensor avoid overhead sending large tensor back cpu", "tags_descriptive": []}