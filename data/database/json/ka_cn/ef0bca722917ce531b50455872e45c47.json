{"title": "A Complete Introduction and Walkthrough ", "description": "Costa Rican Household Poverty Level PredictionWelcome to another Kaggle challenge! The objective of the Costa Rican Household Poverty Level Prediction contest is to develop a machine learning model that can predict the poverty level of households using both individual and household characteristics. This \"data science for good\" project offers the opportunity to put our skills towards a task more beneficial to society than getting people to click on ads! In this notebook, we will walk through a complete machine learning solution: first, get introduced to the problem, then perform a thorough Exploratory Data Analysis of the dataset, work on feature engineering, try out multiple machine learning models, select a model, work to optimize the model, and finally, inspect the outputs of the model and draw conclusions. __While this notebook may not get us to the top of the leaderboard, it is meant to be used as a teaching tool to give you a solid foundation to build on for future machine learning projects. Kaggle projects can teach us a lot about machine learning, but several of the strategies used to get to the very top of the leaderboard are not best practices, so here we'll stick to building a very good - although not quite first place - machine learning solution. While Kaggle projects are competitions, I think they are best described as \"a machine learning education\" disguised as a contest!\" If you are looking to follow-up on this work, I have additional work including a kernel on using Automated Feature Engineering with Featuretools for this problem (with slightly higher leaderboard score). (If you enjoy my writing style and explanations, I write for Towards Data Science) Problem and Data ExplanationThe data for this competition is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual. The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty. This is a supervised multi-class classification machine learning problem:  Supervised: provided with the labels for the training data Multi-class classification: Labels are discrete values with 4 classes  ObjectiveThe objective is to predict poverty on a household level. We are given data on the individual level with each individual having unique features but also information about their household. In order to create a dataset for the task, we'll have to perform some aggregations of the individual data for each household. Moreover, we have to make a prediction for every individual in the test set, but \"ONLY the heads of household are used in scoring\" which means we want to predict poverty on a household basis. Important note: while all members of a household should have the same label in the training data, there are errors where individuals in the same household have different labels. In these cases, we are told to use the label for the head of each household, which can be identified by the rows where parentesco1 == 1.0. We will cover how to correct this in the notebook (for more info take a look at the competition main discussion). The Target values represent poverty levels as follows: 1 = extreme poverty  2 = moderate poverty  3 = vulnerable households  4 = non vulnerable households   The explanations for all 143 columns can be found in the competition documentation, but a few to note are below:  Id: a unique identifier for each individual, this should not be a feature that we use!  idhogar: a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier. parentesco1: indicates if this person is the head of the household. Target: the label, which should be equal for all members in a household  When we make a model, we'll train on a household basis with the label for each household the poverty level of the head of household. The raw data contains a mix of both household and individual characteristics and for the individual data, we will have to find a way to aggregate this for each household. Some of the individuals belong to a household with no head of household which means that unfortunately we can't use this data for training. These issues with the data are completely typical of real-world data and hence this problem is great preparation for the datasets you'll encounter in a data science job! MetricUltimately we want to build a machine learning model that can predict the integer poverty level of a household. Our predictions will be assessed by the Macro F1 Score. You may be familiar with the standard F1 score for binary classification problems which is the harmonic mean of precision and recall: F1=21recall+1precision=2\u22c5precision\u22c5recallprecision+recallF1=21recall+1precision=2\u22c5precision\u22c5recallprecision+recall For mutli-class problems, we have to average the F1 scores for each class. The macro F1 score averages the F1 score for each class without taking into account label imbalances. Macro F1=F1 Class 1+F1 Class 2+F1 Class 3+F1 Class 44Macro F1=F1 Class 1+F1 Class 2+F1 Class 3+F1 Class 44 In other words, the number of occurrences of each label does not figure into the calculation when using macro (while it does when using the \"weighted\" score). (For more information on the differences, look at the Scikit-Learn Documention for F1 Score or this Stack Exchange question and answers. If we want to assess our performance, we can use the code: from sklearn.metrics import f1_score f1_score(y_true, y_predicted, average = 'macro`) For this problem, the labels are imbalanced, which makes it a little strange to use macro averaging for the evaluation metric, but that's a decision made by the organizers and not something we can change! In your own work, you want to be aware of label imbalances and choose a metric accordingly. RoadmapThe end objective is a machine learning model that can predict the poverty level of a household. However, before we get carried away with modeling, it's important to understand the problem and data. Also, we want to evaluate numerous models before choosing one as the \"best\" and after building a model, we want to investigate the predictions. Our roadmap is therefore as follows:  Understand the problem (we're almost there already) Exploratory Data Analysis Feature engineering to create a dataset for machine learning Compare several baseline machine learning models Try more complex machine learning models Optimize the selected model Investigate model predictions in context of problem Draw conclusions and lay out next steps   The steps laid out above are iterative meaning that while we will go through them one at a time, we might go back to an earlier step and revisit some of our decisions. In general, data science is a non-linear pracice where we are constantly evaluating our past decisions and making improvements. In particular, feature engineering, modeling, and optimization are steps that we often repeat because we never know if we got them right the first time! Getting StartedWe have a pretty good grasp of the problem, so we'll move into the Exploratory Data Analysis (EDA) and feature engineering. For the EDA we'll examine any interesting anomalies, trends, correlations, or patterns that can be used for feature engineering and for modeling. We'll make sure to investigate our data both quantitatively (with statistics) and visually (with figures). Once we have a good grasp of the data and any potentially useful relationships, we can do some feature engineering (the most important part of the machine learning pipeline) and establish a baseline model. This won't get us to the top of the leaderboard, but it will provide a strong foundation to build on! With all that info in mind (don't worry if you haven't got all the details), let's get started!", "link": "https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough", "tags": ["Classification", "Exploratory Data Analysis"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pattern", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-08-19 23:17:11", "date_scraped": "2020-12-12 17:59:32", "words": 1338, "sentences": 40, "sum_nltk": "In this notebook, we will walk through a complete machine learning solution: first, get introduced to the problem, then perform a thorough Exploratory Data Analysis of the dataset, work on feature engineering, try out multiple machine learning models, select a model, work to optimize the model, and finally, inspect the outputs of the model and draw conclusions.\nWhile Kaggle projects are competitions, I think they are best described as \"a machine learning education\" disguised as a contest!\" If you are looking to follow-up on this work, I have additional work including a kernel on using Automated Feature Engineering with Featuretools for this problem (with slightly higher leaderboard score).\nThis is a supervised multi-class classification machine learning problem:  Supervised: provided with the labels for the training data Multi-class classification: Labels are discrete values with 4 classes  ObjectiveThe objective is to predict poverty on a household level.\nThe Target values represent poverty levels as follows: 1 = extreme poverty  2 = moderate poverty  3 = vulnerable households  4 = non vulnerable households   The explanations for all 143 columns can be found in the competition documentation, but a few to note are below:  Id: a unique identifier for each individual, this should not be a feature that we use!\nMetricUltimately we want to build a machine learning model that can predict the integer poverty level of a household.", "sum_nltk_words": 229, "sum_nltk_runtime": 0.014, "sum_t5": "the goal of the Costa Rican Household Poverty Level Prediction contest is to develop a machine learning model that can predict the poverty level of households. the notebook is meant to be used as a teaching tool to give you a solid foundation to build on for future machine learning projects. the notebook is meant to be used as a teaching tool to give you a solid foundation to build on for future machine learning projects. the winner will be announced on november 15.", "sum_t5_words": 84, "sum_t5_runtime": 6.509, "runtime": 0.009, "nltk_category": "Economics", "nltk_category_score": 0.2932034730911255, "nltk_category_runtime": 19.672, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9918347597122192, "nltk_subcategory_runtime": 30.71, "category": "Economics", "category_score": 0.2932034730911255, "subcategory": "Machine Learning", "subcategory_score": 0.9918347597122192, "runtime_cat": 50.381, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.8", "language_code": "en", "language_score": "0.9999959770722386", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "costa rican household poverty level predictionwelcome another kaggle challenge objective costa rican household poverty level prediction contest develop machine learning model predict poverty level household using individual household characteristic data science good project offer opportunity put skill towards task beneficial society getting people click ad notebook walk complete machine learning solution first get introduced problem perform thorough exploratory data analysis dataset work feature engineering try multiple machine learning model select model work optimize model finally inspect output model draw conclusion __while notebook may get u top leaderboard meant used teaching tool give solid foundation build future machine learning project kaggle project teach u lot machine learning several strategy used get top leaderboard best practice well stick building good although quite first place machine learning solution kaggle project competition think best described machine learning education disguised contest looking followup work additional work including kernel using automated feature engineering featuretools problem slightly higher leaderboard score enjoy writing style explanation write towards data science problem data explanationthe data competition provided two file traincsv testcsv training set 9557 row 143 column testing set 23856 row 142 column row represents one individual column feature either unique individual household individual training set one additional column target represents poverty level 14 scale label competition value 1 extreme poverty supervised multiclass classification machine learning problem supervised provided label training data multiclass classification label discrete value 4 class objectivethe objective predict poverty household level given data individual level individual unique feature also information household order create dataset task well perform aggregation individual data household moreover make prediction every individual test set head household used scoring mean want predict poverty household basis important note member household label training data error individual household different label case told use label head household identified row parentesco1 10 cover correct notebook info take look competition main discussion target value represent poverty level follows 1 extreme poverty 2 moderate poverty 3 vulnerable household 4 non vulnerable household explanation 143 column found competition documentation note id unique identifier individual feature use idhogar unique identifier household variable feature used group individual household individual household identifier parentesco1 indicates person head household target label equal member household make model well train household basis label household poverty level head household raw data contains mix household individual characteristic individual data find way aggregate household individual belong household head household mean unfortunately cant use data training issue data completely typical realworld data hence problem great preparation datasets youll encounter data science job metricultimately want build machine learning model predict integer poverty level household prediction assessed macro f1 score may familiar standard f1 score binary classification problem harmonic mean precision recall f121recall1precision2precisionrecallprecisionrecallf121recall1precision2precisionrecallprecisionrecall mutliclass problem average f1 score class macro f1 score average f1 score class without taking account label imbalance macro f1f1 class 1f1 class 2f1 class 3f1 class 44macro f1f1 class 1f1 class 2f1 class 3f1 class 44 word number occurrence label figure calculation using macro using weighted score information difference look scikitlearn documention f1 score stack exchange question answer want ass performance use code sklearnmetrics import f1_score f1_scorey_true y_predicted average macro problem label imbalanced make little strange use macro averaging evaluation metric thats decision made organizer something change work want aware label imbalance choose metric accordingly roadmapthe end objective machine learning model predict poverty level household however get carried away modeling important understand problem data also want evaluate numerous model choosing one best building model want investigate prediction roadmap therefore follows understand problem almost already exploratory data analysis feature engineering create dataset machine learning compare several baseline machine learning model try complex machine learning model optimize selected model investigate model prediction context problem draw conclusion lay next step step laid iterative meaning go one time might go back earlier step revisit decision general data science nonlinear pracice constantly evaluating past decision making improvement particular feature engineering modeling optimization step often repeat never know got right first time getting startedwe pretty good grasp problem well move exploratory data analysis eda feature engineering eda well examine interesting anomaly trend correlation pattern used feature engineering modeling well make sure investigate data quantitatively statistic visually figure good grasp data potentially useful relationship feature engineering important part machine learning pipeline establish baseline model wont get u top leaderboard provide strong foundation build info mind dont worry havent got detail let get started", "tags_descriptive": ["Classification", "Exploratory Data Analysis"]}