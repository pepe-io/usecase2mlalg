{"title": "The Effect of Word Embeddings on Bias", "description": "The Quora Insincere Questions Classification spawned several great kernels for more background on word embeddings:  How to: Preprocessing when using embeddings A look at different embeddings.!  In this kernel I'm going to explore the influence of different word embeddings on unintended bias. First, I'm going to benchmark popular word embeddings with the Simple LSTM courtesy of thousandvoices. If you want to skip ahead to the results, click the link for word embeddings comparison. After that I'll introduce operations we can perform on word embeddings and I'll construct a bunch of combinations to benchmark. I'll cover concatenating embeddings and constructing meta embeddings from several different vector spaces. Skip to complete embeddings comparison for the final results. If this helps your model or if you have any ideas for other combinations leave a comment and upvote! I have since added a proper section on BERT embeddings since my initial test scored much lower than expected.    Contents  Without Pretrained Embeddings Fasttext Embeddings GloVe Embeddings Concept Numberbatch Embeddings BERT Embeddings Word Embeddings Comparison Concatenating Fasttext+GloVe Embeddings Weighted Predictions by Bias Score Constructing Meta-Embeddings GloVe+Fasttext Meta-Embeddings Weighted Meta-Embeddings Complete Embeddings Comparison", "link": "https://www.kaggle.com/nholloway/the-effect-of-word-embeddings-on-bias", "tags": ["NLP", "Exploratory Data Analysis", "Feature Engineering"], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "sklearn", "vocabulary", "spacy", "mxnet"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-05-29 20:26:13", "date_scraped": "2020-12-13 12:22:44", "words": 192, "sentences": 8, "runtime": 0.002, "description_category": "Wholesale & Retail", "description_category_score": 0.1829848736524582, "description_category_runtime": 20.245, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.4774015247821808, "description_subcategory_runtime": 33.135, "category": "Wholesale & Retail", "category_score": 0.1829848736524582, "subcategory": "Machine Learning", "subcategory_score": 0.4774015247821808, "runtime_cat": 53.38, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.735", "language_code": "en", "language_score": "0.9999973916840262", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "quora insincere question classification spawned several great kernel background word embeddings preprocessing using embeddings look different embeddings kernel im going explore influence different word embeddings unintended bias first im going benchmark popular word embeddings simple lstm courtesy thousandvoices want skip ahead result click link word embeddings comparison ill introduce operation perform word embeddings ill construct bunch combination benchmark ill cover concatenating embeddings constructing meta embeddings several different vector space skip complete embeddings comparison final result help model idea combination leave comment upvote since added proper section bert embeddings since initial test scored much lower expected content without pretrained embeddings fasttext embeddings glove embeddings concept numberbatch embeddings bert embeddings word embeddings comparison concatenating fasttextglove embeddings weighted prediction bias score constructing metaembeddings glovefasttext metaembeddings weighted metaembeddings complete embeddings comparison", "tags_descriptive": ["Natural Language Processing (NLP)", "Exploratory Data Analysis", "Feature Engineering"]}