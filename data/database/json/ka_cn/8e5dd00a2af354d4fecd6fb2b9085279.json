{"title": "How to: Preprocessing when using embeddings", "description": "In this kernel I want to illustrate how I do come up with meaningful preprocessing when building deep learning NLP models. I start with two golden rules:  Don't use standard preprocessing steps like stemming or stopword removal when you have pre-trained embeddings   Some of you might used standard preprocessing steps when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc.  The reason is simple: You loose valuable information, which would help your NN to figure things out.  Get your vocabulary as close to the embeddings as possible  I will focus in this notebook, how to achieve that. For an example I take the GoogleNews pretrained embeddings, there is no deeper reason for this choice.", "link": "https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings", "tags": ["DL", "NLP"], "kind": ["Project", "(Notebook)"], "ml_libs": ["gensim", "vocabulary"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-11-13 16:44:27", "date_scraped": "2020-12-13 16:20:42", "words": 125, "sentences": 6, "runtime": 0.0, "description_category": "Media & Publishing", "description_category_score": 0.0632021352648735, "description_category_runtime": 11.621, "description_subcategory": "Preventative and Reactive", "description_subcategory_score": 0.430919885635376, "description_subcategory_runtime": 17.965, "category": "Media & Publishing", "category_score": 0.0632021352648735, "subcategory": "Preventative and Reactive", "subcategory_score": 0.430919885635376, "runtime_cat": 29.587, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.801", "language_code": "en", "language_score": "0.9999979796821828", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "kernel want illustrate come meaningful preprocessing building deep learning nlp model start two golden rule dont use standard preprocessing step like stemming stopword removal pretrained embeddings might used standard preprocessing step word count based feature extraction eg tfidf removing stopwords stemming etc reason simple loose valuable information would help nn figure thing get vocabulary close embeddings possible focus notebook achieve example take googlenews pretrained embeddings deeper reason choice", "tags_descriptive": ["Deep Learning (DL)", "Natural Language Processing (NLP)"]}