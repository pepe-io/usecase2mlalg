{"title": "Get Started with Your Questions-EDA+ Model NN", "description": "Competition DetailsComputers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences. Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well\u2026yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong. Unfortunately, it\u2019s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That\u2019s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects. In this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get! Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.", "link": "https://www.kaggle.com/phoenix9032/get-started-with-your-questions-eda-model-nn", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "nltk", "theano", "tensorflow", "sklearn", "spacy"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-11-26 13:49:44", "date_scraped": "2020-12-12 20:20:08", "words": 285, "sentences": 16, "sum_nltk": "But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.\nHumans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well\u2026yet..\nQuestions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem.\nUnfortunately, it\u2019s hard to build better subjective question-answering algorithms because of a lack of data and predictive models.\nThat\u2019s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.\nIn this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering.\nThe question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion.\nOur raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts.\nAs such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task.\nDemonstrating these subjective labels can be predicted reliably can shine a new light on this research area.", "sum_nltk_words": 191, "sum_nltk_runtime": 0.003, "sum_t5": "humans are better at answering questions about opinions, recommendations, or personal experiences. questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. results from this competition will inform the way future intelligent Q&A systems will get built. a new competition is being run by the crowdsource team at google research. a winner will be announced on november 20. if you win, you'll be entered into a competition to see", "sum_t5_words": 79, "sum_t5_runtime": 6.532, "runtime": 0.0, "nltk_category": "Utilities", "nltk_category_score": 0.6422897577285767, "nltk_category_runtime": 18.643, "nltk_subcategory": "Quality", "nltk_subcategory_score": 0.7917901873588562, "nltk_subcategory_runtime": 29.823, "category": "Utilities", "category_score": 0.6422897577285767, "subcategory": "Quality", "subcategory_score": 0.7917901873588562, "runtime_cat": 48.466, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.735", "language_code": "en", "language_score": "0.9999966807011802", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "competition detailscomputers really good answering question single verifiable answer human often still better answering question opinion recommendation personal experience human better addressing subjective question require deeper multidimensional understanding context something computer arent trained wellyet question take many form multisentence elaboration others may simple curiosity fully developed problem multiple intent seek advice opinion may helpful others interesting simple right wrong unfortunately hard build better subjective questionanswering algorithm lack data predictive model thats crowdsource team google research group dedicated advancing nlp type ml science via crowdsourcing collected data number quality scoring aspect competition youre challenged use new dataset build predictive algorithm different subjective aspect questionanswering questionanswer pair gathered nearly 70 different website commonsense fashion raters received minimal guidance training relied largely subjective interpretation prompt prompt crafted intuitive fashion raters could simply use commonsense complete task lessening dependency complicated opaque rating guideline hope increase reuse value data set see get demonstrating subjective label predicted reliably shine new light research area result competition inform way future intelligent qa system get built hopefully contributing becoming humanlike", "tags_descriptive": []}