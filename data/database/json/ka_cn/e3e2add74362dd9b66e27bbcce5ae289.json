{"title": "Amazon_Catboost_SHAP_hyperopt_90%AUC_silvermedal", "description": "Catboost & Hyperopt : Amazon employees dataset Information from Kaggle When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. Given data about current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access.     Catboost:  Yandex, the developers of Catboost, claim that default Catboost provides ~20% logloss improvement over LightGMB & XGBoost. Tuning further improves performance of the model. I will be testing these claims. Catboost uses gradient boosted trees. Great for working on catgorical data and mixed data (with both categorical and numerical features) Data is quantized into bins. The algorithm decides bin 'borders'(We can set our own values too). This quantization supports faster integration into parallel processing workflows.  Symmetric gradient boosted trees are built, each subsequent tree improves the performance of the previous set of trees.  Categorical preprocessing steps like One-Hot-Encoding, text preprocessing steps like tokenization, Bag of Words models can be performed within the Catboost algorithm (No need for additional preprocessing.)       RESULT: One of the columns had duplicated information. After removing this column - the default algorithm gave the best loss publicised by Yandex (~0.137). A kaggle submission showed 90% AUC score. Hyperopt tuning did not improve scores.  Yandex's claims were proven. It had the best loss among the boosting models as shown in table below.       Model Logloss from default   Catboost 0.13516505504697254   Xgboost 0.1554555542790197   LightGBM 0.16383632381872779  Table of Contents Imports & Read in file Explore data Preprocessing Baseline Model Test set performance Hyperparameter tuning Model validation Other Boosting Algorithms    Description of Features:    Label Description   ACTION ACTION is 1 if the resource was approved, 0 if the resource was not   RESOURCE An ID for each resource   EMPLOYEE ID The EMPLOYEE ID of the manager of the current EMPLOYEE ID record; an employee may have only one manager at a time   ROLE_ROLLUP_1 Company role grouping category id 1 (e.g. US Engineering)   ROLE_ROLLUP_2 Company role grouping category id 2 (e.g. US Retail)   ROLE_DEPTNAME Company role department description (e.g. Retail)   ROLE_TITLE Company role business title description (e.g. Senior Engineering Retail Manager)   ROLE_FAMILY_DESC Company role family extended description (e.g. Retail Manager, Software Engineering)   ROLE_FAMILY Company role family description (e.g. Retail Manager)   ROLE_CODE Company role code; this code is unique to each role (e.g. Manager)", "link": "https://www.kaggle.com/orpitadas/amazon-catboost-shap-hyperopt-90-auc-silvermedal", "tags": ["Gradient Boosting", "Classification"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "catboost", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-03-08 23:54:01", "date_scraped": "2020-12-12 16:38:00", "words": 455, "sentences": 25, "sum_nltk": "Catboost & Hyperopt : Amazon employees dataset Information from Kaggle When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role.\nGiven data about current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company.\nCatboost:  Yandex, the developers of Catboost, claim that default Catboost provides ~20% logloss improvement over LightGMB & XGBoost.\nTuning further improves performance of the model.\nSymmetric gradient boosted trees are built, each subsequent tree improves the performance of the previous set of trees.\nCategorical preprocessing steps like One-Hot-Encoding, text preprocessing steps like tokenization, Bag of Words models can be performed within the Catboost algorithm (No need for additional preprocessing.)       RESULT: One of the columns had duplicated information.\nAfter removing this column - the default algorithm gave the best loss publicised by Yandex (~0.137).", "sum_nltk_words": 152, "sum_nltk_runtime": 0.005, "sum_t5": "default Catboost provides 20% logloss improvement over LightGMB & XGBoost. Yandex claim that tuning further improves performance of the model. kaggle submission showed 90% AUC score. hyperopt tuning did not improve scores. a kaggle submission showed 90% AUC.. a kaggle submission showed 90% AUC.. a kaggle submission showed 90% AUC.", "sum_t5_words": 50, "sum_t5_runtime": 6.554, "runtime": 0.003, "nltk_category": "Finance", "nltk_category_score": 0.30434176325798035, "nltk_category_runtime": 15.529, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9394798278808594, "nltk_subcategory_runtime": 24.453, "category": "Finance", "category_score": 0.30434176325798035, "subcategory": "Machine Learning", "subcategory_score": 0.9394798278808594, "runtime_cat": 39.982, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.603", "language_code": "en", "language_score": "0.9999975941481303", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "catboost hyperopt amazon employee dataset information kaggle employee company start work first need obtain computer access necessary fulfill role given data current employee provisioned access model built automatically determine access privilege employee enter leave role within company autoaccess model seek minimize human involvement required grant revoke employee access catboost yandex developer catboost claim default catboost provides 20 logloss improvement lightgmb xgboost tuning improves performance model testing claim catboost us gradient boosted tree great working catgorical data mixed data categorical numerical feature data quantized bin algorithm decides bin borderswe set value quantization support faster integration parallel processing workflow symmetric gradient boosted tree built subsequent tree improves performance previous set tree categorical preprocessing step like onehotencoding text preprocessing step like tokenization bag word model performed within catboost algorithm need additional preprocessing result one column duplicated information removing column default algorithm gave best loss publicised yandex 0137 kaggle submission showed 90 auc score hyperopt tuning improve score yandexs claim proven best loss among boosting model shown table model logloss default catboost 013516505504697254 xgboost 01554555542790197 lightgbm 016383632381872779 table content import read file explore data preprocessing baseline model test set performance hyperparameter tuning model validation boosting algorithm description feature label description action action 1 resource approved 0 resource resource id resource employee id employee id manager current employee id record employee may one manager time role_rollup_1 company role grouping category id 1 eg u engineering role_rollup_2 company role grouping category id 2 eg u retail role_deptname company role department description eg retail role_title company role business title description eg senior engineering retail manager role_family_desc company role family extended description eg retail manager software engineering role_family company role family description eg retail manager role_code company role code code unique role eg manager", "tags_descriptive": ["Gradient Boosting", "Classification"]}