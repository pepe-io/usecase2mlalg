{"title": "Visualizing Kannada MNIST with PCA", "description": "Getting started with Dimensionality Reduction Techniques in PythonA 3 part serieson Dimensionality reduction techniques using the Kannada MNIST dataset  Key Objectives: In this series of notebooks, we shall study about three Dimensionality reduction techniques using the Kannada MNIST dataset. The techniques are PCA, t-SNE and UMAP. Part 1: Visualizing Kannada MNIST with PCA Part 2: Visualizing Kannada MNIST with t-SNE Part 3: Visualizing Kannada MNIST with UMAP  What is Dimensionality Reduction  A lot of Machine Learning problems consists of hundreds to thousands of features. having such a large number of features poses certain problems mainly :  Slows down the training process It becomes hard to find a good solution   This problem is also sometimes termed as The Curse of Dimensionality and Dimensionality Reduction or Dimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables[1]. In other words, the goal is to take something that is very high dimensional and get it down to something that is easier to work with, without losing much of the information. Importance of Dimensionality Reduction : Getting down to two or three features can help us visualize our data which is an important part of data analysis Often a lot of dimensionality in the data is redundant and we can get rid of that that can be sueful for the machine learning process. Reducing the dimensionality can also help us in visualising the data easily.  For instance, the famous MNIST dataset is 784 dimensional when we unfold those digits into long vectors and we shouldn't really need 784 dimensions to describe a datapoint in this dataset. There should be some compact representation of this dataset and we should still be able to get some meaninful result.  Main Approaches for Dimensionality Reduction  There are two main approaches to reducing dimensionality: Projection and Manifold Learning.  Projection : This technique deals with projecting every data point which is in high dimension,  onto a subspace suitable lower-dimensional space in a way which approximately preserves the distances between the points[2]. For instance the figure below, the points in 3D are projected onto a 2D plane. This is a lower-dimensional (2D) subspace of the high-dimensional (3D) space and the axes correspond to new features z1 and z2 (the coordinates of the projections on the plane).   Source : [Beginners Guide To Learn Dimension Reduction Techniques(https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/) Keep in mind that projection may not always be the best method to achieve dimensional reduction.  Manifold Learning : Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.   Jake VanderPlas explains Manifold Learning in a very intuitive way in his book :Python Data Science handbook and here is an excerpt from the book itself; \"manifold learning\u2014a class of unsupervised estimators that seeks to describe datasets as low-dimensional manifolds embedded in high-dimensional spaces. When you think of a manifold, I'd suggest imagining a sheet of paper: this is a two-dimensional object that lives in our familiar three-dimensional world, and can be bent or rolled in that two dimensions. In the parlance of manifold learning, we can think of this sheet as a two-dimensional manifold embedded in three-dimensional space. Rotating, re-orienting, or stretching the piece of paper in three-dimensional space doesn't change the flat geometry of the paper: such operations are akin to linear embeddings. If you bend, curl, or crumple the paper, it is still a two-dimensional manifold, but the embedding into the three-dimensional space is no longer linear. Manifold learning algorithms would seek to learn about the fundamental two-dimensional nature of the paper, even as it is contorted to fill the three-dimensional space.\"  Part1: Principal Component Analysis(PCA) in Python  PCA is a very common technique for dimensionality reduction. The idea behind it is very simple:  Identify a Hyperplane that lies closest to the data Project the data onto the hyperplane.   Projecting 2D-data to a line (PCA However, it is important to choose the right hyperplane so that when the data is projected onto it, it the maximum amount of variation or information about how the original data is distributed. In other words, the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. Principal ComponentsThe axis that explains the maximum amount of variance int he training set is called the principal components. The axis othogonal to this axis is called the second principal component. Thus in 2D, there will be 2 principal components. However, for a higher dimensions, PCA would find a third component orthogonal to the other two components and so on.  Source : A Layman\u2019s Introduction to Principal Components Implementing PCA using Scikit LearnScikit-Learn\u2019s PCA class implements PCA using SVD decomposition. Let's apply PCA on Kannada MNIST data set for visualization. 1. PCA for VisualisationAn effective way to visualize high-dimensional data is to represent each data object by a two-dimensional point in such a way that similar objects are represented by nearby points, and that dissimilar objects are represented by distant points. The resulting two-dimensional points can be visualized in a scatter plot. This leads to a map of the data that reveals the underlying structure of the objects, such as the presence of clusters.Let's see how we can use PCA to do that.", "link": "https://www.kaggle.com/parulpandey/part1-visualizing-kannada-mnist-with-pca", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-02-27 12:17:33", "date_scraped": "2020-12-13 12:26:21", "words": 909, "sentences": 35, "sum_nltk": "Part 1: Visualizing Kannada MNIST with PCA Part 2: Visualizing Kannada MNIST with t-SNE Part 3: Visualizing Kannada MNIST with UMAP  What is Dimensionality Reduction  A lot of Machine Learning problems consists of hundreds to thousands of features.\nImportance of Dimensionality Reduction : Getting down to two or three features can help us visualize our data which is an important part of data analysis Often a lot of dimensionality in the data is redundant and we can get rid of that that can be sueful for the machine learning process.\nProjection : This technique deals with projecting every data point which is in high dimension,  onto a subspace suitable lower-dimensional space in a way which approximately preserves the distances between the points[2].\nJake VanderPlas explains Manifold Learning in a very intuitive way in his book :Python Data Science handbook and here is an excerpt from the book itself; \"manifold learning\u2014a class of unsupervised estimators that seeks to describe datasets as low-dimensional manifolds embedded in high-dimensional spaces.\nManifold learning algorithms would seek to learn about the fundamental two-dimensional nature of the paper, even as it is contorted to fill the three-dimensional space.\"  Part1: Principal Component Analysis(PCA) in Python  PCA is a very common technique for dimensionality reduction.", "sum_nltk_words": 207, "sum_nltk_runtime": 0.01, "sum_t5": "a 3 part serieson three Dimensionality reduction techniques using the Kannada MNIST dataset. the techniques are PCA, t-SNE and UMAP. the goal is to reduce the number of random variables under consideration. reducing dimensionality can help us visualize our data which is an important part of data analysis. the paper will be available in november and will be available in november. back to mail online home. back to the page you came from.", "sum_t5_words": 73, "sum_t5_runtime": 6.649, "runtime": 0.011, "nltk_category": "Media & Publishing", "nltk_category_score": 0.4819682240486145, "nltk_category_runtime": 19.984, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.7583740949630737, "nltk_subcategory_runtime": 31.969, "category": "Media & Publishing", "category_score": 0.4819682240486145, "subcategory": "Machine Learning", "subcategory_score": 0.7583740949630737, "runtime_cat": 51.954, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.695", "language_code": "en", "language_score": "0.99999655338615", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "getting started dimensionality reduction technique pythona 3 part serieson dimensionality reduction technique using kannada mnist dataset key objective series notebook shall study three dimensionality reduction technique using kannada mnist dataset technique pca tsne umap part 1 visualizing kannada mnist pca part 2 visualizing kannada mnist tsne part 3 visualizing kannada mnist umap dimensionality reduction lot machine learning problem consists hundred thousand feature large number feature pose certain problem mainly slows training process becomes hard find good solution problem also sometimes termed curse dimensionality dimensionality reduction dimension reduction process reducing number random variable consideration obtaining set principal variables1 word goal take something high dimensional get something easier work without losing much information importance dimensionality reduction getting two three feature help u visualize data important part data analysis often lot dimensionality data redundant get rid sueful machine learning process reducing dimensionality also help u visualising data easily instance famous mnist dataset 784 dimensional unfold digit long vector shouldnt really need 784 dimension describe datapoint dataset compact representation dataset still able get meaninful result main approach dimensionality reduction two main approach reducing dimensionality projection manifold learning projection technique deal projecting every data point high dimension onto subspace suitable lowerdimensional space way approximately preserve distance points2 instance figure point 3d projected onto 2d plane lowerdimensional 2d subspace highdimensional 3d space ax correspond new feature z1 z2 coordinate projection plane source beginner guide learn dimension reduction techniqueshttpswwwanalyticsvidhyacomblog201507dimensionreductionmethods keep mind projection may always best method achieve dimensional reduction manifold learning manifold learning approach nonlinear dimensionality reduction algorithm task based idea dimensionality many data set artificially high jake vanderplas explains manifold learning intuitive way book python data science handbook excerpt book manifold learninga class unsupervised estimator seek describe datasets lowdimensional manifold embedded highdimensional space think manifold id suggest imagining sheet paper twodimensional object life familiar threedimensional world bent rolled two dimension parlance manifold learning think sheet twodimensional manifold embedded threedimensional space rotating reorienting stretching piece paper threedimensional space doesnt change flat geometry paper operation akin linear embeddings bend curl crumple paper still twodimensional manifold embedding threedimensional space longer linear manifold learning algorithm would seek learn fundamental twodimensional nature paper even contorted fill threedimensional space part1 principal component analysispca python pca common technique dimensionality reduction idea behind simple identify hyperplane lie closest data project data onto hyperplane projecting 2ddata line pca however important choose right hyperplane data projected onto maximum amount variation information original data distributed word axis minimizes mean squared distance original dataset projection onto axis principal componentsthe axis explains maximum amount variance int training set called principal component axis othogonal axis called second principal component thus 2d 2 principal component however higher dimension pca would find third component orthogonal two component source layman introduction principal component implementing pca using scikit learnscikitlearns pca class implement pca using svd decomposition let apply pca kannada mnist data set visualization 1 pca visualisationan effective way visualize highdimensional data represent data object twodimensional point way similar object represented nearby point dissimilar object represented distant point resulting twodimensional point visualized scatter plot lead map data reveals underlying structure object presence clusterslets see use pca", "tags_descriptive": []}