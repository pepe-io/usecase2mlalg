{"title": "2.15 Loss - Simple Split Trick", "description": "In this kernel we will use the dataset in a special way to get better performance and lower loss without any data augmentation.  I have read in one of the forums that the dataset is actually collected by merging 2 datasets together, the first one contains 7000+ samples with 8 features (4 keypoints) for each image, the second one contains 2000+ images that actually belongs to the first dataset but with 30 features (15 keypoints).  Now the question is, how to handle this sneaky dataset to get better results and lower loss (without data augmentation).  you can try 3 approaches for this one:  Drop any sample that doesn't contain the full 15 key points, in this approach you simply ignore the first dataset, you will get an even smaller dataset with 2140 samples, eventually after training and submitting, you will get almost 3.0 loss.  Fill any missing point with the previous available one, in this approach you will end up with 7000+ samples, but most of the features are filled and not accurate, surprisingly this approach will get almost 2.4 loss which is better than the first one, a reasonable explanation for this result is providing the model with 5000 more samples with 4 accurate keypoints and 11 inaccurate filled keypoints lower the loss a bit.  Enhance the 1st approach by using the ignored dataset (1st dataset) to train a separate model to predict only 4 key points. Why would we do that?, Obviously this model (four-keypoints model) will produce more accurate predictions for those specific key points as the training set contains 7000 samples with accurate labels rather than only 2000 samples (notice that those 4 keypoints are just subset of the 15 keypoints). In this case, we have 2 models, fifteen-keypoints model which produces 30-dim vector for each sample, and four_keypoints model 8-dim vector for each sample (which produces more accurate values for certain four key points), then you should replace the predictions of the four-keypoints model with the corresponding predictions of the fifteen-keypoints model. This approach will lower loss to almost 2.1, this simply because we got more accurate predictions for 8 features.    I think with alittle bit of data augmentation after splitting the dataset you can get more decent loss, also i encourage you to take a look at Ole Gee's solution which got 1.28 loss and achieved the 1st place.  The code is simple, concise and fully-commented. Feel free to ask for help or more info or more explanation in the comments, i will be more than happy to help.  Finally if this kernel helps you somehow, kindly don't forget to leave a little upvote up there.  Hope you enjoy.", "link": "https://www.kaggle.com/phylake1337/2-15-loss-simple-split-trick", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-03-14 18:04:42", "date_scraped": "2020-12-12 19:34:30", "words": 457, "sentences": 14, "sum_nltk": "In this kernel we will use the dataset in a special way to get better performance and lower loss without any data augmentation.\nNow the question is, how to handle this sneaky dataset to get better results and lower loss (without data augmentation).\nyou can try 3 approaches for this one:  Drop any sample that doesn't contain the full 15 key points, in this approach you simply ignore the first dataset, you will get an even smaller dataset with 2140 samples, eventually after training and submitting, you will get almost 3.0 loss.\nFill any missing point with the previous available one, in this approach you will end up with 7000+ samples, but most of the features are filled and not accurate, surprisingly this approach will get almost 2.4 loss which is better than the first one, a reasonable explanation for this result is providing the model with 5000 more samples with 4 accurate keypoints and 11 inaccurate filled keypoints lower the loss a bit.\nEnhance the 1st approach by using the ignored dataset (1st dataset) to train a separate model to predict only 4 key points.\nThis approach will lower loss to almost 2.1, this simply because we got more accurate predictions for 8 features.", "sum_nltk_words": 201, "sum_nltk_runtime": 0.004, "sum_t5": "a kernel will use a dataset to get better performance and lower loss. a kernel will use the dataset in a special way to get better results. a kernel will use the dataset in a special way to get better results. a kernel will use the dataset in a special way to get better performance. a kernel will use the dataset in a special way to get better results. a kernel will use the dataset in a special way to get better results.", "sum_t5_words": 83, "sum_t5_runtime": 6.638, "runtime": 0.004, "nltk_category": "Utilities", "nltk_category_score": 0.23365801572799683, "nltk_category_runtime": 16.932, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8186349272727966, "nltk_subcategory_runtime": 26.828, "category": "Utilities", "category_score": 0.23365801572799683, "subcategory": "Machine Learning", "subcategory_score": 0.8186349272727966, "runtime_cat": 43.76, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.613", "language_code": "en", "language_score": "0.9999975314822868", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "kernel use dataset special way get better performance lower loss without data augmentation read one forum dataset actually collected merging 2 datasets together first one contains 7000 sample 8 feature 4 keypoints image second one contains 2000 image actually belongs first dataset 30 feature 15 keypoints question handle sneaky dataset get better result lower loss without data augmentation try 3 approach one drop sample doesnt contain full 15 key point approach simply ignore first dataset get even smaller dataset 2140 sample eventually training submitting get almost 30 loss fill missing point previous available one approach end 7000 sample feature filled accurate surprisingly approach get almost 24 loss better first one reasonable explanation result providing model 5000 sample 4 accurate keypoints 11 inaccurate filled keypoints lower loss bit enhance 1st approach using ignored dataset 1st dataset train separate model predict 4 key point would obviously model fourkeypoints model produce accurate prediction specific key point training set contains 7000 sample accurate label rather 2000 sample notice 4 keypoints subset 15 keypoints case 2 model fifteenkeypoints model produce 30dim vector sample four_keypoints model 8dim vector sample produce accurate value certain four key point replace prediction fourkeypoints model corresponding prediction fifteenkeypoints model approach lower loss almost 21 simply got accurate prediction 8 feature think alittle bit data augmentation splitting dataset get decent loss also encourage take look ole gee solution got 128 loss achieved 1st place code simple concise fullycommented feel free ask help info explanation comment happy help finally kernel help somehow kindly dont forget leave little upvote hope enjoy", "tags_descriptive": []}