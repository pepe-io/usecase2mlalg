{"title": "OSIC AutoEncoder training", "description": "1. OSIC AutoEncoder trainingThis notebooks demonstrates how to train a convolutional AutoEncoder to learn latent features from the 3D CT scans dataset. One of the main applications of AutoEncoders is dimensionality reduction. We will use them for that: reducing 3D images (preprocessed to 1 x 40 x 256 x 256 tensors) to vectors (with 10 dimensions).  Once we have the trained model, the idea is to apply it to extract these latent features and combine them with the OSIC tabular data. My first experiments had a less strangled bottleneck (started with 96 x 2 x 20 x 20), which was already a reduction of over 34:1 (the inputs are 3D images of 1 x 40 x 256 x 256). The AutoEncoder output was great, easy to see. However, using latent features of 96 x 2 x 20 x 20 meant that, in the tabular model, I had to combine 76,800 features (flattened) with the 9 tabular features. In order to have a better balance between tabular and latent features, I decide to strangle the bottleneck further, squeezing the 3D images to 10 features (already flatenned in the AutoEncoder model). As you can see below, the model learns as the loss keeps going down. However, the output of the AutoEncoder is not as visible as with the less strangled bottleneck.", "link": "https://www.kaggle.com/carlossouza/osic-autoencoder-training", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["skimage"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-07-28 20:24:28", "date_scraped": "2020-12-13 14:44:51", "words": 220, "sentences": 11, "sum_nltk": "1. OSIC AutoEncoder trainingThis notebooks demonstrates how to train a convolutional AutoEncoder to learn latent features from the 3D CT scans dataset.\nOne of the main applications of AutoEncoders is dimensionality reduction.\nOnce we have the trained model, the idea is to apply it to extract these latent features and combine them with the OSIC tabular data.\nMy first experiments had a less strangled bottleneck (started with 96 x 2 x 20 x 20), which was already a reduction of over 34:1 (the inputs are 3D images of 1 x 40 x 256 x 256).\nThe AutoEncoder output was great, easy to see.\nHowever, using latent features of 96 x 2 x 20 x 20 meant that, in the tabular model, I had to combine 76,800 features (flattened) with the 9 tabular features.\nIn order to have a better balance between tabular and latent features, I decide to strangle the bottleneck further, squeezing the 3D images to 10 features (already flatenned in the AutoEncoder model).\nAs you can see below, the model learns as the loss keeps going down.\nHowever, the output of the AutoEncoder is not as visible as with the less strangled bottleneck.", "sum_nltk_words": 187, "sum_nltk_runtime": 0.003, "sum_t5": "a convolutional autoencoder is trained to learn latent features from 3D CT scans. the model learns as the loss keeps going down. the output of the autoencoder is not as visible as with the less strangled bottleneck. the autoencoder is now ready to use in a variety of applications. a symlink based autoencoder is available for free download. a symlink based autoencoder is available for free", "sum_t5_words": 66, "sum_t5_runtime": 6.206, "runtime": 0.005, "nltk_category": "Healthcare", "nltk_category_score": 0.46995705366134644, "nltk_category_runtime": 18.162, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8876336216926575, "nltk_subcategory_runtime": 29.271, "category": "Healthcare", "category_score": 0.46995705366134644, "subcategory": "Machine Learning", "subcategory_score": 0.8876336216926575, "runtime_cat": 47.432, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.728", "language_code": "en", "language_score": "0.9999971086387076", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "1 osic autoencoder trainingthis notebook demonstrates train convolutional autoencoder learn latent feature 3d ct scan dataset one main application autoencoders dimensionality reduction use reducing 3d image preprocessed 1 x 40 x 256 x 256 tensor vector 10 dimension trained model idea apply extract latent feature combine osic tabular data first experiment le strangled bottleneck started 96 x 2 x 20 x 20 already reduction 341 input 3d image 1 x 40 x 256 x 256 autoencoder output great easy see however using latent feature 96 x 2 x 20 x 20 meant tabular model combine 76800 feature flattened 9 tabular feature order better balance tabular latent feature decide strangle bottleneck squeezing 3d image 10 feature already flatenned autoencoder model see model learns loss keep going however output autoencoder visible le strangled bottleneck", "tags_descriptive": []}