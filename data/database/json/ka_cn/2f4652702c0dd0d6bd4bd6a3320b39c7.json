{"title": "[PyTorch Training+Inference] EfficientNet B4", "description": "IntroductionThis notebook is forked from https://www.kaggle.com/rhtsingh/pytorch-training-inference-efficientnet-baseline by @rhtsingh - if you are kind enough to upvote my notebook, please also upvote @rhtsingh's. What have I changed?  Because no internet is allowed for submission with this competition, I've created a dataset with EfficientNet resources to enable submission. There is so much data to train on, so little time. So to get started, rather than train on the public training set, this notebook trains on the private training set. From the data documentation: \"the private training set contains only a 100k subset of the total public training set. This 100k subset contains all of the training set images associated with the landmarks in the private test set.\" Given that we're going to use the private training set here, there is no point burning CPU/GPU time training on the public training set. So I've added an option to skip, based on detecting a given id in the test set (which is in the public test set, but evidently not in the private test set). Even with a little tuning, I was still stuck at a LB score of 0.0000 with EfficientNet B0, so I moved up to B4.  Update Added seeds for random engines to try and remove non-determinism. The LB score seems to be right on the 0.0000/1 boundary.  ConclusionA fairly standard EfficientNet baseline (thanks @rhtsingh) doesn't appear to able to get a good result here. Next steps: Study past years' winning solutions and start from there!", "link": "https://www.kaggle.com/andypenrose/pytorch-training-inference-efficientnet-b4", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pytorch", "albumentations"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-08-16 12:19:04", "date_scraped": "2020-12-13 12:43:16", "words": 248, "sentences": 12, "sum_nltk": "IntroductionThis notebook is forked from https://www.kaggle.com/rhtsingh/pytorch-training-inference-efficientnet-baseline by @rhtsingh - if you are kind enough to upvote my notebook, please also upvote @rhtsingh's.\nBecause no internet is allowed for submission with this competition, I've created a dataset with EfficientNet resources to enable submission.\nThere is so much data to train on, so little time.\nSo to get started, rather than train on the public training set, this notebook trains on the private training set.\nFrom the data documentation: \"the private training set contains only a 100k subset of the total public training set.\nThis 100k subset contains all of the training set images associated with the landmarks in the private test set.\" Given that we're going to use the private training set here, there is no point burning CPU/GPU time training on the public training set.\nSo I've added an option to skip, based on detecting a given id in the test set (which is in the public test set, but evidently not in the private test set).\nEven with a little tuning, I was still stuck at a LB score of 0.0000 with EfficientNet B0, so I moved up to B4.\nConclusionA fairly standard EfficientNet baseline (thanks @rhtsingh) doesn't appear to able to get a good result here.", "sum_nltk_words": 200, "sum_nltk_runtime": 0.003, "sum_t5": "this notebook is forked from https://www.kaggle.com/rhtsingh/pytorch-training-inference-efficientnet-baseline. because no internet is allowed for submission with this competition, I've created a dataset with EfficientNet resources. this notebook trains on the private training set. i've added an option to skip, based on detecting a given id in the test set.", "sum_t5_words": 47, "sum_t5_runtime": 5.977, "runtime": 0.0, "nltk_category": "Utilities", "nltk_category_score": 0.4198085367679596, "nltk_category_runtime": 20.968, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8527674674987793, "nltk_subcategory_runtime": 33.898, "category": "Utilities", "category_score": 0.4198085367679596, "subcategory": "Machine Learning", "subcategory_score": 0.8527674674987793, "runtime_cat": 54.865, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.689", "language_code": "en", "language_score": "0.9999961402312136", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introductionthis notebook forked httpswwwkagglecomrhtsinghpytorchtraininginferenceefficientnetbaseline rhtsingh kind enough upvote notebook please also upvote rhtsinghs changed internet allowed submission competition ive created dataset efficientnet resource enable submission much data train little time get started rather train public training set notebook train private training set data documentation private training set contains 100k subset total public training set 100k subset contains training set image associated landmark private test set given going use private training set point burning cpugpu time training public training set ive added option skip based detecting given id test set public test set evidently private test set even little tuning still stuck lb score 00000 efficientnet b0 moved b4 update added seed random engine try remove nondeterminism lb score seems right 000001 boundary conclusiona fairly standard efficientnet baseline thanks rhtsingh doesnt appear able get good result next step study past year winning solution start", "tags_descriptive": []}