{"title": "1st Place Solution", "description": "I'd like to first thank DonorsChoose.org and Kaggle for providing us with an interesting, real world dataset of text based applications to play with! A little about my path: last fall I enrolled in Andrew Ng's Deep Learning course on Coursera and fell under the spell of machine learning.  After Coursera, I read a number of Python machine learning and DNN books.  But my real machine learning education has been through Kaggle.  I participated in the Porto Seguro  and Recruit Restaurant competitions, doing pretty badly but gaining a lot of knowledge through the kernels and discussions shared by other participants.  I particularly enjoy reading about the innovative top solutions posted at the end of the competitions, even competitions I didn't participate in.  Over time I have built up a bag of tricks, and some reusable ML utility code.  DonorsChoose.org was my introduction to NLP and a great dataset for me to put these tools to use in. Summary of  Models: My final ensemble of many models was blended together using the HillClimb algorithm (more about that later) and also stacked using a non-linear XBG stacker: 4 parts HillClimb, 1 part XGB.  So three levels in all.  Quick overview of model groups:  GRU-ATT and GRU-LSTM models as introduced by Peter in the GRU-ATT kernel.  Each model used a different word embedding or had their sentences reversed (a kind of data augmentation). Bi-LSTM models inspired by huiqin's Deep learning is all you need!, some with two different word embeddings in the same model.   I re-used the feature hashing huiqin introduced in my other DNN models that included categorical features. Capsule Network models. Combined Bi-GRU and Conv1D models based on Zafar's The All-in-one Model  kernel. LGB model of depth 17. XBG model of depth 7 (shallow vs LGB's deep).  Each DNN model had a different take on the data, increasing diversity.  DNN provided 54% of the blend vs 46% for gradient boosted trees.", "link": "https://www.kaggle.com/shadowwarrior/1st-place-solution", "tags": ["NLP", "Feature Engineering", "Gradient Boosting"], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "tensorflow", "vocabulary", "theano"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-04-26 01:20:47", "date_scraped": "2020-12-12 19:01:17", "words": 337, "sentences": 19, "sum_nltk": "I'd like to first thank DonorsChoose.org and Kaggle for providing us with an interesting, real world dataset of text based applications to play with!\nA little about my path: last fall I enrolled in Andrew Ng's Deep Learning course on Coursera and fell under the spell of machine learning.\nAfter Coursera, I read a number of Python machine learning and DNN books.\nBut my real machine learning education has been through Kaggle.\nQuick overview of model groups:  GRU-ATT and GRU-LSTM models as introduced by Peter in the GRU-ATT kernel.\nEach model used a different word embedding or had their sentences reversed (a kind of data augmentation).\nBi-LSTM models inspired by huiqin's Deep learning is all you need!, some with two different word embeddings in the same model.\nI re-used the feature hashing huiqin introduced in my other DNN models that included categorical features.\nCombined Bi-GRU and Conv1D models based on Zafar's The All-in-one Model  kernel.\nLGB model of depth 17.\nXBG model of depth 7 (shallow vs LGB's deep).\nEach DNN model had a different take on the data, increasing diversity.\nDNN provided 54% of the blend vs 46% for gradient boosted trees.", "sum_nltk_words": 183, "sum_nltk_runtime": 0.003, "sum_t5": "john sutter: \"donorschoose.org was my introduction to NLP and a great dataset\" he used a hill climb algorithm to blend together several models. sutter: \"i've built up a bag of tricks, and some reusable ML utility code\" he says he's grateful for the dataset and the tools. sutter: \"i'm very happy to have found a way to use it in my own work\"", "sum_t5_words": 63, "sum_t5_runtime": 6.689, "runtime": 0.0, "nltk_category": "Justice, Law and Regulations", "nltk_category_score": 0.45041176676750183, "nltk_category_runtime": 20.07, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.925062894821167, "nltk_subcategory_runtime": 32.251, "category": "Justice, Law and Regulations", "category_score": 0.45041176676750183, "subcategory": "Machine Learning", "subcategory_score": 0.925062894821167, "runtime_cat": 52.322, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.753", "language_code": "en", "language_score": "0.9999977260566194", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "id like first thank donorschooseorg kaggle providing u interesting real world dataset text based application play little path last fall enrolled andrew ng deep learning course coursera fell spell machine learning coursera read number python machine learning dnn book real machine learning education kaggle participated porto seguro recruit restaurant competition pretty badly gaining lot knowledge kernel discussion shared participant particularly enjoy reading innovative top solution posted end competition even competition didnt participate time built bag trick reusable ml utility code donorschooseorg introduction nlp great dataset put tool use summary model final ensemble many model blended together using hillclimb algorithm later also stacked using nonlinear xbg stacker 4 part hillclimb 1 part xgb three level quick overview model group gruatt grulstm model introduced peter gruatt kernel model used different word embedding sentence reversed kind data augmentation bilstm model inspired huiqins deep learning need two different word embeddings model reused feature hashing huiqin introduced dnn model included categorical feature capsule network model combined bigru conv1d model based zafars allinone model kernel lgb model depth 17 xbg model depth 7 shallow v lgb deep dnn model different take data increasing diversity dnn provided 54 blend v 46 gradient boosted tree", "tags_descriptive": ["Natural Language Processing (NLP)", "Feature Engineering", "Gradient Boosting"]}