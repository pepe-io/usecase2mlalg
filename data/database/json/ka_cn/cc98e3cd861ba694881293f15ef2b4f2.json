{"title": "LightGBM hyperparameter optimisation (LB: 0.761)", "description": "Basic end-to-end training of a LightGBM modelFeatures that are illustrated in this kernel:  data reading with memory footprint reduction a bit of feature engineering adding estimated credit length, which boosts AUC ROC by 0.015 on PLB and by 0.035 in local CV categorical feature encoding using one-hot-encoding (OHE) internal category weighting by LightGBM was tuned and no need of resampling is shown gradient-boosted decision trees using LightGBM package early stopping in LightGBM model training to avoid overtraining learning rate decay in LightGBM model training to improve convergence to the minimum hyperparameter optimisation of the model using random search in cross validation submission preparation The main goal is to provide an example of how those features can be used. High ROC AUC score is not the purpose here This kernel inherited ideas and SW solutions from other public kernels and in such cases I will post direct references to the original product, that that you can get some additional insights from the source.", "link": "https://www.kaggle.com/mlisovyi/lightgbm-hyperparameter-optimisation-lb-0-761", "tags": ["Gradient Boosting", "Classification"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pattern", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-08-24 15:03:07", "date_scraped": "2020-12-12 20:47:32", "words": 163, "sentences": 2, "runtime": 0.002, "description_category": "Real Estate, Rental & Leasing", "description_category_score": 0.30596423149108887, "description_category_runtime": 14.504, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.8059449195861816, "description_subcategory_runtime": 23.066, "category": "Real Estate, Rental & Leasing", "category_score": 0.30596423149108887, "subcategory": "Machine Learning", "subcategory_score": 0.8059449195861816, "runtime_cat": 37.572, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.808", "language_code": "en", "language_score": "0.9999966159452367", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "basic endtoend training lightgbm modelfeatures illustrated kernel data reading memory footprint reduction bit feature engineering adding estimated credit length boost auc roc 0015 plb 0035 local cv categorical feature encoding using onehotencoding ohe internal category weighting lightgbm tuned need resampling shown gradientboosted decision tree using lightgbm package early stopping lightgbm model training avoid overtraining learning rate decay lightgbm model training improve convergence minimum hyperparameter optimisation model using random search cross validation submission preparation main goal provide example feature used high roc auc score purpose kernel inherited idea sw solution public kernel case post direct reference original product get additional insight source", "tags_descriptive": ["Gradient Boosting", "Classification"]}