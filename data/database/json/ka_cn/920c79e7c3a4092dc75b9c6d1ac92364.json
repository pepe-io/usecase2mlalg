{"title": "Even more features", "description": "DescriptionThis kernel is a continuation of my previous kernels and is aimed at creating a huge number of features, which were invented in different public kernels (including mine). Please notice, that high number of available features doesn't mean that using all of them at once is a good idea :) Feature selection should be used to limit the number of features. Another important point: some of the code for feature creation is commented, because otherwise kernel hits time limit. I generated these features locally and created a dataset with them: https://www.kaggle.com/artgor/lanl-features Also I wrote another kernel to show various model interpretation and feature selection technics: https://www.kaggle.com/artgor/feature-selection-model-interpretation-and-more", "link": "https://www.kaggle.com/artgor/even-more-features", "tags": ["Regression", "Feature Engineering"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "catboost", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-05-05 15:16:42", "date_scraped": "2020-12-13 13:01:16", "words": 106, "sentences": 4, "runtime": 0.002, "description_category": "Real Estate, Rental & Leasing", "description_category_score": 0.1820497363805771, "description_category_runtime": 11.721, "description_subcategory": "Rental & Leasing", "description_subcategory_score": 0.14494818449020386, "description_subcategory_runtime": 17.885, "category": "Real Estate, Rental & Leasing", "category_score": 0.1820497363805771, "subcategory": "Rental & Leasing", "subcategory_score": 0.14494818449020386, "runtime_cat": 29.607, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.749", "language_code": "en", "language_score": "0.9999961869097729", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "descriptionthis kernel continuation previous kernel aimed creating huge number feature invented different public kernel including mine please notice high number available feature doesnt mean using good idea feature selection used limit number feature another important point code feature creation commented otherwise kernel hit time limit generated feature locally created dataset httpswwwkagglecomartgorlanlfeatures also wrote another kernel show various model interpretation feature selection technics httpswwwkagglecomartgorfeatureselectionmodelinterpretationandmore", "tags_descriptive": ["Regression", "Feature Engineering"]}