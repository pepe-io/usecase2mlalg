{"title": "BERT Base Uncased using PyTorch", "description": "Story:I don't comment much for kernels but for this kernel I will. First of all I am very thankful to: @akensert, @ajinomoto132 and @adityaecdrid who helped me a lot in finding the mistake I was making. It was due to them that I was able to find the mistake in my training code. @akensert shared a kernel with data-processing similar to mine but a different model and loss function. This kernel was written using Tensorflow. You can checkout the kernel here: https://www.kaggle.com/akensert/complete-tf2-1-mixed-precision-implementation Please upvote @akensert's kernel mentioned above! :) Thus, my plan began to replicate the same score in pytorch. Previously I was using BCEWithLogitsLoss. The TF kernel used Cross Entropy loss. This was one of the major differences. Another major difference was using the last two hidden states instead of just the last one. So, I tried and failed. I then tried again, cleaned up my code, started comparing line-by-line and failed again. After 2 days of frustration, I made a discussion post asking for help: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/141019 Then came @ajinomoto132. He mentioned he had replicated the model and gladly shared his code to help me out!!! His code can be found here: https://www.kaggle.com/ajinomoto132/starter-kernel-in-pytorch . Please upvote @ajinomoto132's kernel mentioned above! :) After a few more hours of struggle, I was able to find the mistake I was doing. It was a stupid mistake of not using .from_pretrained when using BertModel. Quite stupid I would say. Since the community helped me so much, I am releasing the fixed version of my code which is also much cleaner than the previous versions of my code. I love this community! Thank you for all the help!", "link": "https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pytorch"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-04-04 22:12:03", "date_scraped": "2020-12-13 17:53:52", "words": 274, "sentences": 19, "sum_nltk": "First of all I am very thankful to: @akensert, @ajinomoto132 and @adityaecdrid who helped me a lot in finding the mistake I was making.\nIt was due to them that I was able to find the mistake in my training code.\n@akensert shared a kernel with data-processing similar to mine but a different model and loss function.\nYou can checkout the kernel here: https://www.kaggle.com/akensert/complete-tf2-1-mixed-precision-implementation Please upvote @akensert's kernel mentioned above!\nThis was one of the major differences.\nI then tried again, cleaned up my code, started comparing line-by-line and failed again.\nAfter 2 days of frustration, I made a discussion post asking for help: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/141019 Then came @ajinomoto132.\nHe mentioned he had replicated the model and gladly shared his code to help me out!!!\nHis code can be found here: https://www.kaggle.com/ajinomoto132/starter-kernel-in-pytorch .\nPlease upvote @ajinomoto132's kernel mentioned above!\n:) After a few more hours of struggle, I was able to find the mistake I was doing.\nIt was a stupid mistake of not using .from_pretrained when using BertModel.\nSince the community helped me so much, I am releasing the fixed version of my code which is also much cleaner than the previous versions of my code.\nThank you for all the help!", "sum_nltk_words": 189, "sum_nltk_runtime": 0.003, "sum_t5": "akensert, @ajinomoto132 and @adityaecdrid helped me find the mistake. he shared a kernel with data-processing similar to mine but a different model and loss function. he is releasing a fixed version of his code which is also much cleaner than the previous versions of his code. he is releasing a kernel with a fixed score of.0.", "sum_t5_words": 56, "sum_t5_runtime": 6.138, "runtime": 0.0, "nltk_category": "Finance", "nltk_category_score": 0.7362926006317139, "nltk_category_runtime": 24.938, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9150814414024353, "nltk_subcategory_runtime": 39.688, "category": "Finance", "category_score": 0.7362926006317139, "subcategory": "Machine Learning", "subcategory_score": 0.9150814414024353, "runtime_cat": 64.626, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.774", "language_code": "en", "language_score": "0.9999963278903251", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "storyi dont comment much kernel kernel first thankful akensert ajinomoto132 adityaecdrid helped lot finding mistake making due able find mistake training code akensert shared kernel dataprocessing similar mine different model loss function kernel written using tensorflow checkout kernel httpswwwkagglecomakensertcompletetf21mixedprecisionimplementation please upvote akenserts kernel mentioned thus plan began replicate score pytorch previously using bcewithlogitsloss tf kernel used cross entropy loss one major difference another major difference using last two hidden state instead last one tried failed tried cleaned code started comparing linebyline failed 2 day frustration made discussion post asking help httpswwwkagglecomctweetsentimentextractiondiscussion141019 came ajinomoto132 mentioned replicated model gladly shared code help code found httpswwwkagglecomajinomoto132starterkernelinpytorch please upvote ajinomoto132s kernel mentioned hour struggle able find mistake stupid mistake using from_pretrained using bertmodel quite stupid would say since community helped much releasing fixed version code also much cleaner previous version code love community thank help", "tags_descriptive": []}