{"title": "Feature engineering + LighGBM with F1_macro", "description": "Do feature engineering to improve LightGBM predictionThis kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster. Several key points:  This kernel runs training on the heads of housholds only (after extracting aggregates over households). This follows the announced scoring startegy: Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored. (from the data description).  It seems to be very important to balance class frequencies. Without balancing a trained model gives ~0.39 PLB / ~0.43 local test, while adding balancing leads to ~0.42 PLB / 0.47 local test. One can do it by hand, one can achieve it by undersampling. But the simplest (and more powerful compared to undersampling) is to set class_weight='balanced' in the LightGBM model constructor in sklearn API, which will assign different weights to different classes proportional to their representation. Note that a better procedure would be to tune those weights in a CV loop instead of blindly assigning 1/n weights This kernel uses macro F1 score to early stopping in training. This is done to align with the scoring strategy. Categoricals are turned into numbers with proper mapping instead of blind label encoding.  OHE is reversed into label encoding, as it is easier to digest for a tree model. This trick would be harmful for non-tree models, so be careful. idhogar is NOT used in training. The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :) Squared features (SQBXXX and agesq) are NOT used in training. These would be useful for a linear model, but are useless for a tree-based model and only confused it (when bagging and resampling is done) There are aggregations done within households and new features are hand-crafted. Note, that there are not so many features that can be aggregated, as most are already quoted on household level. NEW: There are geographical aggregates calculated from households NEW: Models are build and evaluated in a nested CV loop. This is done to reduce fluctuations in early-stopping criterion as well as to average over several performance estimates. A voting classifier is used to average over several LightGBM models. This allows to get better predictions by bagging/averaging.  The main goal is to do feature engineering", "link": "https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-09-10 11:08:57", "date_scraped": "2020-12-12 17:59:32", "words": 416, "sentences": 24, "sum_nltk": "Do feature engineering to improve LightGBM predictionThis kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster.\nSeveral key points:  This kernel runs training on the heads of housholds only (after extracting aggregates over households).\nBut the simplest (and more powerful compared to undersampling) is to set class_weight='balanced' in the LightGBM model constructor in sklearn API, which will assign different weights to different classes proportional to their representation.\nNote that a better procedure would be to tune those weights in a CV loop instead of blindly assigning 1/n weights This kernel uses macro F1 score to early stopping in training.\nWe are fighting with poverty here- exploiting leaks will not reduce poverty in any way :) Squared features (SQBXXX and agesq) are NOT used in training.\nThese would be useful for a linear model, but are useless for a tree-based model and only confused it (when bagging and resampling is done) There are aggregations done within households and new features are hand-crafted.\nNote, that there are not so many features that can be aggregated, as most are already quoted on household level.\nA voting classifier is used to average over several LightGBM models.", "sum_nltk_words": 198, "sum_nltk_runtime": 0.005, "sum_t5": "this kernel uses optimal values from that kernel and thus runs faster. it seems very important to balance class frequencies. without balancing a trained model gives 0.39 PLB / 0.43 local test. it is possible to do it by hand, one can achieve it by undersampling. but the simplest is to set class_weight='balanced' in the LightGBM model constructor in sklearn API.", "sum_t5_words": 61, "sum_t5_runtime": 6.358, "runtime": 0.01, "nltk_category": "Accommodation & Food", "nltk_category_score": 0.20625366270542145, "nltk_category_runtime": 20.485, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9406315088272095, "nltk_subcategory_runtime": 33.074, "category": "Accommodation & Food", "category_score": 0.20625366270542145, "subcategory": "Machine Learning", "subcategory_score": 0.9406315088272095, "runtime_cat": 53.56, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.728", "language_code": "en", "language_score": "0.9999969040511636", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "feature engineering improve lightgbm predictionthis kernel closely follows httpswwwkagglecommlisovyilighgbmhyperoptimisationwithf1macro instead running hyperparameter optimisation us optimal value kernel thus run faster several key point kernel run training head housholds extracting aggregate household follows announced scoring startegy note head household used scoring household member included test sample submission head household scored data description seems important balance class frequency without balancing trained model give 039 plb 043 local test adding balancing lead 042 plb 047 local test one hand one achieve undersampling simplest powerful compared undersampling set class_weightbalanced lightgbm model constructor sklearn api assign different weight different class proportional representation note better procedure would tune weight cv loop instead blindly assigning 1n weight kernel us macro f1 score early stopping training done align scoring strategy categoricals turned number proper mapping instead blind label encoding ohe reversed label encoding easier digest tree model trick would harmful nontree model careful idhogar used training way could info would data leak fighting poverty exploiting leak reduce poverty way squared feature sqbxxx agesq used training would useful linear model useless treebased model confused bagging resampling done aggregation done within household new feature handcrafted note many feature aggregated already quoted household level new geographical aggregate calculated household new model build evaluated nested cv loop done reduce fluctuation earlystopping criterion well average several performance estimate voting classifier used average several lightgbm model allows get better prediction baggingaveraging main goal feature engineering", "tags_descriptive": []}