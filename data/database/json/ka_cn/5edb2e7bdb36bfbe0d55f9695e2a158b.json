{"title": "NB-SVM strong linear baseline", "description": "IntroductionThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the Toxic Comment Classification Challenge competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation. In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes). If you're not familiar with naive bayes and bag of words matrices, I've made a preview available of one of fast.ai's upcoming Practical Machine Learning course videos, which introduces this topic. Here is a link to the section of the video which discusses this: Naive Bayes video.", "link": "https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline", "tags": ["NLP"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-02-03 20:44:11", "date_scraped": "2020-12-13 12:20:06", "words": 120, "sentences": 5, "runtime": 0.005, "description_category": "Utilities", "description_category_score": 0.04631673917174339, "description_category_runtime": 11.998, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.976224958896637, "description_subcategory_runtime": 18.503, "category": "Utilities", "category_score": 0.04631673917174339, "subcategory": "Machine Learning", "subcategory_score": 0.976224958896637, "runtime_cat": 30.502, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.831", "language_code": "en", "language_score": "0.9999976441694274", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introductionthis kernel show use nbsvm naive bayes support vector machine create strong baseline toxic comment classification challenge competition nbsvm introduced sida wang chris manning paper baseline bigram simple good sentiment topic classification kernel use sklearns logistic regression rather svm although practice two nearly identical sklearn us liblinear library behind scene youre familiar naive bayes bag word matrix ive made preview available one fastais upcoming practical machine learning course video introduces topic link section video discus naive bayes video", "tags_descriptive": ["Natural Language Processing (NLP)"]}