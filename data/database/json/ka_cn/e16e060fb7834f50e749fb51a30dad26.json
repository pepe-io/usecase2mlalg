{"title": "XGBoost", "description": "LGMB with random split for early stoppingEdits by Eric Antoine Scuccimarra - This is a fork of  https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro, by Misha Losvyi, with a few changes:  The LightGBM models have been replaced with XGBoost and the code has been updated accordingly. I am also fitting VotingClassifiers of RandomForests and ensembling the results of the XGBs with the RFs. Some additional features have been added. Some features which were previously dropped have been retained. Some of the code has been reorganized. Rather than splitting the data once and using the validation data for the LGBM early stopping, I split the data during the training so the entire training set can be trained on. I found that this works better than a k-fold split in this case.  Some additional features were taken from: https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score, by Kuriyaman. Notes from Original Kernel (edited by EAS): This kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster. Several key points:  This kernel runs training on the heads of housholds only (after extracting aggregates over households). This follows the announced scoring startegy: Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored. (from the data description). However, at the moment it seems that evaluation depends also on non-head household members, see https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115. In practise, ful prediction gives ~0.4 PLB score, while replacing all non-head entries with class 1 leads to a drop down to ~0.2 PLB score It seems to be very important to balance class frequencies. Without balancing a trained model gives ~0.39 PLB / ~0.43 local test, while adding balancing leads to ~0.42 PLB / 0.47 local test. One can do it by hand, one can achieve it by undersampling. But the simplest (and more powerful compared to undersampling) is to set class_weight='balanced' in the LightGBM model constructor in sklearn API. This kernel uses macro F1 score to early stopping in training. This is done to align with the scoring strategy. Categoricals are turned into numbers with proper mapping instead of blind label encoding.  OHE if reversed into label encoding, as it is easier to digest for a tree model. This trick would be harmful for non-tree models, so be careful. idhogar is NOT used in training. The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :) There are aggregations done within households and new features are hand-crafted. Note, that there are not so many features that can be aggregated, as most are already quoted on household level. A voting classifier is used to average over several LightGBM models", "link": "https://www.kaggle.com/skooch/xgboost", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-05-20 08:38:53", "date_scraped": "2020-12-12 17:59:32", "words": 471, "sentences": 28, "sum_nltk": "LGMB with random split for early stoppingEdits by Eric Antoine Scuccimarra - This is a fork of  https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro, by Misha Losvyi, with a few changes:  The LightGBM models have been replaced with XGBoost and the code has been updated accordingly.\nSome additional features were taken from: https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score, by Kuriyaman.\nNotes from Original Kernel (edited by EAS): This kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster.\nSeveral key points:  This kernel runs training on the heads of housholds only (after extracting aggregates over households).\nHowever, at the moment it seems that evaluation depends also on non-head household members, see https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115.\nIn practise, ful prediction gives ~0.4 PLB score, while replacing all non-head entries with class 1 leads to a drop down to ~0.2 PLB score It seems to be very important to balance class frequencies.\nWithout balancing a trained model gives ~0.39 PLB / ~0.43 local test, while adding balancing leads to ~0.42 PLB / 0.47 local test.\nThis kernel uses macro F1 score to early stopping in training.\nWe are fighting with poverty here- exploiting leaks will not reduce poverty in any way :) There are aggregations done within households and new features are hand-crafted.", "sum_nltk_words": 203, "sum_nltk_runtime": 0.005, "sum_t5": "a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a fork of a for", "sum_t5_words": 68, "sum_t5_runtime": 6.965, "runtime": 0.004, "nltk_category": "Economics", "nltk_category_score": 0.24258871376514435, "nltk_category_runtime": 29.944, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.747902512550354, "nltk_subcategory_runtime": 47.671, "category": "Economics", "category_score": 0.24258871376514435, "subcategory": "Machine Learning", "subcategory_score": 0.747902512550354, "runtime_cat": 77.615, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.714", "language_code": "en", "language_score": "0.9999971288324258", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "lgmb random split early stoppingedits eric antoine scuccimarra fork httpswwwkagglecommlisovyifeatureengineeringlighgbmwithf1macro misha losvyi change lightgbm model replaced xgboost code updated accordingly also fitting votingclassifiers randomforests ensembling result xgbs rf additional feature added feature previously dropped retained code reorganized rather splitting data using validation data lgbm early stopping split data training entire training set trained found work better kfold split case additional feature taken httpswwwkagglecomkuriyaman1002reducefeatures14084keepingf1score kuriyaman note original kernel edited ea kernel closely follows httpswwwkagglecommlisovyilighgbmhyperoptimisationwithf1macro instead running hyperparameter optimisation us optimal value kernel thus run faster several key point kernel run training head housholds extracting aggregate household follows announced scoring startegy note head household used scoring household member included test sample submission head household scored data description however moment seems evaluation depends also nonhead household member see httpswwwkagglecomccostaricanhouseholdpovertypredictiondiscussion61403360115 practise ful prediction give 04 plb score replacing nonhead entry class 1 lead drop 02 plb score seems important balance class frequency without balancing trained model give 039 plb 043 local test adding balancing lead 042 plb 047 local test one hand one achieve undersampling simplest powerful compared undersampling set class_weightbalanced lightgbm model constructor sklearn api kernel us macro f1 score early stopping training done align scoring strategy categoricals turned number proper mapping instead blind label encoding ohe reversed label encoding easier digest tree model trick would harmful nontree model careful idhogar used training way could info would data leak fighting poverty exploiting leak reduce poverty way aggregation done within household new feature handcrafted note many feature aggregated already quoted household level voting classifier used average several lightgbm model", "tags_descriptive": []}