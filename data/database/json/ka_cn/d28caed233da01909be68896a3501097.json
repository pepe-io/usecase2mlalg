{"title": "Jigsaw TPU: DistilBERT with Huggingface and Keras", "description": "About this notebookJigsaw Multilingual Toxic Comment Classification is the 3rd annual competition organized by the Jigsaw team. It follows Toxic Comment Classification Challenge, the original 2018 competition, and Jigsaw Unintended Bias in Toxicity Classification, which required the competitors to consider biased ML predictions in their new models. This year, the goal is to use english only training data to run toxicity predictions on many different languages, which can be done using multilingual models, and speed up using TPUs. Many awesome notebooks has already been made so far. Many of them used really cool technologies like Pytorch XLA. This notebook instead aims at constructing a fast, concise, reusable, and beginner-friendly model scaffold. It will focus on the following points:  Using Tensorflow and Keras: Tensorflow is a powerful framework, and Keras makes the training process extremely easy to understand. This is especially good for beginners to learn how to use TPUs, and for experts to focus on the modelling aspect. Huggingface's transformers library: This library is extremely popular, so using this let you easily integrate the end result into your ML pipelines, and can be easily reused for your other projects. Multilingual DistilBERT: DistilBERT is 2 times faster and 25% lighter than multilingual BERT base, all while retaining 92% of its performance. This model let you quickly experiments with different ideas, and when you are ready for the real thing, just change two lines of code to use bert-base-multilingual-cased. Blazing fast tokenization: Huggingface's tokenizers is order of magnitude faster than the default BERT tokenizer, since it is written in Rust, and uses a Python interface. Native TPU usage: The TPU usage is abstracted using the native strategy that was created using Tensorflow's tf.distribute.experimental.TPUStrategy. This avoids getting too much into the lower-level aspect of TPU management. Subset of the data: Instead of using the entire dataset, we will only use the 2018 subset of the data available, which makes this much faster, all while achieving a respectable accuracy.  References Original Author: @xhlulu Original notebook: Link", "link": "https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "tensorflow", "vocabulary"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-04-01 00:22:50", "date_scraped": "2020-12-13 12:15:35", "words": 334, "sentences": 16, "sum_nltk": "It follows Toxic Comment Classification Challenge, the original 2018 competition, and Jigsaw Unintended Bias in Toxicity Classification, which required the competitors to consider biased ML predictions in their new models.\nThis year, the goal is to use english only training data to run toxicity predictions on many different languages, which can be done using multilingual models, and speed up using TPUs. Many awesome notebooks has already been made so far.\nThis notebook instead aims at constructing a fast, concise, reusable, and beginner-friendly model scaffold.\nThis is especially good for beginners to learn how to use TPUs, and for experts to focus on the modelling aspect.\nThis model let you quickly experiments with different ideas, and when you are ready for the real thing, just change two lines of code to use bert-base-multilingual-cased.\nBlazing fast tokenization: Huggingface's tokenizers is order of magnitude faster than the default BERT tokenizer, since it is written in Rust, and uses a Python interface.\nSubset of the data: Instead of using the entire dataset, we will only use the 2018 subset of the data available, which makes this much faster, all while achieving a respectable accuracy.", "sum_nltk_words": 184, "sum_nltk_runtime": 0.004, "sum_t5": "notebook aims to build a fast, concise, reusable, beginner-friendly model scaffold. it will use a powerful framework and a reusable library. it will also use a native strategy that was created using Tensorflow's tf.distribute.experimental.TPUStrategy. the notebook will be available in october.. and will be available in october.", "sum_t5_words": 47, "sum_t5_runtime": 6.407, "runtime": 0.003, "nltk_category": "Biotechnological & Life Sciences", "nltk_category_score": 0.15829463303089142, "nltk_category_runtime": 17.472, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9022514820098877, "nltk_subcategory_runtime": 27.706, "category": "Biotechnological & Life Sciences", "category_score": 0.15829463303089142, "subcategory": "Machine Learning", "subcategory_score": 0.9022514820098877, "runtime_cat": 45.178, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.745", "language_code": "en", "language_score": "0.9999972615751017", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "notebookjigsaw multilingual toxic comment classification 3rd annual competition organized jigsaw team follows toxic comment classification challenge original 2018 competition jigsaw unintended bias toxicity classification required competitor consider biased ml prediction new model year goal use english training data run toxicity prediction many different language done using multilingual model speed using tpus many awesome notebook already made far many used really cool technology like pytorch xla notebook instead aim constructing fast concise reusable beginnerfriendly model scaffold focus following point using tensorflow kera tensorflow powerful framework kera make training process extremely easy understand especially good beginner learn use tpus expert focus modelling aspect huggingfaces transformer library library extremely popular using let easily integrate end result ml pipeline easily reused project multilingual distilbert distilbert 2 time faster 25 lighter multilingual bert base retaining 92 performance model let quickly experiment different idea ready real thing change two line code use bertbasemultilingualcased blazing fast tokenization huggingfaces tokenizers order magnitude faster default bert tokenizer since written rust us python interface native tpu usage tpu usage abstracted using native strategy created using tensorflows tfdistributeexperimentaltpustrategy avoids getting much lowerlevel aspect tpu management subset data instead using entire dataset use 2018 subset data available make much faster achieving respectable accuracy reference original author xhlulu original notebook link", "tags_descriptive": []}