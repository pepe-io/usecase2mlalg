{"title": "Film industry curiosities & box office prediction", "description": "TL;DRCherry-picked InsightsIn case you might be interested in clues to some of the preselected questions, please feel free to follow the corresponding link and start reading from the anchor section.  Let's make missing values NaNs again! Low-budget movies: how many are there? Are there more or less of them recently?  Low-ranking movies: Are there more or less of them recently?  Should we be afraid of film industry apocalypse? ;)  Given that brevity is the sister of talent, do more or less people with talent write movie overviews? And taglines?  Can revenues reveal which decades were really the greatest in cinema? Are there more sequels or movies with word \"love\" in a title? :)  Modeling Field-Aware Factorization Machines: how to make the kaggle-champ library libffm up and running, how to prepare data in the required format, how to train models using 5-fold stratified CV and early stopping, calling all command-line functions from Python LightGBM, 5-fold stratified CV, Bayesian hyperparameters search CatBoost, 5-fold stratified CV, Bayesian hyperparameters search  Kernel overview To have reliable local validation, it is important to mimic the train/test split. So, let us focus on train vs. test EDA first. Based on the EDA findings we would clean data, including filling of missing entries by querying TMDB API. Along the way we would explore potential curiosities about film industry. As a helping step, we would perform adversarial validation. EDA and data cleaning Was train/test split time-based? Budget analysis  Rating analysis Adversarial validation Popularity Analysis Overview and tagline analysis Filling in missing values using TMDB API     Next, we would perform some feature engineering.  Feature engineering Revenue and inflation Sparse categorical features based on columns like crew and cast Text-based features Features based on competition with other movies     The task at hand has quite a few categorical features with high cardinality. Factorization machines have proven itself useful in problems with huge sparsity. We would use libffm library to create a field-aware factorization machine baseline. FFM turned out not to work well enough, please see previous kernel versions for full training. On the bright side we learned how to work with the library which won a couple of competitions. Modeling Field-Aware Factorization Machines     Create gradient boosting baselines (LightGBM, CatBoost) to predict movie box office. We would tune hyperparameters of the gradient boosting models using Bayesian optimization. Modeling LightGBM CatBoost Final Predictions", "link": "https://www.kaggle.com/samusram/film-industry-curiosities-box-office-prediction", "tags": ["Feature Engineering", "Exploratory Data Analysis"], "kind": ["Project", "(Notebook)"], "ml_libs": ["nltk", "lightgbm", "sklearn", "statsmodels", "catboost"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-08-11 05:49:46", "date_scraped": "2020-12-13 17:40:32", "words": 410, "sentences": 16, "sum_nltk": "Low-budget movies: how many are there?\nLow-ranking movies: Are there more or less of them recently?\nShould we be afraid of film industry apocalypse?\n:)  Modeling Field-Aware Factorization Machines: how to make the kaggle-champ library libffm up and running, how to prepare data in the required format, how to train models using 5-fold stratified CV and early stopping, calling all command-line functions from Python LightGBM, 5-fold stratified CV, Bayesian hyperparameters search CatBoost, 5-fold stratified CV, Bayesian hyperparameters search  Kernel overview To have reliable local validation, it is important to mimic the train/test split.\nBased on the EDA findings we would clean data, including filling of missing entries by querying TMDB API.\nAlong the way we would explore potential curiosities about film industry.\nEDA and data cleaning Was train/test split time-based?\nBudget analysis  Rating analysis Adversarial validation Popularity Analysis Overview and tagline analysis Filling in missing values using TMDB API     Next, we would perform some feature engineering.\nWe would use libffm library to create a field-aware factorization machine baseline.\nModeling Field-Aware Factorization Machines     Create gradient boosting baselines (LightGBM, CatBoost) to predict movie box office.\nModeling LightGBM CatBoost Final Predictions", "sum_nltk_words": 189, "sum_nltk_runtime": 0.006, "sum_t5": "low-budget movies: how many are there? are there more or less of them recently? can revenues reveal which decades were really the greatest in cinema? can revenues reveal which decades were really the greatest in cinema? a new tv series will be released in january. a new tv series will be released in january. a new tv series will be released in january. a new tv series will be released in january.", "sum_t5_words": 72, "sum_t5_runtime": 6.907, "runtime": 0.004, "nltk_category": "Utilities", "nltk_category_score": 0.1451118439435959, "nltk_category_runtime": 21.052, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9847522377967834, "nltk_subcategory_runtime": 34.645, "category": "Utilities", "category_score": 0.1451118439435959, "subcategory": "Machine Learning", "subcategory_score": 0.9847522377967834, "runtime_cat": 55.698, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.706", "language_code": "en", "language_score": "0.9999980661411938", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "tldrcherrypicked insightsin case might interested clue preselected question please feel free follow corresponding link start reading anchor section let make missing value nan lowbudget movie many le recently lowranking movie le recently afraid film industry apocalypse given brevity sister talent le people talent write movie overview taglines revenue reveal decade really greatest cinema sequel movie word love title modeling fieldaware factorization machine make kagglechamp library libffm running prepare data required format train model using 5fold stratified cv early stopping calling commandline function python lightgbm 5fold stratified cv bayesian hyperparameters search catboost 5fold stratified cv bayesian hyperparameters search kernel overview reliable local validation important mimic traintest split let u focus train v test eda first based eda finding would clean data including filling missing entry querying tmdb api along way would explore potential curiosity film industry helping step would perform adversarial validation eda data cleaning traintest split timebased budget analysis rating analysis adversarial validation popularity analysis overview tagline analysis filling missing value using tmdb api next would perform feature engineering feature engineering revenue inflation sparse categorical feature based column like crew cast textbased feature feature based competition movie task hand quite categorical feature high cardinality factorization machine proven useful problem huge sparsity would use libffm library create fieldaware factorization machine baseline ffm turned work well enough please see previous kernel version full training bright side learned work library couple competition modeling fieldaware factorization machine create gradient boosting baseline lightgbm catboost predict movie box office would tune hyperparameters gradient boosting model using bayesian optimization modeling lightgbm catboost final prediction", "tags_descriptive": ["Feature Engineering", "Exploratory Data Analysis"]}