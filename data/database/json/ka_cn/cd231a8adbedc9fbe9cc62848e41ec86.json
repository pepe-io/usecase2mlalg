{"title": "[QUEST] Bert-base TF2.0", "description": "Bert-base TensorFlow 2.0This kernel does not explore the data. For that you could check out some of the great EDA kernels: introduction, getting started & another getting started. This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using TensorFow Hub Huggingface transformer.   Update 1 (Commit 7):  removing penultimate dense layer; now there's only one dense layer (output layer) for fine-tuning using BERT's sequence_output instead of pooled_output as input for the dense layer   Update 2 (Commit 8):  adjusting _trim_input() --- now have a q_max_len and a_max_len, instead of 'keeping the ratio the same' while trimming. importantly: now also includes question_title for the input sequence   Update 3 (Commit 9) A lot of experiments can be made with the title + body + answer sequence. Feel free to look into e.g. (1) inventing new tokens (add it to '../input/path-to-bert-folder/assets/vocab.txt'), (2) keeping [SEP] between title and body but modify _get_segments(), (3) using the [PAD] token, or (4) merging title and body without any kind of separation. In this commit I'm doing (2). I also tried (3) offline, and they both perform better than in commit 8, in terms of validation rho.  ignoring first [SEP] token in _get_segments().   Update 4 (Commit 11)  Now using Huggingface transformer instead of TFHub (note major changes in the code). This creates the possibility to easily try out different architectures like XLNet, Roberta etc. As well as easily outputting the hidden states of the transformer. two separate inputs (title+body and answer) for BERT removed snapshot average (now only using last (third) epoch). This will likely decrease performance, but it's not feasible to use ~ 5 x 4 models for a single bert prediction in practice.  only training for 2 epochs instead of 3 (to manage 2h limit)   Update 5 (Commit 12)  transformers now available in 'Latest Available' Docker reducing input size", "link": "https://www.kaggle.com/akensert/quest-bert-base-tf2-0", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-07-26 19:26:13", "date_scraped": "2020-12-12 20:20:08", "words": 319, "sentences": 16, "sum_nltk": "Bert-base TensorFlow 2.0This kernel does not explore the data.\nThis kernel is an example of a TensorFlow 2.0 Bert-base implementation, using TensorFow Hub Huggingface transformer.\nimportantly: now also includes question_title for the input sequence   Update 3 (Commit 9) A lot of experiments can be made with the title + body + answer sequence.\n(1) inventing new tokens (add it to '../input/path-to-bert-folder/assets/vocab.txt'), (2) keeping [SEP] between title and body but modify _get_segments(), (3) using the [PAD] token, or (4) merging title and body without any kind of separation.\nI also tried (3) offline, and they both perform better than in commit 8, in terms of validation rho.\nUpdate 4 (Commit 11)  Now using Huggingface transformer instead of TFHub (note major changes in the code).\nAs well as easily outputting the hidden states of the transformer.\ntwo separate inputs (title+body and answer) for BERT removed snapshot average (now only using last (third) epoch).\nThis will likely decrease performance, but it's not feasible to use ~ 5 x 4 models for a single bert prediction in practice.\nonly training for 2 epochs instead of 3 (to manage 2h limit)   Update 5 (Commit 12)  transformers now available in 'Latest Available' Docker reducing input size", "sum_nltk_words": 197, "sum_nltk_runtime": 0.004, "sum_t5": "this kernel is an example of a TensorFlow 2.0 Bert-base implementation, using TensorFow Hub Huggingface transformer. it's an example of a Bert-base implementation, using TensorFow Hub. it's an example of a bert-base implementation, using a Huggingface transformer. it's an example of a bert-base implementation, using a Huggingface transformer.", "sum_t5_words": 48, "sum_t5_runtime": 6.197, "runtime": 0.003, "nltk_category": "Finance", "nltk_category_score": 0.3009173274040222, "nltk_category_runtime": 22.419, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9772148728370667, "nltk_subcategory_runtime": 36.003, "category": "Finance", "category_score": 0.3009173274040222, "subcategory": "Machine Learning", "subcategory_score": 0.9772148728370667, "runtime_cat": 58.422, "programming_language": "Jupyter Notebook", "ml_score": "0.7", "engagement_score": "0.794", "language_code": "en", "language_score": "0.9999967883212182", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "bertbase tensorflow 20this kernel explore data could check great eda kernel introduction getting started another getting started kernel example tensorflow 20 bertbase implementation using tensorfow hub huggingface transformer update 1 commit 7 removing penultimate dense layer there one dense layer output layer finetuning using berts sequence_output instead pooled_output input dense layer update 2 commit 8 adjusting _trim_input q_max_len a_max_len instead keeping ratio trimming importantly also includes question_title input sequence update 3 commit 9 lot experiment made title body answer sequence feel free look eg 1 inventing new token add inputpathtobertfolderassetsvocabtxt 2 keeping sep title body modify _get_segments 3 using pad token 4 merging title body without kind separation commit im 2 also tried 3 offline perform better commit 8 term validation rho ignoring first sep token _get_segments update 4 commit 11 using huggingface transformer instead tfhub note major change code creates possibility easily try different architecture like xlnet roberta etc well easily outputting hidden state transformer two separate input titlebody answer bert removed snapshot average using last third epoch likely decrease performance feasible use 5 x 4 model single bert prediction practice training 2 epoch instead 3 manage 2h limit update 5 commit 12 transformer available latest available docker reducing input size", "tags_descriptive": []}