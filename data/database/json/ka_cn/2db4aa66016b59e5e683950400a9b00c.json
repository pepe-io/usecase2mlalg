{"title": "Single RNN with 4 folds (CLR)", "description": "I was trying to clean some of my code so I can add more models. However, this can never happen without the awesome kernels from other talented Kagglers. Forgive me if I missed any.  CLR from: https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code Based on SRK's kernel: https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings Vladimir Demidov's 2DCNN textClassifier: https://www.kaggle.com/yekenot/2dcnn-textclassifier Attention layer from Khoi Ngyuen: https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb LSTM model from Strideradu: https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go https://www.kaggle.com/danofer/different-embeddings-with-attention-fork https://www.kaggle.com/ryanzhang/tfidf-naivebayes-logreg-baseline Borrowed some idea from this model: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644 Sentence length seems to a good feature: https://www.kaggle.com/thebrownviking20/analyzing-quora-for-the-insinceres  Some new things here:  Take average of embeddings (Unweighted DME) instead of blending predictions: https://arxiv.org/pdf/1804.07983.pdf The original paper of this idea comes from: Frustratingly Easy Meta-Embedding \u2013 Computing Meta-Embeddings by Averaging Source Word Embeddings Modified the code to choose best threshold Robust method for blending weights: sort the val score and give the final weight  Some thoughts:  Although I pulished a kernel on Transformer, I will not use it Too much randomness in CuDNN. You may get different results by just rerunning this kernel Blending rocks", "link": "https://www.kaggle.com/shujian/single-rnn-with-4-folds-clr", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "gensim"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-11-25 02:20:34", "date_scraped": "2020-12-13 16:20:42", "words": 166, "sentences": 5, "runtime": 0.002, "description_category": "Real Estate, Rental & Leasing", "description_category_score": 0.3485783636569977, "description_category_runtime": 35.661, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.9835853576660156, "description_subcategory_runtime": 58.215, "category": "Real Estate, Rental & Leasing", "category_score": 0.3485783636569977, "subcategory": "Machine Learning", "subcategory_score": 0.9835853576660156, "runtime_cat": 93.876, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.787", "language_code": "en", "language_score": "0.9999984838496485", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "trying clean code add model however never happen without awesome kernel talented kagglers forgive missed clr httpswwwkagglecomhiremefunapikerasf1metriccyclicallearningratecode based srks kernel httpswwwkagglecomsudalairajkumaralookatdifferentembeddings vladimir demidovs 2dcnn textclassifier httpswwwkagglecomyekenot2dcnntextclassifier attention layer khoi ngyuen httpswwwkagglecomsuicaokhoailanglstmattentionbaseline0652lb lstm model strideradu httpswwwkagglecomstrideraduword2vecandgensimgogogo httpswwwkagglecomdanoferdifferentembeddingswithattentionfork httpswwwkagglecomryanzhangtfidfnaivebayeslogregbaseline borrowed idea model httpswwwkagglecomcjigsawtoxiccommentclassificationchallengediscussion52644 sentence length seems good feature httpswwwkagglecomthebrownviking20analyzingquorafortheinsinceres new thing take average embeddings unweighted dme instead blending prediction httpsarxivorgpdf180407983pdf original paper idea come frustratingly easy metaembedding computing metaembeddings averaging source word embeddings modified code choose best threshold robust method blending weight sort val score give final weight thought although pulished kernel transformer use much randomness cudnn may get different result rerunning kernel blending rock", "tags_descriptive": []}