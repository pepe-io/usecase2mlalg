{"title": "Probabilistic Machine Learning: A Diff Approach", "description": "1. Probabilistic Machine Learning: A Different ApproachI'm very grateful to have entered in this competition, as it drove me to learn new things I wouldn't otherwise. As I wrote in other notebooks, my first objective was to learn how to implement AutoEncoders, and discover why generative models are so hype. After reading papers, tutorials, experimenting, analyzing the (poor) results achieved, I started asking questions. Why isn't the model learning good representation of the latent space? Btw, what the heck is a latent space? Why are generative models so cool? How do they actually work? The search for these (and many more answers) drove me to Variational AutoEncoders (which I will share my implementation here later), but most importantly, drove me to Probabilistic Machine Learning, Bayesian Inference, and the very cool field of Probabilistic Programming. As I started to deep dive into papers and books about those subjects, I realized that OSIC Pulmonary Fibrosis problem screams for applying Probabilistic Machine Learning. Mindlessly applying Deep Learning to solve this problem reminded of an oldie but goldie: if all you have is a hammer, everything looks like a nail. I'm so excited about my \"discovery\" of this new field, that I could go on and on writing about what I learned over the past month. But instead, I will leave some nice references that helped me gain insight about discriminative vs generative machine learning, deterministic vs stochastic algorithms, bayesian vs frequentist approach, and dive into a demonstration (there are many many more great readings, these are just some to get started with varied level of depth):  Probabilistic machine learning and artificial intelligence Automating Inference, Learning, and Design using Probabilistic Programming Probabilistic Programming and Bayesian Methods for Hackers Probabilistic Models of Cognition Machine Learning: A Probabilistic Perspective  My idea is to break the problem in 3 sub-problems:  Define and train a simple probabilistic model to make inferences about FVC using only tabular data (baby steps) Define and train an advanced probabilistic model (a Variational AutoEncoder) to learn latent features from the CT scans Combine the 2 models together  1.1. ToolsIMHO the mathematical tools needed are no more advanced than the tools used in Deep Learning. Unfortunately, most of the books and papers are overloaded with hardcore math that may discourage at first. But there are good exceptions (some cited above, like the excelent open books Probabilistic Programming and Bayesian Methods for Hackers, or the Probabilistic Models of Cognition), and I think everyone can learn. In terms of Probabilistic Programming Languages, there are several options. I'd say some of the obvious choices would be those 3:  As a PyTorch user (always found TF too verbose to my taste), my natural choice was Pyro. However, to my surprise, I discovered that I'd have to install it. PyMC3 and TFP work out-of-the-box in Kaggle Kernels, but Pyro doesn't. As this competition do not allow internet, I will use PyMC3. But most importantly, Kaggle please add Pyro to kernels! 1.2. Statistical Modeling ProcessIn the course Bayesian Statistics: Techniques and Models, prof Matthew Heiner summarizes the modeling process as having 8 steps:  Understand the problem Plan and collect data Explore the data Postulate the model Fit the model Check the model Iterate Use the model  We understand the problem well enoguh, and data is already collected. So, let's do a quick exploration of the tabular data and then postulate a model.", "link": "https://www.kaggle.com/carlossouza/probabilistic-machine-learning-a-diff-approach", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "theano"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-08-16 00:22:14", "date_scraped": "2020-12-13 14:44:51", "words": 569, "sentences": 20, "sum_nltk": "The search for these (and many more answers) drove me to Variational AutoEncoders (which I will share my implementation here later), but most importantly, drove me to Probabilistic Machine Learning, Bayesian Inference, and the very cool field of Probabilistic Programming.\nAs I started to deep dive into papers and books about those subjects, I realized that OSIC Pulmonary Fibrosis problem screams for applying Probabilistic Machine Learning.\nBut instead, I will leave some nice references that helped me gain insight about discriminative vs generative machine learning, deterministic vs stochastic algorithms, bayesian vs frequentist approach, and dive into a demonstration (there are many many more great readings, these are just some to get started with varied level of depth):  Probabilistic machine learning and artificial intelligence Automating Inference, Learning, and Design using Probabilistic Programming Probabilistic Programming and Bayesian Methods for Hackers Probabilistic Models of Cognition Machine Learning: A Probabilistic Perspective  My idea is to break the problem in 3 sub-problems:  Define and train a simple probabilistic model to make inferences about FVC using only tabular data (baby steps) Define and train an advanced probabilistic model (a Variational AutoEncoder) to learn latent features from the CT scans Combine the 2 models together  1.1.\nBut there are good exceptions (some cited above, like the excelent open books Probabilistic Programming and Bayesian Methods for Hackers, or the Probabilistic Models of Cognition), and I think everyone can learn.", "sum_nltk_words": 232, "sum_nltk_runtime": 0.006, "sum_t5": "a new book explores the field of Probabilistic machine learning. the book will be a demonstration of the new approach. the book will also cover the field of deterministic vs stochastic algorithms. the book will be available in november. if you're interested in learning more about the book, you can enter the book contest. cnn.com/probability/./probability/probability/probability/probability/probability", "sum_t5_words": 55, "sum_t5_runtime": 6.935, "runtime": 0.006, "nltk_category": "Healthcare", "nltk_category_score": 0.2448706179857254, "nltk_category_runtime": 22.644, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8489997982978821, "nltk_subcategory_runtime": 36.902, "category": "Healthcare", "category_score": 0.2448706179857254, "subcategory": "Machine Learning", "subcategory_score": 0.8489997982978821, "runtime_cat": 59.546, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.701", "language_code": "en", "language_score": "0.9999956666620701", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "1 probabilistic machine learning different approachim grateful entered competition drove learn new thing wouldnt otherwise wrote notebook first objective learn implement autoencoders discover generative model hype reading paper tutorial experimenting analyzing poor result achieved started asking question isnt model learning good representation latent space btw heck latent space generative model cool actually work search many answer drove variational autoencoders share implementation later importantly drove probabilistic machine learning bayesian inference cool field probabilistic programming started deep dive paper book subject realized osic pulmonary fibrosis problem scream applying probabilistic machine learning mindlessly applying deep learning solve problem reminded oldie goldie hammer everything look like nail im excited discovery new field could go writing learned past month instead leave nice reference helped gain insight discriminative v generative machine learning deterministic v stochastic algorithm bayesian v frequentist approach dive demonstration many many great reading get started varied level depth probabilistic machine learning artificial intelligence automating inference learning design using probabilistic programming probabilistic programming bayesian method hacker probabilistic model cognition machine learning probabilistic perspective idea break problem 3 subproblems define train simple probabilistic model make inference fvc using tabular data baby step define train advanced probabilistic model variational autoencoder learn latent feature ct scan combine 2 model together 11 toolsimho mathematical tool needed advanced tool used deep learning unfortunately book paper overloaded hardcore math may discourage first good exception cited like excelent open book probabilistic programming bayesian method hacker probabilistic model cognition think everyone learn term probabilistic programming language several option id say obvious choice would 3 pytorch user always found tf verbose taste natural choice pyro however surprise discovered id install pymc3 tfp work outofthebox kaggle kernel pyro doesnt competition allow internet use pymc3 importantly kaggle please add pyro kernel 12 statistical modeling processin course bayesian statistic technique model prof matthew heiner summarizes modeling process 8 step understand problem plan collect data explore data postulate model fit model check model iterate use model understand problem well enoguh data already collected let quick exploration tabular data postulate model", "tags_descriptive": []}