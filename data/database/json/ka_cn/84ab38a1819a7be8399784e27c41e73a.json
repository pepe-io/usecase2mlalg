{"title": "preprocessing, model averaging by xgb + lgb [1.39]", "description": "In this notebook, we preprocessed the data and feed the data to gradient boosting tree models, and got 1.39 on public leaderboard. the workflow is as follows:  Data preprocessing. The purpose of data preprocessing is to achieve higher time/space efficiency. What we did includes round, constant features removal, duplicate features removal, insignificant features removal, etc. The key here is to ensure the preprocessing shall not hurt the accuracy. Feature transform. The purpose of feature transform is to help the models to better grasp the information in the data, and fight overfitting. What we did includes dropping features which \"live\" on different distributions on training/testing set, adding statistical features, adding low-dimensional representation as features.  Modeling.  We used 2 models: xgboost and lightgbm. We averaged the 2 models for the final prediction.  Stay tuned, more update will come. references:  Distribution of Test vs. Training data Ensemble of LGBM and XGB predict house prices-model tuning & ensemble Stacked Regressions : Top 4% on LeaderBoard", "link": "https://www.kaggle.com/alexpengxiao/preprocessing-model-averaging-by-xgb-lgb-1-39", "tags": ["Ensembling", "Feature Engineering"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-07-09 00:33:23", "date_scraped": "2020-12-13 17:15:37", "words": 166, "sentences": 14, "runtime": 0.002, "description_category": "Real Estate, Rental & Leasing", "description_category_score": 0.4590405523777008, "description_category_runtime": 15.901, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.907768189907074, "description_subcategory_runtime": 25.201, "category": "Real Estate, Rental & Leasing", "category_score": 0.4590405523777008, "subcategory": "Machine Learning", "subcategory_score": 0.907768189907074, "runtime_cat": 41.102, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.776", "language_code": "en", "language_score": "0.9999969257644381", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "notebook preprocessed data feed data gradient boosting tree model got 139 public leaderboard workflow follows data preprocessing purpose data preprocessing achieve higher timespace efficiency includes round constant feature removal duplicate feature removal insignificant feature removal etc key ensure preprocessing shall hurt accuracy feature transform purpose feature transform help model better grasp information data fight overfitting includes dropping feature live different distribution trainingtesting set adding statistical feature adding lowdimensional representation feature modeling used 2 model xgboost lightgbm averaged 2 model final prediction stay tuned update come reference distribution test v training data ensemble lgbm xgb predict house pricesmodel tuning ensemble stacked regression top 4 leaderboard", "tags_descriptive": ["Ensembling", "Feature Engineering"]}