{"title": "Knowledge Distillation with NN + RankGauss", "description": "Introduction\"Distilling the Knowledge in a Neural Network\" was introduced by Geoffrey Hinton, Oriol Vinyals, Jeff Dean in Mar 2015. In this kernel, I would like to share some experiments to distill the knowledge from a LGBM teacher (LB:0.899) to a neural network. The student network has not surpassed the teacher model yet (LB:0.894). But, I hope I can make it happen before this competition ends.", "link": "https://www.kaggle.com/mathormad/knowledge-distillation-with-nn-rankgauss", "tags": ["DL"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-03-11 04:11:08", "date_scraped": "2020-12-13 17:07:48", "words": 65, "sentences": 4, "runtime": 0.0, "description_category": "Education & Research", "description_category_score": 0.346576064825058, "description_category_runtime": 7.605, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.8527556657791138, "description_subcategory_runtime": 11.584, "category": "Education & Research", "category_score": 0.346576064825058, "subcategory": "Machine Learning", "subcategory_score": 0.8527556657791138, "runtime_cat": 19.189, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.756", "language_code": "en", "language_score": "0.999996489706889", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introductiondistilling knowledge neural network introduced geoffrey hinton oriol vinyals jeff dean mar 2015 kernel would like share experiment distill knowledge lgbm teacher lb0899 neural network student network surpassed teacher model yet lb0894 hope make happen competition end", "tags_descriptive": ["Deep Learning (DL)"]}