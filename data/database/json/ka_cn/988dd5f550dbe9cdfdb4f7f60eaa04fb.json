{"title": "Adversarial Learning Challenges - Getting Started", "description": "Getting Started with the NIPS 2017 Adversarial Learning ChallengesCurrent image classifiers can easily be tricked using carefully crafted adversarial images. These images add small changes to the original, correctly-classified image that are virtually imperceptible to the human eye but cause image classifiers to become wrong with high confidence in the incorrect class. There's three related adversarial learning challenges in NIPS 2017.  The first two focus on successfully generating adversarial images. The non-targeted challenge focuses on tricking the classifier with any other class, while the targeted challenge focuses on tricking the classifier into thinking the image is a specific target class. The third defense challenge focuses on training classifiers that are robust against adversarial attacks. The defense challenge is scored based on how well the classifiers work in the face of adversarial attacks from the first two challenges, and the first two challenges are scored based on how well the adversarial attacks trick the classifiers in the third challenge. Here, we'll walk through some code examples on generating non-targeted and targeted adversarial images, and then seeing how the Inception V3 model classifies them. Much of this code is based on Alex's samples. To get started, we'll import the necessary libraries and define some parameters / useful functions.", "link": "https://www.kaggle.com/benhamner/adversarial-learning-challenges-getting-started", "tags": ["Classification", "GAN", "CV"], "kind": ["Project", "(Notebook)"], "ml_libs": ["tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2017-07-06 19:44:40", "date_scraped": "2020-12-13 14:26:28", "words": 207, "sentences": 10, "sum_nltk": "Getting Started with the NIPS 2017 Adversarial Learning ChallengesCurrent image classifiers can easily be tricked using carefully crafted adversarial images.\nThese images add small changes to the original, correctly-classified image that are virtually imperceptible to the human eye but cause image classifiers to become wrong with high confidence in the incorrect class.\nThere's three related adversarial learning challenges in NIPS 2017.\nThe first two focus on successfully generating adversarial images.\nThe non-targeted challenge focuses on tricking the classifier with any other class, while the targeted challenge focuses on tricking the classifier into thinking the image is a specific target class.\nThe third defense challenge focuses on training classifiers that are robust against adversarial attacks.\nThe defense challenge is scored based on how well the classifiers work in the face of adversarial attacks from the first two challenges, and the first two challenges are scored based on how well the adversarial attacks trick the classifiers in the third challenge.\nHere, we'll walk through some code examples on generating non-targeted and targeted adversarial images, and then seeing how the Inception V3 model classifies them.\nMuch of this code is based on Alex's samples.\nTo get started, we'll import the necessary libraries and define some parameters / useful functions.", "sum_nltk_words": 197, "sum_nltk_runtime": 0.003, "sum_t5": "there are three related adversarial learning challenges in the 2017 NIPS. the first two focus on successfully generating adversarial images. the third defense challenge focuses on training classifiers robust against adversarial attacks. the first two challenges are scored based on how well the adversarial attacks trick the classifiers. the third challenge is scored based on how well the classifiers work in the face of adversarial attacks. a test version of the challenge is available for download.", "sum_t5_words": 76, "sum_t5_runtime": 5.031, "runtime": 0.002, "nltk_category": "Accommodation & Food", "nltk_category_score": 0.39576661586761475, "nltk_category_runtime": 18.438, "nltk_subcategory": "Restaurant", "nltk_subcategory_score": 0.4448325037956238, "nltk_subcategory_runtime": 29.72, "category": "Accommodation & Food", "category_score": 0.39576661586761475, "subcategory": "Restaurant", "subcategory_score": 0.4448325037956238, "runtime_cat": 48.159, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.779", "language_code": "en", "language_score": "0.9999953594592783", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "getting started nip 2017 adversarial learning challengescurrent image classifier easily tricked using carefully crafted adversarial image image add small change original correctlyclassified image virtually imperceptible human eye cause image classifier become wrong high confidence incorrect class there three related adversarial learning challenge nip 2017 first two focus successfully generating adversarial image nontargeted challenge focus tricking classifier class targeted challenge focus tricking classifier thinking image specific target class third defense challenge focus training classifier robust adversarial attack defense challenge scored based well classifier work face adversarial attack first two challenge first two challenge scored based well adversarial attack trick classifier third challenge well walk code example generating nontargeted targeted adversarial image seeing inception v3 model classifies much code based alexs sample get started well import necessary library define parameter useful function", "tags_descriptive": ["Classification", "Generative Adversarial Networks (GAN)", "Computer Vision (CV)"]}