{"title": "[RSNA] InceptionV3 Keras + TF1.14.0", "description": "InceptionV3 (previously ResNet50) Keras baseline modelThis notebook takes you through some important steps in building a deep convnet in Keras for multilabel classification of brain CT scans. Update (1):  training for 4 epochs instead of 3. batch size lowered to 16 from 32. training without learning rate decay. Weighted BCE instead of \"plain\" BCE training data lowered to 80% from 90%.  Update (2):  adding competition metric for training using custom Callback for validation and test sets instead of the run() function and 'global epochs' training with \"plain\" BCE again merging TestDataGenerator and TrainDataGenerator into one adding undersampling (see inside on_epoch_end), will now run 6 epochs  Update (3):  skipping/removing windowing (value clipping), but the transformation to Hounsfield Units is kept removing initial layer (doing np.stack((img,)*3, axis=-1)) instead reducing learning rate to 5e-4 and add decay increasing batch size to 32 from 16 Increasing training set to 90% of the data (10% for validation) slight increase in undersampling fixed some hardcoding for input dims/sizes training with weighted BCE again  Update (4):  Trying out InceptionV3, instead of ResNet50 undersampling without weights adding dense layers with dropout before output clipping HUs between -50 and 450 (probably the most relevant value-space?) normalization is now mapping input to 0 to 1 range, instead of -1 to 1. doing 5 epochs instead of 6  Update (5):  Got some inspiration from this great kernel by Ryan Epp Thus I'm trying out the sigmoid (brain + subdural + bone) to see if it improves the log loss Number of epochs reduced to 4, increased undersampling, and validation predictions removed due to limited time  Update (6) (did not improve from (5)):  Going back to raw HUs (with a bit of clipping) Together with a first initial conv layer with sigmoid activation epochs increased to 6 from 4, and input size increased to (256, 256) from (224, 224) simple average of epochs (>1) for the test predictions  Update (7):  Trying windowing based on appian42's repo instead I also include some cleaning based on Jeremy's kernel (hopefully it's correct, atleast the visualization looked good =) weighted average of the epochs (>1) for the test predictions reducing number of epochs to 5  Update (8):  Removing the extra dense layer before output layer (keeping everything else the same)", "link": "https://www.kaggle.com/akensert/rsna-inceptionv3-keras-tf1-14-0", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-07-26 19:34:10", "date_scraped": "2020-12-13 16:47:30", "words": 389, "sentences": 7, "sum_nltk": "Weighted BCE instead of \"plain\" BCE training data lowered to 80% from 90%.\nUpdate (2):  adding competition metric for training using custom Callback for validation and test sets instead of the run() function and 'global epochs' training with \"plain\" BCE again merging TestDataGenerator and TrainDataGenerator into one adding undersampling (see inside on_epoch_end), will now run 6 epochs  Update (3):  skipping/removing windowing (value clipping), but the transformation to Hounsfield Units is kept removing initial layer (doing np.stack((img,)*3, axis=-1)) instead reducing learning rate to 5e-4 and add decay increasing batch size to 32 from 16 Increasing training set to 90% of the data (10% for validation) slight increase in undersampling fixed some hardcoding for input dims/sizes training with weighted BCE again  Update (4):  Trying out InceptionV3, instead of ResNet50 undersampling without weights adding dense layers with dropout before output clipping HUs between -50 and 450 (probably the most relevant value-space?) normalization is now mapping input to 0 to 1 range, instead of -1 to 1.", "sum_nltk_words": 167, "sum_nltk_runtime": 0.004, "sum_t5": "a new convnet is being built in keras for multilabel classification of brain CT scans. batch size lowered to 16 from 32. training without learning rate decay. weighted BCE instead of \"plain\" BCE training data lowered to 80% from 90%. a new kernel is being developed to help with the sigmoid (brain + subdural + bone) classification. a new kernel is being developed to help with the sigmoid (brain + sub", "sum_t5_words": 71, "sum_t5_runtime": 6.954, "runtime": 0.0, "nltk_category": "Utilities", "nltk_category_score": 0.6357777714729309, "nltk_category_runtime": 17.795, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9417986869812012, "nltk_subcategory_runtime": 27.87, "category": "Utilities", "category_score": 0.6357777714729309, "subcategory": "Machine Learning", "subcategory_score": 0.9417986869812012, "runtime_cat": 45.664, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.781", "language_code": "en", "language_score": "0.9999973300899571", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "inceptionv3 previously resnet50 kera baseline modelthis notebook take important step building deep convnet kera multilabel classification brain ct scan update 1 training 4 epoch instead 3 batch size lowered 16 32 training without learning rate decay weighted bce instead plain bce training data lowered 80 90 update 2 adding competition metric training using custom callback validation test set instead run function global epoch training plain bce merging testdatagenerator traindatagenerator one adding undersampling see inside on_epoch_end run 6 epoch update 3 skippingremoving windowing value clipping transformation hounsfield unit kept removing initial layer npstackimg3 axis1 instead reducing learning rate 5e4 add decay increasing batch size 32 16 increasing training set 90 data 10 validation slight increase undersampling fixed hardcoding input dimssizes training weighted bce update 4 trying inceptionv3 instead resnet50 undersampling without weight adding dense layer dropout output clipping hus 50 450 probably relevant valuespace normalization mapping input 0 1 range instead 1 1 5 epoch instead 6 update 5 got inspiration great kernel ryan epp thus im trying sigmoid brain subdural bone see improves log loss number epoch reduced 4 increased undersampling validation prediction removed due limited time update 6 improve 5 going back raw hus bit clipping together first initial conv layer sigmoid activation epoch increased 6 4 input size increased 256 256 224 224 simple average epoch 1 test prediction update 7 trying windowing based appian42s repo instead also include cleaning based jeremys kernel hopefully correct atleast visualization looked good weighted average epoch 1 test prediction reducing number epoch 5 update 8 removing extra dense layer output layer keeping everything else", "tags_descriptive": []}