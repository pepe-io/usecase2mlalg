{"title": "1st-place-solution", "description": "Google Quest Q&A Labeling  1st place solution  by Dmitriy Danevskiy, Oleg Yaroshevskiy, Yury Kashnitsky, and Dmitriy Abulkhanov The purpose of this competition is to analyze StackExchange questions & answers predicting whether the question is interesting, whether the answer is helpful or misleading etc. So in theory, top solutions can help Q&A systems in getting more human-like. In a nutshell, our team trained 4 models: 2 BERT ones, one RoBERTa, and one BART. Key ideas are:  pretraining language models with StackExchange data and auxiliary targets pseudo-labeling postprocessing predictions  Details are outlined in this post, code is shared in this repository.", "link": "https://www.kaggle.com/ddanevskyi/1st-place-solution", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-02-20 11:57:32", "date_scraped": "2020-12-12 20:20:08", "words": 103, "sentences": 4, "runtime": 0.002, "description_category": "Justice, Law and Regulations", "description_category_score": 0.10050012916326523, "description_category_runtime": 11.526, "description_subcategory": "Tools", "description_subcategory_score": 0.27489668130874634, "description_subcategory_runtime": 17.301, "category": "Justice, Law and Regulations", "category_score": 0.10050012916326523, "subcategory": "Tools", "subcategory_score": 0.27489668130874634, "runtime_cat": 28.827, "programming_language": "Jupyter Notebook", "ml_score": "0.7", "engagement_score": "0.752", "language_code": "en", "language_score": "0.9999986425790419", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "google quest qa labeling 1st place solution dmitriy danevskiy oleg yaroshevskiy yury kashnitsky dmitriy abulkhanov purpose competition analyze stackexchange question answer predicting whether question interesting whether answer helpful misleading etc theory top solution help qa system getting humanlike nutshell team trained 4 model 2 bert one one roberta one bart key idea pretraining language model stackexchange data auxiliary target pseudolabeling postprocessing prediction detail outlined post code shared repository", "tags_descriptive": []}