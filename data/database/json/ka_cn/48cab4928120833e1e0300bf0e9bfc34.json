{"title": "How to not overfit?", "description": "General informationIn Don't Overfit! II competition we have a binary classification task. 300 columns, 250 training samples and 79 times more samples in test data! We need to be able to build a model without overfitting. In this kernel I'll write the following things:  EDA on the features and trying to get some insights; Using permutation importance to select most impactful features; Comparing various models: bayer classification, linear models, tree based models; Trying various approaches to feature selection including taking top features from eli5 and shap; Hyperparameter optimization for models; Feature generation; Other things;", "link": "https://www.kaggle.com/artgor/how-to-not-overfit", "tags": ["Classification", "Model Explainability"], "kind": ["Project", "(Notebook)"], "ml_libs": ["xgboost", "lightgbm", "sklearn", "statsmodels", "catboost"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-04-24 16:11:54", "date_scraped": "2020-12-12 19:05:08", "words": 95, "sentences": 3, "runtime": 0.001, "description_category": "Biotechnological & Life Sciences", "description_category_score": 0.1401151716709137, "description_category_runtime": 9.43, "description_subcategory": "General", "description_subcategory_score": 0.9578371644020081, "description_subcategory_runtime": 14.256, "category": "Biotechnological & Life Sciences", "category_score": 0.1401151716709137, "subcategory": "General", "subcategory_score": 0.9578371644020081, "runtime_cat": 23.687, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.792", "language_code": "en", "language_score": "0.9999973165080749", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "general informationin dont overfit ii competition binary classification task 300 column 250 training sample 79 time sample test data need able build model without overfitting kernel ill write following thing eda feature trying get insight using permutation importance select impactful feature comparing various model bayer classification linear model tree based model trying various approach feature selection including taking top feature eli5 shap hyperparameter optimization model feature generation thing", "tags_descriptive": ["Classification", "Model Explainability"]}