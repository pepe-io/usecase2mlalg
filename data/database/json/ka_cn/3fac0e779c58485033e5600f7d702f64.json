{"title": "TPU flowers -  First Love", "description": "About this kernel, about this competitionIntroThis competition will be over in about 2 days and it has been my first Kaggle competition. I am rather a beginner in ML and I want to thank Kaggle for this great opportunity  to learn about tensorflow, classification and tpu's. I started with a public ensemble kernel from Wojtek Rosa and tried a lot of basic hyperparameter tuning. As new discussion entries appeared I could learn a lot about augmentation techniques, under/oversampling, optimizers and other stuff. Kudos to the nice people, that fed the community with their knowlege. I will mention the most important contributions in the later sections. 0) TPU stuffTPUs are impressive and @mgoernergoogle made it easy to understand the basic code, which is necessary to start with tpus. https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu Later he provided a kernel which implemented a custom training loop, that could speed up learning up to 20%. Unfortunately tensorflow 2.1 showed up to be unstable when training 512x512 sized images - this should be fixed in tf 2.2, which has been released 3 days ago. but did not find the way into the Kaggle environment as I write this. https://www.kaggle.com/mgornergoogle/custom-training-loop-with-100-flowers-on-tpu 1) DatasetsWhat could be found in the the beginning was an unbalanced dataset of flowers with 104 classes, which has been nicely assembled from 5 public flower datasets by Martin Goerner. As the competition went on, people incorporated one or more of these public datasets in their training, published them later and it showed that using those datasets could greatly improve LB scores. Thanks to Heng CherKeng, Kirill Blinov and all the others for their contributions. https://www.kaggle.com/c/flower-classification-with-tpus/discussion/140866 https://www.kaggle.com/kirillblinov/tf-flower-photo-tfrec In the last days there has even been a little discussion whether these datasets are allowed to be used. https://www.kaggle.com/c/flower-classification-with-tpus/discussion/148329 2) AugmentationsChris Deotte contributed greatly to this topic, providing notebooks that showed us an implementation Gridmask, CutMix and MixUp augmentations along with his spatial affine transformations. I tried them all and found it very interesting and also introduced me to learn about label smoothing (another technique to handle unbalanced datasets with one-hot encoded class labels). https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96 https://www.kaggle.com/cdeotte/cutmix-and-mixup-on-gpu-tpu https://www.kaggle.com/yihdarshieh/make-chris-deotte-s-data-augmentation-faster https://www.kaggle.com/yihdarshieh/batch-implementation-of-more-data-augmentations https://www.kaggle.com/xiejialun/gridmask-data-augmentation-with-tensorflow Thanks to Micha\u0142 Szachniewicz who implemented AugMix to run under tensorflow 2.x. A pitty that the experiments did not produce nice results. https://www.kaggle.com/szacho/augmix-data-augmentation-on-tpu I found it interesting, how good the rather simple cutout augmentation worked - see the random_blockout() function from a competitor below. I also stumbled about AutoAug, a technique used by Google researchers and in AutoML for classification and now even for object detection, to find the best fitting augmentation parameters for a given dataset. AutoAug can be found in the tensorflow repository on github, but is implemented in tensorflow 1.x and I did not have the time to invest in that. 3) Models and Techniques3.1 ModelsState of the art is the usage of the Effcientnet set of models. Theses models are trained on imagenet and noisy-student - both variations of the weights are available in the Keras version on github. Some people combined one or two Efficientnet model with other models in an ensemble, like it is done in this kernel. Wojtek Rosa provided this starter kernel: https://www.kaggle.com/wrrosa/tpu-enet-b7-densenet 3.2 OptimizersThere is a lot of research going on in this field and computer scientists are proposing a lot of new optimizers these days. I started with Adam and did some experiments, especially with the so called Ranger optimizer (a combination of RectifiedAdam and Lookahead - they can be found in the tensorflow addons library). All in all I did not find success using these, maybe because they converge slower and there is limited training time in this competition -  so I went back to plain Adam. 3.3 Learning Rate and other parametersWhen one is finetuning a model (train all weights of a pretrained model) one should implement a rampup phase for some epochs with a lower learning rate, so one doesn't break the pretrained features. The starter notebook provides a LearningRateScheduler with exponential decay. An alternative would be the usage of a cosine decaying learning rate, which I implemented below in this notebook. I did not try cyclic learning rates, which would have been interesting as well. Btw. - the ranger optimizer likes high flat learning rates in the beginning and cosine annealing. On TPU the initial batch size could be doubled with 512x512 sized images, which really was a big improvement (16  strategy.num_replicas_in_sync  2) I could get a bit better results multiplying the proposed learning rate schedule from the starter kernel by 1.2  3.4 Class WeightsClass weights are a method where one can tell the optimizer to underweight the influence of overrepresented classes. A short piece of code in shown below, but I did not use it at last, because it showed, that the losses are getting smaller more slowly. Maybe more epochs would show that this method leads to a good model, but in this competition we are restricted to a runtime of 3 hours and this is not effective. Further there is doubt whether class weights do work at all in tf2.1 on tpu. 3.5 Oversampling/UndersamplingFor an unbalanced dataset people have found success in training with data, where one filters out examples of the overpresented classes or one extends the dataset with (modified) copies of the underrepresented examples. I did not get lucky with it in this competition. https://www.kaggle.com/yihdarshieh/tutorial-oversample 3.6 Progressive ResizingDuring the competition I read about progressive resizing (to train a model with a smaller image size first and then again with a larger image size) but then a notebook which implemented this using the fastai library, brought me back to this idea in the last days, so I implemented it in this kernel. https://www.kaggle.com/kurianbenoy/classifying-flowers-with-fastaiv2-0-96 3.7 Custom Training LoopAs mentioned above the custom training loop from https://www.kaggle.com/mgornergoogle/custom-training-loop-with-100-flowers-on-tpu can save about 20% training time. 3.8 KFoldsUsing KFolds is the idea of putting together training and validation data in one set and then splitting this set differently K-times, train K models and then aggregate the predictions of these K models. https://www.kaggle.com/ragnar123/4-kfold-densenet201 3.9 TTATTA (test time augmentations) is the idea to augment the test data several times and aggregate the predictions on these data. I did not find success with this, but many successful competitors use it. There is a nice notebook from Caleb about this technique: https://www.kaggle.com/calebeverett/comparison-of-tta-prediction-procedures 3.10 Pseudo labeling test dataI did not try, but well doing competitors probably do. Some time ago Chris Deotte provided a nice summary how it is done: https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969 Using Mish activationsEfficientnet uses the relatively new swish activation function. In the last days I came across an alternative which seems to do better - mish() As exchanging activations functions in Keras on the fly seems to be difficult, I am not sure if I can try something in that direction in the next days. Final wordsMy best result so far (before the last weekend) has been training a 4 model ensemble for 13 epochs with the 512x512 size images with over 40000 images and no TTA (LB score 0.975) and I am rather sure that training for more epochs and using more training images would improve the score a lot. Probably one has to use 224x224 images, For the next weekend I tend to try smaller images sizes with more epochs and images for fun, because the usage of the external datasets is probably not allowed for the final LB run. Maybe some TTA experiments, if time allows. Update I am sorry, I cannot remember whos kernel I copied - but the 4 model one is working great - thx. Training the 4 model ensemble with all external training data pushed me to a LB score of 0.9837 with a training time of 2h20 - this should be a good base for further experiments :) Update Got an 0.984+ score with training 224x224 images. Update Could not test mish() - but training time seems to be 10% slower on EN models with my simple patch. Could not test TTA - I think I simply did it wrong. No way to climb the LB score without these :) Tried to run 48 epochs with a custom training loop, but it seems to use more then 3 hours.... what a pitty :) I really like to try these improvements next week... This notebook will serve as a summary of some of the knowledge I built up and should be able to reach a LB score of 0.967 with the base dataset in one way or another. One should try different models and run it with validation calculations to find the best alpha for ensembling and then submit it training on both (train and val) datasets. Thanks for the fish :)   (to everybody, who doesn't know this quote, pls google Douglas Adams)", "link": "https://www.kaggle.com/romanweilguny/tpu-flowers-first-love", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-05-18 17:58:33", "date_scraped": "2020-12-12 19:56:26", "words": 1454, "sentences": 59, "sum_nltk": "https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu Later he provided a kernel which implemented a custom training loop, that could speed up learning up to 20%.\nhttps://www.kaggle.com/mgornergoogle/custom-training-loop-with-100-flowers-on-tpu 1) DatasetsWhat could be found in the the beginning was an unbalanced dataset of flowers with 104 classes, which has been nicely assembled from 5 public flower datasets by Martin Goerner.\nhttps://www.kaggle.com/c/flower-classification-with-tpus/discussion/148329 2) AugmentationsChris Deotte contributed greatly to this topic, providing notebooks that showed us an implementation Gridmask, CutMix and MixUp augmentations along with his spatial affine transformations.\nhttps://www.kaggle.com/yihdarshieh/tutorial-oversample 3.6 Progressive ResizingDuring the competition I read about progressive resizing (to train a model with a smaller image size first and then again with a larger image size) but then a notebook which implemented this using the fastai library, brought me back to this idea in the last days, so I implemented it in this kernel.\nThere is a nice notebook from Caleb about this technique: https://www.kaggle.com/calebeverett/comparison-of-tta-prediction-procedures 3.10 Pseudo labeling test dataI did not try, but well doing competitors probably do.\nSome time ago Chris Deotte provided a nice summary how it is done: https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969 Using Mish activationsEfficientnet uses the relatively new swish activation function.", "sum_nltk_words": 180, "sum_nltk_runtime": 0.018, "sum_t5": "i am rather a beginner in ML and i want to thank Kaggle for this great opportunity to learn about tensorflow, classification and tpu's. i started with a public ensemble kernel from Wojtek Rosa and tried a lot of basic hyperparameter tuning. as new discussion entries appeared I could learn a lot about augmentation techniques, under/oversampling, optimizers and other stuff. tensorflow 2.1 showed up to be unstable when training", "sum_t5_words": 69, "sum_t5_runtime": 6.963, "runtime": 0.009, "nltk_category": "Accommodation & Food", "nltk_category_score": 0.13681940734386444, "nltk_category_runtime": 27.514, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.6663810610771179, "nltk_subcategory_runtime": 44.197, "category": "Accommodation & Food", "category_score": 0.13681940734386444, "subcategory": "Machine Learning", "subcategory_score": 0.6663810610771179, "runtime_cat": 71.71, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.641", "language_code": "en", "language_score": "0.9999980232189614", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "kernel competitionintrothis competition 2 day first kaggle competition rather beginner ml want thank kaggle great opportunity learn tensorflow classification tpus started public ensemble kernel wojtek rosa tried lot basic hyperparameter tuning new discussion entry appeared could learn lot augmentation technique underoversampling optimizers stuff kudos nice people fed community knowlege mention important contribution later section 0 tpu stufftpus impressive mgoernergoogle made easy understand basic code necessary start tpus httpswwwkagglecommgornergooglegettingstartedwith100flowersontpu later provided kernel implemented custom training loop could speed learning 20 unfortunately tensorflow 21 showed unstable training 512x512 sized image fixed tf 22 released 3 day ago find way kaggle environment write httpswwwkagglecommgornergooglecustomtrainingloopwith100flowersontpu 1 datasetswhat could found beginning unbalanced dataset flower 104 class nicely assembled 5 public flower datasets martin goerner competition went people incorporated one public datasets training published later showed using datasets could greatly improve lb score thanks heng cherkeng kirill blinov others contribution httpswwwkagglecomcflowerclassificationwithtpusdiscussion140866 httpswwwkagglecomkirillblinovtfflowerphototfrec last day even little discussion whether datasets allowed used httpswwwkagglecomcflowerclassificationwithtpusdiscussion148329 2 augmentationschris deotte contributed greatly topic providing notebook showed u implementation gridmask cutmix mixup augmentation along spatial affine transformation tried found interesting also introduced learn label smoothing another technique handle unbalanced datasets onehot encoded class label httpswwwkagglecomcdeotterotationaugmentationgputpu096 httpswwwkagglecomcdeottecutmixandmixupongputpu httpswwwkagglecomyihdarshiehmakechrisdeottesdataaugmentationfaster httpswwwkagglecomyihdarshiehbatchimplementationofmoredataaugmentations httpswwwkagglecomxiejialungridmaskdataaugmentationwithtensorflow thanks micha szachniewicz implemented augmix run tensorflow 2x pitty experiment produce nice result httpswwwkagglecomszachoaugmixdataaugmentationontpu found interesting good rather simple cutout augmentation worked see random_blockout function competitor also stumbled autoaug technique used google researcher automl classification even object detection find best fitting augmentation parameter given dataset autoaug found tensorflow repository github implemented tensorflow 1x time invest 3 model techniques31 modelsstate art usage effcientnet set model thesis model trained imagenet noisystudent variation weight available kera version github people combined one two efficientnet model model ensemble like done kernel wojtek rosa provided starter kernel httpswwwkagglecomwrrosatpuenetb7densenet 32 optimizersthere lot research going field computer scientist proposing lot new optimizers day started adam experiment especially called ranger optimizer combination rectifiedadam lookahead found tensorflow addons library find success using maybe converge slower limited training time competition went back plain adam 33 learning rate parameterswhen one finetuning model train weight pretrained model one implement rampup phase epoch lower learning rate one doesnt break pretrained feature starter notebook provides learningratescheduler exponential decay alternative would usage cosine decaying learning rate implemented notebook try cyclic learning rate would interesting well btw ranger optimizer like high flat learning rate beginning cosine annealing tpu initial batch size could doubled 512x512 sized image really big improvement 16 strategynum_replicas_in_sync 2 could get bit better result multiplying proposed learning rate schedule starter kernel 12 34 class weightsclass weight method one tell optimizer underweight influence overrepresented class short piece code shown use last showed loss getting smaller slowly maybe epoch would show method lead good model competition restricted runtime 3 hour effective doubt whether class weight work tf21 tpu 35 oversamplingundersamplingfor unbalanced dataset people found success training data one filter example overpresented class one extends dataset modified copy underrepresented example get lucky competition httpswwwkagglecomyihdarshiehtutorialoversample 36 progressive resizingduring competition read progressive resizing train model smaller image size first larger image size notebook implemented using fastai library brought back idea last day implemented kernel httpswwwkagglecomkurianbenoyclassifyingflowerswithfastaiv2096 37 custom training loopas mentioned custom training loop httpswwwkagglecommgornergooglecustomtrainingloopwith100flowersontpu save 20 training time 38 kfoldsusing kfolds idea putting together training validation data one set splitting set differently ktimes train k model aggregate prediction k model httpswwwkagglecomragnar1234kfolddensenet201 39 ttatta test time augmentation idea augment test data several time aggregate prediction data find success many successful competitor use nice notebook caleb technique httpswwwkagglecomcalebeverettcomparisonofttapredictionprocedures 310 pseudo labeling test datai try well competitor probably time ago chris deotte provided nice summary done httpswwwkagglecomcdeottepseudolabelingqda0969 using mish activationsefficientnet us relatively new swish activation function last day came across alternative seems better mish exchanging activation function kera fly seems difficult sure try something direction next day final wordsmy best result far last weekend training 4 model ensemble 13 epoch 512x512 size image 40000 image tta lb score 0975 rather sure training epoch using training image would improve score lot probably one use 224x224 image next weekend tend try smaller image size epoch image fun usage external datasets probably allowed final lb run maybe tta experiment time allows update sorry cannot remember who kernel copied 4 model one working great thx training 4 model ensemble external training data pushed lb score 09837 training time 2h20 good base experiment update got 0984 score training 224x224 image update could test mish training time seems 10 slower en model simple patch could test tta think simply wrong way climb lb score without tried run 48 epoch custom training loop seems use 3 hour pitty really like try improvement next week notebook serve summary knowledge built able reach lb score 0967 base dataset one way another one try different model run validation calculation find best alpha ensembling submit training train val datasets thanks fish everybody doesnt know quote pls google douglas adam", "tags_descriptive": []}