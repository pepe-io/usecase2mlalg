{"title": "iMet: ResNet50 Keras Starter", "description": "IntroductionModified from https://www.kaggle.com/mathormad/pretrained-resnet50-focal-loss/notebook?scriptVersionId=12746542 (we have tried to catch up the Pytorch :) To see all my personal modification, please use diff from the original kernel to the latest version. The best score is at version 17. But that version used ResNet50 instead of 'should-be-better' ResNet50V2 In the current version, I try to improve the performance by change back from DenseNet121 to ResNet50. For the previous DenseNet version with Acknowledgement, please take a look at version 9. Modification listResNet50 Change LOG V13 [LB553, CV546] PixSize224, LR1e-4, Batch64, 2048Dense-Head, 30Epochs V14 [LB, CV549] Try VALID_SPLIT0.1, 29Epochs V15 Try batch96 << runtime exceed, from LOG file, there is a memory issue that degrades performance too V16 [CV528] Batch32, Epoch24 << finish too early & final LR too small V17 [CV551, LB560] Batch88, Epoch27  V18 [CV Bad] gamma=3 V19 [CV Bad] gamma=4 V20 [CV 524] change back to gamma=1 and use ResNetV2 weights -- thanks @mathormad again! V21[CV 381!] change Hyperparameter V22[CV526] batch64, LR1e-4 V23[CV511] batch48, LR8e-5 V24 Fix bugs on f2, my_f2 functions, try a bit deeper head to digest more information at the end V25-27 try V17 again [FAILED] V28 [] make a 'variable-size' input augmentation modification (resize later) - ResNetV1 V28-34 Bugs V34 Bug Fixed [V2] V35-36[CV558 / 531] increase LR, adjust ReduceOnPlateau for both V1&V2 -- After play around with Lopuhin's great kernel on pytorch, I try to make a learning process converge faster and better << Better results V37-38 increase LR a bit more to force the network to converge earlier   ToDo  Deal with this issue : http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/", "link": "https://www.kaggle.com/ratthachat/imet-resnet50-keras-starter", "tags": ["DL"], "kind": ["Project", "(Notebook)"], "ml_libs": ["keras", "skimage", "tensorflow", "sklearn", "pytorch"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-05-12 11:39:26", "date_scraped": "2020-12-13 11:40:08", "words": 263, "sentences": 5, "sum_nltk": "IntroductionModified from https://www.kaggle.com/mathormad/pretrained-resnet50-focal-loss/notebook?scriptVersionId=12746542 (we have tried to catch up the Pytorch :) To see all my personal modification, please use diff from the original kernel to the latest version.\nBut that version used ResNet50 instead of 'should-be-better' ResNet50V2 In the current version, I try to improve the performance by change back from DenseNet121 to ResNet50.\nModification listResNet50 Change LOG V13 [LB553, CV546] PixSize224, LR1e-4, Batch64, 2048Dense-Head, 30Epochs V14 [LB, CV549] Try VALID_SPLIT0.1, 29Epochs V15 Try batch96 << runtime exceed, from LOG file, there is a memory issue that degrades performance too V16 [CV528] Batch32, Epoch24 << finish too early & final LR too small V17 [CV551, LB560] Batch88, Epoch27  V18 [CV Bad] gamma=3 V19 [CV Bad] gamma=4 V20 [CV 524] change back to gamma=1 and use ResNetV2 weights -- thanks @mathormad again!\nV21[CV 381!] change Hyperparameter V22[CV526] batch64, LR1e-4 V23[CV511] batch48, LR8e-5 V24 Fix bugs on f2, my_f2 functions, try a bit deeper head to digest more information at the end V25-27 try V17 again [FAILED] V28 [] make a 'variable-size' input augmentation modification (resize later) - ResNetV1 V28-34 Bugs V34 Bug Fixed [V2] V35-36[CV558 / 531] increase LR, adjust ReduceOnPlateau for both V1&V2 -- After play around with Lopuhin's great kernel on pytorch, I try to make a learning process converge faster and better << Better results V37-38 increase LR a bit more to force the network to converge earlier   ToDo  Deal with this issue : http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/", "sum_nltk_words": 239, "sum_nltk_runtime": 0.002, "sum_t5": "the best score is at version 17. But that version used ResNet50 instead of'should-be-better' ResNet50V2 In the current version, I try to improve the performance by change back from DenseNet121 to ResNet50. for the previous DenseNet version with Acknowledgement, please take a look at version 9. For the previous DenseNet version with Acknowledgement, please take a look at version 9.", "sum_t5_words": 60, "sum_t5_runtime": 6.303, "runtime": 0.004, "nltk_category": "Utilities", "nltk_category_score": 0.32232344150543213, "nltk_category_runtime": 36.78, "nltk_subcategory": "Failure", "nltk_subcategory_score": 0.9524608254432678, "nltk_subcategory_runtime": 59.994, "category": "Utilities", "category_score": 0.32232344150543213, "subcategory": "Failure", "subcategory_score": 0.9524608254432678, "runtime_cat": 96.774, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.716", "language_code": "en", "language_score": "0.9999972044853473", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introductionmodified httpswwwkagglecommathormadpretrainedresnet50focallossnotebookscriptversionid12746542 tried catch pytorch see personal modification please use diff original kernel latest version best score version 17 version used resnet50 instead shouldbebetter resnet50v2 current version try improve performance change back densenet121 resnet50 previous densenet version acknowledgement please take look version 9 modification listresnet50 change log v13 lb553 cv546 pixsize224 lr1e4 batch64 2048densehead 30epochs v14 lb cv549 try valid_split01 29epochs v15 try batch96 runtime exceed log file memory issue degrades performance v16 cv528 batch32 epoch24 finish early final lr small v17 cv551 lb560 batch88 epoch27 v18 cv bad gamma3 v19 cv bad gamma4 v20 cv 524 change back gamma1 use resnetv2 weight thanks mathormad v21cv 381 change hyperparameter v22cv526 batch64 lr1e4 v23cv511 batch48 lr8e5 v24 fix bug f2 my_f2 function try bit deeper head digest information end v2527 try v17 failed v28 make variablesize input augmentation modification resize later resnetv1 v2834 bug v34 bug fixed v2 v3536cv558 531 increase lr adjust reduceonplateau v1v2 play around lopuhins great kernel pytorch try make learning process converge faster better better result v3738 increase lr bit force network converge earlier todo deal issue httpblogdatumboxcomthebatchnormalizationlayerofkerasisbroken", "tags_descriptive": ["Deep Learning (DL)"]}