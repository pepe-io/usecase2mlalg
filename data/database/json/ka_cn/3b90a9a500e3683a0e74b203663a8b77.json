{"title": "7th place- clustering, extending, ML merging: 0.75", "description": "Chapter 1: IntroductionThe kernel demonstrates the clustering and expending we used to get to 7# place. Running this kernel on training event 1000 will score ~0.635 after the clustering stage, and 0.735 after expending stage. Every stage takes about 8-10 min on Kaggle, and about half the time on my laptop. In the clustering part of the kernel the algorithm uses 5.500 pairs of z0, 1/2R (more on it below). By increasing the number to about 100.000 pairs the score will plateau at about 0.765 (after expending). How does it work: In each clustering loop the algorithm try to find all tracks originating from (0,0,z0) and with a radius of 1/(2*kt). If a hit (x,y,z) is on a track the helix can be fully defined by the following features (1), (2) rr=(x^2+y^2)^0.5 theta=arctan(y/x) dtheta = arcsin(kt*rr) (1) Theta=theta+dtheta (2) (z-z0)*kt/dtheta To solve the +pi,-pi problem we use sin, cos for theta. To make (2) more uniform, we use arctan((z-z0)/(3.3*dtheta/kt)) After calculating the features, the algorithm tries to cluster all the hits with the same features. This is done by sparse binning \u2013 using np.unique. The disadvantage of sparse binning over dbscan is it\u2019s sensitivity, the advantages are its speed and its sensitivity (almost no outliners). After clustering every hit choose if his cluster is good according to the clusters length. Every 500 loops all hits belonging to tracks which are long enough are removed from the dataset If two hits from the same detector are on the same track, the one which is closest to the track\u2019s center of mass is chosen. The z0, kt pairs are chosen randomly. While running, the algorithm changes the bin width and the length of the minimum track to be extracted from the dataset. Expending is done by selecting the un-clustered hits which are close to the center of mass of the track.", "link": "https://www.kaggle.com/yuval6967/7th-place-clustering-extending-ml-merging-0-75", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-08-17 23:26:35", "date_scraped": "2020-12-13 17:43:28", "words": 309, "sentences": 15, "sum_nltk": "Running this kernel on training event 1000 will score ~0.635 after the clustering stage, and 0.735 after expending stage.\nIn the clustering part of the kernel the algorithm uses 5.500 pairs of z0, 1/2R (more on it below).\nHow does it work: In each clustering loop the algorithm try to find all tracks originating from (0,0,z0) and with a radius of 1/(2*kt).\nIf a hit (x,y,z) is on a track the helix can be fully defined by the following features (1), (2) rr=(x^2+y^2)^0.5 theta=arctan(y/x) dtheta = arcsin(kt*rr) (1) Theta=theta+dtheta (2) (z-z0)*kt/dtheta To solve the +pi,-pi problem we use sin, cos for theta.\nTo make (2) more uniform, we use arctan((z-z0)/(3.3*dtheta/kt)) After calculating the features, the algorithm tries to cluster all the hits with the same features.\nEvery 500 loops all hits belonging to tracks which are long enough are removed from the dataset If two hits from the same detector are on the same track, the one which is closest to the track\u2019s center of mass is chosen.\nWhile running, the algorithm changes the bin width and the length of the minimum track to be extracted from the dataset.\nExpending is done by selecting the un-clustered hits which are close to the center of mass of the track.", "sum_nltk_words": 201, "sum_nltk_runtime": 0.003, "sum_t5": "running this kernel on training event 1000 will score 0.635 after clustering stage, and 0.735 after expending stage. in the clustering part of the kernel the algorithm uses 5.500 pairs of z0, 1/2R. by increasing the number to about 100.000 pairs the score will plateau at about 0.765 (after expending) after calculating the features, the algorithm tries to cluster all the hits with the same features. after clustering every hit choose if his cluster is good according to the clusters length", "sum_t5_words": 81, "sum_t5_runtime": 6.981, "runtime": 0.0, "nltk_category": "Utilities", "nltk_category_score": 0.3185405433177948, "nltk_category_runtime": 24.526, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9577755331993103, "nltk_subcategory_runtime": 39.398, "category": "Utilities", "category_score": 0.3185405433177948, "subcategory": "Machine Learning", "subcategory_score": 0.9577755331993103, "runtime_cat": 63.924, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.68", "language_code": "en", "language_score": "0.9999982850230467", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "chapter 1 introductionthe kernel demonstrates clustering expending used get 7 place running kernel training event 1000 score 0635 clustering stage 0735 expending stage every stage take 810 min kaggle half time laptop clustering part kernel algorithm us 5500 pair z0 12r increasing number 100000 pair score plateau 0765 expending work clustering loop algorithm try find track originating 00z0 radius 12kt hit xyz track helix fully defined following feature 1 2 rrx2y205 thetaarctanyx dtheta arcsinktrr 1 thetathetadtheta 2 zz0ktdtheta solve pipi problem use sin co theta make 2 uniform use arctanzz033dthetakt calculating feature algorithm try cluster hit feature done sparse binning using npunique disadvantage sparse binning dbscan sensitivity advantage speed sensitivity almost outliners clustering every hit choose cluster good according cluster length every 500 loop hit belonging track long enough removed dataset two hit detector track one closest track center mass chosen z0 kt pair chosen randomly running algorithm change bin width length minimum track extracted dataset expending done selecting unclustered hit close center mass track", "tags_descriptive": []}