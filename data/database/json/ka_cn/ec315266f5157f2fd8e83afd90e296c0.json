{"title": "Stochastic Gradient Boosting with XGBoost", "description": "If you like this kernel Greatly Appreciate to UPVOTE .Stochastic Gradient Boosting with XGBoostA simple technique for ensembling decision trees involves training trees on subsamples of the training dataset. Subsets of the the rows in the training data can be taken to train individual trees called bagging. When subsets of rows of the training data are also taken when calculating each split point, this is called random forest. These techniques can also be used in the gradient tree boosting model in a technique called stochastic gradient boosting. In this kernel I will be demonstrating stochastic gradient boosting and how to tune the sampling parameters using XGBoost with scikit-learn in Python. After reading this kernel we will get to know the following point in detail:  The rationale behind training trees on subsamples of data and how this can be used in gradient boosting. How to tune row-based subsampling in XGBoost using scikit-learn? How to tune column-based subsampling by both tree and split-point in XGBoost?  What is Stochastic Gradient Boosting? Let's understand the concept in detailGradient boosting is a greedy procedure. New decision trees are added to the model to correct the residual error of the existing model. Each decision tree is created using a greedy search procedure to select split points that best minimize an objective function. This can result in trees that use the same attributes and even the same split points again and again. Bagging is a technique where a collection of decision trees are created, each from a different random subset of rows from the training data. The effect is that better performance is achieved from the ensemble of trees because the randomness in the sample allows slightly different trees to be created, adding variance to the ensembled predictions. Random forest takes this one step further, by allowing the features (columns) to be subsampled when choosing split points, adding further variance to the ensemble of trees. These same techniques can be used in the construction of decision trees in gradient boosting in a variation called stochastic gradient boosting. It is common to use aggressive sub-samples of the training data such as 40% to 80%. OverviewIn this kernel we are going to look at the effect of different subsampling techniques in gradient boosting. We will tune three different flavors of stochastic gradient boosting supported by the XGBoost library in Python, specifically: 1.  Subsampling of rows in the dataset when creating each tree. 2. Subsampling of columns in the dataset when creating each tree. 3. Subsampling of columns for each split in the dataset when creating each tree. DatasetWe will use the Otto Group Product Classification Challenge dataset available in Kaggle which is available for free. This dataset describes the 93 obfuscated details of more than 61,000 products grouped into 10 product categories (e.g. fashion, electronics, etc.). Input attributes are counts of different events of some kind.The Otto Group is one of the world\u2019s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate & Barrel (USA), Otto.de (Germany) and 3 Suisses (France) selling millions of products worldwide every day, with several thousand products being added to our product line. Data fields id - an anonymous id unique to a product feat_1, feat_2, ..., feat_93 - the various features of a product target - the class of a product  Problem DescriptionThe goal is to make predictions for new products as an array of probabilities for each of the 10 categories and models are evaluated using multiclass logarithmic loss (also called cross entropy). Solution ApproachAs mentioned above let us look at the appraoches one by one.", "link": "https://www.kaggle.com/pavansanagapati/stochastic-gradient-boosting-with-xgboost", "tags": ["Xgboost", "Feature Engineering"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-09-15 07:10:56", "date_scraped": "2020-12-13 14:52:36", "words": 601, "sentences": 28, "sum_nltk": "If you like this kernel Greatly Appreciate to UPVOTE .Stochastic Gradient Boosting with XGBoostA simple technique for ensembling decision trees involves training trees on subsamples of the training dataset.\nWhen subsets of rows of the training data are also taken when calculating each split point, this is called random forest.\nIn this kernel I will be demonstrating stochastic gradient boosting and how to tune the sampling parameters using XGBoost with scikit-learn in Python.\nAfter reading this kernel we will get to know the following point in detail:  The rationale behind training trees on subsamples of data and how this can be used in gradient boosting.\nHow to tune column-based subsampling by both tree and split-point in XGBoost?\nBagging is a technique where a collection of decision trees are created, each from a different random subset of rows from the training data.\nRandom forest takes this one step further, by allowing the features (columns) to be subsampled when choosing split points, adding further variance to the ensemble of trees.\nOverviewIn this kernel we are going to look at the effect of different subsampling techniques in gradient boosting.\n3. Subsampling of columns for each split in the dataset when creating each tree.", "sum_nltk_words": 193, "sum_nltk_runtime": 0.008, "sum_t5": "a simple technique for ensembling decision trees involves training trees on subsamples of the training dataset. bagging is a technique where a collection of decision trees are created, each from a different random subset of rows from the training data. when subsets of rows of the training data are also taken when calculating each split point, this is called random forest. this kernel demonstrates stochastic gradient boosting and how to tune the sampling parameters using XGBoost.", "sum_t5_words": 76, "sum_t5_runtime": 6.774, "runtime": 0.002, "nltk_category": "Education & Research", "nltk_category_score": 0.48347505927085876, "nltk_category_runtime": 18.414, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8902406692504883, "nltk_subcategory_runtime": 29.804, "category": "Education & Research", "category_score": 0.48347505927085876, "subcategory": "Machine Learning", "subcategory_score": 0.8902406692504883, "runtime_cat": 48.219, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.682", "language_code": "en", "language_score": "0.9999979240952883", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "like kernel greatly appreciate upvote stochastic gradient boosting xgboosta simple technique ensembling decision tree involves training tree subsamples training dataset subset row training data taken train individual tree called bagging subset row training data also taken calculating split point called random forest technique also used gradient tree boosting model technique called stochastic gradient boosting kernel demonstrating stochastic gradient boosting tune sampling parameter using xgboost scikitlearn python reading kernel get know following point detail rationale behind training tree subsamples data used gradient boosting tune rowbased subsampling xgboost using scikitlearn tune columnbased subsampling tree splitpoint xgboost stochastic gradient boosting let understand concept detailgradient boosting greedy procedure new decision tree added model correct residual error existing model decision tree created using greedy search procedure select split point best minimize objective function result tree use attribute even split point bagging technique collection decision tree created different random subset row training data effect better performance achieved ensemble tree randomness sample allows slightly different tree created adding variance ensembled prediction random forest take one step allowing feature column subsampled choosing split point adding variance ensemble tree technique used construction decision tree gradient boosting variation called stochastic gradient boosting common use aggressive subsamples training data 40 80 overviewin kernel going look effect different subsampling technique gradient boosting tune three different flavor stochastic gradient boosting supported xgboost library python specifically 1 subsampling row dataset creating tree 2 subsampling column dataset creating tree 3 subsampling column split dataset creating tree datasetwe use otto group product classification challenge dataset available kaggle available free dataset describes 93 obfuscated detail 61000 product grouped 10 product category eg fashion electronics etc input attribute count different event kindthe otto group one world biggest ecommerce company subsidiary 20 country including crate barrel usa ottode germany 3 suisse france selling million product worldwide every day several thousand product added product line data field id anonymous id unique product feat_1 feat_2 feat_93 various feature product target class product problem descriptionthe goal make prediction new product array probability 10 category model evaluated using multiclass logarithmic loss also called cross entropy solution approachas mentioned let u look appraoches one one", "tags_descriptive": ["Xgboost", "Feature Engineering"]}