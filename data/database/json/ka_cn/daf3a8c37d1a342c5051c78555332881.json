{"title": "Know Your Objective", "description": "Let us see if we can improve the basic model in several public kernels by replacing the training objective with the actual objective! Updates:  removed some nonsense features (like mjd_max) changed mjd_diff to det_mjd_diff (detected observations window length) used tensorflow eager mode as an alternative to PyTorch. Hessian is also working now. Note that Hessian is only used as weights in LightGBM/XGBoost, so its scale does not matter. However, the hyperparamter 'min_child_weight'/'min_sum_hessian_in_leaf' can mess with the Hessian scale. It is easier to simply set it to 0. Using Hessian does not seem to impact prediction quality much. learning rate scale is in line with what you can expect from vanilla LightGBM now. There does not seem to be any difference between using clipping or simple log softmax in the objective.", "link": "https://www.kaggle.com/mithrillion/know-your-objective", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pytorch", "tensorflow", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-11-19 02:41:27", "date_scraped": "2020-12-13 15:38:22", "words": 131, "sentences": 8, "runtime": 0.0, "description_category": "Manufacturing", "description_category_score": 0.29865342378616333, "description_category_runtime": 13.987, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.9654934406280518, "description_subcategory_runtime": 21.96, "category": "Manufacturing", "category_score": 0.29865342378616333, "subcategory": "Machine Learning", "subcategory_score": 0.9654934406280518, "runtime_cat": 35.947, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.716", "language_code": "en", "language_score": "0.9999978579438109", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "let u see improve basic model several public kernel replacing training objective actual objective update removed nonsense feature like mjd_max changed mjd_diff det_mjd_diff detected observation window length used tensorflow eager mode alternative pytorch hessian also working note hessian used weight lightgbmxgboost scale matter however hyperparamter min_child_weightmin_sum_hessian_in_leaf mess hessian scale easier simply set 0 using hessian seem impact prediction quality much learning rate scale line expect vanilla lightgbm seem difference using clipping simple log softmax objective", "tags_descriptive": []}