{"title": "Feature Selection", "description": "Introduction: Feature SelectionIn this notebook we will apply feature engineering to the manual engineered features built in two previous kernels. We will reduce the number of features using several methods and then we will test the performance of the features using a fairly basic gradient boosting machine model. The main takeaways from this notebook are:  Going from 1465 total features to 536 and an AUC ROC of 0.783 on the public leaderboard A further optional step to go to 342 features and an AUC ROC of 0.782  The full set of features was built in Part One and Part Two of Manual Feature Engineering We will use three methods for feature selection:  Remove collinear features Remove features with greater than a threshold percentage of missing values Keep only the most relevant features using feature importances from a model  We will also take a look at an example of applying PCA although we will not use this method for feature reduction.", "link": "https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-06-22 03:21:28", "date_scraped": "2020-12-12 20:47:32", "words": 164, "sentences": 3, "runtime": 0.0, "description_category": "Construction & Engineering", "description_category_score": 0.19038164615631104, "description_category_runtime": 13.325, "description_subcategory": "Student", "description_subcategory_score": 0.21779753267765045, "description_subcategory_runtime": 20.225, "category": "Construction & Engineering", "category_score": 0.19038164615631104, "subcategory": "Student", "subcategory_score": 0.21779753267765045, "runtime_cat": 33.551, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.798", "language_code": "en", "language_score": "0.9999988154050226", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introduction feature selectionin notebook apply feature engineering manual engineered feature built two previous kernel reduce number feature using several method test performance feature using fairly basic gradient boosting machine model main takeaway notebook going 1465 total feature 536 auc roc 0783 public leaderboard optional step go 342 feature auc roc 0782 full set feature built part one part two manual feature engineering use three method feature selection remove collinear feature remove feature greater threshold percentage missing value keep relevant feature using feature importance model also take look example applying pca although use method feature reduction", "tags_descriptive": []}