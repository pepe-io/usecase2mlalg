{"title": "Elo Adversarial Validation", "description": "One of the most important things to establish for every Kaggle competition is whether there is a significatn differnece in distributions of the train and test sets. So far the CV validation scores for kernels and for public LB have been pretty close, but the local CV seems to be consistently about 0.01 better than the LB scores. It would be interesting, and potentially very valuable, to find out in a more quantitative and specific way how do these distributions compare. For that purpose we'll build an adverserial validation scheme - we'll run a CV classifier that tries to predict if any given question belongs to the train or the test set. Firts, we'll have to deal with data gregation adn building of the combined train and test datasets. This work has already been doen in many of the kernels, and in this kernel we'll realy on Rahul Bamola's excelent kernel.", "link": "https://www.kaggle.com/tunguz/elo-adversarial-validation", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-12-30 19:39:58", "date_scraped": "2020-12-12 19:19:32", "words": 151, "sentences": 6, "runtime": 0.002, "description_category": "Biotechnological & Life Sciences", "description_category_score": 0.10497668385505676, "description_category_runtime": 13.068, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.6067352294921875, "description_subcategory_runtime": 20.399, "category": "Biotechnological & Life Sciences", "category_score": 0.10497668385505676, "subcategory": "Machine Learning", "subcategory_score": 0.6067352294921875, "runtime_cat": 33.469, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.72", "language_code": "en", "language_score": "0.9999967229669633", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "one important thing establish every kaggle competition whether significatn differnece distribution train test set far cv validation score kernel public lb pretty close local cv seems consistently 001 better lb score would interesting potentially valuable find quantitative specific way distribution compare purpose well build adverserial validation scheme well run cv classifier try predict given question belongs train test set firts well deal data gregation adn building combined train test datasets work already doen many kernel kernel well realy rahul bamolas excelent kernel", "tags_descriptive": []}