{"title": "3 Clusters Per Class - [0.975]", "description": "QDA + Pseudo Labeling + Gaussian Mixture = LB 0.975The dataset for Kaggle competition \"Instant Gratification\" appears to be 512 datasets concatenated where each sub dataset is believed to be created by Sklearn's make_classification. EDA suggests the following parameters: X, y = make_classification(n_samples=1024, n_features=255, n_informative=33+x,             n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=3,             weights=None, flip_y=0.05, class_sep=1.0, hypercube=True, shift=0.0,              scale=1.0, shuffle=True, random_state=None) # where 0<=x<=14   The important parameters to note are n_clusters_per_class=3 and n_informative=33+x. This means that the data resides in 33+x dimensional space within 6 hyper-ellipsoids. Each hyper-ellipsoid is a multivariate Gaussian distribution therefore the best classifiers to use are QDA, Pseudo Labeling, and Gaussian Mixture. (See appendix for EDA showing 3 clusters per class).  Better than Perfect Classifier!If Kaggle generated the data with the above function call to make_classification then Many participants will submit a perfect classifier since the data is easy to separate using QDA and GM. Therefore to win this Kaggle competition, we must submit a better than perfect classifier! The code presented in this kernel has randomness added (and marked in the code below) which allows this classifier to score as much as LB 0.00050 better than a perfect classifier. If the data is made with Sklearn's make_classification, perfect classification is classifying everything correctly except the 2.5% randomly flipped labels. No model can consistently predict the flipped labels correct. However, if you add randomness to your model sometimes it will do better than perfect (and get some flipped targets correct) and sometimes it will do worse. Below is a scatter plot of this kernel's performance. The dotted line is a perfect classifier and each dot is one attempt of this kernel to classify a synthetic dataset (that is similar to this comp's data). The 200 dots represent 10 attempts made on each of 20 different randomly created synthetic datasets. The black dotted lines were determined by modifying make_classification to output the AUC of perfect classification. (See Appendix 3 for more info).  Besides indicating perfect classification on average, this scatter plot also shows that there is no correlation between this kernel's public LB and private LB performance. Therefore for our final submission, we will not choose our highest public LB submission. We will run this kernel 30 times and submit two versions chosen at random regardless of their public LB performance. Then we cross our fingers and hope that those two runs score better than perfect private test dataset classification :P", "link": "https://www.kaggle.com/cdeotte/3-clusters-per-class-0-975", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pattern"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-06-23 14:02:34", "date_scraped": "2020-12-13 11:53:26", "words": 441, "sentences": 19, "sum_nltk": "QDA + Pseudo Labeling + Gaussian Mixture = LB 0.975The dataset for Kaggle competition \"Instant Gratification\" appears to be 512 datasets concatenated where each sub dataset is believed to be created by Sklearn's make_classification.\nBetter than Perfect Classifier!If Kaggle generated the data with the above function call to make_classification then Many participants will submit a perfect classifier since the data is easy to separate using QDA and GM.\nThe code presented in this kernel has randomness added (and marked in the code below) which allows this classifier to score as much as LB 0.00050 better than a perfect classifier.\nIf the data is made with Sklearn's make_classification, perfect classification is classifying everything correctly except the 2.5% randomly flipped labels.\nThe dotted line is a perfect classifier and each dot is one attempt of this kernel to classify a synthetic dataset (that is similar to this comp's data).\nBesides indicating perfect classification on average, this scatter plot also shows that there is no correlation between this kernel's public LB and private LB performance.\nWe will run this kernel 30 times and submit two versions chosen at random regardless of their public LB performance.", "sum_nltk_words": 186, "sum_nltk_runtime": 0.005, "sum_t5": "a dataset for a Kaggle competition appears to be 512 datasets concatenated. the dataset for \"Instant Gratification\" appears to be 512 datasets concatenated. the important parameters to note are n_clusters_per_class=3 and n_informative=33+x. the code presented in this kernel has randomness added. this classifier can score as much as LB 0.00050 better than a perfect classifier.", "sum_t5_words": 55, "sum_t5_runtime": 6.975, "runtime": 0.005, "nltk_category": "Finance", "nltk_category_score": 0.14250290393829346, "nltk_category_runtime": 17.872, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8362036943435669, "nltk_subcategory_runtime": 28.878, "category": "Finance", "category_score": 0.14250290393829346, "subcategory": "Machine Learning", "subcategory_score": 0.8362036943435669, "runtime_cat": 46.75, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.725", "language_code": "en", "language_score": "0.999997621201623", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "qda pseudo labeling gaussian mixture lb 0975the dataset kaggle competition instant gratification appears 512 datasets concatenated sub dataset believed created sklearns make_classification eda suggests following parameter x make_classificationn_samples1024 n_features255 n_informative33x n_redundant0 n_repeated0 n_classes2 n_clusters_per_class3 weightsnone flip_y005 class_sep10 hypercubetrue shift00 scale10 shuffletrue random_statenone 0x14 important parameter note n_clusters_per_class3 n_informative33x mean data resides 33x dimensional space within 6 hyperellipsoids hyperellipsoid multivariate gaussian distribution therefore best classifier use qda pseudo labeling gaussian mixture see appendix eda showing 3 cluster per class better perfect classifierif kaggle generated data function call make_classification many participant submit perfect classifier since data easy separate using qda gm therefore win kaggle competition must submit better perfect classifier code presented kernel randomness added marked code allows classifier score much lb 000050 better perfect classifier data made sklearns make_classification perfect classification classifying everything correctly except 25 randomly flipped label model consistently predict flipped label correct however add randomness model sometimes better perfect get flipped target correct sometimes worse scatter plot kernel performance dotted line perfect classifier dot one attempt kernel classify synthetic dataset similar comp data 200 dot represent 10 attempt made 20 different randomly created synthetic datasets black dotted line determined modifying make_classification output auc perfect classification see appendix 3 info besides indicating perfect classification average scatter plot also show correlation kernel public lb private lb performance therefore final submission choose highest public lb submission run kernel 30 time submit two version chosen random regardless public lb performance cross finger hope two run score better perfect private test dataset classification p", "tags_descriptive": []}