{"title": "BaggingRegressor + RAPIDS Ensemble", "description": "IntroductionIn this competition a significant delta between local CV and LB scores has been reported in some cases (https://www.kaggle.com/c/trends-assessment-prediction/discussion/153256). We have many features to work with... maybe too many. Reducing variance would seem to be a good thing here and I wanted to investigate the BaggingRegressor for that. The idea is to use the BaggingRegressor to build multiple models, each considering only a fraction of the features, then combine their outputs. From the scikit-learn docs: \"A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\" Ridge regression is known to work well on this dataset, so is used as the base regressor here. The use of the BaggingRegressor is considered as part of a high-performing ensemble, combining SVM and Ridge regression. This notebook is heavily based on @aerdem4's excellent SVM notebook and @tunguz's notebook that adds Ridge regression. Those original notebooks can be found here: https://www.kaggle.com/aerdem4/rapids-svm-on-trends-neuroimaging https://www.kaggle.com/tunguz/rapids-ensemble-for-trends-neuroimaging/ ResultsAfter doing an offline sweep of blending weights, the final weights show that for the best local CV, the BaggingRegressor was hardly used for the \"age\" target. However, the BaggingRegressor provided more benefits for the domain variables. In particular for \"domain1_var2\" and \"domain2_var2\" the BaggingRegressor almost completely replaces the basic Ridge regression method. In terms of local CV, the result is almost identical to Bojan's notebook referenced above. On the leaderboard, adding the BaggingRegressor into the ensemble scores 0.1593, an improvement over Bojan's 0.1595. So the local CV to LB delta is successfully reduced, albeit by a little. I find it particularly interesting that only considering small subsets of the features, the BaggingRegressor is competitive for the domain variables but not at all for age.", "link": "https://www.kaggle.com/andypenrose/baggingregressor-rapids-ensemble", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-06-07 13:25:10", "date_scraped": "2020-12-13 17:52:38", "words": 335, "sentences": 16, "sum_nltk": "From the scikit-learn docs: \"A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\nSuch a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\" Ridge regression is known to work well on this dataset, so is used as the base regressor here.\nThe use of the BaggingRegressor is considered as part of a high-performing ensemble, combining SVM and Ridge regression.\nThis notebook is heavily based on @aerdem4's excellent SVM notebook and @tunguz's notebook that adds Ridge regression.\nThose original notebooks can be found here: https://www.kaggle.com/aerdem4/rapids-svm-on-trends-neuroimaging https://www.kaggle.com/tunguz/rapids-ensemble-for-trends-neuroimaging/ ResultsAfter doing an offline sweep of blending weights, the final weights show that for the best local CV, the BaggingRegressor was hardly used for the \"age\" target.\nSo the local CV to LB delta is successfully reduced, albeit by a little.\nI find it particularly interesting that only considering small subsets of the features, the BaggingRegressor is competitive for the domain variables but not at all for age.", "sum_nltk_words": 201, "sum_nltk_runtime": 0.004, "sum_t5": "a new tool is being used to reduce variance in a trend assessment. the idea is to use the BaggingRegressor to build multiple models. the results are based on two notebooks that add Ridge regression. the best local CV was the BaggingRegressor. the best local CV was the LB model. a regressor is a meta-estimator that aggregates multiple predictions. a regressor is a regressor that can be used to reduce", "sum_t5_words": 70, "sum_t5_runtime": 6.949, "runtime": 0.005, "nltk_category": "Justice, Law and Regulations", "nltk_category_score": 0.4274367690086365, "nltk_category_runtime": 23.396, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9605496525764465, "nltk_subcategory_runtime": 38.466, "category": "Justice, Law and Regulations", "category_score": 0.4274367690086365, "subcategory": "Machine Learning", "subcategory_score": 0.9605496525764465, "runtime_cat": 61.862, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.711", "language_code": "en", "language_score": "0.9999974156582624", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introductionin competition significant delta local cv lb score reported case httpswwwkagglecomctrendsassessmentpredictiondiscussion153256 many feature work maybe many reducing variance would seem good thing wanted investigate baggingregressor idea use baggingregressor build multiple model considering fraction feature combine output scikitlearn doc bagging regressor ensemble metaestimator fit base regressors random subset original dataset aggregate individual prediction either voting averaging form final prediction metaestimator typically used way reduce variance blackbox estimator eg decision tree introducing randomization construction procedure making ensemble ridge regression known work well dataset used base regressor use baggingregressor considered part highperforming ensemble combining svm ridge regression notebook heavily based aerdem4s excellent svm notebook tunguz notebook add ridge regression original notebook found httpswwwkagglecomaerdem4rapidssvmontrendsneuroimaging httpswwwkagglecomtunguzrapidsensemblefortrendsneuroimaging resultsafter offline sweep blending weight final weight show best local cv baggingregressor hardly used age target however baggingregressor provided benefit domain variable particular domain1_var2 domain2_var2 baggingregressor almost completely replaces basic ridge regression method term local cv result almost identical bojans notebook referenced leaderboard adding baggingregressor ensemble score 01593 improvement bojans 01595 local cv lb delta successfully reduced albeit little find particularly interesting considering small subset feature baggingregressor competitive domain variable age", "tags_descriptive": []}