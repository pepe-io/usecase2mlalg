{"title": "Taming the BERT - a baseline", "description": "In this kernel, I'm trying to obtain a baseline for the following model:  Use a pre-trained version for the BERT transformer model to obtain contextual word embeddings for the 3 target words in each passage: A, B and Pronoun. Feed this into a multi-layer perceptron (MLP), which learns to solve the coreference resolution problem as a supervised classification task.  I'm using the GitHub repo for the BERT project  to obtain the pre-trained model. See also the BERT paper by Devlin et al. The idea for the architecture (1-2 above) comes from the paper \"What do you learn from context? Probing for sentence structure in contextualized word representations\" by Tenney et al. For coreference resolution, they use the OntoNotes and Definite Pronoun Resolution datasets, but not GAP. As such, the MLP hyperparameters they use may not be the best for our current task. The hyperparameters I use below are quite different from theirs. The data I'm using comes from the 3 GAP files available here. The gap-development file contains the same data as the test_stage_1 file that we're trying to make predictions on. Of course, gap-development also contains the true labels, but I'm not using these when making predictions. I only use the true labels to evaluate the predictions made by my model. The other two files, gap-test and gap-validation, are used for training the model. Updates V7: In the previous version, I was a little worried by the large variance of the model. The current version uses a much smaller MLP for the supervised classification problem, with more regularization. This achieves the same mean CV score, with lower variance. Specifically, the current MLP has:  only one hidden layer of size 37, down from two hidden layers of sizes [59,31] dropout rate of 0.6 in the hidden layer, up from 0.5 L2 regularization in the output layer of 0.1, up from 0.05  Ceshine Lee independently published a kernel with a very neat PyTorch implementation of the same idea. You can check it out here.", "link": "https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-03-06 17:18:55", "date_scraped": "2020-12-12 20:09:27", "words": 338, "sentences": 18, "sum_nltk": "In this kernel, I'm trying to obtain a baseline for the following model:  Use a pre-trained version for the BERT transformer model to obtain contextual word embeddings for the 3 target words in each passage: A, B and Pronoun.\nFeed this into a multi-layer perceptron (MLP), which learns to solve the coreference resolution problem as a supervised classification task.\nI'm using the GitHub repo for the BERT project  to obtain the pre-trained model.\nFor coreference resolution, they use the OntoNotes and Definite Pronoun Resolution datasets, but not GAP.\nAs such, the MLP hyperparameters they use may not be the best for our current task.\nThe gap-development file contains the same data as the test_stage_1 file that we're trying to make predictions on.\nOf course, gap-development also contains the true labels, but I'm not using these when making predictions.\nI only use the true labels to evaluate the predictions made by my model.\nThe other two files, gap-test and gap-validation, are used for training the model.\nUpdates V7: In the previous version, I was a little worried by the large variance of the model.\nThe current version uses a much smaller MLP for the supervised classification problem, with more regularization.", "sum_nltk_words": 191, "sum_nltk_runtime": 0.003, "sum_t5": "a pre-trained version of the BERT transformer model is used to train a model. the model learns to solve the coreference resolution problem as a supervised classification task. the kernel has a much smaller hidden layer, with more regularization. the kernel is available on GitHub. a github repo is available here. a github repo can be found here. a github repo can be found here.", "sum_t5_words": 65, "sum_t5_runtime": 6.493, "runtime": 0.002, "nltk_category": "Education & Research", "nltk_category_score": 0.376557856798172, "nltk_category_runtime": 20.491, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9843489527702332, "nltk_subcategory_runtime": 31.443, "category": "Education & Research", "category_score": 0.376557856798172, "subcategory": "Machine Learning", "subcategory_score": 0.9843489527702332, "runtime_cat": 51.935, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.763", "language_code": "en", "language_score": "0.9999968370712367", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "kernel im trying obtain baseline following model use pretrained version bert transformer model obtain contextual word embeddings 3 target word passage b pronoun feed multilayer perceptron mlp learns solve coreference resolution problem supervised classification task im using github repo bert project obtain pretrained model see also bert paper devlin et al idea architecture 12 come paper learn context probing sentence structure contextualized word representation tenney et al coreference resolution use ontonotes definite pronoun resolution datasets gap mlp hyperparameters use may best current task hyperparameters use quite different data im using come 3 gap file available gapdevelopment file contains data test_stage_1 file trying make prediction course gapdevelopment also contains true label im using making prediction use true label evaluate prediction made model two file gaptest gapvalidation used training model update v7 previous version little worried large variance model current version us much smaller mlp supervised classification problem regularization achieves mean cv score lower variance specifically current mlp one hidden layer size 37 two hidden layer size 5931 dropout rate 06 hidden layer 05 l2 regularization output layer 01 005 ceshine lee independently published kernel neat pytorch implementation idea check", "tags_descriptive": []}