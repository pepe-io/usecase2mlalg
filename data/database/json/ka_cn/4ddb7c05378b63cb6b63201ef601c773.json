{"title": "Word2Vec with ingredients", "description": "I had the idea to apply distributed word vectors (word2vec) to this dataset. Word2vec, in a very high level, is an algorithm capable to learn the relationship between words using the context (neighbouring words), and encodes those relatinships in a vector. Using these vectors, we can cluster the words in or library, or even do operations. The classic example of the latter is; \"king - man + woman = queen.\" Word2vec uses recurrent neural networks to learn, then usually works better with huge datasets (billions of words), but we will see how it performs with the cooking dataset, where each receipt will be a sentence. One of the best features of this algorithm published by Google is the speed. Other recurrent neural networks had been proposed, however they were insanely CPU time consuming. If you want more detailed information about this, I strongly suggest you to read about here, here and here : . Now let\u2019s tackle our dataset.", "link": "https://www.kaggle.com/ccorbi/word2vec-with-ingredients", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "gensim"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2017-03-25 18:53:29", "date_scraped": "2020-12-13 18:21:34", "words": 159, "sentences": 8, "runtime": 0.002, "description_category": "Accommodation & Food", "description_category_score": 0.46127358078956604, "description_category_runtime": 13.889, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.8994930982589722, "description_subcategory_runtime": 22.03, "category": "Accommodation & Food", "category_score": 0.46127358078956604, "subcategory": "Machine Learning", "subcategory_score": 0.8994930982589722, "runtime_cat": 35.919, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.735", "language_code": "en", "language_score": "0.9999957764798877", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "idea apply distributed word vector word2vec dataset word2vec high level algorithm capable learn relationship word using context neighbouring word encodes relatinships vector using vector cluster word library even operation classic example latter king man woman queen word2vec us recurrent neural network learn usually work better huge datasets billion word see performs cooking dataset receipt sentence one best feature algorithm published google speed recurrent neural network proposed however insanely cpu time consuming want detailed information strongly suggest read let tackle dataset", "tags_descriptive": []}