{"title": "Mix of NN models based on Meta-embedding", "description": "I was trying to clean some of my code so I can add more models. However, this can never happen without the awesome kernels from other talented Kagglers. Forgive me if I missed any.  Based on SRK's kernel: https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings Vladimir Demidov's 2DCNN textClassifier: https://www.kaggle.com/yekenot/2dcnn-textclassifier Attention layer from Khoi Ngyuen: https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb LSTM model from Strideradu: https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go https://www.kaggle.com/danofer/different-embeddings-with-attention-fork  Some new things here:  Take average of embeddings (Unweighted DME) instead of blending predictions: https://arxiv.org/pdf/1804.07983.pdf The original paper of this idea comes from: Frustratingly Easy Meta-Embedding \u2013 Computing Meta-Embeddings by Averaging Source Word Embeddings Modified the code to choose best threshold Robust method for blending weights: sort the val score and give the final weight  Some thoughts:  Although I pulished a kernel on Transformer, I will not use it Too much randomness in CuDNN. You may get different results by just rerunning this kernel Blending rocks", "link": "https://www.kaggle.com/shujian/mix-of-nn-models-based-on-meta-embedding", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "gensim"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-11-16 14:06:00", "date_scraped": "2020-12-13 16:20:43", "words": 147, "sentences": 5, "runtime": 0.002, "description_category": "Wholesale & Retail", "description_category_score": 0.2933636009693146, "description_category_runtime": 24.979, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.9761813879013062, "description_subcategory_runtime": 40.659, "category": "Wholesale & Retail", "category_score": 0.2933636009693146, "subcategory": "Machine Learning", "subcategory_score": 0.9761813879013062, "runtime_cat": 65.638, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.745", "language_code": "en", "language_score": "0.9999983266121764", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "trying clean code add model however never happen without awesome kernel talented kagglers forgive missed based srks kernel httpswwwkagglecomsudalairajkumaralookatdifferentembeddings vladimir demidovs 2dcnn textclassifier httpswwwkagglecomyekenot2dcnntextclassifier attention layer khoi ngyuen httpswwwkagglecomsuicaokhoailanglstmattentionbaseline0652lb lstm model strideradu httpswwwkagglecomstrideraduword2vecandgensimgogogo httpswwwkagglecomdanoferdifferentembeddingswithattentionfork new thing take average embeddings unweighted dme instead blending prediction httpsarxivorgpdf180407983pdf original paper idea come frustratingly easy metaembedding computing metaembeddings averaging source word embeddings modified code choose best threshold robust method blending weight sort val score give final weight thought although pulished kernel transformer use much randomness cudnn may get different result rerunning kernel blending rock", "tags_descriptive": []}