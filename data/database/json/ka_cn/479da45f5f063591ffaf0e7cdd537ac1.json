{"title": "The \"Perfect Score\" Script", "description": "Perfect scores were seen on Kaggle's public leaderboards before, but with a different evaluation function, and the authors never published their approach. Someone else commented that a perfect score was obtainable  in this competition after 198 submissions, or, since 3 submissions are allowed per day, 66 days,  using a \"brute force\" method: \"The whole process takes about 198/3 = 66 days, which is shorter than the competition length.\"  I wanted to see how quickly this could really be done, within the rules,  and to try to win the race to the top of the public leaderboard. I ended up using 14 submissions, or 5 days  (Additionally,  I needlessly wasted 1 submission on the \"All 0.5 Benchmark\",   and spent another to actually claim the 0.00000 score, both being technically uninformative)  My approach is informed by information theory. You see, when the Kaggle server gives your submission a  score, it emits up to 21.7  bits of information  about the test labels, but there are only 198 labels with even fewer bits of information in them,  so one could learn all there is to know about the labels in 8 submissions or so. Well, that's a  theoretical limit, and might not be achievable in practice. If you train any two models and choose the one that has a better leaderboard score, you are already using 1 bit of information from your public scores. A generalization of this to any number of models is the boosting attack. However, it would require 4 years to get to the perfect score here. My approach is fundamentally similar, but is much more effective, as it learns more bits from each score. The core algebraic insight needed here is that if we choose 15 probabilities to be sigmoid(- n * epsilon * 2 ** i) where n=198, 0 <= i < 15, and epsilon = 1.05e-5 for example, and choose the rest of the probabilities to be 0.5, then the 15 labels corresponding to those 15 probabilities are easily discoverable from the score we get, because all  32768 possible label combinations lead to different scores. Note that the final rankings are based on the private labels of the second stage. Discovering all public labels helps with those only indirectly, by effectively increasing your training set size by 14%. (I believe the extra 14% are likely critical, given how close Kaggle competitions tend to be) USAGE To use the script, create an empty file called \"scores.txt\", copy \"stage1_sample_submission.csv\" (used to read patient IDs) into the same directory, and create a subdirectory called \"submissions\".  The former should contain the scores the Kaggle server gives you, one per line. It should be empty in the beginning. For example, the first line should be the score corresponding to \"submission_00.csv\". Keep any trailing 0s. There should be 5 digits after the decimal point. You can rerun the script whenever you update \"scores.txt\", but it's not necessary. This will do some partial label inference. When that file contains 14 lines, rerunning the script should also generate \"submission_fin.csv\", which will have all the correct labels. (Don't submit it though. If you wish to verify the labels, you may want to submit 1-labels instead and get the worst score possible: 34.54)", "link": "https://www.kaggle.com/olegtrott/the-perfect-score-script", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": [], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2017-04-07 21:19:14", "date_scraped": "2020-12-12 18:38:03", "words": 551, "sentences": 23, "sum_nltk": "Perfect scores were seen on Kaggle's public leaderboards before, but with a different evaluation function, and the authors never published their approach.\nSomeone else commented that a perfect score was obtainable  in this competition after 198 submissions, or, since 3 submissions are allowed per day, 66 days,  using a \"brute force\" method: \"The whole process takes about 198/3 = 66 days, which is shorter than the competition length.\"  I wanted to see how quickly this could really be done, within the rules,  and to try to win the race to the top of the public leaderboard.\nIf you train any two models and choose the one that has a better leaderboard score, you are already using 1 bit of information from your public scores.\n(I believe the extra 14% are likely critical, given how close Kaggle competitions tend to be) USAGE To use the script, create an empty file called \"scores.txt\", copy \"stage1_sample_submission.csv\" (used to read patient IDs) into the same directory, and create a subdirectory called \"submissions\".\nWhen that file contains 14 lines, rerunning the script should also generate \"submission_fin.csv\", which will have all the correct labels.", "sum_nltk_words": 188, "sum_nltk_runtime": 0.005, "sum_t5": "a perfect score is obtainable after 198 submissions, or 66 days. a researcher used 14 submissions, or 5 days, to try to win the race to the top. he says the approach is informed by information theory. a researcher says the approach is more effective as it learns more bits from each score. a researcher says the best way to get to the perfect score is to use the \"brute force\" method.", "sum_t5_words": 72, "sum_t5_runtime": 6.236, "runtime": 0.0, "nltk_category": "Healthcare", "nltk_category_score": 0.22568978369235992, "nltk_category_runtime": 18.123, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.3980382978916168, "nltk_subcategory_runtime": 29.179, "category": "Healthcare", "category_score": 0.22568978369235992, "subcategory": "Machine Learning", "subcategory_score": 0.3980382978916168, "runtime_cat": 47.302, "programming_language": "Jupyter Notebook", "ml_score": "0.5", "engagement_score": "0.811", "language_code": "en", "language_score": "0.9999974633657089", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "perfect score seen kaggles public leaderboards different evaluation function author never published approach someone else commented perfect score obtainable competition 198 submission since 3 submission allowed per day 66 day using brute force method whole process take 1983 66 day shorter competition length wanted see quickly could really done within rule try win race top public leaderboard ended using 14 submission 5 day additionally needlessly wasted 1 submission 05 benchmark spent another actually claim 000000 score technically uninformative approach informed information theory see kaggle server give submission score emits 217 bit information test label 198 label even fewer bit information one could learn know label 8 submission well thats theoretical limit might achievable practice train two model choose one better leaderboard score already using 1 bit information public score generalization number model boosting attack however would require 4 year get perfect score approach fundamentally similar much effective learns bit score core algebraic insight needed choose 15 probability sigmoid n epsilon 2 n198 0 15 epsilon 105e5 example choose rest probability 05 15 label corresponding 15 probability easily discoverable score get 32768 possible label combination lead different score note final ranking based private label second stage discovering public label help indirectly effectively increasing training set size 14 believe extra 14 likely critical given close kaggle competition tend usage use script create empty file called scorestxt copy stage1_sample_submissioncsv used read patient id directory create subdirectory called submission former contain score kaggle server give one per line empty beginning example first line score corresponding submission_00csv keep trailing 0 5 digit decimal point rerun script whenever update scorestxt necessary partial label inference file contains 14 line rerunning script also generate submission_fincsv correct label dont submit though wish verify label may want submit 1labels instead get worst score possible 3454", "tags_descriptive": []}