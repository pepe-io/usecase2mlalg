{"title": "Carefully dealing with missing values", "description": "Dealing with Missing Values One of the first steps in building a good predictive model is to carefully handle missing values at the start.  There's quite a lot of missing data in this dataset, so in this kernel I've illustrated various ways that we can impute missing values by carefully using the data we already have and I've outlined some key assumptions and rationale made in filling these missing values. I think some of these approaches are better than filling in missing values with the medians for the fields (for example) or letting models deal with them in their default manner (xgboost for instance will have all the missing values as another category) After investigating the data in some detail there also appears to be some fields which represent similar if not the same information which I think we can probably be remove as they are redundant. There are also potentially some inconsistent fields and potentially incorrect data that I discovered on the way and outlined this in this notebook The approaches use to deal with missing values also have to be made to the test data consistently - but I've not illustrated here in the interest of the kernel speed. Ideally after each step taken to deal with missing values you would probably want to carry out some cross-validation to see if it has helped improve your model - I haven't done this here but it would be interested to know if any of the adjustments do help improve the score so please comment if they do! As always if you found the kernel useful please upvote :)", "link": "https://www.kaggle.com/nikunjm88/carefully-dealing-with-missing-values", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2017-09-15 15:38:18", "date_scraped": "2020-12-13 18:50:10", "words": 270, "sentences": 5, "sum_nltk": "There's quite a lot of missing data in this dataset, so in this kernel I've illustrated various ways that we can impute missing values by carefully using the data we already have and I've outlined some key assumptions and rationale made in filling these missing values.\nI think some of these approaches are better than filling in missing values with the medians for the fields (for example) or letting models deal with them in their default manner (xgboost for instance will have all the missing values as another category) After investigating the data in some detail there also appears to be some fields which represent similar if not the same information which I think we can probably be remove as they are redundant.\nThere are also potentially some inconsistent fields and potentially incorrect data that I discovered on the way and outlined this in this notebook The approaches use to deal with missing values also have to be made to the test data consistently - but I've not illustrated here in the interest of the kernel speed.\nIdeally after each step taken to deal with missing values you would probably want to carry out some cross-validation to see if it has helped improve your model - I haven't done this here but it would be interested to know if any of the adjustments do help improve the score so please comment if they do!", "sum_nltk_words": 231, "sum_nltk_runtime": 0.003, "sum_t5": "there's quite a lot of missing data in this dataset. in this kernel i've illustrated various ways to deal with missing values. i think some of these approaches are better than filling in missing values with the medians for the fields. there are also potentially some inconsistent fields and incorrect data. if you found the kernel useful please upvote :). xgboost for instance will have all the missing values as another category.", "sum_t5_words": 72, "sum_t5_runtime": 4.881, "runtime": 0.003, "nltk_category": "Economics", "nltk_category_score": 0.3905751705169678, "nltk_category_runtime": 17.964, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.6203719973564148, "nltk_subcategory_runtime": 28.645, "category": "Economics", "category_score": 0.3905751705169678, "subcategory": "Machine Learning", "subcategory_score": 0.6203719973564148, "runtime_cat": 46.609, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.771", "language_code": "en", "language_score": "0.9999954167443781", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "dealing missing value one first step building good predictive model carefully handle missing value start there quite lot missing data dataset kernel ive illustrated various way impute missing value carefully using data already ive outlined key assumption rationale made filling missing value think approach better filling missing value median field example letting model deal default manner xgboost instance missing value another category investigating data detail also appears field represent similar information think probably remove redundant also potentially inconsistent field potentially incorrect data discovered way outlined notebook approach use deal missing value also made test data consistently ive illustrated interest kernel speed ideally step taken deal missing value would probably want carry crossvalidation see helped improve model havent done would interested know adjustment help improve score please comment always found kernel useful please upvote", "tags_descriptive": []}