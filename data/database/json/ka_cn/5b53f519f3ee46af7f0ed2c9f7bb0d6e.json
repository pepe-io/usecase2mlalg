{"title": "Don't overfit! - Searching true distributions", "description": "Our MissionWe have neither won the competition nor yielded a high score on the leaderboard. Even though we want to share our idea how to model the true distributions that may have been used to generate the data. We assumed that the competition sponsors used 2 gaussian distributions to draw random samples - a small part given as train and a big part given as test. To solve the task we tried to learn how these gaussians look like in sense of their mean and variance values. We setup a semi-supervised gaussian mixture model that starts with intial values for means and variances using the train data. Our idea was then to adjust these values by learning how the gaussians fit the test data best. Hence instead of leaderboard probing we tried to use the information that is already provided by the test data... Unfortunately our model does mad things and learning was not as expected. We are now on our journey to find out what has happened and we like to share our ideas with you. :-) Please feel free to comment, let us know what has negatively influenced the learning process in your opinion. Let's share learning experiences! Happy kaggling ;-) Caution: Still work in progress Table of contents Preparation Exploring the data structure Recursive feature elimination with logistic regression Trying to catch the true distributions Fit on Chris Deottes Top Useful Features What have we learnt?", "link": "https://www.kaggle.com/allunia/don-t-overfit-searching-true-distributions", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-05-17 13:19:57", "date_scraped": "2020-12-12 19:05:09", "words": 239, "sentences": 11, "sum_nltk": "Even though we want to share our idea how to model the true distributions that may have been used to generate the data.\nWe assumed that the competition sponsors used 2 gaussian distributions to draw random samples - a small part given as train and a big part given as test.\nTo solve the task we tried to learn how these gaussians look like in sense of their mean and variance values.\nWe setup a semi-supervised gaussian mixture model that starts with intial values for means and variances using the train data.\nOur idea was then to adjust these values by learning how the gaussians fit the test data best.\nHence instead of leaderboard probing we tried to use the information that is already provided by the test data...\nUnfortunately our model does mad things and learning was not as expected.\nWe are now on our journey to find out what has happened and we like to share our ideas with you.\nLet's share learning experiences!\nHappy kaggling ;-) Caution: Still work in progress Table of contents Preparation Exploring the data structure Recursive feature elimination with logistic regression Trying to catch the true distributions Fit on Chris Deottes Top Useful Features What have we learnt?", "sum_nltk_words": 196, "sum_nltk_runtime": 0.003, "sum_t5": "a small part of the sample was given as train and a big part as test. to solve the task we tried to learn how these gaussian distributions look like in sense of their mean and variance values. instead of leaderboard probing we tried to use the information that is already provided by the test data. unfortunately our model does mad things and learning was not as expected. we are now on our journey to find out what has happened and we like to share our ideas with you.", "sum_t5_words": 89, "sum_t5_runtime": 5.115, "runtime": 0.002, "nltk_category": "Utilities", "nltk_category_score": 0.4335761070251465, "nltk_category_runtime": 16.891, "nltk_subcategory": "Student", "nltk_subcategory_score": 0.6245046854019165, "nltk_subcategory_runtime": 27.16, "category": "Utilities", "category_score": 0.4335761070251465, "subcategory": "Student", "subcategory_score": 0.6245046854019165, "runtime_cat": 44.052, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.636", "language_code": "en", "language_score": "0.9999958619717094", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "missionwe neither competition yielded high score leaderboard even though want share idea model true distribution may used generate data assumed competition sponsor used 2 gaussian distribution draw random sample small part given train big part given test solve task tried learn gaussians look like sense mean variance value setup semisupervised gaussian mixture model start intial value mean variance using train data idea adjust value learning gaussians fit test data best hence instead leaderboard probing tried use information already provided test data unfortunately model mad thing learning expected journey find happened like share idea please feel free comment let u know negatively influenced learning process opinion let share learning experience happy kaggling caution still work progress table content preparation exploring data structure recursive feature elimination logistic regression trying catch true distribution fit chris deottes top useful feature learnt", "tags_descriptive": []}