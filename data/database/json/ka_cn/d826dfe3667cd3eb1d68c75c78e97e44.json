{"title": "Manifold mixup using PyTorch", "description": "Manifold mixupHere I implement the idea from the paper Manifold Mixup: Better Representations by Interpolating Hidden States. It is a new regularization method which allows for training very deep and wide neural networks with much less overfitting. This notebook is a simple PyTorch implementation of CNN with manifold mixup. This is not precise implementation of the paper. Input mixupIt's easier to first understand what is input mixup: mixup: BEYOND EMPIRICAL RISK MINIMIZATION. Input mixup is a regularization done during training procedure. Here is the piece of PyTorch code from the paper: # y1, y2 should be one-hot vectors for(x1, y1), (x2, y2)in zip(loader1, loader2):     lam = numpy.random.beta(alpha, alpha)     x = Variable(lam*x1 + (1. - lam)*x2)     y = Variable(lam*y1 + (1. - lam)*y2)     optimizer.zero_grad()     loss(net(x), y).backward()     optimizer.step() In short: model is given a linear combination of inputs and is asked to return linear combination of outputs. It forces the network to interpolate between samples. Manifold mixupManifold mixup is a similar idea, but the interpolation is done at a random layer inside neural network. Sometimes it is the 0'th layer, which means input mixup. It forces the network to interpolate between hidden representations of samples.", "link": "https://www.kaggle.com/hocop1/manifold-mixup-using-pytorch", "tags": ["Classification", "CNN"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pytorch"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-09-22 21:15:29", "date_scraped": "2020-12-13 12:26:21", "words": 217, "sentences": 13, "sum_nltk": "Manifold mixupHere I implement the idea from the paper Manifold Mixup: Better Representations by Interpolating Hidden States.\nIt is a new regularization method which allows for training very deep and wide neural networks with much less overfitting.\nThis notebook is a simple PyTorch implementation of CNN with manifold mixup.\nThis is not precise implementation of the paper.\nInput mixupIt's easier to first understand what is input mixup: mixup: BEYOND EMPIRICAL RISK MINIMIZATION.\nInput mixup is a regularization done during training procedure.\nHere is the piece of PyTorch code from the paper: # y1, y2 should be one-hot vectors for(x1, y1), (x2, y2)in zip(loader1, loader2):     lam = numpy.random.beta(alpha, alpha)     x = Variable(lam*x1 + (1.\n- lam)*x2)     y = Variable(lam*y1 + (1.\n- lam)*y2)     optimizer.zero_grad()     loss(net(x), y).backward()     optimizer.step() In short: model is given a linear combination of inputs and is asked to return linear combination of outputs.\nIt forces the network to interpolate between samples.\nManifold mixupManifold mixup is a similar idea, but the interpolation is done at a random layer inside neural network.\nSometimes it is the 0'th layer, which means input mixup.\nIt forces the network to interpolate between hidden representations of samples.", "sum_nltk_words": 205, "sum_nltk_runtime": 0.003, "sum_t5": "this notebook is a simple PyTorch implementation of CNN with manifold mixup. it's easier to understand what is input mixup than to understand what is regularization. it forces the network to interpolate between hidden representations of samples. the notebook is not precise implementation of the paper. cnn.com: \"manifold mixup\" is a new regularization method. a similar idea is used in manifold mixup: better representations by interpolating hidden states", "sum_t5_words": 68, "sum_t5_runtime": 6.446, "runtime": 0.005, "nltk_category": "Government and Public Works", "nltk_category_score": 0.5461538434028625, "nltk_category_runtime": 25.245, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9065033793449402, "nltk_subcategory_runtime": 40.484, "category": "Government and Public Works", "category_score": 0.5461538434028625, "subcategory": "Machine Learning", "subcategory_score": 0.9065033793449402, "runtime_cat": 65.729, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.693", "language_code": "en", "language_score": "0.9999951055847869", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "manifold mixuphere implement idea paper manifold mixup better representation interpolating hidden state new regularization method allows training deep wide neural network much le overfitting notebook simple pytorch implementation cnn manifold mixup precise implementation paper input mixupits easier first understand input mixup mixup beyond empirical risk minimization input mixup regularization done training procedure piece pytorch code paper y1 y2 onehot vector forx1 y1 x2 y2in ziploader1 loader2 lam numpyrandombetaalpha alpha x variablelamx1 1 lamx2 variablelamy1 1 lamy2 optimizerzero_grad lossnetx ybackward optimizerstep short model given linear combination input asked return linear combination output force network interpolate sample manifold mixupmanifold mixup similar idea interpolation done random layer inside neural network sometimes 0th layer mean input mixup force network interpolate hidden representation sample", "tags_descriptive": ["Classification", "Convolutional Neural Network (CNN)"]}