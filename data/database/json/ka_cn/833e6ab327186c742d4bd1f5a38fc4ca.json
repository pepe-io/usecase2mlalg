{"title": "Custom Training Loop with 100+ flowers on TPU", "description": "This notebooks shows three ways of training a model on TPU:  Using Keras and model.fit() Using a custom training loop Using a custom training loop specifically optimized for TPU  Optimization that benefit all three models:  use dataset.batch(BATCH_SIZE, drop_remainder=True)  The training dataset is infinitely repeated so drop_remainder=True should not be needed. However, whith the setting, Tensorflow produces batches of a known size and although XLA (the TPU compiler) can now handle variable batches, it is slightly faster on fixed batches.  On the validation dataset, this setting can drop some validation images. It is not the case here because the validation dataset happens to contain an integral number of batches.  Optimizations specific to the TPU-optimized custom training loop:  The training and validation step functions run multiple batches at once. This is achieved by placing a loop using tf.range() in the step function. The loop will be compiled to (thanks to @tf.function) and executed on TPU. The validation dataset is made to repeat indefinitely because handling end-of-dataset exception in a TPU loop implemented with tf.range() is not yet possible. Validation is adjusted to always use exactly or more than the entire validation dataset. This could change numerics. It happens that in this example, the validation dataset is used exactly once per validation. The validation dataset iterator is not reset between validation runs. Since the iterator is passed into the step function which is then compiled for TPU (thanks to @tf.function), passing a fresh iterator for every validation run would trigger a fresh recompilation. With a validation at the end of every epoch this would be slow. Losses are reported through Keras metrics. It is possible to return values from step function and return losses in that way. However, in the optimized version of the custom training loop, using tf.range(), aggregating losses returned from multiple batches becomes impractical.", "link": "https://www.kaggle.com/mgornergoogle/custom-training-loop-with-100-flowers-on-tpu", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-06-24 02:34:39", "date_scraped": "2020-12-12 19:56:26", "words": 311, "sentences": 17, "sum_nltk": "This notebooks shows three ways of training a model on TPU:  Using Keras and model.fit() Using a custom training loop Using a custom training loop specifically optimized for TPU  Optimization that benefit all three models:  use dataset.batch(BATCH_SIZE, drop_remainder=True)  The training dataset is infinitely repeated so drop_remainder=True should not be needed.\nOptimizations specific to the TPU-optimized custom training loop:  The training and validation step functions run multiple batches at once.\nThe loop will be compiled to (thanks to @tf.function) and executed on TPU.\nThe validation dataset is made to repeat indefinitely because handling end-of-dataset exception in a TPU loop implemented with tf.range() is not yet possible.\nValidation is adjusted to always use exactly or more than the entire validation dataset.\nIt happens that in this example, the validation dataset is used exactly once per validation.\nSince the iterator is passed into the step function which is then compiled for TPU (thanks to @tf.function), passing a fresh iterator for every validation run would trigger a fresh recompilation.\nIt is possible to return values from step function and return losses in that way.\nHowever, in the optimized version of the custom training loop, using tf.range(), aggregating losses returned from multiple batches becomes impractical.", "sum_nltk_words": 197, "sum_nltk_runtime": 0.003, "sum_t5": "this notebooks shows three ways of training a model on TPU. using a custom training loop specifically optimized for TPU Optimization. the training and validation step functions run multiple batches at once. the validation dataset is made to repeat indefinitely because handling end-of-dataset exception is not yet possible. a new version of the custom training loop is available for download. a new version of the custom loop is available for download. a new version of the custom loop is available for download.", "sum_t5_words": 82, "sum_t5_runtime": 6.704, "runtime": 0.005, "nltk_category": "Finance", "nltk_category_score": 0.4740432798862457, "nltk_category_runtime": 19.895, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8303086757659912, "nltk_subcategory_runtime": 31.782, "category": "Finance", "category_score": 0.4740432798862457, "subcategory": "Machine Learning", "subcategory_score": 0.8303086757659912, "runtime_cat": 51.677, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.708", "language_code": "en", "language_score": "0.9999954064819864", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "notebook show three way training model tpu using kera modelfit using custom training loop using custom training loop specifically optimized tpu optimization benefit three model use datasetbatchbatch_size drop_remaindertrue training dataset infinitely repeated drop_remaindertrue needed however whith setting tensorflow produce batch known size although xla tpu compiler handle variable batch slightly faster fixed batch validation dataset setting drop validation image case validation dataset happens contain integral number batch optimization specific tpuoptimized custom training loop training validation step function run multiple batch achieved placing loop using tfrange step function loop compiled thanks tffunction executed tpu validation dataset made repeat indefinitely handling endofdataset exception tpu loop implemented tfrange yet possible validation adjusted always use exactly entire validation dataset could change numerics happens example validation dataset used exactly per validation validation dataset iterator reset validation run since iterator passed step function compiled tpu thanks tffunction passing fresh iterator every validation run would trigger fresh recompilation validation end every epoch would slow loss reported kera metric possible return value step function return loss way however optimized version custom training loop using tfrange aggregating loss returned multiple batch becomes impractical", "tags_descriptive": []}