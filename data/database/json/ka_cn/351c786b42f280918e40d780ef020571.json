{"title": "Attension Layer Basic For NLP", "description": "What is Attention? Attention is simply a vector, often the outputs of dense layer using softmax function. Before Attention mechanism, translation relies on reading a complete sentence and compress all information into a fixed-length vector, as you can image, a sentence with hundreds of words represented by several words will surely lead to information loss, inadequate translation, etc.  Attention Architecture with Idea Behind it. The basic idea: each time the model predicts an output word, it only uses parts of an input where the most relevant information is concentrated instead of an entire sentence. In other words, it only pays attention to some input words. Let\u2019s investigate how this is implemented.    Encoder works as usual, and thedifference is only on the decoder\u2019s part. As you can see from a picture, the decoder\u2019s hidden state is computed with a context vector, the previous output and the previous hidden state. But now we use not a single context vector c, but a separate context vector c_i for each target word. These context vectors are computed as a weighted sum of annotations generated by the encoder. In Bahdanau\u2019s paper, they use a Bidirectional LSTM, so these annotations are concatenations of hidden states in forward and backward directions. The weight of each annotation is computed by an alignment model which scores how well the inputs and the output match. An alignment model is a feedforward neural network, for instance. In general, it can be any other model as well. As a result, the alphas\u200a\u2014\u200athe weights of hidden states when computing a context vector\u200a\u2014\u200ashow how important a given annotation is in deciding the next state and generating the output word. These are the attention scores.  Why Attention? The core of Probabilistic Language Model is to assign a probability to a sentence by Markov Assumption. Due to the nature of sentences that consist of different numbers of words, RNN is naturally introduced to model the conditional probability among words.   Vanilla RNN (the classic one) often gets trapped when modeling:  Structure Dilemma: in real world, the length of outputs and inputs can be totally different, while Vanilla RNN can only handle fixed-length problem which is difficult for the alignment. Consider an EN-FR translation examples: \u201che doesn\u2019t like apples\u201d \u2192 \u201cIl n\u2019aime pas les pommes\u201d. Mathematical Nature: it suffers from Gradient Vanishing/Exploding which means it is hard to train when sentences are long enough (maybe at most 4 words). Translation often requires arbitrary input length and out put length, to deal with the deficits above, encoder-decoder model is adopted and basic RNN cell is changed to GRU or LSTM cell, hyperbolic tangent activation is replaced by ReLU. We use GRU cell here.    Embedding layer maps discrete words into dense vectors for computational efficiency. Then embedded word vectors are fed into encoder, aka GRU cells sequentially. What happened during encoding? Information flows from left to right and each word vector is learned according to not only current input but also all previous words. When the sentence is completely read, encoder generates an output and a hidden state at timestep 4 for further processing. For encoding part, decoder (GRUs as well) grabs the hidden state from encoder, trained by teacher forcing (a mode that previous cell\u2019s output as current input), then generate translation words sequentially.  It seems amazing as this model can be applied to N-to-M sequence, yet there still is one main deficit left unsolved: is one hidden state really enough?   How does attention work?  Similar to the basic encoder-decoder architecture, this fancy mechanism plug a context vector into the gap between encoder and decoder. According to the schematic above, blue represents encoder and red represents decoder; and we could see that context vector takes all cells\u2019 outputs as input to compute the probability distribution of source language words for each single word decoder wants to generate. By utilizing this mechanism, it is possible for decoder to capture somewhat global information rather than solely to infer based on one hidden state. And to build context vector is fairly simple. For a fixed target word, first, we loop over all encoders\u2019 states to compare target and source states to generate scores for each state in encoders. Then we could use softmax to normalize all scores, which generates the probability distribution conditioned on target states. At last, the weights are introduced to make context vector easy to train. That\u2019s it. Math is shown below:   To understand the seemingly complicated math, we need to keep three key points in mind:  During decoding,context vectors are computed for every output word. So we will have a 2D matrix whose size is # of target words multiplied by # of source words. Equation (1) demonstrates how to compute a single value given one target word and a set of source word. Once context vector is computed, attention vector could be computed by context vector, target word, and attention function f. We need attention mechanism to be trainable. According to equation (4), both styles offer the trainable weights (W in Luong\u2019s, W1 and W2 in Bahdanau\u2019s). Thus, different styles may result in different performance.  Attention ScoringInputs to the scoring functionLet's start by looking at the inputs we'll give to the scoring function. We will assume we're in the first step in the decoging phase. The first input to the scoring function is the hidden state of decoder (assuming a toy RNN with three hidden nodes -- not usable in real life, but easier to illustrate):", "link": "https://www.kaggle.com/ashishpatel26/attension-layer-basic-for-nlp", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": [], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-01-04 08:43:54", "date_scraped": "2020-12-12 16:01:56", "words": 928, "sentences": 46, "sum_nltk": "The basic idea: each time the model predicts an output word, it only uses parts of an input where the most relevant information is concentrated instead of an entire sentence.\nThese context vectors are computed as a weighted sum of annotations generated by the encoder.\nThe weight of each annotation is computed by an alignment model which scores how well the inputs and the output match.\nAs a result, the alphas\u200a\u2014\u200athe weights of hidden states when computing a context vector\u200a\u2014\u200ashow how important a given annotation is in deciding the next state and generating the output word.\nTranslation often requires arbitrary input length and out put length, to deal with the deficits above, encoder-decoder model is adopted and basic RNN cell is changed to GRU or LSTM cell, hyperbolic tangent activation is replaced by ReLU.\nFor encoding part, decoder (GRUs as well) grabs the hidden state from encoder, trained by teacher forcing (a mode that previous cell\u2019s output as current input), then generate translation words sequentially.\nAccording to the schematic above, blue represents encoder and red represents decoder; and we could see that context vector takes all cells\u2019 outputs as input to compute the probability distribution of source language words for each single word decoder wants to generate.", "sum_nltk_words": 201, "sum_nltk_runtime": 0.018, "sum_t5": "attention is simply a vector, often the outputs of dense layer using softmax function. each time the model predicts an output word, it only uses parts of an input where the most relevant information is concentrated. in other words, it only pays attention to some input words. attention is a function of the attention of the decoder. the core of Probabilistic Language Model is to assign a probability to a sentence by Markov Assumption.", "sum_t5_words": 74, "sum_t5_runtime": 9.118, "runtime": 0.005, "nltk_category": "Utilities", "nltk_category_score": 0.29540300369262695, "nltk_category_runtime": 18.439, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.7749834060668945, "nltk_subcategory_runtime": 29.929, "category": "Utilities", "category_score": 0.29540300369262695, "subcategory": "Machine Learning", "subcategory_score": 0.7749834060668945, "runtime_cat": 48.369, "programming_language": "Jupyter Notebook", "ml_score": "0.5", "engagement_score": "0.726", "language_code": "en", "language_score": "0.999998356058433", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "attention attention simply vector often output dense layer using softmax function attention mechanism translation relies reading complete sentence compress information fixedlength vector image sentence hundred word represented several word surely lead information loss inadequate translation etc attention architecture idea behind basic idea time model predicts output word us part input relevant information concentrated instead entire sentence word pay attention input word let investigate implemented encoder work usual thedifference decoder part see picture decoder hidden state computed context vector previous output previous hidden state use single context vector c separate context vector c_i target word context vector computed weighted sum annotation generated encoder bahdanaus paper use bidirectional lstm annotation concatenation hidden state forward backward direction weight annotation computed alignment model score well input output match alignment model feedforward neural network instance general model well result alpha weight hidden state computing context vector show important given annotation deciding next state generating output word attention score attention core probabilistic language model assign probability sentence markov assumption due nature sentence consist different number word rnn naturally introduced model conditional probability among word vanilla rnn classic one often get trapped modeling structure dilemma real world length output input totally different vanilla rnn handle fixedlength problem difficult alignment consider enfr translation example doesnt like apple il naime pa le pommes mathematical nature suffers gradient vanishingexploding mean hard train sentence long enough maybe 4 word translation often requires arbitrary input length put length deal deficit encoderdecoder model adopted basic rnn cell changed gru lstm cell hyperbolic tangent activation replaced relu use gru cell embedding layer map discrete word dense vector computational efficiency embedded word vector fed encoder aka gru cell sequentially happened encoding information flow left right word vector learned according current input also previous word sentence completely read encoder generates output hidden state timestep 4 processing encoding part decoder grus well grab hidden state encoder trained teacher forcing mode previous cell output current input generate translation word sequentially seems amazing model applied ntom sequence yet still one main deficit left unsolved one hidden state really enough attention work similar basic encoderdecoder architecture fancy mechanism plug context vector gap encoder decoder according schematic blue represents encoder red represents decoder could see context vector take cell output input compute probability distribution source language word single word decoder want generate utilizing mechanism possible decoder capture somewhat global information rather solely infer based one hidden state build context vector fairly simple fixed target word first loop encoders state compare target source state generate score state encoders could use softmax normalize score generates probability distribution conditioned target state last weight introduced make context vector easy train thats math shown understand seemingly complicated math need keep three key point mind decodingcontext vector computed every output word 2d matrix whose size target word multiplied source word equation 1 demonstrates compute single value given one target word set source word context vector computed attention vector could computed context vector target word attention function f need attention mechanism trainable according equation 4 style offer trainable weight w luongs w1 w2 bahdanaus thus different style may result different performance attention scoringinputs scoring functionlets start looking input well give scoring function assume first step decoging phase first input scoring function hidden state decoder assuming toy rnn three hidden node usable real life easier illustrate", "tags_descriptive": []}