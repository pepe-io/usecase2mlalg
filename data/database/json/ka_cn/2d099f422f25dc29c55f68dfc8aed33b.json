{"title": "Train from MLM finetuned XLM-R large", "description": "About this notebookThis notebook trains from the XLM-Roberta large model which was finetuned with masked language modelling on the jigsaw test dataset Link. This notebook also implements a few improvements compared to a previous starter notebook that I shared  1, It trains on translated data 2, It uses different learning rate for the head layer and the transformer 3, It restores the model weights after training to the checkpoint which had the highest validation score  Suggestions/improvements are appreciated!  References: The shared XLM-Roberta large model, finetuned on the Jigsaw multilingual test data with masked language modelling Notebook link / Dataset link My previous starter notebook link This notebook uses the translated versions of the training dataset too, big thanks to Michael Kazachok! link This notebook uses different learning rate for the transformer and the head, I got the ideas from the writeup of the winning team of the Google QUEST Q&A Labeling competition  link, I have seen it described to be useful elsewhere too. This notebook heavily relies on the great notebook by, Xhulu: @xhulu  The tensorflow distrubuted training tutorial: Link", "link": "https://www.kaggle.com/riblidezso/train-from-mlm-finetuned-xlm-roberta-large", "tags": ["NLP"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow", "caffe"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-06-06 23:51:13", "date_scraped": "2020-12-13 12:15:35", "words": 185, "sentences": 3, "runtime": 0.005, "description_category": "Education & Research", "description_category_score": 0.17538532614707947, "description_category_runtime": 16.248, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.973544180393219, "description_subcategory_runtime": 26.388, "category": "Education & Research", "category_score": 0.17538532614707947, "subcategory": "Machine Learning", "subcategory_score": 0.973544180393219, "runtime_cat": 42.637, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.726", "language_code": "en", "language_score": "0.999996088626163", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "notebookthis notebook train xlmroberta large model finetuned masked language modelling jigsaw test dataset link notebook also implement improvement compared previous starter notebook shared 1 train translated data 2 us different learning rate head layer transformer 3 restores model weight training checkpoint highest validation score suggestionsimprovements appreciated reference shared xlmroberta large model finetuned jigsaw multilingual test data masked language modelling notebook link dataset link previous starter notebook link notebook us translated version training dataset big thanks michael kazachok link notebook us different learning rate transformer head got idea writeup winning team google quest qa labeling competition link seen described useful elsewhere notebook heavily relies great notebook xhulu xhulu tensorflow distrubuted training tutorial link", "tags_descriptive": ["Natural Language Processing (NLP)"]}