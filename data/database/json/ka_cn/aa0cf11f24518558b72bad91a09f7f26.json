{"title": "Look ma, no embeddings!", "description": "A non-neural baselineIn this kernel I match the baseline introduced here. Unlike in my previous kernel, I use spaCy for parsing, whose data structures are very easy to work with. Besides the parsing step, the model is completely non-neural and doesn't rely on word embeddings. As expected, it does not come close to the performance of modern deep transfer learning approaches. However, with some tweaking I was able to reach 0.57 log loss. Model architecture Build a global list of candidates using an entity recognizer and make sure both provided candidates are in there. Disqualify some candidates using well-understood grammatical constraints on coreference. Divide remaining candidates into 3 groups A, B, N based on what entity they are likely an instance of. For each group, compute some features based on testing its instances on metrics like prominence, locality, etc. Feed those features into a standard ensemble classifier.  Two take-aways: A major problem is the cascading effect of misparses. Obviously this is an inherent danger of using intermediate representations like dependency parses, and not directly training on the task at hand. A zillion manual tweaks are possible, and some have elegant spaCy implementations, but the return given the effort is rather slim. The thing the model did worst on is detecting cases where neither suggested candidate was correct. I wonder if that is specific to the approach here or if everyone is finding that to some extent.", "link": "https://www.kaggle.com/pheell/look-ma-no-embeddings", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "spacy"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-04-11 02:28:16", "date_scraped": "2020-12-12 20:09:27", "words": 237, "sentences": 15, "sum_nltk": "A non-neural baselineIn this kernel I match the baseline introduced here.\nUnlike in my previous kernel, I use spaCy for parsing, whose data structures are very easy to work with.\nBesides the parsing step, the model is completely non-neural and doesn't rely on word embeddings.\nAs expected, it does not come close to the performance of modern deep transfer learning approaches.\nModel architecture Build a global list of candidates using an entity recognizer and make sure both provided candidates are in there.\nDivide remaining candidates into 3 groups A, B, N based on what entity they are likely an instance of.\nFor each group, compute some features based on testing its instances on metrics like prominence, locality, etc.\nObviously this is an inherent danger of using intermediate representations like dependency parses, and not directly training on the task at hand.\nA zillion manual tweaks are possible, and some have elegant spaCy implementations, but the return given the effort is rather slim.\nThe thing the model did worst on is detecting cases where neither suggested candidate was correct.\nI wonder if that is specific to the approach here or if everyone is finding that to some extent.", "sum_nltk_words": 186, "sum_nltk_runtime": 0.003, "sum_t5": "a non-neural kernel is used to parse data, but the model is non-neural. a major problem is the cascading effect of misparses. a zillion manual tweaks are possible, but the return given the effort is rather slim. a zillion manual tweaks are possible, but the return given the effort is rather slim. a zillion manual tweaks are possible, and some have elegant spaCy implementations.", "sum_t5_words": 64, "sum_t5_runtime": 5.239, "runtime": 0.002, "nltk_category": "Utilities", "nltk_category_score": 0.46363815665245056, "nltk_category_runtime": 16.637, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9348723292350769, "nltk_subcategory_runtime": 26.93, "category": "Utilities", "category_score": 0.46363815665245056, "subcategory": "Machine Learning", "subcategory_score": 0.9348723292350769, "runtime_cat": 43.567, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.648", "language_code": "en", "language_score": "0.999997762673317", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "nonneural baselinein kernel match baseline introduced unlike previous kernel use spacy parsing whose data structure easy work besides parsing step model completely nonneural doesnt rely word embeddings expected come close performance modern deep transfer learning approach however tweaking able reach 057 log loss model architecture build global list candidate using entity recognizer make sure provided candidate disqualify candidate using wellunderstood grammatical constraint coreference divide remaining candidate 3 group b n based entity likely instance group compute feature based testing instance metric like prominence locality etc feed feature standard ensemble classifier two takeaway major problem cascading effect misparses obviously inherent danger using intermediate representation like dependency par directly training task hand zillion manual tweak possible elegant spacy implementation return given effort rather slim thing model worst detecting case neither suggested candidate correct wonder specific approach everyone finding extent", "tags_descriptive": []}