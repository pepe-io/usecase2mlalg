{"title": "DSB 2019: Simple LGBM using aggregated data", "description": "About this notebookYou might have noticed that the train dataset is composed of over 11M data points, but there are only 17k training labels, and 1000k test labels you are predicting. The reason for that is there are many thousand different entries for each installation_id, each representing an event. This notebook simply gathers all the events into 17k groups, each group corresponds to an installation_id. Then, it takes the aggregation (using sums, counts, mean, std, etc.) of those groups, thus resulting in a dataset of summary statistics of each installation_id. After that, it simply fits a model on that dataset. UpdatesV20:  Updated variable names for clarity.  V17:  Removed statistics on event codes, since that created a lot of columns and LGBM seems to overfit on that information.  V16:  Added mode of title accuracy_group (retrieved from training set) as a feature  V10:  Fixed labelling problem. Before that, I was blindly predicting the target without even the title I was trying to assess . I added that now by using the \"title\" column from train_labels.csv, and using the last row of each installation_id from test.csv to construct a test_labels dataframe.  V8:  Added cv_train, a function that trains k-models on each of k-fold CV splits. Then, you can use function cv_predict to use the list of models to predict an output (and blend the results). Added more summary statistics for event_code and game_time, including skewness of the distribution.  References CV idea inspired from this kernel. Thank you! Adding mode as a feature: https://www.kaggle.com/mhviraf/a-baseline-for-dsb-2019", "link": "https://www.kaggle.com/xhlulu/dsb-2019-simple-lgbm-using-aggregated-data", "tags": ["Feature Engineering"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-12-10 01:19:35", "date_scraped": "2020-12-12 18:40:29", "words": 260, "sentences": 15, "sum_nltk": "About this notebookYou might have noticed that the train dataset is composed of over 11M data points, but there are only 17k training labels, and 1000k test labels you are predicting.\nThis notebook simply gathers all the events into 17k groups, each group corresponds to an installation_id.\nThen, it takes the aggregation (using sums, counts, mean, std, etc.) of those groups, thus resulting in a dataset of summary statistics of each installation_id.\nAfter that, it simply fits a model on that dataset.\nV17:  Removed statistics on event codes, since that created a lot of columns and LGBM seems to overfit on that information.\nV16:  Added mode of title accuracy_group (retrieved from training set) as a feature  V10:  Fixed labelling problem.\nI added that now by using the \"title\" column from train_labels.csv, and using the last row of each installation_id from test.csv to construct a test_labels dataframe.\nV8:  Added cv_train, a function that trains k-models on each of k-fold CV splits.\nThen, you can use function cv_predict to use the list of models to predict an output (and blend the results).\nAdded more summary statistics for event_code and game_time, including skewness of the distribution.\nAdding mode as a feature: https://www.kaggle.com/mhviraf/a-baseline-for-dsb-2019", "sum_nltk_words": 194, "sum_nltk_runtime": 0.004, "sum_t5": "notebook gathers events into 17k groups, each group corresponds to an installation_id. it then fits a model on that dataset, using summary statistics of each installation_id. cv_train trains k-models on each of k-fold CV splits. cv_predict can blend the results, and also predict output. a dsb-based test is now available for download. a dsb-based test is available for download.", "sum_t5_words": 59, "sum_t5_runtime": 6.712, "runtime": 0.0, "nltk_category": "Utilities", "nltk_category_score": 0.4892479181289673, "nltk_category_runtime": 22.317, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9446095824241638, "nltk_subcategory_runtime": 35.796, "category": "Utilities", "category_score": 0.4892479181289673, "subcategory": "Machine Learning", "subcategory_score": 0.9446095824241638, "runtime_cat": 58.114, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.744", "language_code": "en", "language_score": "0.9999964835919655", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "notebookyou might noticed train dataset composed 11m data point 17k training label 1000k test label predicting reason many thousand different entry installation_id representing event notebook simply gather event 17k group group corresponds installation_id take aggregation using sum count mean std etc group thus resulting dataset summary statistic installation_id simply fit model dataset updatesv20 updated variable name clarity v17 removed statistic event code since created lot column lgbm seems overfit information v16 added mode title accuracy_group retrieved training set feature v10 fixed labelling problem blindly predicting target without even title trying ass added using title column train_labelscsv using last row installation_id testcsv construct test_labels dataframe v8 added cv_train function train kmodels kfold cv split use function cv_predict use list model predict output blend result added summary statistic event_code game_time including skewness distribution reference cv idea inspired kernel thank adding mode feature httpswwwkagglecommhvirafabaselinefordsb2019", "tags_descriptive": ["Feature Engineering"]}