{"title": "Visualizing BERT, plus an unsupervised solution", "description": "I've been wondering how far one can get in this competition using an unsupervised learning approach. So I decided to play with the contextual embeddings I obtained from BERT in my previous kernel, and try to make predictions without having a training set with ground truth labels. Spoiler alert: I didn't get very far, the best score I could get was around 0.93. Still, I think it's instructive to visualize some of the output of BERT. Specifically, I'm looking at:  The Euclidean distance d(A,P) between the embeddings of the Pronoun and the target word A. The Euclidean distance d(B,P). The Euclidean distance d(A,B) between the two target words.  This is low-dimensional information that you can plot, and then use to make predictions for the coreference resolution problem. The simplest approach would be:  If d(A,B) is large compared to d(A,P) and d(B,P), in a sense that will be made precise later, classify the data as \"Neither\". Otherwise, if d(A,P) < d(B,P), classify the data as \"A\". If d(A,P) > d(B,P), classify the data as \"B\".  This is more or less what I do below. To obtain class probabilities, I pass the distances through a softmax function.", "link": "https://www.kaggle.com/mateiionita/visualizing-bert-plus-an-unsupervised-solution", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-03-10 22:06:56", "date_scraped": "2020-12-12 20:09:27", "words": 199, "sentences": 13, "runtime": 0.002, "description_category": "Education & Research", "description_category_score": 0.125663623213768, "description_category_runtime": 20.704, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.9149966835975647, "description_subcategory_runtime": 33.153, "category": "Education & Research", "category_score": 0.125663623213768, "subcategory": "Machine Learning", "subcategory_score": 0.9149966835975647, "runtime_cat": 53.858, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.688", "language_code": "en", "language_score": "0.9999979022729394", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "ive wondering far one get competition using unsupervised learning approach decided play contextual embeddings obtained bert previous kernel try make prediction without training set ground truth label spoiler alert didnt get far best score could get around 093 still think instructive visualize output bert specifically im looking euclidean distance dap embeddings pronoun target word euclidean distance dbp euclidean distance dab two target word lowdimensional information plot use make prediction coreference resolution problem simplest approach would dab large compared dap dbp sense made precise later classify data neither otherwise dap dbp classify data dap dbp classify data b le obtain class probability pas distance softmax function", "tags_descriptive": []}