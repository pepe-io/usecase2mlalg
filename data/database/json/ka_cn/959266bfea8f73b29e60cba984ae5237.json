{"title": "10-fold LSTM with Attention [0.991 LB]", "description": "TL; DRThis is a fork of  Wiston Van's basic LSTM model https://www.kaggle.com/winstonvan/the-van-plan-for-kaggle-swaggle We slapped an Attention layer on top of the LSTM results, also we leave the prediction probabilites alone instead of rounding it. Anyway, Happy Thanksgiving!", "link": "https://www.kaggle.com/suicaokhoailang/10-fold-lstm-with-attention-0-991-lb", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-11-26 23:27:02", "date_scraped": "2020-12-12 19:03:18", "words": 38, "sentences": 2, "runtime": 0.001, "description_category": "Wholesale & Retail", "description_category_score": 0.10305079817771912, "description_category_runtime": 6.653, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.8552981019020081, "description_subcategory_runtime": 10.139, "category": "Wholesale & Retail", "category_score": 0.10305079817771912, "subcategory": "Machine Learning", "subcategory_score": 0.8552981019020081, "runtime_cat": 16.792, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.734", "language_code": "en", "language_score": "0.9999972432214272", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "tl drthis fork wiston van basic lstm model httpswwwkagglecomwinstonvanthevanplanforkaggleswaggle slapped attention layer top lstm result also leave prediction probabilites alone instead rounding anyway happy thanksgiving", "tags_descriptive": []}