{"title": "EfficientNetB3 data Pipeline and Model", "description": "Update6Using same pipeline with an efficientnetb3, efficientnetb5 using arc face margin 0.15 and NUM_TO_RERANK = 1. Also, retrain the model with image size 512. Update5Using same pipeline with an efficientnetb3 using arc face margin 0.15. Update4Blend efficientnetB1 and B2 and change the NUM_TO_RANK to 3. Blending did not improve the score that much, i believe it is because both efficientnets are correlated. To blend 2 models, just average the embeddings. Update3Change validation to 2%, efficientnetB1 and train with more epochs. The number of unique training classes is 81313 of 81313 total classes The number of unique validation classes is 24574 of 81313 total classes Training with 1548860 images Validating with 31610 images Update2Using retrieval with private train dataset increase public score a lot. Non landmark images and images with a few observations are hard to predict. For this case I use a arcface layer with an efficientnetB0, and the val loss accuracy was 0.730. The nice thing is that leaderboard is much better than last experiment. Well there is a los of ground for improvement. I updated the script so you can experiment with this pipeline. The number of unique training classes is 80937 of 81313 total classes The number of unique validation classes is 64024 of 81313 total classes Update1Used EfficientNetB5 with 100% of the data using 20% validation. The number of unique training classes is 80937 of 81313 total classes The number of unique validation classes is 64024 of 81313 total classes This model got a validation accuracy of 0.86. CommentsSup kaggle, in this pipeline and script i want to share my results trianing my own models so that you can use this as a baseline. Here i trained a basic efficientnetB3 with the total amount of classes \"81313\". I only used 80% of the total training data and my experiments show me that if you train with more data, the validation score improves and also the public leaderboard. Becuase we are only trianing with 80% of the data, we are not actually trianing all the classes because there are some classes that have 2 samples. Here are some basic stats for the trained model. The number of unique training classes is 74450 of 81313 total classes The number of unique validation classes is 66213 of 81313 total classes The validation accuracy score is 0.82 and the gap is 0.80 Important: Im still not sure why my validation is not align with the public leaderboard score, i believe it is because is has another target distribution + non landmark images, but this is just an hypothesis. Some insights that you may find usefull are the following:  Here is a discussion where I comment the dataset im using https://www.kaggle.com/c/landmark-recognition-2020/discussion/180056 Bigger image size gives better score but they need more resources to train, in other words it will take more time Bigger efficientnets give better scores Retrieval methods adjust to this problem much more that normal classification, so you want to try that approach, this is just an example to get you started building and fitting your own models Model was trained in colab tpu, with a regular free account you have 12 hours to train your model, if you need more time just save the model each epoch and then reload it in another session and continue training", "link": "https://www.kaggle.com/ragnar123/efficientnetb3-data-pipeline-and-model", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "keras", "tensorflow"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-09-18 21:07:44", "date_scraped": "2020-12-13 12:43:16", "words": 550, "sentences": 22, "sum_nltk": "The number of unique training classes is 81313 of 81313 total classes The number of unique validation classes is 24574 of 81313 total classes Training with 1548860 images Validating with 31610 images Update2Using retrieval with private train dataset increase public score a lot.\nThe number of unique training classes is 80937 of 81313 total classes The number of unique validation classes is 64024 of 81313 total classes Update1Used EfficientNetB5 with 100% of the data using 20% validation.\nThe number of unique training classes is 80937 of 81313 total classes The number of unique validation classes is 64024 of 81313 total classes This model got a validation accuracy of 0.86.\nI only used 80% of the total training data and my experiments show me that if you train with more data, the validation score improves and also the public leaderboard.\nThe number of unique training classes is 74450 of 81313 total classes The number of unique validation classes is 66213 of 81313 total classes The validation accuracy score is 0.82 and the gap is 0.80 Important: Im still not sure why my validation is not align with the public leaderboard score, i believe it is because is has another target distribution + non landmark images, but this is just an hypothesis.", "sum_nltk_words": 206, "sum_nltk_runtime": 0.006, "sum_t5": "kaggle: i used 80% of the total training data and my experiments show improvement. he says if you train with more data, the validation score improves and the leaderboard. kaggle: i'm not sure if i'll be able to improve the public leaderboard. kaggle: i'll be using the results to improve the public leaderboard. if you want to improve the score, use the cnn tool.", "sum_t5_words": 64, "sum_t5_runtime": 6.868, "runtime": 0.003, "nltk_category": "Finance", "nltk_category_score": 0.208822563290596, "nltk_category_runtime": 18.631, "nltk_subcategory": "Valuation", "nltk_subcategory_score": 0.6905889511108398, "nltk_subcategory_runtime": 29.949, "category": "Finance", "category_score": 0.208822563290596, "subcategory": "Valuation", "subcategory_score": 0.6905889511108398, "runtime_cat": 48.579, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.73", "language_code": "en", "language_score": "0.9999974801817367", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "update6using pipeline efficientnetb3 efficientnetb5 using arc face margin 015 num_to_rerank 1 also retrain model image size 512 update5using pipeline efficientnetb3 using arc face margin 015 update4blend efficientnetb1 b2 change num_to_rank 3 blending improve score much believe efficientnets correlated blend 2 model average embeddings update3change validation 2 efficientnetb1 train epoch number unique training class 81313 81313 total class number unique validation class 24574 81313 total class training 1548860 image validating 31610 image update2using retrieval private train dataset increase public score lot non landmark image image observation hard predict case use arcface layer efficientnetb0 val loss accuracy 0730 nice thing leaderboard much better last experiment well los ground improvement updated script experiment pipeline number unique training class 80937 81313 total class number unique validation class 64024 81313 total class update1used efficientnetb5 100 data using 20 validation number unique training class 80937 81313 total class number unique validation class 64024 81313 total class model got validation accuracy 086 commentssup kaggle pipeline script want share result trianing model use baseline trained basic efficientnetb3 total amount class 81313 used 80 total training data experiment show train data validation score improves also public leaderboard becuase trianing 80 data actually trianing class class 2 sample basic stats trained model number unique training class 74450 81313 total class number unique validation class 66213 81313 total class validation accuracy score 082 gap 080 important im still sure validation align public leaderboard score believe another target distribution non landmark image hypothesis insight may find usefull following discussion comment dataset im using httpswwwkagglecomclandmarkrecognition2020discussion180056 bigger image size give better score need resource train word take time bigger efficientnets give better score retrieval method adjust problem much normal classification want try approach example get started building fitting model model trained colab tpu regular free account 12 hour train model need time save model epoch reload another session continue training", "tags_descriptive": []}