{"title": "Pytorch-XLA: Understanding TPU's and XLA", "description": "About this Notebook In 2018 ,Google open sourced the device which was the backbone of their state of the art results ,the TPU's (more on them in a bit) . They made TPU's available through their cloud services for anyone to use at $2/hour and after some time TPU's were made available for free on Google colab .  As we all know TPU's are very fast and give state of the art results for neural networks , however initially it could only be used with TensorFlow and Keras , leaving the pytorch fans really frustated as they didn't want to shift to TF . This led to a chain of events for the development of way allowing TPU's to be used with Pytorch  Hence Pytorch-XLA module was developed which lets pytorch to run its graph on xla_devices like TPU's .   In the Jigsaw competition TPU's have been used in various different ways , on single cores, multi cores, using different chechpoints etc,etc . However when I tried to understand the Publically shared kernels to explore how to use TPU cores with pytorch, I found it was really difficult to comphrehend. Also for Jigsaw the best performing model is XLM-Roberta which is a fairly large model and throws an error if someone is not careful with memory management. The complexity of TPU's and usage of XLM-Roberta with TPU's left me frustated and angry . I decided to take this TPU thing slowly and started with small models building on that with a lot of experimentations upto XLM-Roberta . In this Notebook I share my experimentations with Pytorch-XLA and TPU's . I will start from basic TPU usage and build on that to show how to use TPUs on multiple cores and also with multithreading. I will also share some tips and tricks which would be useful when using TPU's with Pytorch XLA. If you want to learn to use TPU's the easy way, this might be a good place to start After learning this You will be able to Decode and easily understand ALEX's and Abhishek's kernels", "link": "https://www.kaggle.com/tanulsingh077/pytorch-xla-understanding-tpu-s-and-xla", "tags": ["Transfer Learning"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "pytorch", "tensorflow", "spacy"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-06-01 17:09:16", "date_scraped": "2020-12-13 12:15:35", "words": 350, "sentences": 13, "sum_nltk": "About this Notebook In 2018 ,Google open sourced the device which was the backbone of their state of the art results ,the TPU's (more on them in a bit) .\nAs we all know TPU's are very fast and give state of the art results for neural networks , however initially it could only be used with TensorFlow and Keras , leaving the pytorch fans really frustated as they didn't want to shift to TF .\nHowever when I tried to understand the Publically shared kernels to explore how to use TPU cores with pytorch, I found it was really difficult to comphrehend.\nI decided to take this TPU thing slowly and started with small models building on that with a lot of experimentations upto XLM-Roberta .\nIn this Notebook I share my experimentations with Pytorch-XLA and TPU's .\nI will also share some tips and tricks which would be useful when using TPU's with Pytorch XLA.\nIf you want to learn to use TPU's the easy way, this might be a good place to start After learning this You will be able to Decode and easily understand ALEX's and Abhishek's kernels", "sum_nltk_words": 186, "sum_nltk_runtime": 0.004, "sum_t5": "in 2018,Google open sourced the device which was the backbone of their state of the art results,the TPU's. they made TPU's available through their cloud services for anyone to use at $2/hour and after some time TPU's were made available for free on google colab. this led to a chain of events for the development of way allowing TPU's to be used with Pytorch. Hence Pytorch-XLA module was developed which lets", "sum_t5_words": 71, "sum_t5_runtime": 6.977, "runtime": 0.003, "nltk_category": "Utilities", "nltk_category_score": 0.31515800952911377, "nltk_category_runtime": 17.242, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8494820594787598, "nltk_subcategory_runtime": 27.904, "category": "Utilities", "category_score": 0.31515800952911377, "subcategory": "Machine Learning", "subcategory_score": 0.8494820594787598, "runtime_cat": 45.146, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.708", "language_code": "en", "language_score": "0.9999974857588921", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "notebook 2018 google open sourced device backbone state art result tpus bit made tpus available cloud service anyone use 2hour time tpus made available free google colab know tpus fast give state art result neural network however initially could used tensorflow kera leaving pytorch fan really frustated didnt want shift tf led chain event development way allowing tpus used pytorch hence pytorchxla module developed let pytorch run graph xla_devices like tpus jigsaw competition tpus used various different way single core multi core using different chechpoints etcetc however tried understand publically shared kernel explore use tpu core pytorch found really difficult comphrehend also jigsaw best performing model xlmroberta fairly large model throw error someone careful memory management complexity tpus usage xlmroberta tpus left frustated angry decided take tpu thing slowly started small model building lot experimentation upto xlmroberta notebook share experimentation pytorchxla tpus start basic tpu usage build show use tpus multiple core also multithreading also share tip trick would useful using tpus pytorch xla want learn use tpus easy way might good place start learning able decode easily understand alexs abhisheks kernel", "tags_descriptive": ["Transfer Learning"]}