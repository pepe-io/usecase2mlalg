{"title": "Ensembling on Instance Segmentation (LB 0.419)", "description": "Currently we've many models around with different performances. For model without any post-processing we can expect:  LinkNet: Around LB 0.36 UNet: Around LB 0.37 to 0.43 Mask RCNN: Around LB 0.45 to 0.50  Predictions to deliver for each image for is a list of masks detected (the instances). Now, how can we ensemble models ouputs to get better performances? For models that provide per pixel probabilities (LinkNet, UNet) one can simply do averaging/stacking on the probabilities. But for models that provide instances (with or without scoring), what can be done? This kernel is to try to experiment solutions to ensemble instances. 2018-03-20: First, we apply really basic NMS (Non-Maximum-Suppression) with bounding boxes extracted from predicted masks. 2 predictions provided as a starting point:  Model#0: LB 0.421 (Mask RCNN - Resnet101 backbone, pretrained - Coco, CV fold1) Model#1: LB 0.413 (Mask RCNN - Resnet101 backbone, pretrained - Coco, CV fold2)  Forks and contributions welcome!", "link": "https://www.kaggle.com/mpware/ensembling-on-instance-segmentation-lb-0-419", "tags": ["Ensembling"], "kind": ["Project", "(Notebook)"], "ml_libs": ["skimage", "pattern"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-03-21 22:46:20", "date_scraped": "2020-12-12 18:38:54", "words": 157, "sentences": 6, "runtime": 0.001, "description_category": "Healthcare", "description_category_score": 0.22260287404060364, "description_category_runtime": 16.76, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.9487754106521606, "description_subcategory_runtime": 26.621, "category": "Healthcare", "category_score": 0.22260287404060364, "subcategory": "Machine Learning", "subcategory_score": 0.9487754106521606, "runtime_cat": 43.381, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.76", "language_code": "en", "language_score": "0.9999974369858011", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "currently weve many model around different performance model without postprocessing expect linknet around lb 036 unet around lb 037 043 mask rcnn around lb 045 050 prediction deliver image list mask detected instance ensemble model ouputs get better performance model provide per pixel probability linknet unet one simply averagingstacking probability model provide instance without scoring done kernel try experiment solution ensemble instance 20180320 first apply really basic nm nonmaximumsuppression bounding box extracted predicted mask 2 prediction provided starting point model0 lb 0421 mask rcnn resnet101 backbone pretrained coco cv fold1 model1 lb 0413 mask rcnn resnet101 backbone pretrained coco cv fold2 fork contribution welcome", "tags_descriptive": ["Ensembling"]}