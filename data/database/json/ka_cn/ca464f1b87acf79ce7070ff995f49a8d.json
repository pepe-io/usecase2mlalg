{"title": "LGB + Bayesian parameters finding + Rank average", "description": "Bayesian global optimization with gaussian processes for finding (sub-)optimal parameters of LightGBMAs many of fellow kaggler asking how did I get LightGBM parameters for the kernel Customer Transaction Prediction I published. So, I decided to publish a kernel to optimize parameters. In this kernel I use Bayesian global optimization with gaussian processes for finding optimal parameters. This optimization attempts to find the maximum value of an black box function in as few iterations as possible. In our case the black box function will be a function that I will write to optimize (maximize) the evaluation function (AUC) so that parameters get maximize AUC in training and validation, and expect to do good in the private. The final prediction will be rank average on 5 fold cross validation predictions. Continue to the end of this kernel and upvote it if you find it is interesting.  Image taken from : https://github.com/fmfn/BayesianOptimization", "link": "https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-02-25 00:53:13", "date_scraped": "2020-12-13 17:07:48", "words": 150, "sentences": 8, "runtime": 0.0, "description_category": "Utilities", "description_category_score": 0.023828018456697464, "description_category_runtime": 13.554, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.9297218918800354, "description_subcategory_runtime": 21.285, "category": "Utilities", "category_score": 0.023828018456697464, "subcategory": "Machine Learning", "subcategory_score": 0.9297218918800354, "runtime_cat": 34.839, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.745", "language_code": "en", "language_score": "0.9999941168237065", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "bayesian global optimization gaussian process finding suboptimal parameter lightgbmas many fellow kaggler asking get lightgbm parameter kernel customer transaction prediction published decided publish kernel optimize parameter kernel use bayesian global optimization gaussian process finding optimal parameter optimization attempt find maximum value black box function iteration possible case black box function function write optimize maximize evaluation function auc parameter get maximize auc training validation expect good private final prediction rank average 5 fold cross validation prediction continue end kernel upvote find interesting image taken httpsgithubcomfmfnbayesianoptimization", "tags_descriptive": []}