{"title": "iMet2019 Chainer Starter (SEResNet152 + FocalLoss)", "description": "Chainer Starter Kernel for iMet Collection 2019 - FGVC6What is this kernel?This is a baseline example using Chainer and ChainerCV. I share this kernel mainly for practice of writing kernel, and for sharing some (maybe useful) information e.g. training settings. Summary of model, training, and inferencebase model: SEResNet152 pre-trained weights on ImageNet obtain output of Global Average Pooling Layer (pool5) and feed it to Dense Layer input shape: (ch, height, witdh) = (3, 128, 128) preprocessing for images subtract per-channel mean of all train images, and devide by 255 (after data augmentation)    training fine-tuning all over the model, not freezing any layer data augmentation horizontal flip random_distort random_rotate(angle \u2208 [-10, 10]) random_expand => resize_with_random_interpolation random crop   max epoch: 20 batch size: 128 optimizer: NesterovSGD momentum = 0.9, weight decay = 1e-04   learning schedule: cosine anealing max_lr = 0.01, min_lr = 0.0001 I ran only one cycle. Note: decaying learning rate by epoch, not iteration (for simple implementation)   loss: Focal Loss alpha = 0.5, gamma = 2 Note: Loss for each sample is calculated by summation of focal loss for each class. Loss for mini-batch  is calculated by averaging loss for samples in it. At first I calculated each sample's loss by averaging, but it didn't work well.         validation make one validation set, not perform k-fold cross validation randomly split, not considering target(attribute) frequency train : valid = 4 : 1   check each epoch's f-beta score by threshold = 0.2 since f-beta weights recall higher than precision default threshold of fastai's implementation is 0.2      inference not using TTA using best threshold for validation set Thresholds for all classes are same.", "link": "https://www.kaggle.com/ttahara/imet2019-chainer-starter-seresnet152-focalloss", "tags": ["DL"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-04-10 18:53:56", "date_scraped": "2020-12-13 11:40:08", "words": 294, "sentences": 8, "sum_nltk": "Summary of model, training, and inferencebase model: SEResNet152 pre-trained weights on ImageNet obtain output of Global Average Pooling Layer (pool5) and feed it to Dense Layer input shape: (ch, height, witdh) = (3, 128, 128) preprocessing for images subtract per-channel mean of all train images, and devide by 255 (after data augmentation)    training fine-tuning all over the model, not freezing any layer data augmentation horizontal flip random_distort random_rotate(angle \u2208 [-10, 10]) random_expand => resize_with_random_interpolation random crop   max epoch: 20 batch size: 128 optimizer: NesterovSGD momentum = 0.9, weight decay = 1e-04   learning schedule: cosine anealing max_lr = 0.01, min_lr = 0.0001 I ran only one cycle.\nNote: decaying learning rate by epoch, not iteration (for simple implementation)   loss: Focal Loss alpha = 0.5, gamma = 2 Note: Loss for each sample is calculated by summation of focal loss for each class.\nLoss for mini-batch  is calculated by averaging loss for samples in it.\nvalidation make one validation set, not perform k-fold cross validation randomly split, not considering target(attribute) frequency train : valid = 4 : 1   check each epoch's f-beta score by threshold = 0.2 since f-beta weights recall higher than precision default threshold of fastai's implementation is 0.2      inference not using TTA using best threshold for validation set Thresholds for all classes are same.", "sum_nltk_words": 229, "sum_nltk_runtime": 0.003, "sum_t5": "chainer kernel is a baseline example using Chainer and ChainerCV. it is mainly for practice of writing kernel, and for sharing some (maybe useful) information e.g. training settings. f-beta weights recall higher than precision default threshold of fastai's implementation is 0.2 inference. if you want to test your model, you need to use the following kernel. if you want to test your model, you need to use the following kernel.", "sum_t5_words": 70, "sum_t5_runtime": 6.686, "runtime": 0.0, "nltk_category": "Miscellaneous", "nltk_category_score": 0.29723912477493286, "nltk_category_runtime": 24.726, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9749030470848083, "nltk_subcategory_runtime": 40.157, "category": "Miscellaneous", "category_score": 0.29723912477493286, "subcategory": "Machine Learning", "subcategory_score": 0.9749030470848083, "runtime_cat": 64.883, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.673", "language_code": "en", "language_score": "0.9999963457188275", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "chainer starter kernel imet collection 2019 fgvc6what kernelthis baseline example using chainer chainercv share kernel mainly practice writing kernel sharing maybe useful information eg training setting summary model training inferencebase model seresnet152 pretrained weight imagenet obtain output global average pooling layer pool5 feed dense layer input shape ch height witdh 3 128 128 preprocessing image subtract perchannel mean train image devide 255 data augmentation training finetuning model freezing layer data augmentation horizontal flip random_distort random_rotateangle 10 10 random_expand resize_with_random_interpolation random crop max epoch 20 batch size 128 optimizer nesterovsgd momentum 09 weight decay 1e04 learning schedule cosine anealing max_lr 001 min_lr 00001 ran one cycle note decaying learning rate epoch iteration simple implementation loss focal loss alpha 05 gamma 2 note loss sample calculated summation focal loss class loss minibatch calculated averaging loss sample first calculated sample loss averaging didnt work well validation make one validation set perform kfold cross validation randomly split considering targetattribute frequency train valid 4 1 check epoch fbeta score threshold 02 since fbeta weight recall higher precision default threshold fastais implementation 02 inference using tta using best threshold validation set threshold class", "tags_descriptive": ["Deep Learning (DL)"]}