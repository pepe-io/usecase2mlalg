{"title": "Audio challenge: CNN with concatenated inputs", "description": "OverviewAs I am new to deep learning and my background is more \"classic\" machine learning, I decided to start with random forest (RF) / xgboost (xgb) / logistic regression and then learn how to use neural nets. I started with skimming through articles on musical instrument detection and making a list of important features, coding these features and using these features as an input into RF and xgb. Training both classifiers on the whole dataset and averaging outputs using geometric mean gave 0.844 on the leaderboard, which was already great. Next I started to learn how to use CNNs. \"Learn from what is there\" they say, so I checked what was already done and came across kernel by Zafar - Beginner's Guide to Audio Data, which was a great starting point. Next I did some studying by listening to course by Andrew Ng links from here and played with hyperparameter optimization. As I had dataset for training RF/xgb, I decided to use that dataset along with dataset for CNN as two distinct inputs into one NN which gave good improvement to mapk. Currently I have around 0.92 on the leaderboard. Next steps are to study amazing input done by daisukelab which can be found here. Things I will try:  augmentaions oversampling other NN architectures re-sampling audio at 16k or 24k using RNN or 1d CNN as suggested by Zafar (or time-dependent approach by daisukelab) sequential learning / pseudo-labelling (e.g. definitely it is possible to label all test data in 2-3 days: add to train predictions with highest probability, train, etc.)  What I have noticed  Cross-validated mapk is way more lower then the one on the leaderboard Geometric mean works, arithmetic doesn't (for ensembling) Training classifers on all data can help (cross-validate to tune parameters, then re-train on all data) Ensembling helps TWO SUBMITS IS NOT ENOUGH :D  I will try to share more insights and ideas with the time.", "link": "https://www.kaggle.com/agehsbarg/audio-challenge-cnn-with-concatenated-inputs", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "keras"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-06-04 00:20:47", "date_scraped": "2020-12-12 20:04:19", "words": 323, "sentences": 11, "sum_nltk": "OverviewAs I am new to deep learning and my background is more \"classic\" machine learning, I decided to start with random forest (RF) / xgboost (xgb) / logistic regression and then learn how to use neural nets.\nTraining both classifiers on the whole dataset and averaging outputs using geometric mean gave 0.844 on the leaderboard, which was already great.\nAs I had dataset for training RF/xgb, I decided to use that dataset along with dataset for CNN as two distinct inputs into one NN which gave good improvement to mapk.\nThings I will try:  augmentaions oversampling other NN architectures re-sampling audio at 16k or 24k using RNN or 1d CNN as suggested by Zafar (or time-dependent approach by daisukelab) sequential learning / pseudo-labelling (e.g. definitely it is possible to label all test data in 2-3 days: add to train predictions with highest probability, train, etc.)  What I have noticed  Cross-validated mapk is way more lower then the one on the leaderboard Geometric mean works, arithmetic doesn't (for ensembling) Training classifers on all data can help (cross-validate to tune parameters, then re-train on all data) Ensembling helps TWO SUBMITS IS NOT ENOUGH :D  I will try to share more insights and ideas with the time.", "sum_nltk_words": 205, "sum_nltk_runtime": 0.003, "sum_t5": "i am new to deep learning and my background is more \"classic\" machine learning. i started with random forest (RF) / xgboost (xgb) / logistic regression. then i learned how to use neural nets. i started with training both classifiers on the whole dataset and averaging outputs using geometric mean. i am also learning how to use CNNs. i will try to share more insights and ideas with the time.", "sum_t5_words": 70, "sum_t5_runtime": 6.672, "runtime": 0.004, "nltk_category": "Education & Research", "nltk_category_score": 0.402556449174881, "nltk_category_runtime": 20.502, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9951415657997131, "nltk_subcategory_runtime": 33.366, "category": "Education & Research", "category_score": 0.402556449174881, "subcategory": "Machine Learning", "subcategory_score": 0.9951415657997131, "runtime_cat": 53.868, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.689", "language_code": "en", "language_score": "0.9999972488496641", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "overviewas new deep learning background classic machine learning decided start random forest rf xgboost xgb logistic regression learn use neural net started skimming article musical instrument detection making list important feature coding feature using feature input rf xgb training classifier whole dataset averaging output using geometric mean gave 0844 leaderboard already great next started learn use cnns learn say checked already done came across kernel zafar beginner guide audio data great starting point next studying listening course andrew ng link played hyperparameter optimization dataset training rfxgb decided use dataset along dataset cnn two distinct input one nn gave good improvement mapk currently around 092 leaderboard next step study amazing input done daisukelab found thing try augmentaions oversampling nn architecture resampling audio 16k 24k using rnn 1d cnn suggested zafar timedependent approach daisukelab sequential learning pseudolabelling eg definitely possible label test data 23 day add train prediction highest probability train etc noticed crossvalidated mapk way lower one leaderboard geometric mean work arithmetic doesnt ensembling training classifers data help crossvalidate tune parameter retrain data ensembling help two submits enough try share insight idea time", "tags_descriptive": []}