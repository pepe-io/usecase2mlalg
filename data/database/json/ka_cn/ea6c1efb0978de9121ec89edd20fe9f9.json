{"title": "From prototyping to submission (fastai)", "description": "In the notebook \"Cleaning the data for rapid prototyping\" I showed how to create a small, fast, ready-to-use dataset for prototyping our models. The dataset created in that notebook, along with the metadata files it uses, are now available here. So let's use them to create a model! In this notebook we'll see the whole journey from pre-training using progressive resizing on our prototyping sample, through to fine-tuning on the full dataset, and then submitting to the competition. I'm intentionally not doing any tricky modeling in this notebook, because I want to show the power of simple techniques and simples architectures. You should take this as a starting point and experiment! e.g. try data augmentation methods, architectures, preprocessing approaches, using the DICOM metadata, and so forth... We'll be using the fastai.medical.imaging library here - for more information about this see the notebook Some DICOM gotchas to be aware of. We'll also use the same basic setup that's in the notebook. Update: I'm out of GPU hours and Kaggle isn't freezing when running the current version of the notebook. To see a complete run, see this version. I've commented out the GPU calls in this run so I can run it end to end.", "link": "https://www.kaggle.com/jhoward/from-prototyping-to-submission-fastai", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": [], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-10-30 22:07:22", "date_scraped": "2020-12-13 16:47:30", "words": 203, "sentences": 11, "sum_nltk": "In the notebook \"Cleaning the data for rapid prototyping\" I showed how to create a small, fast, ready-to-use dataset for prototyping our models.\nThe dataset created in that notebook, along with the metadata files it uses, are now available here.\nSo let's use them to create a model!\nIn this notebook we'll see the whole journey from pre-training using progressive resizing on our prototyping sample, through to fine-tuning on the full dataset, and then submitting to the competition.\nI'm intentionally not doing any tricky modeling in this notebook, because I want to show the power of simple techniques and simples architectures.\nYou should take this as a starting point and experiment!\ne.g. try data augmentation methods, architectures, preprocessing approaches, using the DICOM metadata, and so forth...\nWe'll be using the fastai.medical.imaging library here - for more information about this see the notebook Some DICOM gotchas to be aware of.\nWe'll also use the same basic setup that's in the notebook.\nUpdate: I'm out of GPU hours and Kaggle isn't freezing when running the current version of the notebook.\nTo see a complete run, see this version.\nI've commented out the GPU calls in this run so I can run it end to end.", "sum_nltk_words": 192, "sum_nltk_runtime": 0.003, "sum_t5": "the notebook \"Cleaning the data for rapid prototyping\" shows how to create a small, fast, ready-to-use dataset for prototyping. the dataset created in that notebook, along with the metadata files it uses, are now available here. we'll see the whole journey from pre-training using progressive resizing on our prototyping sample, through to fine-tuning on the full dataset, and then submitting to the competition.", "sum_t5_words": 63, "sum_t5_runtime": 4.758, "runtime": 0.0, "nltk_category": "Utilities", "nltk_category_score": 0.22283422946929932, "nltk_category_runtime": 19.588, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.5761531591415405, "nltk_subcategory_runtime": 31.263, "category": "Utilities", "category_score": 0.22283422946929932, "subcategory": "Machine Learning", "subcategory_score": 0.5761531591415405, "runtime_cat": 50.851, "programming_language": "Jupyter Notebook", "ml_score": "0.5", "engagement_score": "0.76", "language_code": "en", "language_score": "0.9999970166180268", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "notebook cleaning data rapid prototyping showed create small fast readytouse dataset prototyping model dataset created notebook along metadata file us available let use create model notebook well see whole journey pretraining using progressive resizing prototyping sample finetuning full dataset submitting competition im intentionally tricky modeling notebook want show power simple technique simple architecture take starting point experiment eg try data augmentation method architecture preprocessing approach using dicom metadata forth well using fastaimedicalimaging library information see notebook dicom gotchas aware well also use basic setup thats notebook update im gpu hour kaggle isnt freezing running current version notebook see complete run see version ive commented gpu call run run end end", "tags_descriptive": []}