{"title": "Unintended EDA [with Tutorial Notes]", "description": "On 2018, Kaggle launched a competition in association with Jigsaw/Conversation AI to classify toxic comments. The original competition can be viewed here. A toxic comment is a comment that is rude, disrespectful or otherwise likely to make someone leave a discussion. The goal of that competition was to build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate. However, the models developed in that competition unfortunately associated the targetted group with toxicity, i.e. \"gay\". For example, a comment like \"I am a gay woman\" would be classified as toxic. This happened as the examples of identities associated with toxicity outnumbered neutral comments regarding the same identity. Therefore, the same team launched a new competition to recognize unintended bias towards identities. We are asked to use a dataset labeled with the associated identity. Let's look into the dataset and understand it first, so we can create a model that can better deal with bias. At the time I started this notebook, I looked at the kernel https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw to have a quick start. So I want to take a moment to thank the Author for the kernel. The purpose of this Kernel is to walk Kaggle newbies through the process of data exploration and visualization. In this notebook, we will use Pandas to do a little bit of data wrangling and Plotly and Seaborn to visualize the result of our wrangling.", "link": "https://www.kaggle.com/ekhtiar/unintended-eda-with-tutorial-notes", "tags": ["Exploratory Data Analysis"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "nltk", "pattern", "gensim"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-06-17 22:51:11", "date_scraped": "2020-12-13 12:22:44", "words": 239, "sentences": 15, "sum_nltk": "On 2018, Kaggle launched a competition in association with Jigsaw/Conversation AI to classify toxic comments.\nThe goal of that competition was to build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.\nHowever, the models developed in that competition unfortunately associated the targetted group with toxicity, i.e.\nFor example, a comment like \"I am a gay woman\" would be classified as toxic.\nThis happened as the examples of identities associated with toxicity outnumbered neutral comments regarding the same identity.\nTherefore, the same team launched a new competition to recognize unintended bias towards identities.\nWe are asked to use a dataset labeled with the associated identity.\nLet's look into the dataset and understand it first, so we can create a model that can better deal with bias.\nAt the time I started this notebook, I looked at the kernel https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw to have a quick start.\nThe purpose of this Kernel is to walk Kaggle newbies through the process of data exploration and visualization.\nIn this notebook, we will use Pandas to do a little bit of data wrangling and Plotly and Seaborn to visualize the result of our wrangling.", "sum_nltk_words": 188, "sum_nltk_runtime": 0.003, "sum_t5": "the goal of the competition was to build a model that\u2019s capable of detecting toxic comments. the models associated the targetted group with toxicity, i.e. \"gay\" the competition was launched to recognize unintended bias towards identities. the goal of this notebook is to walk Kaggle newbies through the process of data exploration and visualization. the goal of this notebook is to walk newbies through the process of data exploration and visualization.", "sum_t5_words": 71, "sum_t5_runtime": 5.793, "runtime": 0.002, "nltk_category": "Finance", "nltk_category_score": 0.44731470942497253, "nltk_category_runtime": 19.552, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.664618194103241, "nltk_subcategory_runtime": 31.142, "category": "Finance", "category_score": 0.44731470942497253, "subcategory": "Machine Learning", "subcategory_score": 0.664618194103241, "runtime_cat": 50.694, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.736", "language_code": "en", "language_score": "0.9999977644965559", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "2018 kaggle launched competition association jigsawconversation ai classify toxic comment original competition viewed toxic comment comment rude disrespectful otherwise likely make someone leave discussion goal competition build multiheaded model thats capable detecting different type toxicity like threat obscenity insult identitybased hate however model developed competition unfortunately associated targetted group toxicity ie gay example comment like gay woman would classified toxic happened example identity associated toxicity outnumbered neutral comment regarding identity therefore team launched new competition recognize unintended bias towards identity asked use dataset labeled associated identity let look dataset understand first create model better deal bias time started notebook looked kernel httpswwwkagglecomnz0722simpleedatextpreprocessingjigsaw quick start want take moment thank author kernel purpose kernel walk kaggle newbie process data exploration visualization notebook use panda little bit data wrangling plotly seaborn visualize result wrangling", "tags_descriptive": ["Exploratory Data Analysis"]}