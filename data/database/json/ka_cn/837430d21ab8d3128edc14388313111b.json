{"title": "14 Simple Tips to save RAM memory for 1+GB dataset", "description": "Introduction While working in python jupyter notebooks using pandas with small datasets (100MB), performance degradation is not seen however when we work on large datasets (1 GB or above), performance issue of the notebook is clearly seen as it takes longer time for the cell to execute resulting in either longer execution time of the cell or the notebook failing due to insufficient memory. Tools like Spark can handle large datasets (100's of GB to TB), taking full advantage of their capabilities usually requires more expensive hardware which is practically not possible while we try to work POC's or doing research. And unlike pandas, they lack rich feature sets for high quality data cleaning, exploration, and analysis. For medium-sized data, we\u2019re better off trying to get more out of pandas, rather than switching to a different tool. DatasetI have choosen a existing kaggle dataset from competition data namely Jigsaw Multilingual Toxic Comment Classification which has sizable data to demonstrate what I am trying to convey in this notebook. Brief about the data:Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results \"translate\" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages. As our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential. Files jigsaw-toxic-comment-train.csv - data from our first competition. The dataset is made up of English comments from Wikipedia\u2019s talk page edits. jigsaw-unintended-bias-train.csv - data from our second competition. This is an expanded version of the Civil Comments dataset with a range of additional labels. sample_submission.csv - a sample submission file in the correct format test.csv - comments from Wikipedia talk pages in different non-English languages. validation.csv - comments from Wikipedia talk pages in different non-English languages. jigsaw-toxic-comment-train-processed-seqlen128.csv - training data preprocessed for BERT jigsaw-unintended-bias-train-processed-seqlen128.csv - training data preprocessed for BERT validation-processed-seqlen128.csv - validation data preprocessed for BERT test-processed-seqlen128.csv - test data preprocessed for BERT  Let\u2019s start by importing both pandas and our data in Python and taking a look at the first five rows.", "link": "https://www.kaggle.com/pavansanagapati/14-simple-tips-to-save-ram-memory-for-1-gb-dataset", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": [], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2020-09-03 17:47:59", "date_scraped": "2020-12-13 12:15:35", "words": 417, "sentences": 18, "sum_nltk": "Tools like Spark can handle large datasets (100's of GB to TB), taking full advantage of their capabilities usually requires more expensive hardware which is practically not possible while we try to work POC's or doing research.\nFor medium-sized data, we\u2019re better off trying to get more out of pandas, rather than switching to a different tool.\nDatasetI have choosen a existing kaggle dataset from competition data namely Jigsaw Multilingual Toxic Comment Classification which has sizable data to demonstrate what I am trying to convey in this notebook.\nBrief about the data:Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list).\nYour training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages.\nFiles jigsaw-toxic-comment-train.csv - data from our first competition.\nThe dataset is made up of English comments from Wikipedia\u2019s talk page edits.\njigsaw-unintended-bias-train.csv - data from our second competition.\nsample_submission.csv - a sample submission file in the correct format test.csv - comments from Wikipedia talk pages in different non-English languages.\nvalidation.csv - comments from Wikipedia talk pages in different non-English languages.", "sum_nltk_words": 191, "sum_nltk_runtime": 0.005, "sum_t5": "develop strategies to build effective multilingual models. develop strategies to build effective multilingual models. jupyter notebooks are designed to be able to work with large datasets. a kaggle dataset is a kaggle dataset. a kaggle notebook is a kaggle dataset. a kaggle notebook is a kaggle dataset. a kaggle notebook is a kaggle dataset.", "sum_t5_words": 54, "sum_t5_runtime": 6.729, "runtime": 0.005, "nltk_category": "Education & Research", "nltk_category_score": 0.4550980031490326, "nltk_category_runtime": 19.645, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.9333317875862122, "nltk_subcategory_runtime": 31.516, "category": "Education & Research", "category_score": 0.4550980031490326, "subcategory": "Machine Learning", "subcategory_score": 0.9333317875862122, "runtime_cat": 51.162, "programming_language": "Jupyter Notebook", "ml_score": "0.5", "engagement_score": "0.75", "language_code": "en", "language_score": "0.9999977979373631", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "introduction working python jupyter notebook using panda small datasets 100mb performance degradation seen however work large datasets 1 gb performance issue notebook clearly seen take longer time cell execute resulting either longer execution time cell notebook failing due insufficient memory tool like spark handle large datasets 100 gb tb taking full advantage capability usually requires expensive hardware practically possible try work pocs research unlike panda lack rich feature set high quality data cleaning exploration analysis mediumsized data better trying get panda rather switching different tool dataseti choosen existing kaggle dataset competition data namely jigsaw multilingual toxic comment classification sizable data demonstrate trying convey notebook brief datajigsaws api perspective serf toxicity model others growing set language see documentation full list past year field seen impressive multilingual capability latest model innovation including zeroshot learning excited learn whether result translate pun intended toxicity classification training data english data provided previous two competition test data wikipedia talk page comment several different language computing resource modeling capability grow potential support healthy conversation across globe develop strategy build effective multilingual model youll help conversation ai entire industry realize potential file jigsawtoxiccommenttraincsv data first competition dataset made english comment wikipedias talk page edits jigsawunintendedbiastraincsv data second competition expanded version civil comment dataset range additional label sample_submissioncsv sample submission file correct format testcsv comment wikipedia talk page different nonenglish language validationcsv comment wikipedia talk page different nonenglish language jigsawtoxiccommenttrainprocessedseqlen128csv training data preprocessed bert jigsawunintendedbiastrainprocessedseqlen128csv training data preprocessed bert validationprocessedseqlen128csv validation data preprocessed bert testprocessedseqlen128csv test data preprocessed bert let start importing panda data python taking look first five row", "tags_descriptive": []}