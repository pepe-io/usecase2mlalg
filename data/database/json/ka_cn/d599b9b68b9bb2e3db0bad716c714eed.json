{"title": "Xtra Credit: XGB / LGB + TFIDF + Feature Stacking", "description": "Word Vectors and Features I have been wanting to try out a model that combines word vectors with feature engineering. I also wanted my model to use KFold CV with a validation set within each fold. Finally, I wanted to ensemble the predictions to produce a combined model that out performs its individual parts. Over time, this kernel has grown to include XGBoost, LightGBM and CatBoost models. CatBoost has been dropped in favor of NB-SVM. The final output of each mode is ensembled using a technique from another kernel in this competition. This isn't the prettiest kernel with the highest leaderboard score but I hope that it's useful for those less proficient in python than I am. For those farther along, please suggest improvements! I will add more annotations, explanations, modify parameters and make changes over the course of the competition. Stay tuned...", "link": "https://www.kaggle.com/jmbull/xtra-credit-xgb-lgb-tfidf-feature-stacking", "tags": ["NLP", "Gradient Boosting", "Xgboost", "Feature Engineering"], "kind": ["Project", "(Notebook)"], "ml_libs": ["nltk", "xgboost", "lightgbm", "sklearn", "pattern"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-04-24 00:25:11", "date_scraped": "2020-12-12 19:01:17", "words": 143, "sentences": 9, "runtime": 0.0, "description_category": "Finance", "description_category_score": 0.23161572217941284, "description_category_runtime": 12.665, "description_subcategory": "Machine Learning", "description_subcategory_score": 0.8166000843048096, "description_subcategory_runtime": 20.188, "category": "Finance", "category_score": 0.23161572217941284, "subcategory": "Machine Learning", "subcategory_score": 0.8166000843048096, "runtime_cat": 32.853, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.706", "language_code": "en", "language_score": "0.9999966998507833", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "word vector feature wanting try model combine word vector feature engineering also wanted model use kfold cv validation set within fold finally wanted ensemble prediction produce combined model performs individual part time kernel grown include xgboost lightgbm catboost model catboost dropped favor nbsvm final output mode ensembled using technique another kernel competition isnt prettiest kernel highest leaderboard score hope useful le proficient python farther along please suggest improvement add annotation explanation modify parameter make change course competition stay tuned", "tags_descriptive": ["Natural Language Processing (NLP)", "Gradient Boosting", "Xgboost", "Feature Engineering"]}