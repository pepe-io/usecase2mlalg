{"title": "Don't waste your time decrypting the texts", "description": "SummaryThe title of this kernel is quite bold and, as all bold affirmations it is not completely true but, it is a good clickbait, isn't it? :-) I think we only need to identify the white space character. There are some interesting kernels trying to decrypt all the texts but, at least for the case of difficulty 1 (and maybe 2), it wouldn't be strictly neccessary. If we knew how the white space is encrypted we could tokenize properly (to models like Bag of Words the real words doesn't matter: if chars substitution has been applied it should work the same) I have tried to know what are the most probable encripted white space character using brute force: my assumption is that, the same model (Logistic Regression) should perform better if we tokenize using the right character.", "link": "https://www.kaggle.com/lbronchal/don-t-waste-your-time-decrypting-the-texts", "tags": ["NLP", "Classification"], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "vocabulary", "pattern"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2018-12-19 12:16:26", "date_scraped": "2020-12-12 16:01:56", "words": 137, "sentences": 3, "runtime": 0.0, "description_category": "Government and Public Works", "description_category_score": 0.2411222755908966, "description_category_runtime": 12.331, "description_subcategory": "Student", "description_subcategory_score": 0.2999420464038849, "description_subcategory_runtime": 18.858, "category": "Government and Public Works", "category_score": 0.2411222755908966, "subcategory": "Student", "subcategory_score": 0.2999420464038849, "runtime_cat": 31.189, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.644", "language_code": "en", "language_score": "0.9999961776170414", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "summarythe title kernel quite bold bold affirmation completely true good clickbait isnt think need identify white space character interesting kernel trying decrypt text least case difficulty 1 maybe 2 wouldnt strictly neccessary knew white space encrypted could tokenize properly model like bag word real word doesnt matter char substitution applied work tried know probable encripted white space character using brute force assumption model logistic regression perform better tokenize using right character", "tags_descriptive": ["Natural Language Processing (NLP)", "Classification"]}