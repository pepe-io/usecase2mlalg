{"title": "Parameter Tuning in One Function with Hyperopt", "description": "Hyperopt Made Simple!Automated Parameter Tuning in One Function: quick_hyperopt()  Parameter tuning can be a chore. Which parameters should be changed? And by how much? There are an almost infinite combination, so how can we find the best ones? Simple methods like GridSearch can manually run through some preset combinations but will not necessarily pick the best possible parameters, just the best in the set you provided. And this grid will have to be changed when you add new features or otherwise modify your data. A more nuanced approach is to use Bayesian optimisation, probabilistically selecting the optimum values for each parameter after every round of evaluation. This kernel provides a single function for automated Bayesian hyperparameter optimisation with LightGBM, XGBoost and CatBoost via the Hyperopt package. Simply enter your data, target, and the type of model, and it will output a parameter dictionary you can then supply to your model-training kernels. While Hyperopt is a powerful tool, it can be difficult to implement and utilise. This function does the hard(-ish) work for you! I owe a great debt to Will Koehrsen for his exhaustive kernel on automated parameter tuning. While I have streamlined, modified and generalised much of his work for easy use with different model frameworks, I strongly recommend reading his kernel from top to bottom. It contains a thorough examination of how Bayesian parameter tuning works better than random selection, along with a step-by-step guide on outputting the Hyperopt progress logs. If your aim is to truly understand the how and why of parameter tuning, his kernels are some of the best resources you'll find. The basic form of the function in this kernel is as follows: optimum_parameter_dictionary = quick_hyperopt(data, labels, 'model_type', NUM_EVALS) where 'model_type' is either 'lgbm' for LightGBM, 'xgb' for XGBoost or 'cb' for CatBoost, and NUM_EVALS is the number of parameter optimisation rounds. optimum_paramater_dictionary is the parameter dictionary you can supply to the relevant model. Usage Notes  Parameter tuning can be a lengthy and intensive process. If the kernel is crashing or failing to finish, you may be using too much of your data; consider sampling it and running multiple kernels with each sample. You can then take the average/majority vote of each parameter that hyperopt selects. You can also reduce the number of evaluation rounds with the global parameter NUM_EVALS.  This function is written for regression tasks, but includes instructions on what must be modified if you are performing a classification task instead. I have tried to make this as simple as possible. By default it will use ROC-AUC as its objective metric for LightGBM/XGBoost and Log Loss for CatBoost.  For speed, when optimising a CatBoost model this function is designed to work with GPU. Some CatBoost parameters aren't yet written to work with GPU so I've left them out. LightGBM and XGBoost work quickly with Hyperopt on CPU alone but I've left options for activating them in the code - simply uncomment the relevant lines. For LightGBM you'll have to compile the GPU version in your kernel by following the steps here.  You may want to include other parameters in the search space or remove others; hopefully the code is transparent enough for you to work out how to modify it yourself according to your needs. For example, you may want to use different evaluation metrics, learning rate ranges or model objectives.  The default objective metric for this function is Mean Absolute Error (MAE). For many regression tasks, Root Mean Squared Error (RMSE) is the preferred metric. Be sure to change this in the code below if required.   Data Preparation Your data does not need any special preperation before using quick_hyperopt() beyond the requirements of the desired model (CatBoost for example may complain about NaNs and infinite values). Simply separate the features from the labels/targets. The training features can be either a Pandas DataFrame or 2-D NumPy array. The labels can be either a Pandas Series, a single-column Pandas DataFrame or a flattened 1-D NumPy array.", "link": "https://www.kaggle.com/bigironsphere/parameter-tuning-in-one-function-with-hyperopt", "tags": [], "kind": ["Project", "(Notebook)"], "ml_libs": ["sklearn", "xgboost", "catboost", "lightgbm"], "host": "kaggle.com", "license": "Apache-2.0", "language": "english", "date_project": "2019-05-01 03:47:16", "date_scraped": "2020-12-13 13:01:16", "words": 671, "sentences": 33, "sum_nltk": "A more nuanced approach is to use Bayesian optimisation, probabilistically selecting the optimum values for each parameter after every round of evaluation.\nThis kernel provides a single function for automated Bayesian hyperparameter optimisation with LightGBM, XGBoost and CatBoost via the Hyperopt package.\nSimply enter your data, target, and the type of model, and it will output a parameter dictionary you can then supply to your model-training kernels.\nIt contains a thorough examination of how Bayesian parameter tuning works better than random selection, along with a step-by-step guide on outputting the Hyperopt progress logs.\nThe basic form of the function in this kernel is as follows: optimum_parameter_dictionary = quick_hyperopt(data, labels, 'model_type', NUM_EVALS) where 'model_type' is either 'lgbm' for LightGBM, 'xgb' for XGBoost or 'cb' for CatBoost, and NUM_EVALS is the number of parameter optimisation rounds.\nBy default it will use ROC-AUC as its objective metric for LightGBM/XGBoost and Log Loss for CatBoost.\nFor speed, when optimising a CatBoost model this function is designed to work with GPU.\nLightGBM and XGBoost work quickly with Hyperopt on CPU alone but I've left options for activating them in the code - simply uncomment the relevant lines.", "sum_nltk_words": 185, "sum_nltk_runtime": 0.009, "sum_t5": "this kernel provides a single function for automated Bayesian hyperparameter optimisation with LightGBM, XGBoost and CatBoost. if the kernel isn't working, it will be unable to handle the optimisation of the parameters. if the kernel isn't working, it will be unable to handle the optimisation of the parameters. if the kernel isn't working, it will be unable to handle the optimisation of the parameters.", "sum_t5_words": 64, "sum_t5_runtime": 6.632, "runtime": 0.004, "nltk_category": "Finance", "nltk_category_score": 0.4305322766304016, "nltk_category_runtime": 20.658, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.7579899430274963, "nltk_subcategory_runtime": 33.572, "category": "Finance", "category_score": 0.4305322766304016, "subcategory": "Machine Learning", "subcategory_score": 0.7579899430274963, "runtime_cat": 54.23, "programming_language": "Jupyter Notebook", "ml_score": "1.0", "engagement_score": "0.744", "language_code": "en", "language_score": "0.9999986594917798", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "hyperopt made simpleautomated parameter tuning one function quick_hyperopt parameter tuning chore parameter changed much almost infinite combination find best one simple method like gridsearch manually run preset combination necessarily pick best possible parameter best set provided grid changed add new feature otherwise modify data nuanced approach use bayesian optimisation probabilistically selecting optimum value parameter every round evaluation kernel provides single function automated bayesian hyperparameter optimisation lightgbm xgboost catboost via hyperopt package simply enter data target type model output parameter dictionary supply modeltraining kernel hyperopt powerful tool difficult implement utilise function hardish work owe great debt koehrsen exhaustive kernel automated parameter tuning streamlined modified generalised much work easy use different model framework strongly recommend reading kernel top bottom contains thorough examination bayesian parameter tuning work better random selection along stepbystep guide outputting hyperopt progress log aim truly understand parameter tuning kernel best resource youll find basic form function kernel follows optimum_parameter_dictionary quick_hyperoptdata label model_type num_evals model_type either lgbm lightgbm xgb xgboost cb catboost num_evals number parameter optimisation round optimum_paramater_dictionary parameter dictionary supply relevant model usage note parameter tuning lengthy intensive process kernel crashing failing finish may using much data consider sampling running multiple kernel sample take averagemajority vote parameter hyperopt selects also reduce number evaluation round global parameter num_evals function written regression task includes instruction must modified performing classification task instead tried make simple possible default use rocauc objective metric lightgbmxgboost log loss catboost speed optimising catboost model function designed work gpu catboost parameter arent yet written work gpu ive left lightgbm xgboost work quickly hyperopt cpu alone ive left option activating code simply uncomment relevant line lightgbm youll compile gpu version kernel following step may want include parameter search space remove others hopefully code transparent enough work modify according need example may want use different evaluation metric learning rate range model objective default objective metric function mean absolute error mae many regression task root mean squared error rmse preferred metric sure change code required data preparation data need special preperation using quick_hyperopt beyond requirement desired model catboost example may complain nan infinite value simply separate feature labelstargets training feature either panda dataframe 2d numpy array label either panda series singlecolumn panda dataframe flattened 1d numpy array", "tags_descriptive": []}