{"title": "Learning Set-equivariant Functions with SWARM Mappings", "link": "https://arxiv.org/abs/1906.09400", "tags": ["RNN", "Fashion"], "description": "In this work we propose a new neural network architecture that efficiently implements and learns general purpose set-equivariant functions. Such a function f maps a set of entities x = {x1, . . . , xn} from one domain to a set of same cardinality y = f (x) = {y1, . . . , yn} in another domain regardless of the ordering of the entities. The architecture is based on a gated recurrent network which is iteratively applied to all entities individually and at the same time syncs with the progression of the whole population. In reminiscence to this pattern, which can be frequently observed in nature, we call our approach SWARM mapping. Set-equivariant and generally permutation invariant functions are important building blocks for many state of the art machine learning approaches. Even in applications where the permutation invariance is not of primary interest, as to be seen in the recent success of attention based transformer models (Vaswani et. al. 2017). Accordingly, we demonstrate the power and usefulness of SWARM mappings in different applications. We compare the performance of our approach with another recently proposed set-equivariant function, the Set Transformer (Lee this http URL. 2018) and we demonstrate that models solely based on SWARM layers gives state of the art results.", "category": "Education & Research", "category_score": 1, "subcategory": "Machine Learning", "subcategory_score": 1, "ml_score": "1", "host": "zalando.com", "kind": "Article", "date_scraped": "2021-01-17 00:00:00", "learn_score": 0.5, "explore_score": 0, "compete_score": 0.75, "words": 212, "sentences": 17, "sum_nltk": "In this work we propose a new neural network architecture that efficiently implements and learns general purpose set-equivariant functions.\nSuch a function f maps a set of entities x = {x1, .\n, xn} from one domain to a set of same cardinality y = f (x) = {y1, .\n, yn} in another domain regardless of the ordering of the entities.\nThe architecture is based on a gated recurrent network which is iteratively applied to all entities individually and at the same time syncs with the progression of the whole population.\nIn reminiscence to this pattern, which can be frequently observed in nature, we call our approach SWARM mapping.\nSet-equivariant and generally permutation invariant functions are important building blocks for many state of the art machine learning approaches.\nEven in applications where the permutation invariance is not of primary interest, as to be seen in the recent success of attention based transformer models (Vaswani et.\nAccordingly, we demonstrate the power and usefulness of SWARM mappings in different applications.\nWe compare the performance of our approach with another recently proposed set-equivariant function, the Set Transformer (Lee this http URL.\n2018) and we demonstrate that models solely based on SWARM layers gives state of the art results.", "sum_nltk_words": 196, "sum_nltk_runtime": 0.01, "sum_t5": "the architecture is based on a gated recurrent network. it is iteratively applied to all entities individually and at the same time syncs with the progression of the whole population. set-equivariant and generally permutation invariant functions are important building blocks for many state of the art machine learning approaches. however, in applications where the permutation invariance is not of primary interest, as to be seen in the recent success of attention based transformer models.", "sum_t5_words": 74, "sum_t5_runtime": 7.583, "language_code": "en", "language": "english", "language_score": "0.9999976804067623", "runtime": 0.003, "description_lemmatized": "work propose new neural network architecture efficiently implement learns general purpose setequivariant function function f map set entity x x1 xn one domain set cardinality f x y1 yn another domain regardless ordering entity architecture based gated recurrent network iteratively applied entity individually time syncs progression whole population reminiscence pattern frequently observed nature call approach swarm mapping setequivariant generally permutation invariant function important building block many state art machine learning approach even application permutation invariance primary interest seen recent success attention based transformer model vaswani et al 2017 accordingly demonstrate power usefulness swarm mapping different application compare performance approach another recently proposed setequivariant function set transformer lee http url 2018 demonstrate model solely based swarm layer give state art result", "tags_descriptive": ["Recurrent Neural Network (RNN)", "Fashion"]}