{"title": "Contextual BERT: Conditioning the Language Model Using a Global State", "link": "https://arxiv.org/abs/2010.15778", "tags": ["NLP", "Fashion"], "description": "BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly.", "category": "Education & Research", "category_score": 1, "subcategory": "Machine Learning", "subcategory_score": 1, "ml_score": "1", "host": "zalando.com", "kind": "Article", "date_scraped": "2021-01-17 00:00:00", "learn_score": 0.5, "explore_score": 0, "compete_score": 0.75, "words": 121, "sentences": 5, "language_code": "en", "language": "english", "language_score": "0.9999981080960278", "runtime": 0.001, "description_lemmatized": "bert popular language model whose main pretraining task fill blank ie predicting word masked sentence based remaining word application however additional context help model make right prediction eg taking domain time writing account motivates u advance bert architecture adding global state conditioning fixedsized context present two novel approach apply industry usecase complete fashion outfit missing article conditioned specific customer experimental comparison method literature show method improve personalization significantly", "tags_descriptive": ["Natural Language Processing (NLP)", "Fashion"]}