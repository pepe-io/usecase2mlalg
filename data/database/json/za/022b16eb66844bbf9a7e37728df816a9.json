{"title": "A Bandit Framework for Optimal Selection of Reinforcement Learning Agents", "link": "https://arxiv.org/abs/1902.03657", "tags": ["DL", "RL", "Fashion"], "description": "Deep Reinforcement Learning has been shown to be very successful in complex games, e.g. Atari or Go. These games have clearly defined rules, and hence allow simulation. In many practical applications, however, interactions with the environment are costly and a good simulator of the environment is not available. Further, as environments differ by application, the optimal inductive bias (architecture, hyperparameters, etc.) of a reinforcement agent depends on the application. In this work, we propose a multi-arm bandit framework that selects from a set of different reinforcement learning agents to choose the one with the best inductive bias. To alleviate the problem of sparse rewards, the reinforcement learning agents are augmented with surrogate rewards. This helps the bandit framework to select the best agents early, since these rewards are smoother and less sparse than the environment reward. The bandit has the double objective of maximizing the reward while the agents are learning and selecting the best agent after a finite number of learning steps. Our experimental results on standard environments show that the proposed framework is able to consistently select the optimal agent after a finite number of steps, while collecting more cumulative reward compared to selecting a sub-optimal architecture or uniformly alternating between different agents.", "category": "Education & Research", "category_score": 1, "subcategory": "Machine Learning", "subcategory_score": 1, "ml_score": "1", "host": "zalando.com", "kind": "Article", "date_scraped": "2021-01-17 00:00:00", "learn_score": 0.5, "explore_score": 0, "compete_score": 0.75, "words": 205, "sentences": 10, "sum_nltk": "Deep Reinforcement Learning has been shown to be very successful in complex games, e.g. Atari or Go. These games have clearly defined rules, and hence allow simulation.\nIn many practical applications, however, interactions with the environment are costly and a good simulator of the environment is not available.\nFurther, as environments differ by application, the optimal inductive bias (architecture, hyperparameters, etc.) of a reinforcement agent depends on the application.\nIn this work, we propose a multi-arm bandit framework that selects from a set of different reinforcement learning agents to choose the one with the best inductive bias.\nTo alleviate the problem of sparse rewards, the reinforcement learning agents are augmented with surrogate rewards.\nThis helps the bandit framework to select the best agents early, since these rewards are smoother and less sparse than the environment reward.\nThe bandit has the double objective of maximizing the reward while the agents are learning and selecting the best agent after a finite number of learning steps.\nOur experimental results on standard environments show that the proposed framework is able to consistently select the optimal agent after a finite number of steps, while collecting more cumulative reward compared to selecting a sub-optimal architecture or uniformly alternating between different agents.", "sum_nltk_words": 198, "sum_nltk_runtime": 0.046, "sum_t5": "a bandit framework is proposed to select the best reinforcement learning agent. reinforcement learning agents are augmented with surrogate rewards. the framework is able to select the best agent after a finite number of steps. the results are compared to selecting a sub-optimal architecture or uniformly alternating between different agents. a symbiosis of the framework and the environment is also proposed. a symbiosis of the framework is also proposed.", "sum_t5_words": 69, "sum_t5_runtime": 6.627, "language_code": "en", "language": "english", "language_score": "0.9999980928472666", "runtime": 0.002, "description_lemmatized": "deep reinforcement learning shown successful complex game eg atari go game clearly defined rule hence allow simulation many practical application however interaction environment costly good simulator environment available environment differ application optimal inductive bias architecture hyperparameters etc reinforcement agent depends application work propose multiarm bandit framework selects set different reinforcement learning agent choose one best inductive bias alleviate problem sparse reward reinforcement learning agent augmented surrogate reward help bandit framework select best agent early since reward smoother le sparse environment reward bandit double objective maximizing reward agent learning selecting best agent finite number learning step experimental result standard environment show proposed framework able consistently select optimal agent finite number step collecting cumulative reward compared selecting suboptimal architecture uniformly alternating different agent", "tags_descriptive": ["Deep Learning (DL)", "Reinforcement Learning (RL)", "Fashion"]}