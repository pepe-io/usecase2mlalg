{"title": "The Hewlett Foundation: Automated Essay Scoring", "description": "Develop an automated scoring algorithm for student-written essays.The William and Flora Hewlett Foundation (Hewlett) is sponsoring the Automated Student Assessment Prize (ASAP).\u00a0 Hewlett is appealing to data scientists and machine learning specialists to help solve an important social problem.\u00a0 We need fast, effective and affordable solutions for automated grading of student-written essays. Hewlett is sponsoring the following prizes:  $60,000:\u00a0 1 place $30,000:\u00a0 2 place $10,000:\u00a0 3 place  You are provided access to hand scored essays, so that you can build, train and test scoring engines against a wide field of competitors.\u00a0 Your success depends upon how closely you can deliver scores to those of human expert graders.\u00a0 While we believe that these financial incentives are important, we also intend to introduce top performers both to leading vendors in the industry and/or an established base of interested buyers.\u00a0 Hewlett is opening the field of automated student assessment to you.\u00a0 We want to induce a breakthrough that is both personally satisfying and game-changing for improving public education. Today, state departments of education are developing new forms of testing and grading methods, to assess the new common core standards.\u00a0 In this environment the need for more sophisticated and affordable options is vital.\u00a0 For example, we know that essays are an important expression of academic achievement, but they are expensive and time consuming for states to grade them by hand.\u00a0 So, we are frequently limited to multiple-choice standardized tests.\u00a0 We believe that automated scoring systems can yield fast, effective and affordable solutions that would allow states to introduce essays and other sophisticated testing tools.\u00a0 We believe that you can help us pave the way towards a breakthrough.\u00a0 ASAP is designed to achieve the following goals:  Challenge developers of automated student assessment systems to demonstrate their current capabilities. Compare the efficacy and cost of automated scoring to that of human graders. Reveal product capabilities to state departments of education and other key decision makers interested in adopting them.  The graded essays are selected according to specific data characteristics.\u00a0 On average, each essay is approximately 150 to 550 words in length. \u00a0Some are more dependent upon source materials than others.\u00a0 This range of essay type is provided so that we can better understand the strengths of your solution.\u00a0 It is our intent to showcase quality and reliability, based on how well you can match expert human graders for each essay. You will be provided with training data for each essay prompt.\u00a0 The number of training essays does vary.\u00a0 For example, the lowest amount of training data is 1,190 essays, randomly selected from a total of 1,982.\u00a0 The data will contain ASCII formatted text for each essay followed by one or more human scores, and (where necessary) a final resolved human score.\u00a0 Where it is relevant, you are provided with more than one human score, so that you may evaluate the reliability of the human scorers, but - keep in mind - that you will be predicting to the resolved score.\u00a0 Also, please note that most essays are scored using a holistic scoring rubric.\u00a0 However, one data set uses a trait scoring rubric.\u00a0 The variability is intended to test the limits of your scoring engine\u2019s capabilities. Following a period of 3 months to build and/or train your engine, you will be provided with test data that will contain new essays, randomly selected for blind evaluation.\u00a0 However, you will notice that the rater and resolved score columns will be blank.\u00a0 You will be asked to supply, based on your engine's predictions for each essay, your score in the resolved score column and then submit your new data set on this site. As part of the file that you will submit with your predictive scores, you will be asked to submit additional information.\u00a0 We would like to understand both the time and capital that you\u2019ve spent developing your engine, the profile of your team (or you as an individual if you are working alone) and the projected cost to implement your solution on a larger scale, along with any known limitations.\u00a0 Basically, you will have the opportunity to present your case for who you are, why your model is commercially viable and to what extent you can use your model to satisfy the interests of potential buyers.\u00a0 This other information will not be used to determine any prize rewards, and it is optional.\u00a0 But, if you provide it, it will be used to evaluate whether or not your model should be presented to state departments of education and others who stand to benefit from your work. Also, please note that it is our intention to stage other follow-on ASAP phases in the months ahead.\u00a0 We are starting with graded essays and will follow with new data:  Phase 1: Demonstration for long-form constructed response (essays); Phase 2: Demonstration for short-form constructed response (short answers); Phase 3: Demonstration for symbolic mathematical/logic reasoning (charts/graphs).  In every instance, we seek to drive innovation for new solutions to automated student assessment.\u00a0 We hope that you will enjoy this process.\u00a0 May the best model win!", "link": "https://www.kaggle.com/c/asap-aes", "tags": [], "kind": ["Project", "(Competition)", "(Dataset)"], "host": "kaggle.com", "date_project": "2012-05-01 01:59:59", "words": 846, "sentences": 12, "sum_nltk": "We need fast, effective and affordable solutions for automated grading of student-written essays.\nHewlett is sponsoring the following prizes:  $60,000:\u00a0 1 place $30,000:\u00a0 2 place $10,000:\u00a0 3 place  You are provided access to hand scored essays, so that you can build, train and test scoring engines against a wide field of competitors.\nToday, state departments of education are developing new forms of testing and grading methods, to assess the new common core standards.\nWe believe that automated scoring systems can yield fast, effective and affordable solutions that would allow states to introduce essays and other sophisticated testing tools.\nFollowing a period of 3 months to build and/or train your engine, you will be provided with test data that will contain new essays, randomly selected for blind evaluation.\nYou will be asked to supply, based on your engine's predictions for each essay, your score in the resolved score column and then submit your new data set on this site.\nBut, if you provide it, it will be used to evaluate whether or not your model should be presented to state departments of education and others who stand to benefit from your work.", "sum_nltk_words": 187, "sum_nltk_runtime": 0.011, "sum_t5": "develop an automated scoring algorithm for student-written essays. the automated student assessment prize is $60,000: 1 place $30,000: 2 place $10,000: 3 place. the prize is open to data scientists and machine learning specialists. john sutter: ASAP is a great opportunity to help improve public education. sutter: we need to improve the quality of student-written essays. a winner will be announced on november 15. a winner will be announced on november 15.", "sum_t5_words": 72, "sum_t5_runtime": 6.804, "runtime": 0.005, "nltk_category": "Education & Research", "nltk_category_score": 0.6023948192596436, "nltk_category_runtime": 16.454, "nltk_subcategory": "Student", "nltk_subcategory_score": 0.8594368100166321, "nltk_subcategory_runtime": 26.404, "category": "Education & Research", "category_score": 0.6023948192596436, "subcategory": "Student", "subcategory_score": 0.8594368100166321, "runtime_cat": 42.859, "engagement_score": "0.53", "language_code": "en", "language": "english", "language_score": "0.9999978292562244", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "develop automated scoring algorithm studentwritten essaysthe william flora hewlett foundation hewlett sponsoring automated student assessment prize asap hewlett appealing data scientist machine learning specialist help solve important social problem need fast effective affordable solution automated grading studentwritten essay hewlett sponsoring following prize 60000 1 place 30000 2 place 10000 3 place provided access hand scored essay build train test scoring engine wide field competitor success depends upon closely deliver score human expert grader believe financial incentive important also intend introduce top performer leading vendor industry andor established base interested buyer hewlett opening field automated student assessment want induce breakthrough personally satisfying gamechanging improving public education today state department education developing new form testing grading method ass new common core standard environment need sophisticated affordable option vital example know essay important expression academic achievement expensive time consuming state grade hand frequently limited multiplechoice standardized test believe automated scoring system yield fast effective affordable solution would allow state introduce essay sophisticated testing tool believe help u pave way towards breakthrough asap designed achieve following goal challenge developer automated student assessment system demonstrate current capability compare efficacy cost automated scoring human grader reveal product capability state department education key decision maker interested adopting graded essay selected according specific data characteristic average essay approximately 150 550 word length dependent upon source material others range essay type provided better understand strength solution intent showcase quality reliability based well match expert human grader essay provided training data essay prompt number training essay vary example lowest amount training data 1190 essay randomly selected total 1982 data contain ascii formatted text essay followed one human score necessary final resolved human score relevant provided one human score may evaluate reliability human scorer keep mind predicting resolved score also please note essay scored using holistic scoring rubric however one data set us trait scoring rubric variability intended test limit scoring engine capability following period 3 month build andor train engine provided test data contain new essay randomly selected blind evaluation however notice rater resolved score column blank asked supply based engine prediction essay score resolved score column submit new data set site part file submit predictive score asked submit additional information would like understand time capital youve spent developing engine profile team individual working alone projected cost implement solution larger scale along known limitation basically opportunity present case model commercially viable extent use model satisfy interest potential buyer information used determine prize reward optional provide used evaluate whether model presented state department education others stand benefit work also please note intention stage followon asap phase month ahead starting graded essay follow new data phase 1 demonstration longform constructed response essay phase 2 demonstration shortform constructed response short answer phase 3 demonstration symbolic mathematicallogic reasoning chartsgraphs every instance seek drive innovation new solution automated student assessment hope enjoy process may best model win", "tags_descriptive": []}