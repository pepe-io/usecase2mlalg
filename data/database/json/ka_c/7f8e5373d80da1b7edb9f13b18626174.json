{"title": "TREC-COVID Information Retrieval", "description": "Build a pandemic document retrieval systemLAUNCHED   This competition was launched and opened for submissions on May 27th 2020. Submissions will close in 1 week at 11:00 AM UTC on June 3rd 2020. The public leaderboard is based on the TREC-COVID Round 2 dataset. The private leaderboard will be based on the Round 3 dataset, which will be evaluated after the competition closes. Review the Data page for more details.  Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presents a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge will identify answers for some of today's questions while building infrastructure to improve tomorrow's search systems. Kaggle first teamed up with the Allen Institute for AI in the launch of the COVID-19 Open Research Dataset (CORD-19). TREC-COVID builds on the CORD-19 Challenge by using the same document set, a collection of biomedical literature articles that has been updated on a weekly rolling basis.  This is the 3rd Round of the TREC-COVID Challenge. Prior runs were hosted directly on the TREC-COVID Site. For this round, you have the option to submit on Kaggle or directly to the TREC-COVID platform. The organizers have added 5 additional COVID-related topics to the 35 topics from the first two rounds, for a total of 40 topics. You will create a retrieval system that returns ranked lists of documents from CORD-19 for (a) each of these additional Round 3 topics (\"runs\") and as well as (b) residual rankings on the completed Round 1 & 2 topics, i.e., for any documents not judged in the CORD-19 dataset (not previously included as a ranked document).  The eligible population of documents for Round 3 is anything included in the CORD-19 release up to Round 3's launch date, last updated on May 19th 2020. Following the close of Round 3, NIST will gather the collective set of participants' runs, to include those participants submitting directly through TREC-COVID. The organizers will then assess some reasonable subset of these submissions for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, will then be used to score the submitted runs. It is important to understand that not all documents will be assessed, and thus the private leaderboard score will be based on partial document assessment. With your help, the final document and topic sets together with the cumulative relevance judgments will comprise a COVID test collection. The incremental nature of the collection will support research on search systems for dynamic environments. Acknowledgments The Text REtrieval Conference (TREC) was founded in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. The TREC-COVID Challenge is being organized by the Allen Institute for Artificial Intelligence (AI2), the National Institute of Standards and Technology (NIST), the National Library of Medicine (NLM), Oregon Health and Science University (OHSU), and the University of Texas Health Science Center at Houston (UTHealth). See the NIST press release for more information.", "link": "https://www.kaggle.com/c/trec-covid-information-retrieval", "tags": [], "kind": ["Project", "(Competition)", "(Dataset)"], "host": "kaggle.com", "date_project": "2020-06-03 13:00:00", "words": 550, "sentences": 25, "sum_nltk": "Build a pandemic document retrieval systemLAUNCHED   This competition was launched and opened for submissions on May 27th 2020.\nThe public leaderboard is based on the TREC-COVID Round 2 dataset.\nThe private leaderboard will be based on the Round 3 dataset, which will be evaluated after the competition closes.\nThe results of the TREC-COVID Challenge will identify answers for some of today's questions while building infrastructure to improve tomorrow's search systems.\nKaggle first teamed up with the Allen Institute for AI in the launch of the COVID-19 Open Research Dataset (CORD-19).\nTREC-COVID builds on the CORD-19 Challenge by using the same document set, a collection of biomedical literature articles that has been updated on a weekly rolling basis.\nFor this round, you have the option to submit on Kaggle or directly to the TREC-COVID platform.\nThe eligible population of documents for Round 3 is anything included in the CORD-19 release up to Round 3's launch date, last updated on May 19th 2020.\nFollowing the close of Round 3, NIST will gather the collective set of participants' runs, to include those participants submitting directly through TREC-COVID.\nThe results of the human annotation, known as relevance judgments, will then be used to score the submitted runs.", "sum_nltk_words": 196, "sum_nltk_runtime": 0.007, "sum_t5": "the competition was launched and opened for submissions on may 27th 2020. the public leaderboard is based on the TREC-COVID Round 2 dataset. the private leaderboard will be based on the Round 3 dataset. the results of the TREC-COVID Challenge will identify answers for some of today's questions. the organizers have added 5 additional COVID-related topics to the 35 topics from the first two rounds. a total of 40 topics will be retrieved from the 3rd", "sum_t5_words": 76, "sum_t5_runtime": 7.032, "runtime": 0.01, "nltk_category": "Biotechnological & Life Sciences", "nltk_category_score": 0.5817021727561951, "nltk_category_runtime": 19.676, "nltk_subcategory": "Preventative and Reactive", "nltk_subcategory_score": 0.6942471265792847, "nltk_subcategory_runtime": 30.863, "category": "Biotechnological & Life Sciences", "category_score": 0.5817021727561951, "subcategory": "Preventative and Reactive", "subcategory_score": 0.6942471265792847, "runtime_cat": 50.539, "engagement_score": "0.362", "language_code": "en", "language": "english", "language_score": "0.99999520723174", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "build pandemic document retrieval systemlaunched competition launched opened submission may 27th 2020 submission close 1 week 1100 utc june 3rd 2020 public leaderboard based treccovid round 2 dataset private leaderboard based round 3 dataset evaluated competition close review data page detail researcher clinician policy maker involved response covid19 constantly searching reliable information virus impact present unique opportunity information retrieval ir text processing community contribute response pandemic well study method quickly standing information system similar future event result treccovid challenge identify answer today question building infrastructure improve tomorrow search system kaggle first teamed allen institute ai launch covid19 open research dataset cord19 treccovid build cord19 challenge using document set collection biomedical literature article updated weekly rolling basis 3rd round treccovid challenge prior run hosted directly treccovid site round option submit kaggle directly treccovid platform organizer added 5 additional covidrelated topic 35 topic first two round total 40 topic create retrieval system return ranked list document cord19 additional round 3 topic run well b residual ranking completed round 1 2 topic ie document judged cord19 dataset previously included ranked document eligible population document round 3 anything included cord19 release round 3 launch date last updated may 19th 2020 following close round 3 nist gather collective set participant run include participant submitting directly treccovid organizer ass reasonable subset submission relevance human annotator biomedical expertise result human annotation known relevance judgment used score submitted run important understand document assessed thus private leaderboard score based partial document assessment help final document topic set together cumulative relevance judgment comprise covid test collection incremental nature collection support research search system dynamic environment acknowledgment text retrieval conference trec founded 1992 support research within information retrieval community providing infrastructure necessary largescale evaluation text retrieval methodology treccovid challenge organized allen institute artificial intelligence ai2 national institute standard technology nist national library medicine nlm oregon health science university ohsu university texas health science center houston uthealth see nist press release information", "tags_descriptive": []}