{"title": "Jigsaw Unintended Bias in Toxicity Classification", "description": "Detect toxicity across a diverse range of conversationsCan you help detect toxic comments \u2015 and minimize unintended model bias? That's your challenge in this competition. The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.  Last year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.  Here\u2019s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users. In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive. Acknowledgments The Conversation AI team would like to thank Civil Comments for making this dataset available publicly and the Online Hate Index Research Project at D-Lab, University of California, Berkeley, whose labeling survey/instrument informed the dataset labeling. We'd also like to thank everyone who has contributed to Conversation AI's research, especially those who took part in our last competition, the success of which led to the creation of this challenge.    This is a Kernels-only competition. Refer to Kernels Requirements for details.", "link": "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification", "tags": ["Exploratory Data Analysis"], "kind": ["Project", "(Competition)", "(Dataset)"], "host": "kaggle.com", "date_project": "2019-07-18 21:35:00", "words": 392, "sentences": 18, "sum_nltk": "Detect toxicity across a diverse range of conversationsCan you help detect toxic comments \u2015 and minimize unintended model bias?\nThis year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.\nHere\u2019s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity.\nModels predicted a high likelihood of toxicity for comments containing those identities (e.g.\nIn this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities.\nDevelop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.\nAcknowledgments The Conversation AI team would like to thank Civil Comments for making this dataset available publicly and the Online Hate Index Research Project at D-Lab, University of California, Berkeley, whose labeling survey/instrument informed the dataset labeling.\nWe'd also like to thank everyone who has contributed to Conversation AI's research, especially those who took part in our last competition, the success of which led to the creation of this challenge.", "sum_nltk_words": 200, "sum_nltk_runtime": 0.005, "sum_t5": "the Conversation AI team builds technology to protect voices in conversation. last year, you built models to recognize toxicity and several subtypes of toxicity. this year's competition is a related challenge: building toxicity models that operate fairly across conversations. you'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.", "sum_t5_words": 73, "sum_t5_runtime": 6.715, "runtime": 0.0, "nltk_category": "Utilities", "nltk_category_score": 0.5618791580200195, "nltk_category_runtime": 17.231, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8960931897163391, "nltk_subcategory_runtime": 28.155, "category": "Utilities", "category_score": 0.5618791580200195, "subcategory": "Machine Learning", "subcategory_score": 0.8960931897163391, "runtime_cat": 45.387, "engagement_score": "0.702", "language_code": "en", "language": "english", "language_score": "0.9999984876437911", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "detect toxicity across diverse range conversationscan help detect toxic comment minimize unintended model bias thats challenge competition conversation ai team research initiative founded jigsaw google part alphabet build technology protect voice conversation main area focus machine learning model identify toxicity online conversation toxicity defined anything rude disrespectful otherwise likely make someone leave discussion last year toxic comment classification challenge built multiheaded model recognize toxicity several subtypes toxicity year competition related challenge building toxicity model operate fairly across diverse range conversation here background conversation ai team first built toxicity model found model incorrectly learned associate name frequently attacked identity toxicity model predicted high likelihood toxicity comment containing identity eg gay even comment actually toxic gay woman happens training data pulled available source unfortunately certain identity overwhelmingly referred offensive way training model data imbalance risk simply mirroring bias back user competition youre challenged build model recognizes toxicity minimizes type unintended bias respect mention identity youll using dataset labeled identity mention optimizing metric designed measure unintended bias develop strategy reduce unintended bias machine learning model youll help conversation ai team entire industry build model work well wide range conversation disclaimer dataset competition contains text may considered profane vulgar offensive acknowledgment conversation ai team would like thank civil comment making dataset available publicly online hate index research project dlab university california berkeley whose labeling surveyinstrument informed dataset labeling wed also like thank everyone contributed conversation ai research especially took part last competition success led creation challenge kernelsonly competition refer kernel requirement detail", "tags_descriptive": ["Exploratory Data Analysis"]}