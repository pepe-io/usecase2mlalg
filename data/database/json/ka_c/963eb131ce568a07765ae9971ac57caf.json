{"title": "Freesound General-Purpose Audio Tagging Challenge", "description": "Can you automatically recognize sounds from a wide range of real-world environments?Some sounds are distinct and instantly recognizable, like a baby\u2019s laugh or the strum of a guitar. Other sounds aren\u2019t clear and are difficult to pinpoint. If you close your eyes, can you tell which of the sounds below is a chainsaw versus a blender? Moreover, we often experience a mix of sounds that create an ambience \u2013 like the clamoring of construction, a hum of traffic from outside the door, blended with loud laughter from the room, and the ticking of the clock on your wall. The sound clip below is of a busy food court in the UK. Partly because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. Currently, a lot of manual effort is required for tasks like annotating sound collections and providing captions for non-speech events in audiovisual content. To tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 370,000 Creative Commons Licensed sounds) and Google Research\u2019s Machine Perception Team (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this competition. You\u2019re challenged to build a general-purpose automatic audio tagging system using a dataset of audio files covering a wide range of real-world environments. Sounds in the dataset include things like musical instruments, human sounds, domestic sounds, and animals from Freesound\u2019s library, annotated using a vocabulary of more than 40 labels from Google\u2019s AudioSet ontology. To succeed in this competition your systems will need to be able to recognize an increased number of sound events of very diverse nature, and to leverage subsets of training data featuring annotations of varying reliability (see Data section for more information).", "link": "https://www.kaggle.com/c/freesound-audio-tagging", "tags": ["Feature Engineering", "Exploratory Data Analysis"], "kind": ["Project", "(Competition)", "(Dataset)"], "host": "kaggle.com", "date_project": "2018-08-01 13:59:00", "words": 299, "sentences": 10, "sum_nltk": "Can you automatically recognize sounds from a wide range of real-world environments?Some sounds are distinct and instantly recognizable, like a baby\u2019s laugh or the strum of a guitar.\nPartly because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist.\nCurrently, a lot of manual effort is required for tasks like annotating sound collections and providing captions for non-speech events in audiovisual content.\nTo tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 370,000 Creative Commons Licensed sounds) and Google Research\u2019s Machine Perception Team (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this competition.\nYou\u2019re challenged to build a general-purpose automatic audio tagging system using a dataset of audio files covering a wide range of real-world environments.\nTo succeed in this competition your systems will need to be able to recognize an increased number of sound events of very diverse nature, and to leverage subsets of training data featuring annotations of varying reliability (see Data section for more information).", "sum_nltk_words": 180, "sum_nltk_runtime": 0.003, "sum_t5": "no reliable automatic general-purpose audio tagging systems exist. a lot of manual effort is required for tasks like annotating sound collections. a dataset of audio files covering a wide range of real-world environments. sounds in the dataset include things like musical instruments, human sounds, domestic sounds, and animals. if you close your eyes, can you tell which of the sounds below is a chainsaw versus a blender?. if you close your eyes, can you tell which of the sounds below", "sum_t5_words": 80, "sum_t5_runtime": 6.653, "runtime": 0.002, "nltk_category": "Utilities", "nltk_category_score": 0.3710709810256958, "nltk_category_runtime": 17.276, "nltk_subcategory": "General", "nltk_subcategory_score": 0.8270766139030457, "nltk_subcategory_runtime": 27.694, "category": "Utilities", "category_score": 0.3710709810256958, "subcategory": "General", "subcategory_score": 0.8270766139030457, "runtime_cat": 44.971, "engagement_score": "0.613", "language_code": "en", "language": "english", "language_score": "0.9999977086146478", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "automatically recognize sound wide range realworld environmentssome sound distinct instantly recognizable like baby laugh strum guitar sound arent clear difficult pinpoint close eye tell sound chainsaw versus blender moreover often experience mix sound create ambience like clamoring construction hum traffic outside door blended loud laughter room ticking clock wall sound clip busy food court uk partly vastness sound experience reliable automatic generalpurpose audio tagging system exist currently lot manual effort required task like annotating sound collection providing caption nonspeech event audiovisual content tackle problem freesound initiative mtgupf maintains collaborative database 370000 creative common licensed sound google research machine perception team creator audioset largescale dataset manually annotated audio event 500 class teamed develop dataset competition youre challenged build generalpurpose automatic audio tagging system using dataset audio file covering wide range realworld environment sound dataset include thing like musical instrument human sound domestic sound animal freesounds library annotated using vocabulary 40 label google audioset ontology succeed competition system need able recognize increased number sound event diverse nature leverage subset training data featuring annotation varying reliability see data section information", "tags_descriptive": ["Feature Engineering", "Exploratory Data Analysis"]}