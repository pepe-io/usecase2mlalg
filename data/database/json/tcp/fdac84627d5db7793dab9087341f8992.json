{"title": "Support Vector Machines (SVM) in Machine Learning", "description": "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this article, I will develop the intuition behind support vector machines and their use in classification problems.I will begin with the standard imports:Let\u2019s consider the simple case of a classification task, in which the two classes of points are well separated:A linear discriminative classifier would attempt to draw a straight line separating the two sets of data and thereby create a model for classification. For two dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can correctly discriminate between the two classes.I will draw them as follows:These are three\u00a0very\u00a0different separators, which, nevertheless, correctly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the \u201cX\u201d in this plot) will be assigned a different label! Our simple intuition of \u201cdrawing a line between classes\u201d is not enough, and we need to think a bit deeper.Support vector machines offer one way to improve on this. The intuition is this: rather than merely drawing a zero-width line between the classes, we can bring around each edge a\u00a0margin\u00a0of some width, up to the nearest point. Here is an example of how this might look:In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a\u00a0maximum margin\u00a0estimator.Let\u2019s see the result of an exact fit for this data: we will use Scikit-Learn\u2019s support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the\u00a0C\u00a0parameter to a very large number:To better visualize what\u2019s happening here, let\u2019s create a quick convenience function that will plot SVM decision boundaries for us:This is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin: the black circles in this figure indicate them. These points are the pivotal elements of this fit, and are known as the\u00a0support vectors, and give the algorithm its name. In Scikit-Learn, the identity of these points are stored in the\u00a0support_vectors_\u00a0the attribute of the classifier:A key to this classifier\u2019s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the right side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:As an example of support vector machines in action, let\u2019s take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn:Let\u2019s plot a few of these faces to see what we\u2019re working with:Each image contains [62\u00d747] or nearly 3,000 pixels. We could proceed by merely using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here, we will use a principal component analysis to remove 150 fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:For the sake of testing our classifier output, we will split the data into a training and testing set:Finally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust\u00a0C\u00a0(which controls the margin hardness) and\u00a0gamma\u00a0(which controls the size of the radial basis function kernel), and determine the best model:The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen:Let\u2019s take a look at a few of the test images along with their predicted values:Out of this small sample, our optimal estimator mislabeled only a single face (Bush\u2019s face in the bottom row was mislabeled as Blair). We can get a better sense of our estimator\u2019s performance using the classification report, which lists recovery statistics label by label:We might also display the confusion matrix between these classes:With those traits in mind, I generally only turn to Support Vector Machines once another simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs. Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results. I hope you liked this article, feel free to ask questions on SVMs, or any other topic, in the comments section below.", "link": "https://thecleverprogrammer.com/2020/07/06/support-vector-machines-svm-in-machine-learning/", "tags": ["CV", "Classification", "Regression", "SVM"], "kind": "Project", "ml_libs": ["sklearn"], "host": "thecleverprogrammer.com", "language": "english", "date_project": "2020-07-06 15:49:55", "date_scraped": "2020-12-20 00:00:00", "words": 863, "sentences": 20, "sum_nltk": "In this article, I will develop the intuition behind support vector machines and their use in classification problems.I will begin with the standard imports:Let\u2019s consider the simple case of a classification task, in which the two classes of points are well separated:A linear discriminative classifier would attempt to draw a straight line separating the two sets of data and thereby create a model for classification.\nSupport vector machines are an example of such a\u00a0maximum margin\u00a0estimator.Let\u2019s see the result of an exact fit for this data: we will use Scikit-Learn\u2019s support vector classifier to train an SVM model on this data.\nTechnically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:As an example of support vector machines in action, let\u2019s take a look at the facial recognition problem.", "sum_nltk_words": 173, "sum_nltk_runtime": 0.008, "sum_t5": "support vector machines (SVMs) are a powerful class of supervised algorithms. they are used for both classification and regression. this article will develop the intuition behind support vector machines. it will be used to train a classifier on two dimensional data. the dividing line that maximizes the margin between the two sets of points is known as the support vectors. the black circles in this figure indicate them. the dividing line is the one that maximizes the margin between the two sets of points.", "sum_t5_words": 84, "sum_t5_runtime": 6.839, "runtime": 0.011, "nltk_category": "Utilities", "nltk_category_score": 0.037852004170417786, "nltk_category_runtime": 15.738, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8530141711235046, "nltk_subcategory_runtime": 25.058, "category": "Utilities", "category_score": 0.037852004170417786, "subcategory": "Machine Learning", "subcategory_score": 0.8530141711235046, "runtime_cat": 40.796, "programming_language": "Python", "ml_score": "1.0", "language_code": "en", "language_score": "0.9999988226109265", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "support vector machine svms particularly powerful flexible class supervised algorithm classification regression article develop intuition behind support vector machine use classification problemsi begin standard importslets consider simple case classification task two class point well separateda linear discriminative classifier would attempt draw straight line separating two set data thereby create model classification two dimensional data like shown task could hand immediately see problem one possible dividing line correctly discriminate two classesi draw followsthese three different separator nevertheless correctly discriminate sample depending choose new data point eg one marked x plot assigned different label simple intuition drawing line class enough need think bit deepersupport vector machine offer one way improve intuition rather merely drawing zerowidth line class bring around edge margin width nearest point example might lookin support vector machine line maximizes margin one choose optimal model support vector machine example maximum margin estimatorlets see result exact fit data use scikitlearns support vector classifier train svm model data time use linear kernel set c parameter large numberto better visualize whats happening let create quick convenience function plot svm decision boundary usthis dividing line maximizes margin two set point notice training point touch margin black circle figure indicate point pivotal element fit known support vector give algorithm name scikitlearn identity point stored support_vectors_ attribute classifiera key classifier success fit position support vector matter point margin right side modify fit technically point contribute loss function used fit model position number matter long cross marginwe see example plot model learned first 60 point first 120 point datasetas example support vector machine action let take look facial recognition problem use labeled face wild dataset consists several thousand collated photo various public figure fetcher dataset built scikitlearnlets plot face see working witheach image contains 6247 nearly 3000 pixel could proceed merely using pixel value feature often effective use sort preprocessor extract meaningful feature use principal component analysis remove 150 fundamental component feed support vector machine classifier straightforwardly packaging preprocessor classifier single pipelinefor sake testing classifier output split data training testing setfinally use grid search crossvalidation explore combination parameter adjust c control margin hardness gamma control size radial basis function kernel determine best modelthe optimal value fall toward middle grid fell edge would want expand grid make sure found true optimumnow crossvalidated model predict label test data model yet seenlets take look test image along predicted valuesout small sample optimal estimator mislabeled single face bush face bottom row mislabeled blair get better sense estimator performance using classification report list recovery statistic label labelwe might also display confusion matrix classeswith trait mind generally turn support vector machine another simpler faster le tuningintensive method shown insufficient need nevertheless cpu cycle commit training crossvalidating svm data method lead excellent result hope liked article feel free ask question svms topic comment section", "tags_descriptive": ["Computer Vision (CV)", "Classification", "Regression", "Support Vector Machines (SVM)"]}