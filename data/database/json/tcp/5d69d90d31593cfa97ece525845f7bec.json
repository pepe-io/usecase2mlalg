{"title": "K-Means in Machine Learning", "description": "Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as k-means clustering, which is implemented in sklearn.cluster.KMeans.The\u00a0k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like:Those two assumptions are the basis of the\u00a0k-means model. We will soon dive into exactly\u00a0how\u00a0the algorithm reaches this solution, but for now, let\u2019s take a look at a simple dataset and see the\u00a0k-means result.First, let\u2019s generate a two-dimensional dataset containing four distinct blobs. To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization.By eye, it is relatively easy to pick out the four clusters. The\u00a0k-means algorithm does this automatically, and in Scikit-Learn uses the standard estimator API:Let\u2019s visualize the results by plotting the data coloured by these labels. We will also plot the cluster centers as determined by the\u00a0k-means estimator:The good news is that the\u00a0k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye. But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the name of data points\u2014an exhaustive search would be very, very costly. Fortunately for us, such a thorough search is not necessary: instead, the typical approach to\u00a0k-means involves an intuitive, iterative approach known as\u00a0expectation\u2013maximization.Expectation\u2013maximization (E\u2013M) is a robust algorithm that comes up in a variety of contexts within data science.\u00a0k-means is a particularly easy-to-understand and straightforward application of the algorithm, and we will walk through it briefly here. In short, the expectation-maximization approach here consists of the following procedure:Here the \u201cE-step\u201d or \u201cExpectation step\u201d is so-named because it involves updating our expectation of which cluster each point belongs to. The \u201cM-step\u201d or \u201cMaximization step\u201d is so-named because it consists in maximizing some fitness function that defines the location of the cluster centers\u2014in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.The\u00a0k-Means algorithm is simple enough that we can write it in a few lines of code. The following is a very basic implementation:To start, let\u2019s take a look at applying\u00a0k-means on the same simple digits data. Here we will attempt to use\u00a0k-means to try to identify similar digits\u00a0without using the original label information; this might be similar to a first step in extracting meaning from a new dataset about which you don\u2019t have any\u00a0a priori\u00a0label information.We will start by loading the digits and then finding the KMeans clusters. Recall that the numbers consist of 1,797 samples with 64 features, where each of the 64 elements is the brightness of one pixel in an 8\u00d78 image:Now let\u2019s do the clustering:The result is 10 clusters in 64 dimensions. Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the \u201ctypical\u201d digit within the cluster. Let\u2019s see what these cluster centers look like:We see that\u00a0even without the labels, KMeans can find clusters whose centers are recognizable digits, with perhaps the exception of 1 and 8.Because\u00a0k-means knows nothing about the identity of the cluster, the 0\u20139 labels may be permuted. We can fix this by matching each learned cluster label with the correct names found in them:Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:With just a simple\u00a0k-means algorithm, we discovered the correct grouping for 80% of the input digits! Let\u2019s check the confusion matrix for this:As we might expect from the cluster centers we visualized before, the main point of confusion is between the eights and ones. But this still shows that using\u00a0k-means, we can essentially build a digit classifier\u00a0without reference to any known labels.I hope you liked this article on k-means algorithm. Feel free to ask your valuable questions in the comments section. Don\u2019t forget to subscribe for my daily newsletters below to get email notifications if you like my work.", "link": "https://thecleverprogrammer.com/2020/07/12/k-means-in-machine-learning/", "tags": ["K-means", "Clustering", "Classification"], "kind": "Project", "ml_libs": ["sklearn"], "host": "thecleverprogrammer.com", "language": "english", "date_project": "2020-07-12 23:54:13", "date_scraped": "2020-12-20 00:00:00", "words": 672, "sentences": 19, "sum_nltk": "Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as k-means clustering, which is implemented in sklearn.cluster.KMeans.The\u00a0k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset.\nThe\u00a0k-means algorithm does this automatically, and in Scikit-Learn uses the standard estimator API:Let\u2019s visualize the results by plotting the data coloured by these labels.\nWe will also plot the cluster centers as determined by the\u00a0k-means estimator:The good news is that the\u00a0k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye.\nThe following is a very basic implementation:To start, let\u2019s take a look at applying\u00a0k-means on the same simple digits data.\nWe can fix this by matching each learned cluster label with the correct names found in them:Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:With just a simple\u00a0k-means algorithm, we discovered the correct grouping for 80% of the input digits!", "sum_nltk_words": 167, "sum_nltk_runtime": 0.007, "sum_t5": "k-means is an algorithm that searches for a pre-determined number of clusters. it accomplishes this using a simple conception of what the optimal clustering looks like. the k-means algorithm does this automatically, and in Scikit-Learn uses the standard estimator API. the good news is that the k-means algorithm assigns the points to clusters very similarly to how we might assign them by eye.", "sum_t5_words": 63, "sum_t5_runtime": 6.15, "runtime": 0.009, "nltk_category": "Utilities", "nltk_category_score": 0.4017012119293213, "nltk_category_runtime": 18.302, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8334684371948242, "nltk_subcategory_runtime": 29.36, "category": "Utilities", "category_score": 0.4017012119293213, "subcategory": "Machine Learning", "subcategory_score": 0.8334684371948242, "runtime_cat": 47.663, "programming_language": "Python", "ml_score": "1.0", "language_code": "en", "language_score": "0.99999688809641", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "many clustering algorithm available scikitlearn elsewhere perhaps simplest understand algorithm known kmeans clustering implemented sklearnclusterkmeansthe kmeans algorithm search predetermined number cluster within unlabeled multidimensional dataset accomplishes using simple conception optimal clustering look likethose two assumption basis kmeans model soon dive exactly algorithm reach solution let take look simple dataset see kmeans resultfirst let generate twodimensional dataset containing four distinct blob emphasize unsupervised algorithm leave label visualizationby eye relatively easy pick four cluster kmeans algorithm automatically scikitlearn us standard estimator apilets visualize result plotting data coloured label also plot cluster center determined kmeans estimatorthe good news kmeans algorithm least simple case assigns point cluster similarly might assign eye might wonder algorithm find cluster quickly number possible combination cluster assignment exponential name data pointsan exhaustive search would costly fortunately u thorough search necessary instead typical approach kmeans involves intuitive iterative approach known expectationmaximizationexpectationmaximization em robust algorithm come variety context within data science kmeans particularly easytounderstand straightforward application algorithm walk briefly short expectationmaximization approach consists following procedurehere estep expectation step sonamed involves updating expectation cluster point belongs mstep maximization step sonamed consists maximizing fitness function defines location cluster centersin case maximization accomplished taking simple mean data clusterthe kmeans algorithm simple enough write line code following basic implementationto start let take look applying kmeans simple digit data attempt use kmeans try identify similar digit without using original label information might similar first step extracting meaning new dataset dont priori label informationwe start loading digit finding kmeans cluster recall number consist 1797 sample 64 feature 64 element brightness one pixel 88 imagenow let clusteringthe result 10 cluster 64 dimension notice cluster center 64dimensional point interpreted typical digit within cluster let see cluster center look likewe see even without label kmeans find cluster whose center recognizable digit perhaps exception 1 8because kmeans know nothing identity cluster 09 label may permuted fix matching learned cluster label correct name found themnow check accurate unsupervised clustering finding similar digit within datawith simple kmeans algorithm discovered correct grouping 80 input digit let check confusion matrix thisas might expect cluster center visualized main point confusion eight one still show using kmeans essentially build digit classifier without reference known labelsi hope liked article kmeans algorithm feel free ask valuable question comment section dont forget subscribe daily newsletter get email notification like work", "tags_descriptive": ["K-means", "Clustering", "Classification"]}