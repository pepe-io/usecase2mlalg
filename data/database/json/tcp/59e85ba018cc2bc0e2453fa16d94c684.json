{"title": "SMS Spam Detection with Machine Learning", "description": "This Article is based on SMS Spam detection classification with Machine Learning. I will be using the\u00a0multinomial Naive Bayes\u00a0implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input. On the other hand\u00a0Gaussian Naive Bayes\u00a0is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution.Lets Start by importing the librariesDownload and read the data setDropping the unwanted columns Unnamed:2, Unnamed: 3 and Unnamed:4Number\u00a0of\u00a0observations\u00a0in\u00a0each\u00a0label\u00a0spam\u00a0and\u00a0hamWhat we have here in our data set is a large collection of text data (5,572 rows of data). Most Machine Learning algorithms rely on numerical data to be fed into them as input, and email/sms messages are usually text heavy. We need a way to represent text data for machine learning algorithm and the bag-of-words model helps us to achieve that task. It is a way of extracting features from the text for use in machine learning algorithms. In this approach, we use the tokenized words for each observation and find out the frequency of each token. Using a process which we will go through now, we can convert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrence of each word or token in that document.For example:Lets say we have 4 documents as follows:[\u2018Hello, how are you!\u2019, \u2018Win money, win from home.\u2019, \u2018Call me now\u2019, \u2018Hello, Call you tomorrow?\u2019]Our objective here is to convert this set of text to a frequency distribution matrix, as follows:\u00a0Here as we can see, the documents are numbered in the rows, and each word is a column name, with the corresponding value being the frequency of that word in the document.Lets break this down and see how we can do this conversion using a small set of documents.To handle this, we will be using sklearns count vectorizer method which does the following:Step 1: Convert all strings to their lower case form.Step 2: Removing all punctuationsStep 3: TokenizationStep 4: Count frequenciesHere we will look to create a frequency matrix on a smaller document set to make sure we understand how the document-term matrix generation happens. We have created a sample document set \u2018documents\u2019.documents = [\u2018Hello, how are you!\u2019, \u2018Win money, win from home.\u2019, \u2018Call me now.\u2019, \u2018Hello, Call hello you tomorrow?\u2019]In above step, we implemented a version of the CountVectorizer() method from scratch that entailed cleaning our data first. This cleaning involved converting all of our data to lower case and removing all punctuation marks. CountVectorizer() has certain parameters which take care of these steps for us. They are:lowercase = TrueThe lowercase parameter has a default value of True which converts all of our text to its lower case form.token_pattern = (?u)\\b\\w\\w+\\bThe token_pattern parameter has a default regular expression value of (?u)\\b\\w\\w+\\b which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words.stop_wordsThe stop_words parameter, if set to english will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. Considering the size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not be setting this parameter value.I will use sklearns\u00a0sklearn.naive_bayes\u00a0method to make predictions on our dataset for SMS Spam Detection.Specifically, we will be using the\u00a0multinomial Naive Bayes\u00a0implementation. This particular classifier is suitable for classification with discrete features. It takes in integer word counts as its input. On the other hand\u00a0Gaussian Naive Bayes\u00a0is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution.Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. There are various mechanisms for doing so, but first let\u2019s do quick recap of them.Accuracy\u00a0measures how often the classifier makes the correct prediction. It\u2019s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).Precision\u00a0tells us what proportion of messages we classified as spam, actually were spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of[True Positives/(True Positives + False Positives)]Recall(sensitivity)\u00a0tells us what proportion of messages that actually were spam were classified by us as spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of[True Positives/(True Positives + False Negatives)]For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren\u2019t, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score.We will be using all 4 metrics to make sure our model does well. For all 4 metrics whose values can range from 0 to 1, having a score as close to 1 as possible is a good indicator of how well our model is doing.", "link": "https://thecleverprogrammer.com/2020/06/12/sms-spam-detection-with-machine-learning/", "tags": ["Classification", "Naive Bayes", "Classification"], "kind": "Project", "ml_libs": ["pattern", "sklearn", "nltk"], "host": "thecleverprogrammer.com", "language": "english", "date_project": "2020-06-12 23:15:05", "date_scraped": "2020-12-20 00:00:00", "words": 975, "sentences": 27, "sum_nltk": "Using a process which we will go through now, we can convert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrence of each word or token in that document.For example:Lets say we have 4 documents as follows:[\u2018Hello, how are you!\u2019, \u2018Win money, win from home.\u2019, \u2018Call me now\u2019, \u2018Hello, Call you tomorrow?\u2019]Our objective here is to convert this set of text to a frequency distribution matrix, as follows:\u00a0Here as we can see, the documents are numbered in the rows, and each word is a column name, with the corresponding value being the frequency of that word in the document.Lets break this down and see how we can do this conversion using a small set of documents.To handle this, we will be using sklearns count vectorizer method which does the following:Step 1: Convert all strings to their lower case form.Step 2: Removing all punctuationsStep 3: TokenizationStep 4: Count frequenciesHere we will look to create a frequency matrix on a smaller document set to make sure we understand how the document-term matrix generation happens.\nIt is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of[True Positives/(True Positives + False Negatives)]For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren\u2019t, accuracy by itself is not a very good metric.", "sum_nltk_words": 268, "sum_nltk_runtime": 0.009, "sum_t5": "based on text classification, this article is based on SMS Spam detection. we will be using the multinomial Naive Bayes implementation. it takes in integer word counts as its input. on the other hand, Gaussian Naive Bayes is better suited for continuous data. it assumes that the input data has a Gaussian(normal) distribution. sklearns count vectorizer method does the following:Step 1: Convert all documents to a matrix", "sum_t5_words": 67, "sum_t5_runtime": 6.825, "runtime": 0.006, "nltk_category": "Finance", "nltk_category_score": 0.37377333641052246, "nltk_category_runtime": 25.582, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.7249265313148499, "nltk_subcategory_runtime": 40.641, "category": "Finance", "category_score": 0.37377333641052246, "subcategory": "Machine Learning", "subcategory_score": 0.7249265313148499, "runtime_cat": 66.224, "programming_language": "Python", "ml_score": "1.0", "language_code": "en", "language_score": "0.9999956695614666", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "article based sm spam detection classification machine learning using multinomial naive bayes implementation particular classifier suitable classification discrete feature case word count text classification take integer word count input hand gaussian naive bayes better suited continuous data assumes input data gaussiannormal distributionlets start importing librariesdownload read data setdropping unwanted column unnamed2 unnamed 3 unnamed4number observation label spam hamwhat data set large collection text data 5572 row data machine learning algorithm rely numerical data fed input emailsms message usually text heavy need way represent text data machine learning algorithm bagofwords model help u achieve task way extracting feature text use machine learning algorithm approach use tokenized word observation find frequency token using process go convert collection document matrix document row wordtoken column corresponding rowcolumn value frequency occurrence word token documentfor examplelets say 4 document followshello win money win home call hello call tomorrowour objective convert set text frequency distribution matrix follows see document numbered row word column name corresponding value frequency word documentlets break see conversion using small set documentsto handle using sklearns count vectorizer method followingstep 1 convert string lower case formstep 2 removing punctuationsstep 3 tokenizationstep 4 count frequencieshere look create frequency matrix smaller document set make sure understand documentterm matrix generation happens created sample document set documentsdocuments hello win money win home call hello call hello tomorrowin step implemented version countvectorizer method scratch entailed cleaning data first cleaning involved converting data lower case removing punctuation mark countvectorizer certain parameter take care step u arelowercase truethe lowercase parameter default value true convert text lower case formtoken_pattern ubwwbthe token_pattern parameter default regular expression value ubwwb ignores punctuation mark treat delimiters accepting alphanumeric string length greater equal 2 individual token wordsstop_wordsthe stop_words parameter set english remove word document set match list english stop word defined scikitlearn considering size dataset fact dealing sm message larger text source like email setting parameter valuei use sklearns sklearnnaive_bayes method make prediction dataset sm spam detectionspecifically using multinomial naive bayes implementation particular classifier suitable classification discrete feature take integer word count input hand gaussian naive bayes better suited continuous data assumes input data gaussiannormal distributionnow made prediction test set next goal evaluate well model various mechanism first let quick recap themaccuracy measure often classifier make correct prediction ratio number correct prediction total number prediction number test data pointsprecision tell u proportion message classified spam actually spam ratio true positiveswords classified spam actually spam positivesall word classified spam irrespective whether correct classification word ratio oftrue positivestrue positive false positivesrecallsensitivity tell u proportion message actually spam classified u spam ratio true positiveswords classified spam actually spam word actually spam word ratio oftrue positivestrue positive false negativesfor classification problem skewed classification distribution like case example 100 text message 2 spam rest 98 werent accuracy good metric could classify 90 message spamincluding 2 spam classify spam hence would false negative 10 spamall 10 false positive still get reasonably good accuracy score case precision recall come handy two metric combined get f1 score weighted average precision recall score score range 0 1 1 best possible f1 scorewe using 4 metric make sure model well 4 metric whose value range 0 1 score close 1 possible good indicator well model", "tags_descriptive": ["Classification", "Naive Bayes", "Classification"]}