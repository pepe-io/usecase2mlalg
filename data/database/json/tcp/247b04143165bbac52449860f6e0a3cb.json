{"title": "PCA in Machine Learning", "description": "In this article, you will explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.After a brief conceptual discussion of the PCA algorithm, we will see a couple examples of these further applications. I will start with the standard imports:Principal component analysis (PCA) is a fast and flexible unsupervised method for dimensionality reduction in data. Its behavior is easiest to visualize by looking at a two-dimensional dataset. Consider the following 200 points:By eye, it is clear that there is a nearly linear relationship between the x and y variables. But the problem here states: rather than attempting to\u00a0predict\u00a0the y values from the x values, the unsupervised learning problem attempts to learn about the\u00a0relationship\u00a0between the x and y values.In principal component analysis (PCA), this relationship is quantified by finding a list of the\u00a0principal axes\u00a0in the data, and using those axes to describe the dataset. Using Scikit-Learn\u2019s\u00a0PCA\u00a0estimator, we can compute this as follows:The fit learns some quantities from the data, most importantly the \u201ccomponents\u201d and \u201cexplained variance\u201d:To see what these numbers mean, let\u2019s visualize them as vectors over the input data, using the \u201ccomponents\u201d to define the direction of the vector, and the \u201cexplained variance\u201d to define the squared-length of the vector:These vectors represent the\u00a0principal axes\u00a0of the data, and the length of the vector is an indication of how \u201cimportant\u201d that axis is in describing the distribution of the data\u2014more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the \u201cprincipal components\u201d of the data.Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.Here is an example of using PCA as a dimensionality reduction transform:The transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:The light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \u201cinformation\u201d is discarded in this reduction of dimensionality.The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data. To see this, let\u2019s take a quick look at the application of PCA to the digits data. I will start by loading the data:To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:We can now plot the first two principal components of each point to learn about the data:the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance. Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised manner\u2014that is, without reference to the labels.A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data. This can be determined by looking at the cumulative\u00a0explained variance ratio\u00a0as a function of the number of components:Here we will take a look and explore the Labeled Faces in the Wild dataset made available through Scikit-Learn:Let\u2019s take a look at the principal axes that span this dataset. Because this is a large dataset, we will use\u00a0RandomizedPCA\u2014it contains a randomized method to approximate the first N principal components much more quickly than the standard\u00a0PCA\u00a0estimator, and thus is very useful for high-dimensional data (here, a dimensionality of nearly 3,000). We will take a look at the first 150 components:In this case, it can be interesting to visualize the images associated with the first several principal components:The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips. Let\u2019s take a look at the cumulative variance of these components to see how much of the data information the projection is preserving:We see that these 150 components account for just over 90% of the variance. That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data. To make this more concrete, we can compare the input images with the images reconstructed from these 150 components:The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features. This visualization makes clear why the PCA feature selection used in\u00a0was so successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image. What this means is that our classification algorithm needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which depending on the particular algorithm we choose, can lead to a much more efficient classification.", "link": "https://thecleverprogrammer.com/2020/07/08/pca-in-machine-learning/", "tags": ["Supervised Learning", "Classification", "Unsupervised Learning"], "kind": "Project", "ml_libs": ["sklearn"], "host": "thecleverprogrammer.com", "language": "english", "date_project": "2020-07-08 00:31:00", "date_scraped": "2020-12-20 00:00:00", "words": 970, "sentences": 22, "sum_nltk": "Using Scikit-Learn\u2019s\u00a0PCA\u00a0estimator, we can compute this as follows:The fit learns some quantities from the data, most importantly the \u201ccomponents\u201d and \u201cexplained variance\u201d:To see what these numbers mean, let\u2019s visualize them as vectors over the input data, using the \u201ccomponents\u201d to define the direction of the vector, and the \u201cexplained variance\u201d to define the squared-length of the vector:These vectors represent the\u00a0principal axes\u00a0of the data, and the length of the vector is an indication of how \u201cimportant\u201d that axis is in describing the distribution of the data\u2014more precisely, it is a measure of the variance of the data when projected onto that axis.\nThis makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.\nThis visualization makes clear why the PCA feature selection used in\u00a0was so successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image.", "sum_nltk_words": 175, "sum_nltk_runtime": 0.009, "sum_t5": "principal component analysis (PCA) is a fast and flexible unsupervised method. it can be used for visualization, noise filtering, feature extraction and engineering. the algorithm can be used to reduce dimensionality and to extract the maximum data variance. it can also be used to extract the smallest data points and to extract the largest data points. the algorithm can be used to extract the largest data points and to extract the largest data points. a dimensionality reduction transform can be used to extract the largest data points.", "sum_t5_words": 87, "sum_t5_runtime": 6.829, "runtime": 0.009, "nltk_category": "Utilities", "nltk_category_score": 0.35566943883895874, "nltk_category_runtime": 17.817, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.8385360836982727, "nltk_subcategory_runtime": 28.5, "category": "Utilities", "category_score": 0.35566943883895874, "subcategory": "Machine Learning", "subcategory_score": 0.8385360836982727, "runtime_cat": 46.317, "programming_language": "Python", "ml_score": "1.0", "language_code": "en", "language_score": "0.9999957797073724", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "article explore perhaps one broadly used unsupervised algorithm principal component analysis pca pca fundamentally dimensionality reduction algorithm also useful tool visualization noise filtering feature extraction engineering much moreafter brief conceptual discussion pca algorithm see couple example application start standard importsprincipal component analysis pca fast flexible unsupervised method dimensionality reduction data behavior easiest visualize looking twodimensional dataset consider following 200 pointsby eye clear nearly linear relationship x variable problem state rather attempting predict value x value unsupervised learning problem attempt learn relationship x valuesin principal component analysis pca relationship quantified finding list principal ax data using ax describe dataset using scikitlearns pca estimator compute followsthe fit learns quantity data importantly component explained varianceto see number mean let visualize vector input data using component define direction vector explained variance define squaredlength vectorthese vector represent principal ax data length vector indication important axis describing distribution datamore precisely measure variance data projected onto axis projection data point onto principal ax principal component datausing pca dimensionality reduction involves zeroing one smallest principal component resulting lowerdimensional projection data preserve maximal data variancehere example using pca dimensionality reduction transformthe transformed data reduced single dimension understand effect dimensionality reduction perform inverse transform reduced data plot along original datathe light point original data dark point projected version make clear pca dimensionality reduction mean information along least important principal axis ax removed leaving component data highest variance fraction variance cut proportional spread point line formed figure roughly measure much information discarded reduction dimensionalitythe usefulness dimensionality reduction may entirely apparent two dimension becomes much clear looking highdimensional data see let take quick look application pca digit data start loading datato gain intuition relationship point use pca project manageable number dimension say twowe plot first two principal component point learn datathe full data 64dimensional point cloud point projection data point along direction largest variance essentially found optimal stretch rotation 64dimensional space allows u see layout digit two dimension done unsupervised mannerthat without reference labelsa vital part using pca practice ability estimate many component needed describe data determined looking cumulative explained variance ratio function number componentshere take look explore labeled face wild dataset made available scikitlearnlets take look principal ax span dataset large dataset use randomizedpcait contains randomized method approximate first n principal component much quickly standard pca estimator thus useful highdimensional data dimensionality nearly 3000 take look first 150 componentsin case interesting visualize image associated first several principal componentsthe result interesting give u insight image vary example first eigenfaces seem associated angle lighting face later principal vector seem picking certain feature eye nose lip let take look cumulative variance component see much data information projection preservingwe see 150 component account 90 variance would lead u believe using 150 component would recover essential characteristic data make concrete compare input image image reconstructed 150 componentsthe top row show input image bottom row show reconstruction image 150 3000 initial feature visualization make clear pca feature selection used successful although reduces dimensionality data nearly factor 20 projected image contain enough information might eye recognize individual image mean classification algorithm need trained 150dimensional data rather 3000dimensional data depending particular algorithm choose lead much efficient classification", "tags_descriptive": ["Supervised Learning", "Classification", "Unsupervised Learning"]}