{"title": "Manifold Learning", "description": "Rotating, re-orienting, or stretching the piece of paper in three-dimensional space doesn\u2019t change the flat geometry of the article: such operations are akin to linear embeddings. If you bend, curl, or crumple the paper, it is still a two-dimensional manifold, but the embedding into the three-dimensional space is no longer linear. Manifold learning algorithms would seek to learn about the fundamental two-dimensional nature of the paper, even as it is contorted to fill the three-dimensional space.Here I will demonstrate several different methods, going most deeply into a couple of techniques: multidimensional scaling (MDS), locally linear embedding (LLE), and isometric mapping (IsoMap).I will begin with the standard imports:To make these concepts more clear, let\u2019s start by generating some two-dimensional data that we can use to define a manifold. Here is a function that will create data in the shape of the word \u201cHELLO\u201d:Let\u2019s call the function and visualize the resulting data:The output is two dimensional, and consists of points drawn in the shape of the word, \u201cHELLO\u201d. This data form will help us to see visually what these algorithms are doing.Looking at data like this, we can see that the particular choice of\u00a0x\u00a0and\u00a0y\u00a0values of the dataset are not the most fundamental description of the data: we can scale, shrink, or rotate the data, and the \u201cHELLO\u201d will still be apparent. For example, if we use a rotation matrix to switch the data, the\u00a0x\u00a0and\u00a0y\u00a0values change, but the data is still fundamentally the same:This tells us that the\u00a0x\u00a0and\u00a0y\u00a0values are not necessarily fundamental to the relationships in the data. What\u00a0is\u00a0fundamental, in this case, is the\u00a0distance\u00a0between each point and the other points in the dataset. A common way to represent this is to use a distance matrix: for $N$ points, we construct an N \\times N array such that entry (i, j) contains the distance between point i and point j. Let\u2019s use Scikit-Learn\u2019s efficient\u00a0pairwise_distances\u00a0function to do this for our original data:As promised, for our\u00a0N=1,000 points, we obtain a 1000\u00d71000 matrix, which can be visualised as shown here:This distance matrix gives us a representation of our data that is invariant to rotations and translations, but the visualisation of the matrix above is not entirely intuitive. In the image shown in this figure, we have lost any visible sign of the impressive structure in the data: the \u201cHELLO\u201d that we saw before.Further, while computing this distance matrix from the (x, y) coordinates is straightforward, transforming the distances back into\u00a0x\u00a0and\u00a0y\u00a0coordinates is somewhat tricky. This is precisely what the multidimensional scaling algorithm aims to do: given a distance matrix between points, it recovers a $D$-dimensional coordinate representation of the data. Let\u2019s see how it works for our distance matrix, using the\u00a0precomputed\u00a0dissimilarity to specify that we are passing a distance matrix:The MDS algorithm recovers one of the possible two-dimensional coordinate representations of our data, using\u00a0only\u00a0the N\\times N distance matrix describing the relationship between the data points.The usefulness of this becomes more apparent when we consider the fact that distance matrices can be computed from data in\u00a0any\u00a0dimension. So, for example, instead of simply rotating the data in the two-dimensional plane, we can project it into three dimensions using the following function:Let\u2019s visualize these points to see what we\u2019re working with:We can now ask the\u00a0MDS\u00a0estimator to input this three-dimensional data, compute the distance matrix and then determine the optimal two-dimensional embedding for this distance matrix. The result recovers a representation of the original data:This is essentially the goal of a manifold learning estimator: given high-dimensional embedded data, it seeks a low-dimensional representation of the data that preserves individual relationships within the data. In the case of MDS, the quantity protected is the distance between every pair of points.One place manifold learning is often used is in understanding the relationship between high-dimensional data points. A typical case of high-dimensional data is images: for example, a set of pictures with 1,000 pixels each can be thought of as a collection of points in 1,000 dimensions \u2013 the brightness of each pixel in each image defines the coordinate in that dimension.Here let\u2019s apply Isomap on some faces data. I will use the Labeled Faces in the Wild dataset provided by scikit-learn.We have 2,370 images, each with 2,914 pixels. In other words, the images can be thought of as data points in a 2,914-dimensional space.Let\u2019s quickly visualize several of these images to see what we\u2019re working with:I would like to plot a low-dimensional embedding of the 2,914-dimensional data to learn the fundamental relationships between the images. One useful way to start is to compute a PCA, and examine the explained variance ratio, which will give us an idea of how many linear features are required to describe the data:We see that for this data, nearly 100 components are required to preserve 90% of the variance: this tells us that the data is intrinsically very high dimensional\u2014it can\u2019t be described linearly with just a few components.When this is the case, nonlinear manifold embeddings like LLE and Isomap can be helpful. We can compute an Isomap embedding on these faces using the same pattern shown before:The output is a two-dimensional projection of all the input images. To get a better idea of what the forecast tells us, let\u2019s define a function that will output image thumbnails at the locations of the projections:Calling this function now, we see the result:The result is impressive: the first two Isomap dimensions seem to describe global image features: the overall darkness or lightness of the image from left to right, and the general orientation of the face from bottom to top. This gives us an excellent visual indication of some of the fundamental features in our data.I hope you liked this article on manifold learning in machine learning. Feel free to ask questions in the comments section below on any topic you want.", "link": "https://thecleverprogrammer.com/2020/07/08/manifold-learning/", "tags": [], "kind": "Project", "ml_libs": ["pattern", "sklearn"], "host": "thecleverprogrammer.com", "language": "english", "date_project": "2020-07-08 17:06:40", "date_scraped": "2020-12-20 00:00:00", "words": 961, "sentences": 23, "sum_nltk": "Manifold learning algorithms would seek to learn about the fundamental two-dimensional nature of the paper, even as it is contorted to fill the three-dimensional space.Here I will demonstrate several different methods, going most deeply into a couple of techniques: multidimensional scaling (MDS), locally linear embedding (LLE), and isometric mapping (IsoMap).I will begin with the standard imports:To make these concepts more clear, let\u2019s start by generating some two-dimensional data that we can use to define a manifold.\nSo, for example, instead of simply rotating the data in the two-dimensional plane, we can project it into three dimensions using the following function:Let\u2019s visualize these points to see what we\u2019re working with:We can now ask the\u00a0MDS\u00a0estimator to input this three-dimensional data, compute the distance matrix and then determine the optimal two-dimensional embedding for this distance matrix.\nIn other words, the images can be thought of as data points in a 2,914-dimensional space.Let\u2019s quickly visualize several of these images to see what we\u2019re working with:I would like to plot a low-dimensional embedding of the 2,914-dimensional data to learn the fundamental relationships between the images.", "sum_nltk_words": 178, "sum_nltk_runtime": 0.01, "sum_t5": "a function that will create data in the shape of the word, \u201cHELLO\u201d will help us to see visually what these algorithms are doing. if we use a rotation matrix to switch the data, the x and y values change, but the data is still fundamentally the same. if we use a rotation matrix to switch the data, the x and y values change, but the data is still fundamentally the same. if we bend, curl, or crumple the paper, it is", "sum_t5_words": 82, "sum_t5_runtime": 6.807, "runtime": 0.008, "nltk_category": "Media & Publishing", "nltk_category_score": 0.052924152463674545, "nltk_category_runtime": 18.531, "nltk_subcategory": "Machine Learning", "nltk_subcategory_score": 0.14196017384529114, "nltk_subcategory_runtime": 29.712, "category": "Media & Publishing", "category_score": 0.052924152463674545, "subcategory": "Machine Learning", "subcategory_score": 0.14196017384529114, "runtime_cat": 48.242, "programming_language": "Python", "ml_score": "1.0", "language_code": "en", "language_score": "0.9999968468591907", "learn_score": 1, "explore_score": 0, "compete_score": 0, "description_lemmatized": "rotating reorienting stretching piece paper threedimensional space doesnt change flat geometry article operation akin linear embeddings bend curl crumple paper still twodimensional manifold embedding threedimensional space longer linear manifold learning algorithm would seek learn fundamental twodimensional nature paper even contorted fill threedimensional spacehere demonstrate several different method going deeply couple technique multidimensional scaling md locally linear embedding lle isometric mapping isomapi begin standard importsto make concept clear let start generating twodimensional data use define manifold function create data shape word hellolets call function visualize resulting datathe output two dimensional consists point drawn shape word hello data form help u see visually algorithm doinglooking data like see particular choice x value dataset fundamental description data scale shrink rotate data hello still apparent example use rotation matrix switch data x value change data still fundamentally samethis tell u x value necessarily fundamental relationship data fundamental case distance point point dataset common way represent use distance matrix n point construct n time n array entry j contains distance point point j let use scikitlearns efficient pairwise_distances function original dataas promised n1000 point obtain 10001000 matrix visualised shown herethis distance matrix give u representation data invariant rotation translation visualisation matrix entirely intuitive image shown figure lost visible sign impressive structure data hello saw beforefurther computing distance matrix x coordinate straightforward transforming distance back x coordinate somewhat tricky precisely multidimensional scaling algorithm aim given distance matrix point recovers ddimensional coordinate representation data let see work distance matrix using precomputed dissimilarity specify passing distance matrixthe md algorithm recovers one possible twodimensional coordinate representation data using ntimes n distance matrix describing relationship data pointsthe usefulness becomes apparent consider fact distance matrix computed data dimension example instead simply rotating data twodimensional plane project three dimension using following functionlets visualize point see working withwe ask md estimator input threedimensional data compute distance matrix determine optimal twodimensional embedding distance matrix result recovers representation original datathis essentially goal manifold learning estimator given highdimensional embedded data seek lowdimensional representation data preserve individual relationship within data case md quantity protected distance every pair pointsone place manifold learning often used understanding relationship highdimensional data point typical case highdimensional data image example set picture 1000 pixel thought collection point 1000 dimension brightness pixel image defines coordinate dimensionhere let apply isomap face data use labeled face wild dataset provided scikitlearnwe 2370 image 2914 pixel word image thought data point 2914dimensional spacelets quickly visualize several image see working withi would like plot lowdimensional embedding 2914dimensional data learn fundamental relationship image one useful way start compute pca examine explained variance ratio give u idea many linear feature required describe datawe see data nearly 100 component required preserve 90 variance tell u data intrinsically high dimensionalit cant described linearly componentswhen case nonlinear manifold embeddings like lle isomap helpful compute isomap embedding face using pattern shown beforethe output twodimensional projection input image get better idea forecast tell u let define function output image thumbnail location projectionscalling function see resultthe result impressive first two isomap dimension seem describe global image feature overall darkness lightness image left right general orientation face bottom top give u excellent visual indication fundamental feature datai hope liked article manifold learning machine learning feel free ask question comment section topic want", "tags_descriptive": []}