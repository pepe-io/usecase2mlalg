date;description;license;link;ml_detected;ml_libs;ml_slugs;ml_terms;reference;score_views;score_votes;scraped_at;sources;tags;title;type;views;votes;score_private;score_public
2020-11-27 07:24:12;Vectorization;Apache 2.0;https://www.kaggle.com/ananthu017/classification-tfidf-logistic;1.0;['sklearn'];['ner', 'ai', 'dl', 'nn', 'ml'];['filter', 'regression', 'train', 'model', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge;0.627;0.292;2020-12-12 16:01:56;20 Newsgroups Ciphertext Challenge;[];Classification - TFIDF + Logistic;Python notebook;714.0;9;;
2019-02-05 11:19:49;Thanks to this kernel:https://www.kaggle.com/collinsjosh/xgboost-classifier;Apache 2.0;https://www.kaggle.com/ashishpatel26/everything-you-want-to-know-about-20-ngctc;1.0;['xgboost', 'sklearn', 'nltk'];['ai', 'nn', 'ann', 'cv'];['filter', 'training data', 'train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge;0.652;0.362;2020-12-12 16:01:56;20 Newsgroups Ciphertext Challenge;['classification, text data, text mining, +1 moreindia'];Everything you want to know about 20  NGCTC;Python notebook;1137.0;19;;
2018-12-16 10:09:19;Examine the properties by target;Apache 2.0;https://www.kaggle.com/delayedkarma/some-basic-explorations-lgb-baseline;1.0;['xgboost', 'sklearn', 'lightgbm', 'nltk'];['ner', 'ai', 'rl', 'gbm'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge;0.702;0.268;2020-12-12 16:01:56;multiple data sources;['beginner, data visualization, exploratory data analysis'];Some Basic Explorations + LGB Baseline;Python notebook;3206.0;7;;
2018-12-26 12:30:02;;Apache 2.0;https://www.kaggle.com/jazivxt/enigma-layers-template;1.0;['pattern', 'sklearn'];['ai', 'rl', 'cv'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge;0.637;0.268;2020-12-12 16:01:56;20 Newsgroups Ciphertext Challenge;['puzzles'];Enigma Layers Template;Python notebook;864.0;7;0.49744;0.49744
2018-12-15 19:50:59;;Apache 2.0;https://www.kaggle.com/opanichev/lightgbm-and-simple-features;1.0;['pattern', 'lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge;0.665;0.334;2020-12-12 16:01:56;20 Newsgroups Ciphertext Challenge;['beginner, feature engineering'];LightGBM and simple features;Python script;1453.0;14;0.35245;0.35245
2019-09-17 21:53:25;Translations of coordinates from https://www.kaggle.com/lopuhin/lyft-3d-join-all-lidars-annotations-from-scratch;Apache 2.0;https://www.kaggle.com/fartuk1/3d-segmentation-approach;1.0;['tensorflow', 'skimage', 'keras'];['ner', 'ai', 'dl', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.689;0.387;2020-12-12 16:08:03;Lyft 3D Object Detection for Autonomous Vehicles;['gpu'];3d segmentation approach;Python notebook;2412.0;25;;
2019-09-14 13:25:13;"This is certainly an interesting and challenging competition.  Lets take a look at the 85GB unique dataset! Disclaimer: I do not know anything about 3D object detection nor about Autonomous Vehicles.  The notebook is provided ""as is"", without warranty of any kind... :)";Apache 2.0;https://www.kaggle.com/gaborfodor/eda-3d-object-detection-challenge;0.5;[];['ai', 'nn', 'ann', 'rl'];['object detection', 'filter', 'train', 'predict', 'ground truth'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.713;0.449;2020-12-12 16:08:03;Lyft 3D Object Detection for Autonomous Vehicles;[];EDA - 3D Object Detection Challenge;Python notebook;4078.0;52;0.000;0.000
2020-01-28 20:29:21;;Apache 2.0;https://www.kaggle.com/hmendonca/kaggle-pytorch-utility-script;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'loss'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.733;0.497;2020-12-12 16:08:01;multiple data sources;['deep learning, utility script'];Kaggle pytorch utility script;Python script;6731.0;96;;
2019-09-15 17:26:35;Main goal here is to learn to work with the data as-is (because not all needed features are presnet in the SDK), join all lidars into one point cloud and apply annotations to make sure we're not missing anything. EDIT as @alexamadori pointed out in comments, most scenes have data only from one lidar, so merging all 3 lidars might not make much sense. Still writing this kernel was useful to me to understand the data and coordinate systems better. Overview of what we're doing below:  we load lidar data (3d points), these points are in coordinate system of the lidar, rotated and translated relative to the car using sensor information of the lidar, we translate points from lidar coordinate frame to car coordinate frame, this allows us to merge data from all 3 lidars annotations and submission are in global coordinates. We translate annotations into the car coordinates, which allows to have both the data and annotations in the same (car) coordinate frame, which can then be used for training  We learn:  how the raw data looks like, so that we understand how SDK works better and can extend it if needed (also if you see any missing features, file issues at https://github.com/lyft/nuscenes-devkit/issues/) how to translate objects between various coordinate frames  In terms of custom packages, we'll use only pyquaternion which is also used by Lyft SDK.;Apache 2.0;https://www.kaggle.com/lopuhin/lyft-3d-join-all-lidars-annotations-from-scratch;0.5;[];['ai', 'nn', 'ann'];['train', 'resnet', 'predict'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.699;0.464;2020-12-12 16:08:03;Lyft 3D Object Detection for Autonomous Vehicles;[];Lyft 3D: Join all lidars, annotations from scratch;Python notebook;2961.0;63;;
2019-09-20 10:51:56;This kernel generates BEV (Bird's Eye View) images for the test data set. We will be feeding these image into NN for test set predictions. Instead of processing all the images everytime we want to make a prediction, it's better to generate it once and reuse it. You can find Guido Zuidof's original kernel here.;Apache 2.0;https://www.kaggle.com/meaninglesslives/lyft3d-test-dataset;0.5;[];['ner', 'ai', 'cv', 'rl', 'nn', 'ann'];['model', 'test data', 'predict'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.669;0.367;2020-12-12 16:08:03;Lyft 3D Object Detection for Autonomous Vehicles;[];Lyft3D Test Dataset;Python notebook;1593.0;20;;
2019-09-30 12:08:25;Let's understand the datasetGiven files train_data.zip and test_data.zip - contains JSON files with multiple tables. The most important is sample_data.json, which contains the primary identifiers used in the competition, as well as links to key image / lidar information.  train_images.zip and test_images.zip - contains .jpeg files corresponding to samples in sample_data.json  train_lidar.zip and test_lidar.zip - contains .bin files corresponding to samples in sample_data.json train_maps.zip and test_maps.zip - contains maps of the entire sample area. train.csv - contains all sample_tokens in the train set, as well as annotations in the required format for all train set objects. sample_submission.csv - contains all sample_tokens in the test set, with empty predictions.;Apache 2.0;https://www.kaggle.com/rishabhiitbhu/eda-understanding-the-dataset-with-3d-plots;0.5;[];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'label', 'predict', 'ground truth'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.75;0.514;2020-12-12 16:08:01;multiple data sources;['beginner, data visualization, exploratory data analysis'];EDA: understanding the dataset with 3D plots;Python notebook;10203.0;122;;
2019-10-16 18:48:33;Visualizing the predictions;Apache 2.0;https://www.kaggle.com/rishabhiitbhu/visualizing-predictions;1.0;['opencv-python', 'pillow'];['ner', 'ai', 'cv', 'nn', 'ml'];['train', 'label', 'ground truth', 'predict'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.652;0.375;2020-12-12 16:08:03;multiple data sources;[];Visualizing predictions;Python notebook;1144.0;22;;
2019-10-10 23:52:30;In this kernel we convert LEVEL5 Lyft data (NuScenes format) to KITTI format, which is usually used in public repositories. After this you can search for repos, that solve KITTI 3d-detection task.;Apache 2.0;https://www.kaggle.com/stalkermustang/converting-lyft-dataset-to-kitty-format;0.5;[];['ai', 'nn', 'ann'];['train', 'predict'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.718;0.465;2020-12-12 16:08:03;Lyft 3D Object Detection for Autonomous Vehicles;[];Converting LYFT dataset to KITTY-format;Python notebook;4664.0;64;;
2019-09-15 16:45:27;Introduction;Apache 2.0;https://www.kaggle.com/tarunpaparaju/lyft-competition-understanding-the-data;1.0;['sklearn', 'opencv-python', 'pillow'];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'object detection', 'train', 'model', 'label', 'recommend', 'classification'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.794;0.626;2020-12-12 16:07:59;Lyft 3D Object Detection for Autonomous Vehicles;['data visualization, exploratory data analysis, image data'];Lyft Competition : Understanding the data;Python notebook;38016.0;705;;
2019-09-21 08:11:54;"About this kernelThis kernel takes a look at the dataset for the Lyft competition, with some visual exploration (display images, animations, etc.). I also convert some of the JSON files into CSV, so feel free to use this kernel output as supplementary data. UpdatesV3: Added Animations! Please go to the ""Animating the images"" section at the end! References Starter Devkit Lyft3D: https://www.kaggle.com/jesucristo/starter-devkit-lyft3d Devkit for the public 2019 Lyft Level 5 AV Dataset: https://github.com/lyft/nuscenes-devkit";Apache 2.0;https://www.kaggle.com/xhlulu/lyft-eda-animations-generating-csvs;0.5;[];['ner', 'ai', 'cv', 'ml', 'nn', 'ann'];['train', 'training data', 'predict'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.724;0.487;2020-12-12 16:08:01;Lyft 3D Object Detection for Autonomous Vehicles;['beginner, data visualization, exploratory data analysis, +1 moredata cleaning'];Lyft: EDA, Animations, generating CSVs;Python notebook;5289.0;85;;
2020-08-22 14:10:44;;Apache 2.0;https://www.kaggle.com/zikazika/what-is-object-detection-yolo;1.0;['pattern'];['ner', 'ai', 'cnn', 'cv', 'rl', 'nn', 'ann'];['r-cnn', 'image classification', 'object detection', 'linear regression', 'regression', 'train', 'model', 'neural network', 'deep learning', 'layer', 'vgg', 'alexnet', 'label', 'predict', 'computer vision', 'understanding', 'classification', 'convolutional neural network'];https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles;0.695;0.485;2020-12-12 16:08:03;Lyft 3D Object Detection for Autonomous Vehicles;[];What is object detection? (YOLO);Python notebook;2769.0;82;;
2020-02-14 19:40:43;forked from: https://www.kaggle.com/t88take/check-the-purpose Version 1:  Added gridlines For each task, showing all the train pairs instead of only the first one Showing evaluation set also  Version 2: added task index and filename (for easy lookup);Apache 2.0;https://www.kaggle.com/boliu0/visualizing-all-task-pairs-with-gridlines;0.5;[];['ai', 'dl'];['train', 'label'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.778;0.579;2020-12-12 16:09:11;Abstraction and Reasoning Challenge;[];visualizing all task pairs with gridlines;Python notebook;22494.0;317;;
2020-05-28 04:17:21;;Apache 2.0;https://www.kaggle.com/golubev/7-solved-tasks-via-trees;1.0;['skimage', 'sklearn'];['dl', 'ner', 'ai', 'nn'];['train', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.668;0.418;2020-12-12 16:09:12;Abstraction and Reasoning Challenge;[];7 solved tasks via Trees;Python notebook;1551.0;36;;
2020-02-13 18:50:11;You thought that https://www.kaggle.com/t88take/check-the-purpose notebook is big? (btw thanks @t88take - this is a fork of their notebook). Now this is even bigger, we plot all train examples, not just the first one, because some puzzles are ambiguous without extra examples.;Apache 2.0;https://www.kaggle.com/lopuhin/check-all-train-and-test-examples;0.5;[];['ai'];['train'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.713;0.446;2020-12-12 16:09:12;Abstraction and Reasoning Challenge;[];Check all train and test examples;Python notebook;4075.0;50;;
2020-02-14 12:42:00;ref. https://www.kaggle.com/c/abstraction-and-reasoning-challenge/discussion/130360;Apache 2.0;https://www.kaggle.com/nagiss/manual-coding-for-the-first-10-tasks;1.0;['pattern'];['ai'];['train'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.755;0.54;2020-12-12 16:09:11;Abstraction and Reasoning Challenge;['exploratory data analysis'];Manual Coding for the First 10 Tasks;Python notebook;11709.0;177;;
2020-04-02 22:22:21;The Idea:Look at training examples, where the size of the input is the same as the size of the output, and the new color of the pixel at coordinate i,j can depend on the old color and i mod Q1 and j mod Q2, where Q1 and Q2 are some small integers.;Apache 2.0;https://www.kaggle.com/szabo7zoltan/colorandcountingmoduloq;0.5;[];['ner', 'ai'];['train'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.697;0.468;2020-12-12 16:09:12;Abstraction and Reasoning Challenge;[];ColorAndCountingModuloQ;Python notebook;2895.0;66;0.990;0.990
2020-02-13 16:57:05;;Apache 2.0;https://www.kaggle.com/t88take/check-the-purpose;0.5;[];['ai'];['train'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.718;0.485;2020-12-12 16:09:12;Abstraction and Reasoning Challenge;[];check the purpose;Python notebook;4654.0;82;;
2020-02-26 07:07:02;Introduction;Apache 2.0;https://www.kaggle.com/tarunpaparaju/arc-competition-eda-pytorch-cnn;1.0;['pytorch', 'tensorflow', 'keras', 'pattern'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ml'];['machine learning', 'training data', 'train', 'model', 'epoch', 'loss', 'label', 'predict', 'relu', 'propagation'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.763;0.564;2020-12-12 16:09:11;Abstraction and Reasoning Challenge;['data visualization, exploratory data analysis, deep learning, +2 morecnn, artificial intelligence'];ARC Competition : EDA + PyTorch CNN ðŸ’¥ ;Python notebook;14865.0;254;;
2020-02-29 20:48:12;arseny-n showed us that Cellular Automata can be used as a language for solving ARC challenges, and provided us with a few example solutions. In this notebook we'll explore how we can use neural networks to create cellular automata. We'll start with Conway's Game of Life, and then move on to see if we can replicate his results using learned cellular automata. (See part II).;Apache 2.0;https://www.kaggle.com/teddykoker/training-cellular-automata-part-i-game-of-life;0.5;[];['ai', 'dl', 'ml', 'nn', 'ann'];['train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.696;0.435;2020-12-12 16:09:12;Abstraction and Reasoning Challenge;[];Training Cellular Automata Part I: Game of Life;Python notebook;2797.0;44;;
2020-03-01 17:48:17;In my previous notebook we explored how we could use a CNN to create a cellular automata (CA) by recurrently passing the state of the grid through itself. Now we'll solve one of the tasks arseny-n solved with a hard coded CA by learning the CA instead!;Apache 2.0;https://www.kaggle.com/teddykoker/training-cellular-automata-part-ii-learning-tasks;0.5;[];['ner', 'ai', 'dl', 'cnn', 'nn', 'ml'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.736;0.519;2020-12-12 16:09:11;Abstraction and Reasoning Challenge;['gpu'];Training Cellular Automata Part II: Learning Tasks;Python notebook;7264.0;130;;
2020-04-21 15:00:33;Intro;Apache 2.0;https://www.kaggle.com/zaharch/arc-c-approach;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['train', 'model'];https://www.kaggle.com/c/abstraction-and-reasoning-challenge;0.674;0.439;2020-12-12 16:09:12;Abstraction and Reasoning Challenge;[];ARC C++ approach;Python notebook;1743.0;46;0.990;0.990
2020-03-17 22:21:58;;Apache 2.0;https://www.kaggle.com/sainiknitin/nitin-saini-walking-through-customer-transaction;1.0;['pattern', 'sklearn'];['ai', 'rl', 'cv', 'nn', 'ann'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'label', 'logistic regression', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/acquire-valued-shoppers-challenge;0.667;0.0;2020-12-12 16:09:18;Acquire Valued Shoppers Challenge;[];kernel21fd4a7c59;Python notebook;1531.0;0;;
2019-03-09 01:47:07;;Apache 2.0;https://www.kaggle.com/ateplyuk/keras-transfer-densenet121;1.0;['tensorflow', 'keras'];['ai', 'nn', 'cv'];['predict', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/aerial-cactus-identification;0.597;0.367;2020-12-12 16:10:46;Aerial Cactus Identification;['gpu'];Keras _Transfer_DenseNet121;Python notebook;430.0;20;;
2019-03-09 01:51:03;;Apache 2.0;https://www.kaggle.com/ateplyuk/keras-transfer-vgg16;1.0;['tensorflow', 'keras'];['ai', 'cv'];['predict', 'train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'relu'];https://www.kaggle.com/c/aerial-cactus-identification;0.696;0.452;2020-12-12 16:10:46;Aerial Cactus Identification;['gpu'];Keras _Transfer_VGG16;Python notebook;2825.0;54;;
2019-04-20 04:28:45;;Apache 2.0;https://www.kaggle.com/gabrielmv/aerial-cactus-identification-keras;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/aerial-cactus-identification;0.679;0.346;2020-12-12 16:10:46;Aerial Cactus Identification;['beginner, deep learning, earth and nature, +1 moreclassification'];Aerial Cactus Identification - Keras;Python notebook;1963.0;16;0.9999;0.9999
2019-03-22 02:01:41;;Apache 2.0;https://www.kaggle.com/kenseitrg/simple-fastai-exercise;1.0;['pytorch'];['ner', 'ai', 'nn', 'cnn'];['train', 'model', 'epoch', 'label', 'loss', 'classification'];https://www.kaggle.com/c/aerial-cactus-identification;0.729;0.464;2020-12-12 16:10:46;Aerial Cactus Identification;['gpu'];Simple_FastAI_exercise;Python notebook;5982.0;63;1.0000;1.0000
2019-03-29 08:56:20;Exploration;Apache 2.0;https://www.kaggle.com/mariammohamed/simple-cnn;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/aerial-cactus-identification;0.695;0.375;2020-12-12 16:10:46;Aerial Cactus Identification;['gpu, deep learning, classification'];Simple CNN;Python notebook;2745.0;22;0.9995;0.9995
2019-03-10 13:07:04;Cactus Identification fastai baseline;Apache 2.0;https://www.kaggle.com/mnpinto/cactus-identification-fastai-v1-0-46-ensemble;1.0;['sklearn'];['dl', 'ai', 'nn', 'cnn'];['train', 'model', 'epoch', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/aerial-cactus-identification;0.682;0.352;2020-12-12 16:10:46;multiple data sources;['gpu'];Cactus Identification fastai v1.0.46 ensemble;Python notebook;2059.0;17;0.9996;0.9996
2019-06-20 07:28:57;Introduction;Apache 2.0;https://www.kaggle.com/shahules/getting-started-with-cnn-and-vgg16;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cnn', 'cv', 'nn', 'ann'];['activation function', 'filter', 'train', 'fitting', 'model', 'neural network', 'epoch', 'validation data', 'layer', 'vgg', 'loss', 'label', 'relu', 'predict', 'computer vision', 'classification', 'convolutional neural network'];https://www.kaggle.com/c/aerial-cactus-identification;0.767;0.559;2020-12-12 16:10:46;Aerial Cactus Identification;['gpu, beginner, cnn, +1 morebinary classification'];Getting started with CNN and VGG16;Python notebook;16306.0;234;;
2019-06-14 06:56:34;;Apache 2.0;https://www.kaggle.com/umangjpatel/aerial-cactus-cnn;1.0;['pytorch'];['ner', 'ai', 'nn', 'cnn'];['train', 'model', 'epoch', 'label', 'loss', 'classification'];https://www.kaggle.com/c/aerial-cactus-identification;0.682;0.357;2020-12-12 16:10:46;Aerial Cactus Identification;['classification, cnn, neural networks, +2 moretransfer learning, research'];Aerial Cactus CNN;Python notebook;2091.0;18;0.9999;0.9999
2015-12-13 07:43:08;;Apache 2.0;https://www.kaggle.com/datadave/ndcg-score-r;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['filter', 'training data', 'train', 'deep learning', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.734;0.319;2020-12-12 16:26:37;Airbnb New User Bookings;[];nDCG score R;R script;6902.0;12;0.85669;0.85359
2016-02-04 22:06:51;;Apache 2.0;https://www.kaggle.com/davidgasquez/ndcg-scorer;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['test data', 'random forest', 'train', 'fitting', 'model', 'understanding', 'deep learning', 'label', 'predict', 'rank', 'recommend', 'classification', 'ground truth'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.777;0.4;2020-12-12 16:26:37;Airbnb New User Bookings;[];NDCG Scorer;Python script;22041.0;29;;
2015-12-13 11:31:26;;Apache 2.0;https://www.kaggle.com/davidgasquez/user-data-exploration;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'deep learning', 'label', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.798;0.532;2020-12-12 16:26:37;Airbnb New User Bookings;[];User Data Exploration;Python notebook;43183.0;156;;
2015-12-04 18:35:52;;Apache 2.0;https://www.kaggle.com/dietcoke/score-predictions-using-ndcg;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['gru', 'training data', 'train', 'fitting', 'deep learning', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.733;0.292;2020-12-12 16:26:37;Airbnb New User Bookings;[];Score predictions using NDCG;Python script;6697.0;9;;
2015-11-26 20:32:31;;Apache 2.0;https://www.kaggle.com/hulkbulk/fisrtscript;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.715;0.311;2020-12-12 16:26:37;Airbnb New User Bookings;[];FisrtScript;Python script;4273.0;11;0.00000;0.00000
2015-12-09 23:55:19;;Apache 2.0;https://www.kaggle.com/indradenbakker/rscript-0-86547;1.0;['xgboost'];['ner', 'ai', 'gan', 'ml', 'nlp', 'nn', 'ann'];['test data', 'generation', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.754;0.379;2020-12-12 16:26:37;Airbnb New User Bookings;[];Rscript_0.86547;R script;11342.0;23;0.86841;0.86454
2016-01-12 16:16:31;;Apache 2.0;https://www.kaggle.com/joetheshow/airbnb-visualizations;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.742;0.292;2020-12-12 16:26:37;Airbnb New User Bookings;[];Airbnb Visualizations;R notebook;8427.0;9;;
2019-02-20 05:44:03;;Apache 2.0;https://www.kaggle.com/justk1/airbnb;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'label', 'predict', 'decision tree'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.663;0.281;2020-12-12 16:26:37;Airbnb New User Bookings;['gpu'];AirBNB;Python notebook;1420.0;8;0.86972;0.86509
2016-02-01 16:54:17;;Apache 2.0;https://www.kaggle.com/kevinwu06/airbnb-exploratory-analysis;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.778;0.431;2020-12-12 16:26:37;Airbnb New User Bookings;[];Airbnb Exploratory Analysis;Python notebook;23090.0;42;;
2016-02-04 23:44:55;;Apache 2.0;https://www.kaggle.com/kevinwu06/feature-importance-w-xgboost;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.716;0.281;2020-12-12 16:26:37;Airbnb New User Bookings;[];Feature Importance w/ XGBoost;Python script;4383.0;8;0.86271;0.85934
2020-05-18 12:02:59;Visualizations;Apache 2.0;https://www.kaggle.com/krutarthhd/airbnb-eda-and-xgboost;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'nn', 'ann'];['test data', 'filter', 'train', 'label', 'predict'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.713;0.476;2020-12-12 16:26:37;Airbnb New User Bookings;['data visualization, exploratory data analysis, feature engineering, +1 morexgboost'];Airbnb EDA and Xgboost;Python notebook;4079.0;73;;
2017-09-13 07:18:06;**Airbnb New User Bookings**;Apache 2.0;https://www.kaggle.com/nikhiljangam/exploratory-data-analysis;1.0;['sklearn'];['ai', 'nn', 'ann', 'rl'];['training data', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.711;0.334;2020-12-12 16:26:37;Airbnb New User Bookings;[];Exploratory Data  Analysis;Python notebook;3910.0;14;;
2015-12-02 09:53:41;;Apache 2.0;https://www.kaggle.com/omarelgabry/airbnb-user-bookings;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'ml', 'nlp', 'nn', 'ann'];['machine learning', 'random forest', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.766;0.371;2020-12-12 16:26:37;Airbnb New User Bookings;[];Airbnb User Bookings;Python notebook;15964.0;21;0.00000;0.00000
2015-12-08 13:24:11;;Apache 2.0;https://www.kaggle.com/pappukrjha/session-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'classification', 'training data', 'deep learning'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.712;0.292;2020-12-12 16:26:37;Airbnb New User Bookings;[];session_Data;Python script;4002.0;9;;
2015-12-09 03:51:32;;Apache 2.0;https://www.kaggle.com/scottbrenstuhl/holidays-make-people-hate-travel;1.0;['pattern', 'h2o'];['ner', 'ai', 'nlu', 'dl', 'gbm', 'gan', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['filter', 'train', 'deep learning', 'vgg', 'label', 'predict', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.755;0.416;2020-12-12 16:26:37;Airbnb New User Bookings;[];Holidays make people hate travel;Rmarkdown script;11699.0;35;;
2016-02-27 01:56:07;;Apache 2.0;https://www.kaggle.com/svpons/feature-engineering;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ann'];['test data', 'generation', 'train', 'model', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.732;0.346;2020-12-12 16:26:37;Airbnb New User Bookings;[];feature_engineering;Python script;6500.0;16;;
2015-12-08 05:18:23;;Apache 2.0;https://www.kaggle.com/svpons/script-0-8655;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['machine learning', 'test data', 'train', 'fitting', 'model', 'understanding', 'deep learning', 'loss', 'label', 'predict', 'decision tree', 'classification', 'labeled'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.78;0.442;2020-12-12 16:26:37;Airbnb New User Bookings;[];script_0.8655;Python script;24276.0;48;0.86952;0.86555
2015-11-29 10:34:01;;Apache 2.0;https://www.kaggle.com/wendykan/ndcg-example;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn', 'ann'];['rank', 'classification', 'train', 'deep learning'];https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings;0.776;0.371;2020-12-12 16:26:37;Airbnb New User Bookings;[];NDCG example;Python script;21275.0;21;;
2018-09-10 22:56:00;Overview;Apache 2.0;https://www.kaggle.com/iafoss/unet34-dice-0-87;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['image segmentation', 'test data', 'train', 'fitting', 'model', 'epoch', 'layer', 'loss', 'label', 'u-net', 'predict', 'relu', 'resnet', 'classification', 'labeled', 'ground truth'];https://www.kaggle.com/c/airbus-ship-detection;0.777;0.521;2020-12-12 16:28:05;multiple data sources;['gpu, deep learning, cnn, +1 moreneural networks'];Unet34 (dice 0.87+);Python notebook;22112.0;134;;
2018-10-09 16:16:51;Overview;Apache 2.0;https://www.kaggle.com/iafoss/unet34-submission-tta-0-699-new-public-lb;1.0;['sklearn'];['ner', 'ai', 'dl', 'cv', 'nn'];['training data', 'train', 'model', 'epoch', 'layer', 'label', 'predict', 'relu', 'resnet', 'u-net'];https://www.kaggle.com/c/airbus-ship-detection;0.776;0.53;2020-12-12 16:28:05;multiple data sources;['gpu, deep learning, cnn, +1 moreneural networks'];Unet34 submission (TTA + new data);Python notebook;21678.0;153;0.83344;0.69998
2018-07-30 19:45:35;;Apache 2.0;https://www.kaggle.com/inversion/run-length-decoding-quick-start;1.0;['skimage'];['ai', 'nn', 'rl'];['train'];https://www.kaggle.com/c/airbus-ship-detection;0.794;0.58;2020-12-12 16:28:05;Airbus Ship Detection Challenge;[];Run Length Decoding - Quick Start;Python notebook;37542.0;326;;
2018-09-20 16:10:17;Introduction;Apache 2.0;https://www.kaggle.com/meaninglesslives/airbus-ship-detection-data-visualization;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ai', 'rl', 'cv', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'label'];https://www.kaggle.com/c/airbus-ship-detection;0.782;0.528;2020-12-12 16:28:05;Airbus Ship Detection Challenge;['beginner, data visualization'];Airbus Ship Detection: Data Visualization;Python notebook;25519.0;149;;
2018-08-01 05:07:36;OverviewAccording to this post from Max Diebold, an empty submission can get you  a score of 0.847. So, this baseline model will always predict that there is no ship.;Apache 2.0;https://www.kaggle.com/npatta01/naive-model;0.5;[];['ai'];['train', 'model', 'test data', 'predict'];https://www.kaggle.com/c/airbus-ship-detection;0.729;0.44;2020-12-12 16:28:05;Airbus Ship Detection Challenge;[];Naive Model;Python notebook;6099.0;47;0.00000;0.00000
2018-08-16 19:12:02;;Apache 2.0;https://www.kaggle.com/rackovic1994/convolutional-neural-network;1.0;['tensorflow', 'skimage', 'keras'];['ner', 'ai', 'dl', 'cnn', 'rl', 'ml', 'nn', 'ann'];['filter', 'object detection', 'train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'predict', 'computer vision', 'relu', 'u-net', 'convolutional neural network', 'ground truth'];https://www.kaggle.com/c/airbus-ship-detection;0.739;0.456;2020-12-12 16:28:05;Airbus Ship Detection Challenge;[];Convolutional Neural Network;Python notebook;7808.0;57;;
2020-05-31 07:54:11;Faster YCbCr DecodingAs discussed in the forums, most methods of loading a JPEG image output an RGB array. Converting this back to YCbCr can result in tiny errors which could be significant since the stenographic techniques used here rely on tiny changes in DCT space. RÃ©mi Cogranne has kindly provided us a method to decode from JPEG directly to YCbCr using jpegio, skipping the RGB step. https://www.kaggle.com/remicogranne/jpeg-explanations This method can be a bit slow as discussed here. Let's use vectorised operations in NumPy to speed this up;Apache 2.0;https://www.kaggle.com/anjum48/faster-ycbcr-decoding;0.2;[];['ai', 'dl', 'gan', 'nn', 'ann'];[];https://www.kaggle.com/c/alaska2-image-steganalysis;0.662;0.431;2020-12-12 16:29:24;ALASKA2 Image Steganalysis;[];Faster YCbCr Decoding;Python notebook;1394.0;42;;
2020-05-03 15:09:39;Introduction;Apache 2.0;https://www.kaggle.com/eswarchandt/alaska2-eda-and-efficientnet;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'gan', 'cv', 'rl', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/alaska2-image-steganalysis;0.699;0.446;2020-12-12 16:29:24;ALASKA2 Image Steganalysis;['tpu'];Alaska2: EDA and EfficientNet;Python notebook;3016.0;50;;
2020-05-18 19:57:15;;Apache 2.0;https://www.kaggle.com/pranshu29/more-image-processing-ideas-analysis;1.0;['pattern', 'skimage'];['ner', 'ai', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'object detection', 'train', 'model', 'computer vision', 'understanding', 'classification'];https://www.kaggle.com/c/alaska2-image-steganalysis;0.67;0.452;2020-12-12 16:29:24;ALASKA2 Image Steganalysis;['beginner, data visualization, computer vision'];more Image processing Ideas & analysis;Python notebook;1613.0;54;;
2020-05-14 15:58:37;ALASKA2 : Image Steganalysis - All you need to knowIntroductionKaggle has recently launched a competition ALASKA2 Image Steganalysis. But wait, what is Image Steganalysis. So, the objective of this notebook is to describe Image Steganalysis in detail so that Kaggle users can understand it. So, let's get started.;Apache 2.0;https://www.kaggle.com/prashant111/alaska2-image-steganalysis-all-you-need-to-know;0.5;[];['ner', 'ai', 'dl', 'cnn', 'gan', 'rl', 'nn', 'ann'];['machine learning', 'regression', 'train', 'model', 'neural network', 'supervised learning', 'deep learning', 'label', 'predict', 'classification', 'convolutional neural network'];https://www.kaggle.com/c/alaska2-image-steganalysis;0.725;0.545;2020-12-12 16:29:23;ALASKA2 Image Steganalysis;['beginner'];ALASKA2: Image Steganalysis - All you need to know;Python notebook;5496.0;190;;
2020-05-13 10:14:54;;Apache 2.0;https://www.kaggle.com/remicogranne/jpeg-explanations;0.2;[];['ai', 'nn', 'ann', 'gan'];[];https://www.kaggle.com/c/alaska2-image-steganalysis;0.697;0.462;2020-12-12 16:29:24;ALASKA2 Image Steganalysis;[];JPEG Explanations : YCbCr & QualityFactor meaning;Python notebook;2894.0;61;;
2016-11-28 08:49:42;;Apache 2.0;https://www.kaggle.com/ameshkov/r-xgb-1107-23-unskew-encode-fold;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/allstate-claims-severity;0.739;0.439;2020-12-12 16:36:59;Allstate Claims Severity;[];[R] XGB 1107.23: Unskew + Encode + Fold;R script;7754.0;46;;
2016-10-15 23:07:40;;Apache 2.0;https://www.kaggle.com/danijelk/keras-starter-with-bagging-lb-1120-596;1.0;['xgboost', 'theano', 'sklearn', 'tensorflow', 'keras'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ml'];['training data', 'test data', 'neuron', 'train', 'fitting', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'recommend', 'classification', 'hidden layer'];https://www.kaggle.com/c/allstate-claims-severity;0.768;0.48;2020-12-12 16:36:59;Allstate Claims Severity;[];Keras starter with bagging (LB: 1120.596);Python script;17215.0;77;;
2016-10-17 04:03:18;;Apache 2.0;https://www.kaggle.com/dmi3kno/all-the-allstate-states-eda;0.5;[];['dl', 'ai', 'nn'];['train', 'loss'];https://www.kaggle.com/c/allstate-claims-severity;0.757;0.524;2020-12-12 16:36:59;Allstate Claims Severity;[];All the Allstate states (EDA);Rmarkdown script;12480.0;141;;
2016-10-10 23:12:11;;Apache 2.0;https://www.kaggle.com/dmi3kno/allstate-eda;0.5;[];['ai'];['regression', 'train', 'label', 'loss', 'linear regression'];https://www.kaggle.com/c/allstate-claims-severity;0.761;0.476;2020-12-12 16:36:59;Allstate Claims Severity;[];Allstate EDA;Rmarkdown script;13811.0;73;;
2016-10-22 01:49:47;;Apache 2.0;https://www.kaggle.com/dmi3kno/allstate-fingerprints-eda;1.0;['pattern', 'xgboost'];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'fitting', 'model', 'layer', 'clustering', 'loss', 'label', 'predict', 'rank'];https://www.kaggle.com/c/allstate-claims-severity;0.729;0.452;2020-12-12 16:36:59;Allstate Claims Severity;[];Allstate Fingerprints (EDA);Rmarkdown script;5988.0;54;;
2016-10-26 19:17:50;;Apache 2.0;https://www.kaggle.com/iglovikov/xgb-1114;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['linear regression', 'filter', 'regression', 'train', 'model', 'understanding', 'epoch', 'deep learning', 'gradient descent', 'loss', 'label', 'logistic regression', 'predict', 'decision tree', 'classification', 'bayesian'];https://www.kaggle.com/c/allstate-claims-severity;0.772;0.509;2020-12-12 16:36:59;Allstate Claims Severity;[];xgb 1114;Python script;18857.0;113;;
2016-10-11 02:46:22;;Apache 2.0;https://www.kaggle.com/laurae2/sneak-peak-at-the-data-1;1.0;['xgboost'];['ner', 'ai', 'gan', 'cv', 'rl'];['filter', 'train', 'fitting', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/allstate-claims-severity;0.723;0.397;2020-12-12 16:36:59;Allstate Claims Severity;[];Sneak peak at the data;Rmarkdown script;5206.0;28;;
2016-11-29 02:33:03;;Apache 2.0;https://www.kaggle.com/mariusbo/xgb-lb-1106-33084;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'regression', 'train', 'fitting', 'model', 'validation data', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/allstate-claims-severity;0.753;0.447;2020-12-12 16:36:59;Allstate Claims Severity;[];XGB (LB 1106.33084);Python script;11236.0;51;;
2016-11-14 16:30:18;;Apache 2.0;https://www.kaggle.com/misfyre/encoding-feature-comb-modkzs-1108-72665;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/allstate-claims-severity;0.746;0.416;2020-12-12 16:36:59;Allstate Claims Severity;[];Encoding + Feature Comb (modkzs) (1108.72665);Python script;9298.0;35;;
2016-12-14 08:28:46;;Apache 2.0;https://www.kaggle.com/mmueller/categorical-embedding-with-xgb;1.0;['xgboost', 'sklearn', 'keras'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'model', 'neural network', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/allstate-claims-severity;0.745;0.408;2020-12-12 16:36:59;Allstate Claims Severity;[];Categorical Embedding with XGB;Python script;8950.0;32;;
2016-10-11 23:26:01;;Apache 2.0;https://www.kaggle.com/mmueller/stacking-starter;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['machine learning', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'rank', 'understanding', 'classification'];https://www.kaggle.com/c/allstate-claims-severity;0.795;0.551;2020-12-12 16:36:58;Allstate Claims Severity;[];Stacking Starter;Python script;38832.0;207;;
2016-10-12 07:34:17;;Apache 2.0;https://www.kaggle.com/mmueller/yet-another-xgb-starter;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/allstate-claims-severity;0.736;0.411;2020-12-12 16:36:59;Allstate Claims Severity;[];Yet Another XGB Starter;Python script;7180.0;33;;
2016-11-09 01:39:40;;Apache 2.0;https://www.kaggle.com/mtinti/keras-starter-with-bagging-1111-84364;1.0;['keras', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['activation function', 'test data', 'neuron', 'train', 'fitting', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'predict', 'relu', 'recommend', 'classification', 'hidden layer', 'bayesian'];https://www.kaggle.com/c/allstate-claims-severity;0.771;0.481;2020-12-12 16:36:59;Allstate Claims Severity;[];Keras starter with bagging 1111.84364;Python script;18522.0;78;;
2016-10-13 00:06:08;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/farons-xgb-starter-ported-to-r;1.0;['xgboost'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['training data', 'test data', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'gradient boosting', 'predict', 'understanding', 'classification', 'bayesian'];https://www.kaggle.com/c/allstate-claims-severity;0.755;0.453;2020-12-12 16:36:59;Allstate Claims Severity;[];Farons XGB starter ported to R;R script;11715.0;55;;
2016-11-18 01:25:05;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/r-script-scoring-1113-93-on-plb;1.0;['xgboost', 'theano', 'h2o', 'mxnet', 'keras'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['training data', 'train', 'fitting', 'model', 'neural network', 'deep learning', 'loss', 'label', 'predict', 'computer vision', 'understanding', 'classification'];https://www.kaggle.com/c/allstate-claims-severity;0.735;0.408;2020-12-12 16:36:59;Allstate Claims Severity;[];R script scoring 1113.93 on PLB;R script;6970.0;32;;
2016-10-13 04:35:15;;Apache 2.0;https://www.kaggle.com/tilii7/bias-correction-xgboost;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ml'];['recommend', 'linear regression', 'training data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'predict', 'decision tree', 'classification', 'bayesian'];https://www.kaggle.com/c/allstate-claims-severity;0.78;0.5;2020-12-12 16:36:59;Allstate Claims Severity;[];Bias Correction + XGBoost;Python script;24396.0;100;;
2020-02-15 15:29:44;;Apache 2.0;https://www.kaggle.com/caoyi41/allstate-pg1;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nn', 'ann'];['training data', 'test data', 'regression', 'random forest', 'train', 'fitting', 'model', 'validation data', 'loss', 'label', 'gradient boosting', 'predict', 'rank', 'decision tree', 'classification'];https://www.kaggle.com/c/allstate-purchase-prediction-challenge;0.67;0.188;2020-12-12 16:37:06;Allstate Purchase Prediction Challenge;[];allstate_pg1;Python notebook;1607.0;3;;
2020-07-03 00:58:17;;Apache 2.0;https://www.kaggle.com/chinmayiudaybhaskar/my-first-project;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'ann'];['regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'random forest'];https://www.kaggle.com/c/amazon-employee-access-challenge;0.518;0.214;2020-12-12 16:38:00;Amazon.com - Employee Access Challenge;[];my first project;Python notebook;128.0;4;;
2019-04-10 00:17:24;;Apache 2.0;https://www.kaggle.com/dmitrylarko/kaggledays-sf-1-amazon-baseline;1.0;['catboost', 'sklearn'];['cv', 'ai', 'dl', 'gan'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/amazon-employee-access-challenge;0.701;0.416;2020-12-12 16:38:00;Amazon.com - Employee Access Challenge;['feature engineering']; KaggleDays SF: 1. Amazon - Baseline;Python notebook;3160.0;35;0.87634;0.87977
2020-02-05 11:59:03;;Apache 2.0;https://www.kaggle.com/lucamassaron/deep-learning-for-tabular-data;1.0;['catboost', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'gan', 'gbm', 'cv', 'rl', 'nn'];['filter', 'train', 'fitting', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'gradient boosting', 'predict', 'rank', 'relu'];https://www.kaggle.com/c/amazon-employee-access-challenge;0.643;0.327;2020-12-12 16:38:00;Amazon.com - Employee Access Challenge;[];Deep_learning_for_tabular_data;Python notebook;964.0;13;0.87937;0.88335
2019-01-23 22:15:24;;Apache 2.0;https://www.kaggle.com/lucamassaron/kaggle-days-paris-skopt-catboost-solution;1.0;['catboost', 'sklearn'];['ai', 'dl', 'gan', 'cv', 'nn', 'ml'];['filter', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/amazon-employee-access-challenge;0.689;0.34;2020-12-12 16:38:00;Amazon.com - Employee Access Challenge;[];Kaggle Days Paris - Skopt + CatBoost solution;Python notebook;2420.0;15;;
2020-10-15 18:08:04;Amazon.com - Employee Access Challenge;Apache 2.0;https://www.kaggle.com/nehalbandal/predicting-an-employee-s-access-needs;1.0;['catboost', 'xgboost', 'sklearn'];['ai', 'dl', 'cv', 'ml', 'nn', 'ann'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'loss', 'label', 'logistic regression', 'predict', 'random forest', 'ground truth'];https://www.kaggle.com/c/amazon-employee-access-challenge;0.462;0.099;2020-12-12 16:38:00;Amazon.com - Employee Access Challenge;['beginner, binary classification, gradient boosting, +1 morecategorical data'];Predicting an Employee's Access Needs;Python notebook;61.0;1;;
2020-06-24 12:53:33;Import the required libraries;Apache 2.0;https://www.kaggle.com/vibeeshk/amazon-employees-access-prediction;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'ann'];['machine learning', 'regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'decision tree', 'random forest', 'naive bayes'];https://www.kaggle.com/c/amazon-employee-access-challenge;0.551;0.236;2020-12-12 16:38:00;Amazon.com - Employee Access Challenge;[];Amazon employees access prediction;Python notebook;206.0;5;0.68703;0.68736
2020-03-21 13:00:48;;Apache 2.0;https://www.kaggle.com/yihang123/kernel51242a6135;1.0;['lightgbm', 'sklearn'];['cv', 'ai', 'nn', 'gbm'];['train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/amazon-employee-access-challenge;0.489;0.0;2020-12-12 16:38:01;Amazon.com - Employee Access Challenge;['gpu'];kernel51242a6135;Python notebook;87.0;0;;
2019-06-28 12:36:00;Cool Imports;Apache 2.0;https://www.kaggle.com/abhishek/very-simple-pytorch-training-0-59;1.0;['pytorch'];['ai', 'nn'];['train', 'model', 'epoch', 'layer', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/aptos2019-blindness-detection;0.763;0.547;2020-12-12 16:39:24;multiple data sources;['gpu'];very simple pytorch training [0.59+];Python notebook;14789.0;194;;
2019-07-13 06:27:35;;Apache 2.0;https://www.kaggle.com/chanhu/eye-inference-num-class-1-ver3;1.0;['pytorch', 'sklearn'];['ai', 'rl', 'nn', 'cv'];['train', 'model', 'label', 'filter'];https://www.kaggle.com/c/aptos2019-blindness-detection;0.749;0.533;2020-12-12 16:39:24;multiple data sources;['gpu'];Eye -- Inference(num class=1) ver3;Python notebook;10120.0;160;0.901678;0.777870
2019-07-18 09:05:11;"this kernel is from  https://www.kaggle.com/manojprabhaakr/similar-duplicate-images-in-aptos-data and  https://www.kaggle.com/maxwell110/duplicated-list-csv-file/ I do three things:  change phash to md5 according see-'s comment https://www.kaggle.com/maxwell110/duplicated-list-csv-file/comments#575422; Duplicated with different label Duplicated in both train and test.";Apache 2.0;https://www.kaggle.com/h4211819/more-information-about-duplicate;0.5;[];['ai', 'cv'];['train', 'label'];https://www.kaggle.com/c/aptos2019-blindness-detection;0.703;0.495;2020-12-12 16:39:24;APTOS 2019 Blindness Detection;[];More information about duplicate;Python notebook;3309.0;94;;
2019-09-07 09:23:56;Various possible Pre-processing options;Apache 2.0;https://www.kaggle.com/jeru666/aptos-preprocessing-update-histogram-matching;1.0;['skimage'];['ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'label', 'filter'];https://www.kaggle.com/c/aptos2019-blindness-detection;0.72;0.514;2020-12-12 16:39:24;APTOS 2019 Blindness Detection;[];APTOS Preprocessing [Update: Histogram Matching];Python notebook;4861.0;121;;
2019-07-28 16:19:51;Updates28/07/2019: Added circle_crop_v2. Here we resize the image after the first crop before drawing the circle. The result is that a larger portion of the zoomed in images is retained, though it will be somewhat stretched. I think a slightly stretch is better than losing so much information.;Apache 2.0;https://www.kaggle.com/taindow/pre-processing-train-and-test-images;0.5;[];['ner', 'ai', 'cv'];['train', 'test data', 'training data', 'loss'];https://www.kaggle.com/c/aptos2019-blindness-detection;0.747;0.526;2020-12-12 16:39:24;APTOS 2019 Blindness Detection;[];Pre-processing train and test images;Python notebook;9529.0;144;;
2019-07-08 20:36:07;Loading all needed modules;Apache 2.0;https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading;1.0;['skimage'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model'];https://www.kaggle.com/c/aptos2019-blindness-detection;0.751;0.496;2020-12-12 16:39:24;multiple data sources;[];Benchmark 2019: Speed of image reading;Python notebook;10674.0;95;;
2020-07-11 02:37:08;twitter.com;Apache 2.0;https://www.kaggle.com/mpwolke/hewlett-foundation-essay-scoring;0.5;[];['ner', 'ai', 'dl', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/asap-aes;0.647;0.253;2020-12-12 16:39:36;The Hewlett Foundation: Automated Essay Scoring;[];Hewlett Foundation - Essay scoring;Python notebook;1041.0;6;;
2020-08-31 16:16:43;;Apache 2.0;https://www.kaggle.com/rathimadhav/notebook79ae0de477;1.0;['keras', 'sklearn', 'nltk', 'gensim'];['ner', 'ai', 'dl', 'cv', 'nn'];['regression', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu'];https://www.kaggle.com/c/asap-aes;0.587;0.099;2020-12-12 16:39:36;The Hewlett Foundation: Automated Essay Scoring;[];notebook79ae0de477;Python notebook;361.0;1;;
2019-10-29 23:19:17;;Apache 2.0;https://www.kaggle.com/isaienkov/keras-nn-with-embeddings-for-cat-features-1-15;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nn', 'rl'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/ashrae-energy-prediction;0.75;0.522;2020-12-12 16:41:48;ASHRAE - Great Energy Predictor III;['beginner, deep learning, feature engineering'];Keras NN with embeddings for cat. features (1.15);Python notebook;10390.0;137;;
2019-10-25 17:17:14;;Apache 2.0;https://www.kaggle.com/isaienkov/lightgbm-fe-1-19;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nn'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/ashrae-energy-prediction;0.761;0.54;2020-12-12 16:41:48;ASHRAE - Great Energy Predictor III;['beginner, feature engineering'];Simple LightGBM LB 1.24;Python notebook;13906.0;176;1.404;1.191
2019-10-23 22:40:19;;Apache 2.0;https://www.kaggle.com/jaseziv83/a-deep-dive-eda-into-all-variables;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl'];['filter', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/ashrae-energy-prediction;0.761;0.573;2020-12-12 16:41:48;ASHRAE - Great Energy Predictor III;['data visualization, exploratory data analysis, feature engineering, +1 moredata cleaning'];A deep dive EDA into ALL variables;Rmarkdown script;14025.0;288;;
2019-11-07 19:59:04;;Apache 2.0;https://www.kaggle.com/kailex/ac-dc;1.0;['lightgbm'];['ner', 'ai', 'dl', 'gan', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['test data', 'regression', 'train', 'model', 'deep learning', 'label', 'gradient boosting', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/ashrae-energy-prediction;0.731;0.543;2020-12-12 16:41:48;ASHRAE - Great Energy Predictor III;['feature engineering, data cleaning, regression, +1 moregradient boosting'];AC/DC;R script;6276.0;185;1.309;1.106
2019-10-30 15:58:37;Introduction In this competition we are to create a model to predict an energy usage per building and per meter. We are given a dataset of around 1450 buildings containing such information as building id, which is a unique identifier of a building across both train and test dataset. Meter - is the type of meter that measures the data - energy consumption in kWh. Different buildings might have different meters installed. Some of them have only one type of meter, some have all four. So we need to make a predictions for each meter of each building. ContentLoading data Amount of data and NaNs Mean meter reading by day Number of observations by day Features: Meter Site_id primary_use square_feet year_built floor_count building_id air_temperature cloud_coverage dew_temperature precip_depth_1_hr wind_direction & wind_speed  Engineered features: Observation hour Observation day of month Observation day of week;Apache 2.0;https://www.kaggle.com/nroman/eda-for-ashrae;0.5;[];['ner', 'ai', 'nn', 'rl'];['filter', 'test data', 'train', 'model', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/ashrae-energy-prediction;0.773;0.596;2020-12-12 16:41:48;ASHRAE - Great Energy Predictor III;[];EDA for ASHRAE ;Python notebook;19747.0;419;;
2019-10-31 03:14:04;"Hey guys! In ""External Data Disclosure Thread"", I just found an intersting external dataset of weather data: https://www.kaggle.com/selfishgene/historical-hourly-weather-data After comparing the dataset with competition ones, amazingly I found those corresponding cites for most sites (not exact but at least very close). Hope this helps and look forward to more findings!";Apache 2.0;https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'predict'];https://www.kaggle.com/c/ashrae-energy-prediction;0.733;0.54;2020-12-12 16:41:48;multiple data sources;['exploratory data analysis'];Locate cities according weather temperatureðŸŒ‡;Python notebook;6676.0;175;;
2019-10-21 01:17:33;Competition Objective:Energy savings is one of the important area of focus our current world. Energy savings has two key elements:  Forecasting future energy usage without improvements Forecasting energy use after a specific set of improvements have been implemented  Once we have implemented a set of improvements, the value of energy efficiency improvements can be challenging as there's no way to truly know how much energy a building would have used without the improvements. The best we can do is to build counterfactual models. his competition challenges you to build these counterfactual models across four energy types (chilled water, electricity, hot water, and steam) based on historic usage rates and observed weather.  Picture Source: wur.nl Notebook Objective:The objective of this notebook is to explore the data and make some inferences on the way. Dataset:The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world. We are given two main files - train.csv and test.csv just like other competitions. In addition to these files, we also have couple of more files  building_metadata.csv - meta data about the buildings weather.csv - weather information  First let us load the train file and look at the top few rows to get an idea about the data.;Apache 2.0;https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-ashrae;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['training data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/ashrae-energy-prediction;0.734;0.536;2020-12-12 16:41:48;ASHRAE - Great Energy Predictor III;[];Simple Exploration Notebook - ASHRAE;Python notebook;6785.0;166;;
2019-12-10 12:56:34;All Leak DatasetAs you already know there are huge data leak in this competition. Until now site-0, site-1, site-2, site-4 and site-15 building meter reading data was discovered by great kagglers. This Kernel collects all leak data revealed by following great kernels:  ASHRAE - UCF Spider and EDA (Full Test Labels) v3 UCL: Data Leakage (Episode 2) v1 ASU train and scraped test data v7 UCB: Data Leakage (Site 4) v15 ASHRAE-site15-cornell v3  Thank you @gunesevitan, @mpware, @poedator, @pdnartreb, @serengil and @pp2file . You are Great Kagglers!!;Apache 2.0;https://www.kaggle.com/yamsam/ashrae-leak-data-station;0.5;[];['ner', 'ai'];['train', 'label', 'test data', 'predict'];https://www.kaggle.com/c/ashrae-energy-prediction;0.755;0.527;2020-12-12 16:41:48;multiple data sources;[];ASHRAE: Leak Data Station;Python notebook;11883.0;146;;
2020-07-14 01:57:31;ç”±æ–¼æ–‡ä»¶çš„Rowæ•¸é‡å¤ªéŽé¾å¤§ ï¼ˆè¶…éŽ4åƒè¬ç­†ï¼‰ ï¼ŒPandas DataFrameç„¡æ³•è®€å–é€™éº¼å¤§çš„è³‡æ–™é‡ï¼Œå› æ­¤è³‡æ–™ä»¥æ¯100è¬ç­†ç‚ºä¸€æ‰¹æ¬¡åˆ†æ‰¹è®€å–ï¼Œä¸¦ä¸”å¾žæ¯å€‹æ‰¹æ¬¡çš„è³‡æ–™ä¸­ï¼Œéš¨æ©ŸæŠ½æ¨£5ï¼…çš„è³‡æ–™ï¼Œé›†æˆä¸€å€‹æ–°çš„train setã€‚;Apache 2.0;https://www.kaggle.com/akishen74/ctr-practice;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ann'];['train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/avazu-ctr-prediction;0.703;0.311;2020-12-12 16:42:00;multiple data sources;[];CTR Practice;Python notebook;3241.0;11;;
2020-08-15 10:02:53;;Apache 2.0;https://www.kaggle.com/jejutphyunchangwoo/logisticregression-practice-0815;1.0;['sklearn'];['ai', 'ml', 'cv'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/avazu-ctr-prediction;0.634;0.0;2020-12-12 16:42:00;Click-Through Rate Prediction;[];LogisticRegression_Practice_0815;Python notebook;807.0;0;;
2015-06-05 12:43:16;;Apache 2.0;https://www.kaggle.com/abhishek/beating-the-benchmark-1;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['predict', 'train', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/avito-context-ad-clicks;0.733;0.411;2020-12-12 16:52:17;Avito Context Ad Clicks;[];"Beating the Benchmark ;)";Python script;6619.0;33;;
2015-07-27 08:24:47;;Apache 2.0;https://www.kaggle.com/adamwong/script-aminos-glm-logit;0.5;[];['nlp', 'ai', 'nn', 'ner'];['test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/avito-context-ad-clicks;0.593;0.152;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Script_aminos_glm_logit;R script;397.0;2;;
2015-07-17 19:44:22;;Apache 2.0;https://www.kaggle.com/alledluviette/ctr-test;1.0;['sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn'];['test data', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/avito-context-ad-clicks;0.707;0.214;2020-12-12 16:52:17;Avito Context Ad Clicks;[];CTR test;Python script;3588.0;4;;
2015-06-19 18:27:14;;Apache 2.0;https://www.kaggle.com/brianbai/inspect-tables-with-pandas;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-context-ad-clicks;0.683;0.214;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Inspect tables with pandas;Python script;2139.0;4;;
2015-06-22 19:00:10;;Apache 2.0;https://www.kaggle.com/btwardow/r-dplyr-sqlite;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-context-ad-clicks;0.686;0.253;2020-12-12 16:52:17;Avito Context Ad Clicks;[];R+dplyr+SQLite;R script;2279.0;6;;
2015-07-21 03:09:51;;Apache 2.0;https://www.kaggle.com/datayo/python-data-explorer;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['regression', 'train', 'model', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/avito-context-ad-clicks;0.719;0.236;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Python Data Explorer;Python script;4698.0;5;;
2015-06-03 00:33:40;;Apache 2.0;https://www.kaggle.com/jeffmoser/database-schema;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-context-ad-clicks;0.706;0.268;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Database Schema;Sqlite script;3469.0;7;;
2015-06-22 23:53:52;;Apache 2.0;https://www.kaggle.com/jeffmoser/sample-rows-from-each-table;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-context-ad-clicks;0.704;0.281;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Sample Rows From Each Table;Sqlite script;3320.0;8;;
2015-07-03 12:47:56;;Apache 2.0;https://www.kaggle.com/olivermeyfarth/logistic-regression-on-histctr;0.5;[];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'regression', 'train', 'model', 'validation data', 'deep learning', 'loss', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/avito-context-ad-clicks;0.726;0.319;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Logistic Regression on HistCTR;R script;5658.0;12;;
2019-02-15 18:26:12;This Kernel is only for Learning purpose ,not for competition;Apache 2.0;https://www.kaggle.com/rahulpatel11315/read-data-from-tsv-file-using-pandas-dataframe;0.5;[];['ner', 'ai'];['train'];https://www.kaggle.com/c/avito-context-ad-clicks;0.766;0.253;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Read Data from TSV file using pandas DataFrame;Python notebook;16214.0;6;;
2015-06-26 20:00:17;;Apache 2.0;https://www.kaggle.com/roenbaeck/number-of-records-in-every-table;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-context-ad-clicks;0.665;0.236;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Number of records in every table;Sqlite script;1463.0;5;;
2015-07-14 14:54:59;;Apache 2.0;https://www.kaggle.com/rootua/apache-spark-scala-logistic-regression;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'deep learning', 'loss', 'label', 'logistic regression', 'predict', 'classification', 'labeled'];https://www.kaggle.com/c/avito-context-ad-clicks;0.745;0.188;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Apache Spark (Scala) Logistic regression;Python script;9020.0;3;;
2015-07-13 11:41:59;;Apache 2.0;https://www.kaggle.com/ryutek/join-test;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-context-ad-clicks;0.688;0.236;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Join Test;Sqlite script;2342.0;5;;
2015-07-10 08:59:19;;Apache 2.0;https://www.kaggle.com/satomacoto/logistic-regression;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['test data', 'regression', 'train', 'model', 'deep learning', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/avito-context-ad-clicks;0.697;0.214;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Logistic Regression;Python script;2846.0;4;0.05112;0.05067
2015-10-26 13:05:01;;Apache 2.0;https://www.kaggle.com/sionek/histctr-price;0.5;[];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'regression', 'train', 'model', 'validation data', 'deep learning', 'loss', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/avito-context-ad-clicks;0.667;0.188;2020-12-12 16:52:17;Avito Context Ad Clicks;[];HistCTR+price;R script;1528.0;3;;
2015-07-05 07:57:21;;Apache 2.0;https://www.kaggle.com/thakurrajanand/logistic-regression-on-histctr;0.5;[];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'regression', 'train', 'model', 'validation data', 'deep learning', 'loss', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/avito-context-ad-clicks;0.685;0.214;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Logistic Regression on HistCTR;R script;2221.0;4;0.05106;0.05061
2015-07-08 17:12:47;;Apache 2.0;https://www.kaggle.com/tinoswe/split-of-python-dataframe-chunk;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-context-ad-clicks;0.705;0.214;2020-12-12 16:52:17;Avito Context Ad Clicks;[];chunks of large .sqlite to dataframes;Python script;3399.0;4;;
2015-06-29 17:37:35;;Apache 2.0;https://www.kaggle.com/tinoswe/split-python-dataframe;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-context-ad-clicks;0.801;0.375;2020-12-12 16:52:17;Avito Context Ad Clicks;[];.tsv/.csv files to pandas dataframe;Python script;47761.0;22;;
2015-06-09 02:47:21;;Apache 2.0;https://www.kaggle.com/triskelion/sample-clicks;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-context-ad-clicks;0.613;0.214;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Number of empty IsClick fields in train;Sqlite script;555.0;4;;
2015-07-09 15:13:57;;Apache 2.0;https://www.kaggle.com/yejiming/combining-datasets;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['test data', 'training data', 'train', 'deep learning', 'classification'];https://www.kaggle.com/c/avito-context-ad-clicks;0.709;0.214;2020-12-12 16:52:17;Avito Context Ad Clicks;[];Combining datasets;Python script;3721.0;4;;
2018-05-04 22:32:44;;Apache 2.0;https://www.kaggle.com/alxmamaev/how-to-easy-preprocess-russian-text;1.0;['nltk'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/avito-demand-prediction;0.778;0.483;2020-12-12 16:57:27;Avito Demand Prediction Challenge;[];How to easy preprocess Russian text ðŸ‡·ðŸ‡º;Python script;22447.0;80;;
2018-05-03 18:00:21;;Apache 2.0;https://www.kaggle.com/bguberfain/vgg16-train-features;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ann'];['test data', 'train', 'model', 'deep learning', 'layer', 'vgg', 'predict', 'classification'];https://www.kaggle.com/c/avito-demand-prediction;0.725;0.472;2020-12-12 16:57:27;multiple data sources;['gpu'];VGG16 Train features;Python script;5404.0;70;;
2018-05-16 09:16:09;;Apache 2.0;https://www.kaggle.com/christofhenkel/using-train-active-for-training-word-embeddings;1.0;['vocabulary', 'tensorflow', 'keras', 'gensim'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/avito-demand-prediction;0.719;0.471;2020-12-12 16:57:27;Avito Demand Prediction Challenge;[];Using train_active for training word embeddings;Python script;4765.0;69;;
2018-05-12 16:38:56;;Apache 2.0;https://www.kaggle.com/codename007/avito-eda-fe-time-series-dt-visualization;1.0;['pattern', 'textblob', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'label', 'gradient boosting', 'predict', 'decision tree'];https://www.kaggle.com/c/avito-demand-prediction;0.765;0.545;2020-12-12 16:57:27;Avito Demand Prediction Challenge;['beginner, exploratory data analysis, feature engineering, +1 moretime series analysis'];Avito EDA, FE, Time Series, DT Visualization âœ“âœ“;Python notebook;15624.0;190;;
2018-06-14 18:02:14;;Apache 2.0;https://www.kaggle.com/him4318/avito-lightgbm-with-ridge-feature-v-2-0;1.0;['pattern', 'lightgbm', 'sklearn', 'nltk'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ann'];['training data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'gradient boosting', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/avito-demand-prediction;0.754;0.491;2020-12-12 16:57:27;Avito Demand Prediction Challenge;['beginner, feature engineering, nlp, +1 moregradient boosting'];avito_LightGBM with Ridge Feature V 2.0;Python script;11449.0;89;0.22563;0.22208
2018-05-31 23:00:10;;Apache 2.0;https://www.kaggle.com/kailex/xgb-text2vec-tfidf-0-2237;1.0;['vocabulary', 'xgboost', 'nltk', 'sklearn', 'pattern'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn'];['filter', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/avito-demand-prediction;0.772;0.516;2020-12-12 16:57:27;Avito Demand Prediction Challenge;['beginner, classification, feature engineering, +2 morenlp, text mining'];xgb+text2vec+tfidf [0.2237];R script;18850.0;126;;
2018-05-26 22:04:36;;Apache 2.0;https://www.kaggle.com/nicapotato/bow-meta-text-and-dense-features-lgbm;1.0;['vocabulary', 'lightgbm', 'nltk', 'sklearn', 'pattern'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'deep learning', 'label', 'gradient boosting', 'predict', 'computer vision', 'classification'];https://www.kaggle.com/c/avito-demand-prediction;0.747;0.489;2020-12-12 16:57:27;Avito Demand Prediction Challenge;['nlp, regression, gradient boosting'];BoW, Meta text, and Dense Features - LGBM;Python script;9529.0;87;0.22606;0.22269
2018-05-24 09:51:55;;Apache 2.0;https://www.kaggle.com/nicapotato/simple-catboost;1.0;['catboost', 'sklearn'];['ner', 'ai', 'gan', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'label', 'gradient boosting', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/avito-demand-prediction;0.775;0.487;2020-12-12 16:57:27;Avito Demand Prediction Challenge;['binary classification, decision tree, gradient boosting'];Simple CatBoost;Python script;20918.0;85;0.23266;0.22903
2018-05-07 07:19:31;In-Depth Exploratory Analysis and Visualizations - AvitoAvito is the Russiaâ€™s largest classified advertisements website. Sellers on their platform sometimes feel frustrated with both too little demand (indicating something is wrong with the product or the product listing) or too much demand (indicating a hot item with a good description was underpriced). In this competition, the aim is to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive. In this notebook, I have analysed the dataset shared by Avito and prepared visualizations to understand it better. Contents1. Dataset Preparation 2. Feature Engineering 3. Translating Columns 4. Dataset Snapshot Â  Â Â  Â  4.1. Snapshot of Training Data Â  Â Â  Â  4.2 Snapshot of Training Periods Data 5. Understanding the Variable Distributions Â  Â Â  Â  5.1 What is the Distribution of Deal Probability Â  Â Â  Â  5.2 Deal Probability Bins Â  Â Â  Â  5.3 Distribution of Parent Category Â  Â Â  Â  5.4 Distribution of Category Â  Â Â  Â  5.5 Regions and their Item Counts Â  Â Â  Â  5.6 Visualizing the Regions on Map Â  Â Â  Â  5.7 Visualizing the Cities on Map Â  Â Â  Â  5.8 Cities and their item counts Â  Â Â  Â  5.9 Param_1, Param_2, Param_3 distributions Â  Â Â  Â  5.10 Month Day and Week Day - Number of Items Â  Â Â  Â  5.11 Title Word Count, Description Word Count and their distributions Â  Â Â  Â  5.12 Image Top1 and User Id Â  Â Â  Â  5.13 Distribution of User Type Â  Â Â  Â  5.14 For how much Days ads are run Â  Â Â  Â  5.15 Top brands present in the title 6. Multi Variate Analysis Â  Â Â  Â  6.1 Correlation among the variables Â  Â Â  Â  6.2 Deal Probability by Parent Category and User Type  Â  Â Â  Â  6.3 Deal Probability by Region and User Type Â  Â Â  Â  6.4 Understanding Log of Price with respect to deal probability Class Â  Â Â  Â  6.5 How many items are there under different Parent Category and different Deal Class Â  Â Â  Â  6.6 Items from different Regions and Different Deal Probability Â  Â Â  Â  6.7 Different Regions and the mean price of items having different deal probability Â  Â Â  Â  6.8 What is the Mean Price of Items having different Deal Probability and Parent Category Â  Â Â  Â  6.9 Deal Class values on different Week Days and their Mean Price Â  Â Â  Â  6.10 Different Regions, Different Day of the Week and Value of Mean Price of Items Â  Â Â  Â  6.11 Week Day and Region vise Value of Max Price of Items Â  Â Â  Â  6.12 Max value of Deal Proabability by different Regions and Day of the Week 7. Charateristics of Items having High or Low Mean Price Â  Â Â  Â  7.1 Mean Price of Items by Week Day and Month Day Â  Â Â  Â  7.2 Mean price of items by Regions and Cities Â  Â Â  Â  7.3 Mean Price of Items by Categories and Parent Categories Â  Â Â  Â  7.4 Mean Price by Count of Words present in Title and Description 8. Characteristics of Items having Very High (or Very Low) Deal Percentage Â  Â Â  Â  8.1 Words present in the title Â  Â Â  Â  8.2 Comparing all other features of Items with Low and High deal percentages Â  Â Â  Â  8.3 Top N-grams used in the description of items 9. Images Related with Items Â  Â Â  Â  9.1 Images having High Deal Probability Â  Â Â  Â  9.2 Images having Low Deal Probability  1. Dataset PreparationIn this section, I have included the required libraries and loaded the dataset into memory using pandas;Apache 2.0;https://www.kaggle.com/shivamb/in-depth-analysis-visualisations-avito;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'layer', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/avito-demand-prediction;0.778;0.59;2020-12-12 16:57:26;Avito Demand Prediction Challenge;['beginner, data visualization, exploratory data analysis, +2 morefeature engineering, text data'];In-Depth Analysis & Visualisations - Avito;Python notebook;23000.0;381;;
2018-05-15 14:26:45;;Apache 2.0;https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lb-0-2241;1.0;['xgboost', 'lightgbm', 'nltk', 'sklearn', 'pattern'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn', 'ann'];['regression', 'train', 'model', 'deep learning', 'label', 'gradient boosting', 'predict', 'computer vision', 'classification', 'labeled'];https://www.kaggle.com/c/avito-demand-prediction;0.764;0.523;2020-12-12 16:57:27;Avito Demand Prediction Challenge;[];BoW, Meta text , and Dense Features - [LB 0.2241];Python script;15291.0;138;;
2016-05-06 15:53:51;;Apache 2.0;https://www.kaggle.com/abhishek/logistict-regression;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['regression', 'generation', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.656;0.214;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];Logistict Regression;Python script;1236.0;4;0.64158;0.64127
2016-06-18 19:41:40;;Apache 2.0;https://www.kaggle.com/anokas/wget-data;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.662;0.236;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];wget data;Python script;1391.0;5;;
2016-07-28 21:16:56;;Apache 2.0;https://www.kaggle.com/bguberfain/get-hash-from-images-4x-faster;0.2;[];['ner', 'ai', 'cv'];[];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.681;0.152;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];Get Hash from Images (4x faster);Python notebook;2049.0;2;;
2016-05-29 05:15:08;;Apache 2.0;https://www.kaggle.com/brandao/test-ad-1;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['generation', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.654;0.268;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];test ad#1;R script;1186.0;7;0.78575;0.78435
2016-05-16 11:52:22;;Apache 2.0;https://www.kaggle.com/chevli/get-hash-from-images-in-parallel;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.602;0.152;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];Get Hash from Images - in Parallel;Python script;462.0;2;;
2016-07-08 16:43:27;;Apache 2.0;https://www.kaggle.com/company/adad-1;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['generation', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.74;0.292;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];ADAD_1;R script;7947.0;9;;
2016-05-09 00:41:15;;Apache 2.0;https://www.kaggle.com/company/adad1;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['regression', 'generation', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.578;0.152;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];ADAD1;Python script;314.0;2;0.64158;0.64127
2016-07-10 20:13:31;;Apache 2.0;https://www.kaggle.com/drarfc/r-xgboost-example;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['generation', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.651;0.152;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];R XGBoost example;R script;1116.0;2;0.79644;0.79483
2016-05-12 16:54:22;;Apache 2.0;https://www.kaggle.com/edwingraham/r-xgboost-example;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['generation', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.743;0.319;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];R XGBoost example;R script;8631.0;12;0.77900;0.77726
2016-05-23 17:13:04;;Apache 2.0;https://www.kaggle.com/iezepov/get-hash-from-images-slightly-daster;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.702;0.236;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];Get Hash from Images (slightly) faster;Python script;3223.0;5;;
2016-06-13 12:06:20;;Apache 2.0;https://www.kaggle.com/pietromarinelli/test-ad-1;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['generation', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.588;0.152;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];test ad#1;R script;368.0;2;0.78559;0.78407
2016-05-14 10:08:25;;Apache 2.0;https://www.kaggle.com/remap1/visualize-images;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'generation', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.657;0.236;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];visualize_images;Python script;1250.0;5;;
2016-05-10 10:49:32;;Apache 2.0;https://www.kaggle.com/rightfit/get-hash-from-images;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['rank', 'classification', 'train', 'deep learning'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.748;0.34;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];Get Hash from Images;Python script;9791.0;15;;
2016-05-11 12:50:54;;Apache 2.0;https://www.kaggle.com/rudikruger/no-images;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.668;0.236;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];String Distance;R script;1572.0;5;0.62937;0.62882
2016-05-13 23:52:31;;Apache 2.0;https://www.kaggle.com/rudikruger/r-xgboost-example;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['generation', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.672;0.152;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];R XGBoost example;R script;1686.0;2;0.78524;0.78365
2016-05-06 07:41:18;;Apache 2.0;https://www.kaggle.com/thakurrajanand/glm-0-61;0.5;[];['ner', 'ai', 'cnn', 'nlp', 'nn'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.737;0.437;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];GLM - 0.61 ;R script;7391.0;45;0.62000;0.61951
2016-05-06 10:40:25;;Apache 2.0;https://www.kaggle.com/vaheed/logistict-regression;1.0;['sklearn'];['ner', 'ai', 'gan', 'nlp', 'nn'];['regression', 'generation', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.724;0.357;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];Logistict Regression;Python script;5375.0;18;0.64158;0.64127
2016-06-16 20:25:27;;Apache 2.0;https://www.kaggle.com/weizhezhang/python-xgboost-starter-0-74;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['test data', 'generation', 'train', 'model', 'validation data', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.641;0.152;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];Python XGBoost Starter - 0.74;Python script;928.0;2;0.75373;0.75363
2016-07-10 13:19:53;;Apache 2.0;https://www.kaggle.com/xameleoh/kaggle-uploader-py;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.64;0.268;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];kaggle-uploader.py;Python script;902.0;7;;
2016-05-19 12:54:56;;Apache 2.0;https://www.kaggle.com/zfturbo/python-xgboost-starter;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ml'];['test data', 'generation', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/avito-duplicate-ads-detection;0.749;0.405;2020-12-12 17:07:49;Avito Duplicate Ads Detection;[];Python XGBoost Starter - 0.74;Python script;9992.0;31;0.75201;0.75204
2020-06-25 07:03:39;;Apache 2.0;https://www.kaggle.com/cdeotte/how-to-compete-with-gpus-workshop;1.0;['albumentations', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn'];['generation', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/bengaliai-cv19;0.74;0.545;2020-12-12 17:09:20;multiple data sources;['gpu'];How To Compete with GPUs Workshop;Python notebook;7959.0;189;;
2020-01-25 19:21:35;Bengali.AI Handwritten Grapheme - Getting StartedIntroductionBengali is the 5th most spoken language in the world with hundreds of million of speakers. Optical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to Englishâ€™s 250 graphemic units). Bangladesh-based non-profit Bengali.AI is focused on helping to solve this problem. They build and release crowdsourced, metadata-rich datasets and open source them through research competitions. Through this work, Bengali.AI hopes to democratize and accelerate research in Bengali language technologies and to promote machine learning education. For this competition, we are given the image of a handwritten Bengali grapheme and are challenged to separately classify three constituent elements in the image: grapheme root, vowel diacritics, and consonant diacritics.;Apache 2.0;https://www.kaggle.com/gpreda/bengali-ai-handwritten-grapheme-getting-started;0.5;[];['ner', 'ai', 'cv', 'rl', 'nn', 'ann'];['train', 'recognition', 'label', 'machine learning'];https://www.kaggle.com/c/bengaliai-cv19;0.729;0.521;2020-12-12 17:09:20;Bengali.AI Handwritten Grapheme Classification;['beginner, data visualization, exploratory data analysis, +1 morecomputer vision'];Bengali.AI Handwritten Grapheme - Getting Started;Python notebook;5996.0;134;;
2020-02-09 03:14:22;;Apache 2.0;https://www.kaggle.com/h030162/version1-0-9696;1.0;['caffe'];['ai', 'nn', 'ann', 'cv'];['train', 'model', 'layer', 'relu', 'resnet'];https://www.kaggle.com/c/bengaliai-cv19;0.751;0.501;2020-12-12 17:09:20;multiple data sources;['gpu'];version1 : 0.9696;Python notebook;10583.0;102;0.9261;0.9696
2020-01-28 20:29:21;;Apache 2.0;https://www.kaggle.com/hmendonca/kaggle-pytorch-utility-script;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'loss'];https://www.kaggle.com/c/bengaliai-cv19;0.733;0.497;2020-12-12 17:09:20;multiple data sources;['deep learning, utility script'];Kaggle pytorch utility script;Python script;6732.0;96;;
2020-01-01 05:28:05;DescriptionThis kernel performs inference for Grapheme fast.ai starter kernel. Check it for more training details. The image preprocessing pipline is provided here.;Apache 2.0;https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-inference;0.5;[];['dl', 'ai', 'nn', 'cv'];['filter', 'train', 'model', 'layer', 'label', 'predict', 'relu'];https://www.kaggle.com/c/bengaliai-cv19;0.743;0.526;2020-12-12 17:09:20;multiple data sources;['gpu, deep learning, classification'];Grapheme fast.ai starter [inference];Python notebook;8576.0;144;0.9340;0.9639
2019-12-21 03:55:58;Bengali.AI Quick data explorationIn the next couple of days, I'll continue to explore the BengaliAI dataset, stay tuned.;Apache 2.0;https://www.kaggle.com/pestipeti/bengali-quick-eda;0.5;[];['ai', 'cv'];['train', 'label'];https://www.kaggle.com/c/bengaliai-cv19;0.722;0.513;2020-12-12 17:09:20;multiple data sources;['data visualization, exploratory data analysis'];Bengali - Quick EDA;Python notebook;5087.0;120;;
2020-01-22 17:29:01;Augmented prediction visualizer;Apache 2.0;https://www.kaggle.com/pestipeti/visualize-your-model-s-augmented-predictions;1.0;['pytorch', 'albumentations'];['ner', 'ai', 'cv', 'ml', 'nn', 'ann'];['image classification', 'filter', 'train', 'model', 'layer', 'label', 'predict', 'relu', 'resnet', 'classification'];https://www.kaggle.com/c/bengaliai-cv19;0.67;0.489;2020-12-12 17:09:20;multiple data sources;['gpu, data visualization, computer vision'];Visualize your model's augmented predictions;Python notebook;1635.0;87;;
2019-12-23 09:13:47;In this competition, we have three targets for each sample, thus I think using iterative stratifications (https://github.com/trent-b/iterative-stratification) is helpful according to previous competition:imet top 1 solution https://www.kaggle.com/c/imet-2019-fgvc6/discussion/94687;Apache 2.0;https://www.kaggle.com/yiheng/iterative-stratification;0.5;[];['ai', 'ml', 'cv'];['train', 'label'];https://www.kaggle.com/c/bengaliai-cv19;0.721;0.502;2020-12-12 17:09:20;Bengali.AI Handwritten Grapheme Classification;[];iterative stratification;Python notebook;5010.0;104;;
2020-04-22 11:29:38;BigQuery ML Intersection Data Analysis;Apache 2.0;https://www.kaggle.com/anshuls235/google-big-query-bokeh-visualizations;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;0.599;0.346;2020-12-12 17:10:49;BigQuery-Geotab Intersection Congestion;['data visualization, exploratory data analysis, bigquery'];Google Big Query + Bokeh visualizations;Python notebook;444.0;16;;
2019-10-15 04:43:50;Data Cleaning and Preprocessing;Apache 2.0;https://www.kaggle.com/dcaichara/feature-engineering-and-lightgbm;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ai', 'gbm', 'cv', 'rl', 'nn', 'ml'];['regression', 'train', 'model', 'label', 'predict', 'bayesian'];https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;0.722;0.48;2020-12-12 17:10:49;BigQuery-Geotab Intersection Congestion;['beginner, feature engineering, regression'];Feature Engineering and LightGBM;Python notebook;5085.0;77;63.651650;66.742440
2020-04-03 08:35:30;The dataset includes intersection wait times and stopping distances with hour, month and weekend/weekday discrepancy in 4 major US cities Philadelphia, Boston, Atlanta and Chicago.;Apache 2.0;https://www.kaggle.com/fatihbilgin/data-visualization-and-eda-for-geotab-bigquery;0.5;[];['ai', 'nn', 'ann'];['train', 'label', 'test data'];https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;0.688;0.447;2020-12-12 17:10:49;BigQuery-Geotab Intersection Congestion;['beginner, data visualization, exploratory data analysis'];Data Visualization and EDA for Geotab BigQuery;Python notebook;2345.0;51;;
2019-10-15 20:41:10;;Apache 2.0;https://www.kaggle.com/gaborfodor/5-combine-models;1.0;['xgboost'];['ner', 'ai'];['train', 'fitting'];https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;0.647;0.362;2020-12-12 17:10:49;multiple data sources;[];5 Combine Models ðŸš¦ðŸš— ;Python notebook;1030.0;19;62.460670;63.985284
2019-09-26 18:36:27;IntroThis notebook explores the dynamics of single intersections. I like to take this sort of micro-level look to learn about the data and get ideas for feature engineering. The potential features might be derived from the given data or they might require data from other sources. Using such features and/or ones like them will help improve model performance. Here's a quick look at the data with the target-like columns separated.;Apache 2.0;https://www.kaggle.com/jpmiller/eda-to-break-through-rmse-68;0.5;[];['ner', 'ai', 'nn', 'gan'];['train', 'model', 'label'];https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;0.654;0.367;2020-12-12 17:10:49;BigQuery-Geotab Intersection Congestion;['exploratory data analysis, feature engineering'];EDA to break through RMSE 68 ðŸš™;Python notebook;1191.0;20;;
2019-09-14 10:57:48;About the Competition;Apache 2.0;https://www.kaggle.com/pradeepmuniasamy/comparative-study-of-models-geotab-inertsection;1.0;['statsmodels', 'catboost', 'tensorflow', 'sklearn'];['dl', 'ner', 'ai', 'nn'];['training data', 'test data', 'regression', 'train', 'model', 'predict'];https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;0.635;0.371;2020-12-12 17:10:49;BigQuery-Geotab Intersection Congestion;['gpu'];Comparative Study of Models - Geotab Inertsection;Python notebook;826.0;21;76.266128;78.602994
2019-09-15 18:25:05;About the Competition;Apache 2.0;https://www.kaggle.com/pradeepmuniasamy/extensive-eda-and-modelling-geotab-inertsection;1.0;['pattern', 'catboost', 'tensorflow'];['ner', 'ai', 'dl', 'nn', 'ml'];['training data', 'test data', 'train', 'model', 'predict', 'understanding'];https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;0.702;0.463;2020-12-12 17:10:49;BigQuery-Geotab Intersection Congestion;['beginner, data visualization, exploratory data analysis'];Extensive EDA and Modelling - Geotab Inertsection ;Python notebook;3226.0;62;;
2019-10-24 02:19:37;Create UID Intersection;Apache 2.0;https://www.kaggle.com/whatust/geotab-congestion;1.0;['pytorch'];['ai', 'nn', 'ml', 'gan'];['train', 'model', 'epoch', 'label', 'loss', 'rank', 'relu'];https://www.kaggle.com/c/bigquery-geotab-intersection-congestion;0.661;0.346;2020-12-12 17:10:49;multiple data sources;[];Fork of kernel56e53f4445;Python notebook;1342.0;16;77.412310;80.737351
2016-08-27 04:53:43;;Apache 2.0;https://www.kaggle.com/apapiu/predicting-bike-sharing-with-xgboost;1.0;['xgboost'];['ai'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/bike-sharing-demand;0.76;0.346;2020-12-12 17:15:12;Bike Sharing Demand;[];Predicting bike sharing with xgboost;R notebook;13405.0;16;;
2015-04-03 22:47:25;;Apache 2.0;https://www.kaggle.com/benhamner/bike-rentals-by-time;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/bike-sharing-demand;0.75;0.34;2020-12-12 17:15:12;Bike Sharing Demand;[];Bike Rentals By Time;R script;10289.0;15;;
2015-05-04 20:57:26;;Apache 2.0;https://www.kaggle.com/benhamner/bike-rentals-by-time-and-temperature;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/bike-sharing-demand;0.785;0.449;2020-12-12 17:15:12;Bike Sharing Demand;[];Bike Rentals By Time And Temperature;R script;28558.0;52;;
2015-04-10 01:25:15;;Apache 2.0;https://www.kaggle.com/benhamner/random-forest-benchmark-2;0.5;[];['nlp', 'ai', 'nn', 'ner'];['training data', 'random forest', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/bike-sharing-demand;0.77;0.393;2020-12-12 17:15:12;Bike Sharing Demand;[];Random Forest Benchmark;R script;17852.0;27;0.59522;0.59522
2020-01-16 10:22:44;In this kernel we will explore the dataset.Dataset has data for two years spanning 19 days in each month.Then we will predict the bike Count using Machine learning.This kernel is a work in process.If you like my work please do vote.;Apache 2.0;https://www.kaggle.com/biphili/why-car-when-you-can-bike;0.5;[];['ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'train', 'label', 'predict'];https://www.kaggle.com/c/bike-sharing-demand;0.63;0.352;2020-12-12 17:15:12;Bike Sharing Demand;[];Why Car when you Can Bike?;Python notebook;752.0;17;;
2017-03-09 10:43:27;;Apache 2.0;https://www.kaggle.com/casalicchio/tuning-with-mlr;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/bike-sharing-demand;0.74;0.379;2020-12-12 17:15:12;Bike Sharing Demand;[];Tuning with mlr;R script;7925.0;23;0.37977;0.37977
2016-10-19 20:08:46;;Apache 2.0;https://www.kaggle.com/h19881812/data-vizualization;0.5;[];['ner', 'ai', 'nlu', 'nlp', 'nn'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/bike-sharing-demand;0.761;0.362;2020-12-12 17:15:12;Bike Sharing Demand;[];Data Vizualization;R script;13844.0;19;;
2020-03-04 04:24:40;Dicas de Jupyter NotebookEste programa que estamos usando no navegador se chama Jupyter Notebook e Ã© um interpretador de Python interativo. Podemos digitar um comando de Python, apertar Shift + Enter e esse comando Ã© executado, tendo seu resultado impresso na tela. Existem alguns atalhos bastante Ãºteis para trabalhar com Jupyter Notebook: 0. 'Esc' - SaÃ­ do modo ediÃ§Ã£o da cÃ©lula atual (modo navegaÃ§Ã£o) 1. 'Enter' - Edita a cÃ©lula selecionada (modo navegaÃ§Ã£o) 2. 'Shift + Enter' - Executa a cÃ©lula selecionada (modo ediÃ§Ã£o) 3. 'A' - Adiciona uma nova cÃ©lula acima da cÃ©lula atual (modo navegaÃ§Ã£o) 4. 'B' - Adiciona uma nova cÃ©lula abaixo da cÃ©lula atual (modo navegaÃ§Ã£o) 5. 'X' - Recorta a cÃ©lula selecionada (modo navegaÃ§Ã£o) 6. 'Z' - Desfaz a deleÃ§Ã£o de uma cÃ©lula (modo navegaÃ§Ã£o) 7. 'Ctrl + Z' - Desfaz as Ãºltimas ediÃ§Ãµes (modo ediÃ§Ã£o) 8. 'Ctrl + Shift + Z' - Refaz o Ãºltimo comando de desfazer (modo ediÃ§Ã£o) 9. 'H' - Abre um arquivo de ajuda com todos os atalhos (modo navegaÃ§Ã£o) 10. 'Tab' - Faz sugestÃµes para completar o cÃ³digo (modo ediÃ§Ã£o) 11. 'Shift + Tab' - Faz sugestÃµes de assinatura de um mÃ©todo ou funÃ§Ã£o (modo ediÃ§Ã£o) 12. '?funÃ§Ã£o' - Mostra a documentaÃ§Ã£o de uma dada funÃ§Ã£o (modo ediÃ§Ã£o);Apache 2.0;https://www.kaggle.com/joaoavf/python-e-pandas-exercicio;0.5;[];['ai', 'ml', 'rl'];['train', 'gru', 'machine learning', 'deep learning'];https://www.kaggle.com/c/bike-sharing-demand;0.714;0.327;2020-12-12 17:15:12;Bike Sharing Demand;[];Python e Pandas - ExercÃ­cio;Python notebook;4228.0;13;;
2018-11-20 16:47:14;;Apache 2.0;https://www.kaggle.com/roshanchoudhary/bike-sharing-in-r-programming;1.0;['pattern'];['ner', 'ai', 'nlp', 'nn', 'ml'];['test data', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'linear regression', 'classification'];https://www.kaggle.com/c/bike-sharing-demand;0.706;0.334;2020-12-12 17:15:12;Bike Sharing Demand;[];Bike Sharing in R Programming;R script;3508.0;14;;
2019-11-10 19:26:00;;Apache 2.0;https://www.kaggle.com/swinalmeshram/bike-sharing;1.0;['sklearn'];['ai'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/bike-sharing-demand;0.653;0.34;2020-12-12 17:15:12;Bike Sharing Demand;['exploratory data analysis, feature engineering'];Bike Sharing;Python notebook;1160.0;15;0.46366;0.46366
2015-04-26 03:56:52;;Apache 2.0;https://www.kaggle.com/triskelion/check-adherence-to-benford-s-law;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/bike-sharing-demand;0.732;0.393;2020-12-12 17:15:12;Bike Sharing Demand;[];Check adherence to Benford's Law;Python script;6567.0;27;;
2020-02-04 04:14:19;;Apache 2.0;https://www.kaggle.com/anastasia484/predicting-a-biological-response;1.0;['sklearn'];['ai', 'cv'];['regression', 'train', 'model', 'validation data', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/bioresponse;0.595;0.099;2020-12-12 17:15:59;Predicting a Biological Response;[];kernel598f682400;Python script;410.0;1;;
2018-09-25 03:33:14;;Apache 2.0;https://www.kaggle.com/ludi666/lr-regression;1.0;['sklearn'];['ai'];['regression', 'train', 'model', 'logistic regression', 'predict'];https://www.kaggle.com/c/bioresponse;0.626;0.0;2020-12-12 17:15:59;Predicting a Biological Response;[];LR_regression;Python notebook;703.0;0;0.51349;0.59424
2019-01-29 17:25:10;;Apache 2.0;https://www.kaggle.com/quinnd/basicsubmission;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/bioresponse;0.594;0.0;2020-12-12 17:15:59;Predicting a Biological Response;[];basicsubmission;Python script;409.0;0;;
2020-07-03 21:22:13;;Apache 2.0;https://www.kaggle.com/vernondsouza123/predict-biological-response-through-lightgbm;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'train', 'model', 'loss', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/bioresponse;0.561;0.099;2020-12-12 17:15:59;Predicting a Biological Response;['gradient boosting'];Predict Biological Response through LightGBM;Python notebook;242.0;1;0.38584;0.42249
2018-12-15 22:25:10;Predicting a Biological Response.;Apache 2.0;https://www.kaggle.com/zmey56/my-first-kernels-from-coursera;1.0;['sklearn'];['ai', 'nn', 'gbm'];['machine learning', 'random forest', 'train', 'fitting', 'model', 'loss', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/bioresponse;0.642;0.152;2020-12-12 17:15:59;multiple data sources;['beginner'];My first Kernels from Coursera.  ;Python notebook;943.0;2;;
2020-06-17 19:52:32;General informationThere are already many projects underway to extensively monitor birds by continuously recording natural soundscapes over long periods. However, as many living and nonliving things make noise, the analysis of these datasets is often done manually by domain experts. These analyses are painstakingly slow, and results are often incomplete. In this competition we predict which bird are in the audio Work, obviously, is in progress :);Apache 2.0;https://www.kaggle.com/artgor/which-bird-is-it;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['test data', 'train', 'recognition', 'label', 'predict'];https://www.kaggle.com/c/birdsong-recognition;0.712;0.483;2020-12-12 17:18:49;Cornell Birdcall Identification;['data visualization, exploratory data analysis, classification, +1 moreaudio data'];Which bird is it?;Python notebook;4003.0;80;;
2020-06-17 11:30:35;In this notebook we will import the test data and make a simple prediction.;Apache 2.0;https://www.kaggle.com/cwthompson/birdsong-making-a-prediction;0.5;[];['dl', 'ai', 'nn', 'ann'];['test data', 'train', 'recognition', 'model', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/birdsong-recognition;0.712;0.518;2020-12-12 17:18:49;Cornell Birdcall Identification;['beginner, earth and nature'];[Birdsong] Making a Prediction;Python notebook;4005.0;128;0.000;0.002
2020-08-09 02:13:57;Introduction;Apache 2.0;https://www.kaggle.com/frlemarchand/bird-song-classification-using-an-efficientnet;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'rl', 'nn', 'rnn', 'ann'];['filter', 'training data', 'train', 'recognition', 'model', 'output layer', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/birdsong-recognition;0.74;0.521;2020-12-12 17:18:49;multiple data sources;['gpu, deep learning, classification, +1 morelstm'];Bird Song Classification using an EfficientNet;Python notebook;7880.0;135;0.000;0.000
2020-06-29 17:16:47;;Apache 2.0;https://www.kaggle.com/hamditarek/audio-data-analysis-using-librosa;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'recognition', 'model', 'layer', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/birdsong-recognition;0.735;0.516;2020-12-12 17:18:49;multiple data sources;['gpu, audio data'];Audio Data Analysis Using librosa ðŸ“ˆ;Python notebook;6989.0;126;0.584;0.560
2020-08-14 12:26:17;Update;Apache 2.0;https://www.kaggle.com/hidehisaarai1213/introduction-to-sound-event-detection;1.0;['pytorch', 'tensorflow', 'sklearn'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'ml', 'nn', 'ann'];['speech recognition', 'filter', 'train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/birdsong-recognition;0.789;0.603;2020-12-12 17:18:49;multiple data sources;['gpu, deep learning, audio data'];Introduction to Sound Event Detection;Python notebook;31491.0;473;;
2020-09-14 18:05:29;Birds! Birds come in so many varieties of colours, shapes, sizes... and voices? Humans are quite adept at recognizing each other's voices and so are ML models now. But how good are ML models in recognizing voices of bird species in their natural habitat along with the sounds of nature? We will have an answer to it at the end of this competition.;Apache 2.0;https://www.kaggle.com/rohanrao/birdcall-eda-chirp-hoot-and-flutter;0.5;[];['ai', 'rl', 'ml', 'nn', 'ann'];['training data', 'test data', 'train', 'recognition', 'model', 'label', 'predict'];https://www.kaggle.com/c/birdsong-recognition;0.678;0.478;2020-12-12 17:18:49;multiple data sources;['beginner, data visualization, exploratory data analysis'];Birdcall EDA: Chirp, Hoot and Flutter;Python notebook;1914.0;75;;
2020-08-06 21:38:44;1. Introduction There are already many projects underway to extensively monitor birds by continuously recording natural soundscapes over long periods. However, as many living and nonliving things make noise, the analysis of these datasets is often done manually by domain experts. These analyses are painstakingly slow, and results are often incomplete.   In this competition, you will identify a wide variety of bird vocalizations in soundscape recordings. Due to the complexity of the recordings, they contain weak labels. There might be anthropogenic sounds (e.g., airplane overflights) or other bird and non-bird (e.g., chipmunk) calls in the background, with a particular labeled bird species in the foreground. Bring your new ideas to build effective detectors and classifiers for analyzing complex soundscape recordings!  Which bird is this?;Apache 2.0;https://www.kaggle.com/rohitsingh9990/eda-visualizations-simple-baseline;0.5;[];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'train', 'recognition', 'model', 'label', 'labeled'];https://www.kaggle.com/c/birdsong-recognition;0.688;0.462;2020-12-12 17:18:50;Cornell Birdcall Identification;['data visualization, exploratory data analysis'];EDA + Visualizations + simple baseline;Python notebook;2334.0;61;;
2020-06-29 09:03:48;Sample SubmissionThis notebook without model. It will help for making submission without Exception (jump start);Apache 2.0;https://www.kaggle.com/shonenkov/sample-submission-using-custom-check;0.5;[];['ai'];['train', 'recognition', 'model', 'predict'];https://www.kaggle.com/c/birdsong-recognition;0.704;0.5;2020-12-12 17:18:49;multiple data sources;[];[Sample Submission] Using Custom Check;Python notebook;3348.0;101;;
2020-08-31 08:32:20;V3 Added Cut-out;Apache 2.0;https://www.kaggle.com/tanulsingh077/audio-albumentations-transform-your-audio;1.0;['pytorch', 'albumentations'];['ner', 'ai', 'cv', 'nn', 'ann'];['train', 'recognition', 'model', 'label', 'computer vision'];https://www.kaggle.com/c/birdsong-recognition;0.687;0.484;2020-12-12 17:18:49;multiple data sources;['audio data'];Audio Albumentations : Transform Your Audio;Python notebook;2308.0;81;;
2020-06-24 05:26:07;;Apache 2.0;https://www.kaggle.com/tarunpaparaju/birdcall-identification-spectrogram-resnet;1.0;['sklearn', 'pytorch', 'albumentations', 'tensorflow', 'keras'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ann'];['filter', 'recognition', 'predict', 'training data', 'object detection', 'train', 'epoch', 'lstm', 'classification', 'image classification', 'model', 'neural network', 'layer', 'loss', 'understanding', 'resnet', 'test data', 'output layer', 'label', 'computer vision'];https://www.kaggle.com/c/birdsong-recognition;0.72;0.485;2020-12-12 17:18:49;multiple data sources;['gpu, deep learning, classification, +2 moreaudio data, signal processing'];Birdcall Identification: Spectrogram ResNet ðŸ¦;Python notebook;4813.0;82;;
2020-07-15 21:41:09;Birdsong Pytorch Baseline: ResNeSt50-fast (Inference);Apache 2.0;https://www.kaggle.com/ttahara/inference-birdsong-baseline-resnest50-fast;1.0;['pytorch', 'sklearn', 'mxnet', 'pattern'];['ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'recognition', 'model', 'layer', 'relu', 'label', 'predict', 'computer vision', 'resnet', 'classification'];https://www.kaggle.com/c/birdsong-recognition;0.753;0.543;2020-12-12 17:18:49;multiple data sources;['gpu, deep learning'];[Inference] Birdsong Baseline: ResNeSt50-fast;Python notebook;11064.0;183;0.591;0.568
2020-07-05 12:54:50;Birdsong Pytorch Baseline: ResNeSt50-fast (Training);Apache 2.0;https://www.kaggle.com/ttahara/training-birdsong-baseline-resnest50-fast;1.0;['pytorch', 'sklearn', 'pillow'];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['gru', 'filter', 'train', 'recognition', 'model', 'epoch', 'label', 'loss', 'relu', 'resnet'];https://www.kaggle.com/c/birdsong-recognition;0.766;0.564;2020-12-12 17:18:49;multiple data sources;['gpu, deep learning'];[Training] Birdsong Baseline: ResNeSt50-fast;Python notebook;15911.0;251;;
2020-07-12 18:31:02;Initialization;Apache 2.0;https://www.kaggle.com/hinamxx/blue-book-for-bulldozers-advanced-approach;1.0;['pattern', 'sklearn'];['ner', 'ai', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/bluebook-for-bulldozers;0.518;0.214;2020-12-12 17:19:45;Blue Book for Bulldozers;['gpu, data visualization, random forest'];Blue Book for Bulldozers - Advanced Approach;Python notebook;128.0;4;;
2019-12-15 06:45:24;;Apache 2.0;https://www.kaggle.com/kaushal2896/blue-book-for-bulldozers;1.0;['pattern', 'lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nn', 'ml'];['training data', 'regression', 'train', 'model', 'validation data', 'predict'];https://www.kaggle.com/c/bluebook-for-bulldozers;0.574;0.152;2020-12-12 17:19:45;Blue Book for Bulldozers;[];Blue Book for Bulldozers;Python notebook;294.0;2;;
2020-09-11 20:33:24;Loading the Data;Apache 2.0;https://www.kaggle.com/sumitsingh20/price-prediction-bulldozers;1.0;['pattern', 'sklearn'];['ai', 'cv'];['training data', 'test data', 'train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/bluebook-for-bulldozers;0.556;0.281;2020-12-12 17:19:45;Blue Book for Bulldozers;['gpu'];Price-Prediction-Bulldozers;Python notebook;222.0;8;;
2018-10-21 13:07:57;Understanding and testing perfomance of pandas.read_csv;Apache 2.0;https://www.kaggle.com/timetraveller98/testing-pandas-read-csv-performance;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'train', 'model', 'understanding'];https://www.kaggle.com/c/bluebook-for-bulldozers;0.769;0.268;2020-12-12 17:19:45;multiple data sources;['beginner, data cleaning'];Testing pandas.read_csv performance;Python notebook;17480.0;7;;
2020-09-17 14:36:33;Bluebook for Bulldozers;Apache 2.0;https://www.kaggle.com/vishakbharadwaj/ensemble-learning-with-bulldozer-auction-data;1.0;['pattern', 'sklearn'];['ai', 'dl'];['filter', 'train', 'fitting', 'model', 'predict', 'random forest'];https://www.kaggle.com/c/bluebook-for-bulldozers;0.459;0.152;2020-12-12 17:19:45;multiple data sources;['random forest, ensembling'];Ensemble Learning with Bulldozer auction data;Python notebook;59.0;2;;
2016-03-24 20:13:20;;Apache 2.0;https://www.kaggle.com/chabir/extratreesclassifier-score-0-45-v5;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'predict', 'decision tree', 'classification', 'naive bayes'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.763;0.375;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];ExtraTreesClassifier002;Python script;14817.0;22;0.46251;0.46248
2016-02-04 17:22:53;;Apache 2.0;https://www.kaggle.com/director/simple-xgboost-0-46146;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['machine learning', 'training data', 'random forest', 'train', 'fitting', 'model', 'deep learning', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.76;0.39;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];Simple XGBoost (0.46146);Python script;13601.0;26;0.67886;0.67894
2016-02-13 23:21:13;;Apache 2.0;https://www.kaggle.com/jimthompson/using-the-boruta-package-to-determine-fe;1.0;['xgboost'];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'random forest', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'understanding', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.764;0.379;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];Boruta Package-Feature Relevance;Rmarkdown script;15346.0;23;;
2016-02-18 05:07:27;;Apache 2.0;https://www.kaggle.com/jstaker7/tensorflow-starter;1.0;['tensorflow', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.753;0.311;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];Tensorflow Starter;Python script;11177.0;11;0.53183;0.53077
2016-04-16 09:56:41;;Apache 2.0;https://www.kaggle.com/justfor/xgb-cross-val-and-feat-select;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['gru', 'filter', 'training data', 'test data', 'train', 'fitting', 'model', 'understanding', 'deep learning', 'loss', 'label', 'predict', 'rank', 'recommend', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.786;0.439;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];xgb-cross-val_and_feat_selected;R script;28905.0;46;0.45898;0.46087
2016-02-04 07:39:13;;Apache 2.0;https://www.kaggle.com/mlandry/h2o-starter-gbm;1.0;['h2o'];['ner', 'ai', 'dl', 'gbm', 'nlp', 'nn', 'ml'];['predict', 'training data', 'train', 'model', 'validation data', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.735;0.357;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];H2O Starter GBM;R script;7045.0;18;0.46692;0.46899
2016-02-13 11:50:13;;Apache 2.0;https://www.kaggle.com/mpearmain/bayesianoptimization-of-random-forest;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['predict', 'random forest', 'train', 'fitting', 'model', 'deep learning', 'loss', 'recommend', 'classification', 'bayesian'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.777;0.444;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];Bayesian Hyperparam Optimization of RF;Python script;22008.0;49;;
2016-03-18 15:57:39;;Apache 2.0;https://www.kaggle.com/omarelgabry/bnp-correlation-predictions;1.0;['xgboost', 'sklearn'];['ai', 'gbm', 'cv', 'ml', 'nn', 'ann'];['machine learning', 'regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'random forest'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.725;0.311;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];BNP Correlation & Predictions;Python notebook;5426.0;11;0.46861;0.47086
2016-03-04 22:17:28;;Apache 2.0;https://www.kaggle.com/rdslater/more-visualization;1.0;['sklearn'];['ai', 'ml'];['train', 'label'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.706;0.302;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];More visualization;Python notebook;3495.0;10;;
2018-04-01 05:52:02;;Apache 2.0;https://www.kaggle.com/rsakata/xgboost-with-combination-of-factors;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn', 'ml'];['training data', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.757;0.435;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];XGBOOST with combination of factors;R script;12622.0;44;0.43251;0.43441
2016-03-22 11:51:19;;Apache 2.0;https://www.kaggle.com/rushter/et-classifier;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ml'];['training data', 'test data', 'regression', 'train', 'model', 'deep learning', 'logistic regression', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.76;0.352;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];ET classifier;Python script;13629.0;17;0.45279;0.45374
2016-03-05 12:45:35;;Apache 2.0;https://www.kaggle.com/scirpus/benouilli-naive-bayes;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['predict', 'random forest', 'train', 'model', 'deep learning', 'loss', 'classification', 'naive bayes'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.729;0.34;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];Benouilli Naive Bayes;Python script;6098.0;15;0.51605;0.51741
2016-03-07 01:31:18;;Apache 2.0;https://www.kaggle.com/timesler/xgboost-15-02-2016;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['filter', 'test data', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.747;0.311;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];XGBoost 15-02-2016 (0.45769);R script;9495.0;11;;
2016-02-12 17:21:28;;Apache 2.0;https://www.kaggle.com/trottefox/blending-trees;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['predict', 'train', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.746;0.397;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];Blending trees;Python script;9193.0;28;0.46139;0.46225
2016-04-15 20:17:12;;Apache 2.0;https://www.kaggle.com/trottefox/nearest-neighbour-linear-features;1.0;['sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['regression', 'train', 'fitting', 'model', 'reward', 'deep learning', 'loss', 'predict', 'linear regression', 'classification'];https://www.kaggle.com/c/bnp-paribas-cardif-claims-management;0.752;0.455;2020-12-12 17:27:17;BNP Paribas Cardif Claims Management;[];Nearest Neighbour Linear Features;Python script;10983.0;56;0.45029;0.45148
2016-09-12 22:37:58;;Apache 2.0;https://www.kaggle.com/cartographic/bish-bash-xgboost;1.0;['xgboost'];['ai'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/bosch-production-line-performance;0.751;0.456;2020-12-12 17:31:53;Bosch Production Line Performance;[];bish, bash, xgboost;Rmarkdown script;10657.0;57;0.21381;0.20686
2016-09-11 11:19:10;;Apache 2.0;https://www.kaggle.com/cartographic/de-dupe-categoricals;0.5;[];['ai'];['train', 'model', 'label'];https://www.kaggle.com/c/bosch-production-line-performance;0.714;0.379;2020-12-12 17:31:53;Bosch Production Line Performance;[];De-dupe categoricals;Rmarkdown script;4171.0;23;;
2016-11-14 12:55:48;;Apache 2.0;https://www.kaggle.com/danielfg/xgboost-reg-linear-lb-0-485;1.0;['xgboost'];['ner', 'ai', 'gan', 'nlp', 'nn'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/bosch-production-line-performance;0.738;0.352;2020-12-12 17:31:53;Bosch Production Line Performance;[];XGBoost_reg:linear_LB:0.485;R script;7500.0;17;0.48548;0.48578
2016-09-11 14:47:42;EDA of important features;Apache 2.0;https://www.kaggle.com/dollardollar/eda-of-important-features;1.0;['pattern', 'xgboost'];['ai'];['train', 'model', 'training data', 'predict'];https://www.kaggle.com/c/bosch-production-line-performance;0.749;0.452;2020-12-12 17:31:53;Bosch Production Line Performance;[];EDA of important features;Python notebook;9940.0;54;;
2016-10-10 23:45:17;This is a visualization of the first 10,000 jobs. Node size indicates in-degree (how many jobs come into this station). Node colors indicate the Line. Alpha of paths indicates how often jobs go from this station to another. One can nicely see parallel station setups and overflow / backup lines.;Apache 2.0;https://www.kaggle.com/gingerman/shopfloor-visualization;0.5;[];['ai', 'nn'];['train', 'label', 'filter'];https://www.kaggle.com/c/bosch-production-line-performance;0.774;0.478;2020-12-12 17:31:53;Bosch Production Line Performance;[];Shopfloor Visualization;R notebook;20096.0;75;;
2016-10-26 11:32:07;This is a new version of my older script which instead of visualizing a number of jobs presents the whole data. This allows assessing typical paths across the production facility. I provide three variants - one for all jobs and two additional ones filtered to Response 0 and 1. Widths of the edges indicate the number of jobs on this station pair.;Apache 2.0;https://www.kaggle.com/gingerman/shopfloor-visualization-2-0;0.5;[];['ai', 'nn'];['train', 'label', 'filter'];https://www.kaggle.com/c/bosch-production-line-performance;0.743;0.442;2020-12-12 17:31:53;Bosch Production Line Performance;[];Shopfloor Visualization 2.0;R notebook;8592.0;48;;
2016-09-22 22:39:02;Error/Failure Rate EDA and Feature Engineering IdeasI wanted to know if certain lines or stations were correlated to higher error rates. While exploring, I found this code is also somewhat useful for feature engineering, such as taking the min/max values at each station. Station 32A total of 24,543 samples run through station 32, with a 4.7% error rate, compared the mean error rate 0.6%. It only has one feature, L3_S32_F3850, which has come up on other Kernels ranking feature importance. Coincidentally (or maybe not), Station 31 is associated with the lowest failure rate at 0.27%;Apache 2.0;https://www.kaggle.com/jeffd23/what-s-wrong-with-station-32;0.5;[];['ai'];['rank', 'label', 'train'];https://www.kaggle.com/c/bosch-production-line-performance;0.743;0.437;2020-12-12 17:31:53;Bosch Production Line Performance;[];What's Wrong with Station 32;Python notebook;8518.0;45;;
2016-09-10 01:50:01;;Apache 2.0;https://www.kaggle.com/jeffhebert/visualize-station-parameters-over-time;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/bosch-production-line-performance;0.702;0.383;2020-12-12 17:31:53;Bosch Production Line Performance;[];Visualize Station Parameters Over Time;R script;3234.0;24;;
2016-09-09 01:32:42;;Apache 2.0;https://www.kaggle.com/jeffhebert/visualize-station-performance;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/bosch-production-line-performance;0.725;0.408;2020-12-12 17:31:53;Bosch Production Line Performance;[];Visualize Station Performance;R script;5478.0;32;;
2016-09-09 15:28:19;;Apache 2.0;https://www.kaggle.com/jpmiller/flowpath-viz;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/bosch-production-line-performance;0.738;0.427;2020-12-12 17:31:53;Bosch Production Line Performance;[];Flowpath Viz;R script;7510.0;40;;
2016-09-09 13:30:11;;Apache 2.0;https://www.kaggle.com/laurae2/what-s-in-the-kaggle-docker;1.0;['xgboost', 'h2o', 'mxnet', 'pattern', 'skimage'];['ner', 'ai', 'dl', 'cnn', 'gbm', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['autoencoder', 'gru', 'filter', 'regression', 'generation', 'train', 'model', 'neural network', 'layer', 'clustering', 'loss', 'label', 'predict', 'rank', 'recommend', 'classification', 'labeled', 'bayesian'];https://www.kaggle.com/c/bosch-production-line-performance;0.715;0.367;2020-12-12 17:31:53;Bosch Production Line Performance;[];What's in the Kaggle Docker?;Rmarkdown script;4323.0;20;;
2016-10-18 08:33:51;;Apache 2.0;https://www.kaggle.com/mcosch/leaky-36-in-r;1.0;['xgboost'];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/bosch-production-line-performance;0.72;0.4;2020-12-12 17:31:53;Bosch Production Line Performance;[];leaky .36 in R;R script;4826.0;29;;
2016-10-08 22:13:38;;Apache 2.0;https://www.kaggle.com/mmueller/road-2-0-4;1.0;['pattern', 'xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/bosch-production-line-performance;0.783;0.524;2020-12-12 17:31:53;Bosch Production Line Performance;[];Road-2-0.4+;Python script;26743.0;140;;
2016-09-30 13:11:03;;Apache 2.0;https://www.kaggle.com/scirpus/loo-template-for-low-memory;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['training data', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/bosch-production-line-performance;0.733;0.423;2020-12-12 17:31:53;Bosch Production Line Performance;[];Loo Template for Low Memory;Python script;6621.0;38;;
2019-04-09 09:13:03;There have been many discussions about links between samples in the train and test data, because the robot's absolute orientation does not change much in one run. This means that a) it is dangerously easy to create a model which looks like it predicts surfaces when in fact it predicts orientation and b) we might gain some additional knowledge about the data. Let's settle this once and for all.;Apache 2.0;https://www.kaggle.com/friedchips/the-missing-link;0.5;[];['ai', 'nn', 'ann'];['train', 'model', 'test data', 'predict'];https://www.kaggle.com/c/career-con-2019;0.681;0.458;2020-12-12 17:33:04;CareerCon 2019 - Help Navigate Robots;['data visualization, exploratory data analysis, feature engineering'];The Missing Link...;Python notebook;2022.0;58;;
2019-04-29 14:05:37;Fast Fourier Tranform DenoisingIn this kernel I will show a quick trick to denoise your signal.;Apache 2.0;https://www.kaggle.com/ilhamfp31/fast-fourier-transform-denoising;0.5;[];['ai', 'nn', 'ann'];['train', 'label', 'filter'];https://www.kaggle.com/c/career-con-2019;0.719;0.446;2020-12-12 17:33:04;CareerCon 2019 - Help Navigate Robots;[];Fast Fourier Transform Denoising;Python notebook;4744.0;50;;
2019-03-26 20:37:39;Credits:  https://www.kaggle.com/jsaguiar/surface-recognition-baseline https://www.kaggle.com/gpreda/santander-eda-and-prediction;Apache 2.0;https://www.kaggle.com/pluceroo/new-features-lgbm-and-simple-rf;1.0;['lightgbm', 'sklearn'];['ai', 'gbm', 'cv', 'rl', 'nn'];['predict', 'test data', 'train', 'recognition', 'model', 'label', 'loss', 'random forest'];https://www.kaggle.com/c/career-con-2019;0.68;0.4;2020-12-12 17:33:04;CareerCon 2019 - Help Navigate Robots;[];New features + LGBM and  Simple RF;Python notebook;1974.0;29;0.5779;0.7055
2019-04-12 02:26:15;;Apache 2.0;https://www.kaggle.com/prith189/starter-code-for-3rd-place-solution;1.0;['tensorflow', 'sklearn', 'keras'];['dl', 'ai', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu'];https://www.kaggle.com/c/career-con-2019;0.712;0.431;2020-12-12 17:33:04;CareerCon 2019 - Help Navigate Robots;['gpu'];Starter Code for 3rd place Solution;Python notebook;3999.0;42;0.8739;0.8281
2019-03-22 23:25:29;My hope was, that different surface types yield (visible) differences in the frequency spectrum of the sensor measurements. Machine learning techniques might learn frequency filters on their own, but why don't give the machine a little head start? So I computed the the cyclic FFT for the angular velocity and linear acceleration sensors and plotted mean and standard deviation of the absolute values of the frequency components per training surface category (leaving out the frequency 0 (i.e. constants like sensor bias, earth gravity, ...). The sensors show some different frequency characterists (see plots below), but unfortunately the surface categories have all similar (to the human eye) shapes, varying mostly in total power, and the standard deviations are high (compared to differences in the means). So there are no nice strong characteristic peaks for surface types. But that does not mean, that there is nothing detectable by more sophisticated statistical methods. This article http://www.kaggle.com/christoffer/establishing-sampling-frequency makes a convincing case, that the sampling frequency is around 400Hz, so according to that you would see the frequency range to 3-200 Hz in the diagrams (and aliased higher frequencies).;Apache 2.0;https://www.kaggle.com/trohwer64/simple-fourier-analysis;0.5;[];['ai', 'nn'];['train', 'filter', 'label', 'machine learning'];https://www.kaggle.com/c/career-con-2019;0.688;0.405;2020-12-12 17:33:04;CareerCon 2019 - Help Navigate Robots;[];Simple Fourier Analysis;Python notebook;2345.0;31;;
2017-08-04 07:15:24;;Apache 2.0;https://www.kaggle.com/alekseit/simple-bounding-boxes;1.0;['skimage'];['ai', 'cv'];['train'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.779;0.367;2020-12-12 17:38:34;Carvana Image Masking Challenge;['data visualization, computer vision'];Simple bounding boxes;Python notebook;23582.0;20;;
2017-07-27 18:20:04;;Apache 2.0;https://www.kaggle.com/arseni/h5py-dataset-caching-with-shuffled-batch-generator;1.0;['skimage'];['ner', 'ai', 'dl', 'ml', 'nlp', 'nn', 'ann'];['training data', 'train', 'model', 'epoch', 'deep learning', 'computer vision', 'classification'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.704;0.34;2020-12-12 17:38:34;Carvana Image Masking Challenge;['computer vision'];h5py dataset caching with shuffled batch generator;Python script;3335.0;15;;
2017-07-27 18:21:38;;Apache 2.0;https://www.kaggle.com/arsenyinfo/h5py-caching-for-fast-data-access;1.0;['tensorflow', 'skimage'];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.707;0.39;2020-12-12 17:38:34;Carvana Image Masking Challenge;[];h5py caching for fast data access ;Python script;3570.0;26;;
2017-07-28 13:11:59;;Apache 2.0;https://www.kaggle.com/bguberfain/naive-keras;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'cv', 'rl', 'nn', 'ann'];['predict', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.755;0.465;2020-12-12 17:38:33;Carvana Image Masking Challenge;['neural networks, computer vision, advanced'];Naive Keras;Python notebook;11968.0;64;;
2017-07-27 10:24:46;;Apache 2.0;https://www.kaggle.com/hackerpoet/even-faster-run-length-encoder;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'predict', 'computer vision', 'classification'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.72;0.411;2020-12-12 17:38:33;Carvana Image Masking Challenge;['computer vision'];Even Faster Run Length Encoder;Python script;4867.0;33;;
2017-07-30 17:27:39;Getting a meaning of the scoreIn this notebook I will take the train masks and by using erode and dilate try to get a sense of what the different dice scores mean.;Apache 2.0;https://www.kaggle.com/ironbar/getting-a-meaning-of-the-score;0.5;[];['ai', 'cv'];['train', 'label', 'predict'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.682;0.367;2020-12-12 17:38:34;Carvana Image Masking Challenge;['computer vision'];Getting a meaning of the score;Python notebook;2094.0;20;;
2017-08-30 09:47:11;;Apache 2.0;https://www.kaggle.com/loveall/tpot-on-cervicalcancer-top10-factors;1.0;['xgboost', 'sklearn', 'tpot'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.692;0.397;2020-12-12 17:38:33;multiple data sources;['healthcare'];Tpot  on CervicalCancer_Top10_factors;Python script;2589.0;28;;
2017-08-28 05:17:15;;Apache 2.0;https://www.kaggle.com/lyakaap/weighing-boundary-pixels-loss-script-by-keras2;1.0;['pytorch', 'tensorflow', 'keras'];['ner', 'ai', 'gan', 'cv', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'deep learning', 'loss', 'understanding', 'classification'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.735;0.357;2020-12-12 17:38:34;Carvana Image Masking Challenge;[];Weighing boundary pixels loss script by Keras2;Python script;6950.0;18;;
2017-08-26 17:36:00;;Apache 2.0;https://www.kaggle.com/mlagunas/naive-unet-with-pytorch-tensorboard-logging;1.0;['pytorch', 'tensorflow'];['ner', 'ai', 'dl', 'gan', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'label', 'loss', 'relu', 'classification', 'u-net'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.745;0.352;2020-12-12 17:38:34;Carvana Image Masking Challenge;[];Naive Unet with Pytorch + Tensorboard logging;Python script;9057.0;17;;
2018-03-11 04:35:23;;Apache 2.0;https://www.kaggle.com/paulorzp/run-length-encode-and-decode;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.781;0.483;2020-12-12 17:38:33;Carvana Image Masking Challenge;[];Run-Length Encode and Decode;Python script;24731.0;80;;
2017-08-01 09:30:13;Fast run length encoding, tested on the provided training mask data.;Apache 2.0;https://www.kaggle.com/stainsby/fast-tested-rle;0.5;[];['ner', 'ai', 'rl'];['train'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.749;0.475;2020-12-12 17:38:33;Carvana Image Masking Challenge;['computer vision, advanced'];Fast, tested RLE;Python notebook;10116.0;72;;
2017-07-29 01:26:24;;Apache 2.0;https://www.kaggle.com/tunguz/baseline-2-optimal-mask;0.5;[];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'loss'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.67;0.327;2020-12-12 17:38:34;Carvana Image Masking Challenge;[];Baseline 2 [Optimal mask];Python script;1630.0;13;0.82340;0.82196
2017-08-22 03:55:04;;Apache 2.0;https://www.kaggle.com/uiiurz1/how-does-the-image-scale-affect-dice;1.0;['skimage'];['ai', 'cv'];['train', 'label'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.678;0.39;2020-12-12 17:38:34;Carvana Image Masking Challenge;[]; How does the image scale affect dice?;Python notebook;1919.0;26;;
2017-07-27 08:37:23;Visual Data Analysis;Apache 2.0;https://www.kaggle.com/vfdev5/data-visualization;0.5;[];['ai', 'nn', 'cv'];['train', 'test data', 'model', 'training data'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.79;0.538;2020-12-12 17:38:33;Carvana Image Masking Challenge;['computer vision'];Data visualization;Python notebook;33398.0;170;;
2017-07-26 20:39:34;;Apache 2.0;https://www.kaggle.com/zfturbo/baseline-optimal-mask;0.5;[];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'computer vision', 'classification', 'deep learning'];https://www.kaggle.com/c/carvana-image-masking-challenge;0.732;0.446;2020-12-12 17:38:33;Carvana Image Masking Challenge;['computer vision'];Baseline [Optimal mask];Python script;6442.0;50;;
2019-09-19 17:51:24;1. Background;Apache 2.0;https://www.kaggle.com/billynguyen/using-r-for-eda-modeling;0.5;[];['ner', 'ai', 'dl', 'rl', 'cv'];['filter', 'machine learning', 'regression', 'training data', 'train', 'test data', 'model', 'natural language processing', 'predict', 'linear regression', 'natural language'];https://www.kaggle.com/c/cat-in-the-dat;0.668;0.416;2020-12-12 17:40:12;Categorical Feature Encoding Challenge;[];Using R for EDA & modeling;R notebook;1564.0;35;;
2019-12-31 19:32:17;;Apache 2.0;https://www.kaggle.com/caesarlupum/cat-with-null-importance-target-permutation;1.0;['lightgbm', 'sklearn'];['ai', 'gbm', 'cv', 'rl', 'ml'];['filter', 'train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/cat-in-the-dat;0.668;0.418;2020-12-12 17:40:12;Categorical Feature Encoding Challenge;['exploratory data analysis, utility script, categorical data'];ðŸ± Cat with Null Importance - Target Permutation;Python notebook;1563.0;36;;
2019-09-09 18:27:09;;Apache 2.0;https://www.kaggle.com/ccccat/r-glmnet;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['filter', 'train', 'model', 'deep learning', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/cat-in-the-dat;0.675;0.423;2020-12-12 17:40:12;Categorical Feature Encoding Challenge;[];R GLMNET;R script;1812.0;38;0.80252;0.80807
2020-10-30 02:27:31;Data Science Best Practices with pandas;Apache 2.0;https://www.kaggle.com/faressayah/data-science-best-practices-with-pandas-part-i;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'machine learning'];https://www.kaggle.com/c/cat-in-the-dat;0.693;0.475;2020-12-12 17:40:12;multiple data sources;['data cleaning'];Data Science BEST Practices with Pandas - PART I;Python notebook;2648.0;72;;
2019-12-11 08:53:08;Load data;Apache 2.0;https://www.kaggle.com/pavelvpster/cat-in-dat-ohe-vs-thermometer-logit;1.0;['sklearn'];['ai', 'dl', 'cv'];['filter', 'regression', 'train', 'model', 'logistic regression', 'predict'];https://www.kaggle.com/c/cat-in-the-dat;0.692;0.405;2020-12-12 17:40:12;multiple data sources;['logistic regression'];Cat in dat: OHE vs Thermometer + Logit;Python notebook;2587.0;31;0.80262;0.80812
2019-08-24 01:52:10;;Apache 2.0;https://www.kaggle.com/peterhurford/why-not-logistic-regression;1.0;['tensorflow', 'sklearn'];['ai', 'cv'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/cat-in-the-dat;0.736;0.519;2020-12-12 17:40:12;Categorical Feature Encoding Challenge;[];Why Not Logistic Regression?;Python notebook;7205.0;131;0.80181;0.80739
2019-10-23 05:21:06;Introduction;Apache 2.0;https://www.kaggle.com/shahules/an-overview-of-encoding-techniques;1.0;['sklearn'];['ai'];['test data', 'regression', 'train', 'fitting', 'model', 'label', 'logistic regression', 'predict'];https://www.kaggle.com/c/cat-in-the-dat;0.774;0.617;2020-12-12 17:40:12;Categorical Feature Encoding Challenge;['gpu, beginner, categorical data'];An Overview of Encoding Techniques;Python notebook;20469.0;598;;
2020-01-02 17:16:10;;Apache 2.0;https://www.kaggle.com/abhishek/same-old-entity-embeddings;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'rl'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/cat-in-the-dat-ii;0.763;0.536;2020-12-12 17:42:12;Categorical Feature Encoding Challenge II;['gpu'];same old entity embeddings;Python notebook;14924.0;167;0.78751;0.78623
2020-01-02 14:39:18;;Apache 2.0;https://www.kaggle.com/caesarlupum/2020-20-lines-target-encoding;1.0;['keras', 'sklearn'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['recommend', 'linear regression', 'regression', 'train', 'model', 'supervised learning', 'deep learning', 'label', 'logistic regression', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/cat-in-the-dat-ii;0.72;0.486;2020-12-12 17:42:12;Categorical Feature Encoding Challenge II;['utility script, data cleaning, categorical data'];2020- 20 lines Target Encoding;Python script;4816.0;84;0.78646;0.78500
2020-10-30 02:27:31;Data Science Best Practices with pandas;Apache 2.0;https://www.kaggle.com/faressayah/data-science-best-practices-with-pandas-part-i;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'machine learning'];https://www.kaggle.com/c/cat-in-the-dat-ii;0.693;0.475;2020-12-12 17:42:12;multiple data sources;['data cleaning'];Data Science BEST Practices with Pandas - PART I;Python notebook;2649.0;72;;
2020-01-02 17:36:21;Categorical Feature Encoding Challenge II;Apache 2.0;https://www.kaggle.com/marcovasquez/basic-eda-categoricals-values;1.0;['sklearn'];['ai', 'cv', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'test data', 'train', 'fitting', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/cat-in-the-dat-ii;0.617;0.379;2020-12-12 17:42:13;Categorical Feature Encoding Challenge II;[];Basic EDA + Categoricals values ðŸ“Š;Python notebook;599.0;23;;
2020-01-07 12:01:13;;Apache 2.0;https://www.kaggle.com/springmanndaniel/keras-r-embeddings-baseline;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'relu', 'loss', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/cat-in-the-dat-ii;0.658;0.387;2020-12-12 17:42:13;Categorical Feature Encoding Challenge II;['neural networks, binary classification'];keras[R] embeddings baseline;R script;1279.0;25;0.78744;0.78626
2020-11-02 00:23:30;Categorical Data EDA & Visualizationtechniques for Categorical Data EDA & Viz   I participated in last competition so much, I will participate again. First of all, I'm going to do EDA to come up with an idea of the overall distribution or idea of the data. Related Work  11 Categorical Encoders and Benchmark;Apache 2.0;https://www.kaggle.com/subinium/categorical-data-eda-visualization;0.5;[];['dl', 'ai', 'nn', 'ann'];['train', 'label', 'test data'];https://www.kaggle.com/c/cat-in-the-dat-ii;0.715;0.463;2020-12-12 17:42:12;Categorical Feature Encoding Challenge II;['data visualization, exploratory data analysis'];Categorical Data EDA & Visualization;Python notebook;4336.0;62;;
2020-01-26 20:53:57;;Apache 2.0;https://www.kaggle.com/tunguz/cat-ii-with-rapids-knn;1.0;['xgboost', 'sklearn', 'pillow'];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['regression', 'train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/cat-in-the-dat-ii;0.631;0.387;2020-12-12 17:42:12;multiple data sources;['gpu'];Cat II with Rapids kNN;Python notebook;766.0;25;0.61880;0.61351
2020-03-28 15:02:31;;Apache 2.0;https://www.kaggle.com/vikassingh1996/don-t-underestimate-the-power-of-a-logistic-reg;1.0;['sklearn'];['dl', 'ai', 'nn', 'cv'];['filter', 'machine learning', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/cat-in-the-dat-ii;0.713;0.493;2020-12-12 17:42:12;Categorical Feature Encoding Challenge II;['beginner, classification'];ðŸ±Don't Underestimate the Power of a Logistic Reg;Python notebook;4150.0;92;0.78309;0.78149
2020-01-27 09:52:30;SKF LightGBM - Target Encoding;Apache 2.0;https://www.kaggle.com/vincentlugat/skf-lightgbm-target-encoding;1.0;['lightgbm', 'sklearn'];['ai', 'gbm', 'cv', 'rl', 'ml'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/cat-in-the-dat-ii;0.679;0.397;2020-12-12 17:42:12;Categorical Feature Encoding Challenge II;['beginner, data visualization, classification, +1 morefeature engineering'];SKF LightGBM - Target Encoding;Python notebook;1962.0;28;0.78694;0.78550
2017-09-14 21:28:13;;Apache 2.0;https://www.kaggle.com/bguberfain/just-showing-a-few-images;1.0;['skimage'];['ai'];['train'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.712;0.393;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Just showing a few images;Python notebook;3992.0;27;;
2017-09-14 22:57:12;Create the dataframe;Apache 2.0;https://www.kaggle.com/bguberfain/naive-statistics;1.0;['skimage'];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.699;0.421;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Naive statistics;Python notebook;2968.0;37;;
2017-09-15 09:19:42;;Apache 2.0;https://www.kaggle.com/blazeka/validate-download-with-sha256-hash;0.5;[];['ai'];['train'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.631;0.34;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Validate download with sha256 hash;Python notebook;775.0;15;;
2017-11-04 09:42:06;;Apache 2.0;https://www.kaggle.com/ezietsman/inception-v3-finetune;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.756;0.362;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Inception V3 finetune ;Python notebook;12193.0;19;;
2017-09-23 23:06:12;;Apache 2.0;https://www.kaggle.com/mpekalski/convert-bson-to-tfrecord;1.0;['tensorflow', 'skimage'];['nlp', 'ai', 'nn', 'ner'];['train', 'image classification', 'classification', 'deep learning'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.669;0.34;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Convert BSON to TFRecord;Python script;1594.0;15;;
2017-09-29 22:33:53;Tested:## the resulting file is 85GB , run time on 8 cores: 53 minutes;Apache 2.0;https://www.kaggle.com/sophieg/convert-bson-to-hdf5;0.5;[];['ai', 'nn', 'ann', 'cv'];['train', 'understanding'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.711;0.352;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Convert bson to hdf5;Python notebook;3945.0;17;;
2017-09-23 18:35:33;Read Files;Apache 2.0;https://www.kaggle.com/sophieg/explore-dataset;0.5;[];['ner', 'ai', 'cv', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.702;0.397;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Explore Dataset;Python notebook;3225.0;28;;
2017-10-13 22:11:44;Data visualization kernelHere is a new image-based competition hosted by a french e-commerce company Cdiscount. Dataset announced features:  Almost 9 million products: half of the current catalogue More than 15 million images at 180x180 resolution More than 5000 categories: yes this is quite an extreme multi-class classification!  Let's explore this in details Content First images in train and test datasets Random item access Explore categories  PS. Thanks to this and this very helpful kernels !;Apache 2.0;https://www.kaggle.com/vfdev5/data-visualization-and-analysis;0.5;[];['ner', 'ai', 'cv', 'nn', 'ann'];['training data', 'test data', 'train', 'label', 'predict', 'classification'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.746;0.465;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Data visualization and analysis;Python notebook;9225.0;64;;
2017-09-15 10:15:10;Random item access from BSON fileHere is one of some other methods to access item without iterating through the whole BSON file Idea is to store offsets and lenghts of all items and seek/read from binary file. Following code creates a dictionary with key indexing item _id and values (offset, length). It takes around 3 mins to execute.;Apache 2.0;https://www.kaggle.com/vfdev5/random-item-access;0.5;[];['ai', 'rl', 'cv'];['train'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.675;0.357;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Random item access;Python notebook;1778.0;18;;
2017-10-03 10:52:43;;Apache 2.0;https://www.kaggle.com/zfturbo/squeeze-baseline-lb-0-14;1.0;['h2o', 'sklearn', 'tensorflow', 'keras', 'tpot'];['ner', 'ai', 'nlu', 'dl', 'cnn', 'gbm', 'gan', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['image classification', 'gru', 'train', 'model', 'epoch', 'deep learning', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/cdiscount-image-classification-challenge;0.683;0.34;2020-12-12 17:44:11;Cdiscountâ€™s Image Classification Challenge;[];Squeeze Baseline [LB: 0.14];Python script;2128.0;15;0.16766;0.16718
2016-01-27 08:52:21;;Apache 2.0;https://www.kaggle.com/abhishek/hmmmmm;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/cervical-cancer-screening;0.62;0.099;2020-12-12 17:51:11;Cervical Cancer Screening;[];hmmmmm...;Python script;630.0;1;;
2016-01-26 04:16:51;;Apache 2.0;https://www.kaggle.com/benhamner/data-tables;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/cervical-cancer-screening;0.665;0.0;2020-12-12 17:51:11;Cervical Cancer Screening;[];Data Tables;Sqlite script;1470.0;0;;
2016-02-03 09:41:13;;Apache 2.0;https://www.kaggle.com/gaborfodor/last-week;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['rank', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/cervical-cancer-screening;0.655;0.099;2020-12-12 17:51:11;Cervical Cancer Screening;[];Last week;Python script;1196.0;1;;
2016-02-14 05:02:22;;Apache 2.0;https://www.kaggle.com/hurlburt/afterdeadline;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'deep learning', 'label', 'classification', 'labeled'];https://www.kaggle.com/c/cervical-cancer-screening;0.609;0.0;2020-12-12 17:51:11;Cervical Cancer Screening;[];AfterDeadline;Python notebook;526.0;0;;
2016-02-11 21:22:22;;Apache 2.0;https://www.kaggle.com/hurlburt/age-and-screeners;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/cervical-cancer-screening;0.644;0.099;2020-12-12 17:51:11;Cervical Cancer Screening;[];Age and Screeners;Python notebook;984.0;1;;
2016-02-13 00:57:39;;Apache 2.0;https://www.kaggle.com/hurlburt/diagnosiscodesetcv2;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'deep learning', 'label', 'classification', 'labeled'];https://www.kaggle.com/c/cervical-cancer-screening;0.611;0.0;2020-12-12 17:51:11;Cervical Cancer Screening;[];DiagnosisCodesEtcV2;Python notebook;537.0;0;;
2016-01-29 21:41:51;;Apache 2.0;https://www.kaggle.com/hurlburt/getting-installed-python-modules;1.0;['statsmodels', 'xgboost', 'nltk', 'theano', 'gensim', 'sklearn', 'h2o', 'mxnet', 'pillow', 'tensorflow', 'spacy', 'textblob', 'keras', 'skimage', 'tpot'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'model', 'deep learning', 'layer', 'classification'];https://www.kaggle.com/c/cervical-cancer-screening;0.639;0.0;2020-12-12 17:51:11;Cervical Cancer Screening;[];getting installed python modules;Python script;891.0;0;;
2016-01-29 20:37:37;;Apache 2.0;https://www.kaggle.com/hurlburt/plot-test3;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/cervical-cancer-screening;0.627;0.152;2020-12-12 17:51:11;Cervical Cancer Screening;[];Plot test3;Python notebook;712.0;2;;
2016-02-10 01:27:44;;Apache 2.0;https://www.kaggle.com/hurlburt/plottest4;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/cervical-cancer-screening;0.588;0.0;2020-12-12 17:51:11;Cervical Cancer Screening;[];PlotTest4;Python notebook;369.0;0;;
2016-02-12 22:11:46;;Apache 2.0;https://www.kaggle.com/hurlburt/plottest5;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/cervical-cancer-screening;0.637;0.099;2020-12-12 17:51:11;Cervical Cancer Screening;[];PlotTest5;Python notebook;857.0;1;;
2016-02-09 16:55:19;;Apache 2.0;https://www.kaggle.com/paulperry/more-sqlite-testing;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/cervical-cancer-screening;0.614;0.0;2020-12-12 17:51:11;Cervical Cancer Screening;[];more sqlite testing;Python script;570.0;0;;
2016-01-29 16:08:41;;Apache 2.0;https://www.kaggle.com/paulperry/testing-sqlite;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['classification', 'deep learning'];https://www.kaggle.com/c/cervical-cancer-screening;0.653;0.0;2020-12-12 17:51:11;Cervical Cancer Screening;[];testing sqlite;Python notebook;1155.0;0;;
2016-02-12 22:42:53;;Apache 2.0;https://www.kaggle.com/paweljankiewicz/pawel-maks-insights;1.0;['xgboost'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'model', 'deep learning', 'natural language processing', 'label', 'predict', 'classification', 'natural language'];https://www.kaggle.com/c/cervical-cancer-screening;0.709;0.302;2020-12-12 17:51:11;Cervical Cancer Screening;[];Pawel & Maks insights;Python notebook;3745.0;10;;
2020-10-03 06:38:26;;Apache 2.0;https://www.kaggle.com/agrjarastogi/ic-sentimentanalysis;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'recognition', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.458;0.0;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;[];IC_SentimentAnalysis;Python notebook;58.0;0;;
2020-10-03 09:43:11;;Apache 2.0;https://www.kaggle.com/agrjarastogi/ic-sentimentanalysis-e9dd27;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'ml', 'nn', 'ann'];['filter', 'train', 'recognition', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.477;0.0;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;[];IC_SentimentAnalysis e9dd27;Python notebook;74.0;0;;
2020-11-27 12:22:54;;Apache 2.0;https://www.kaggle.com/alexbsantos/ic-reconhecimento-emocoes-deep;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'ml', 'nn', 'ann'];['filter', 'train', 'recognition', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.583;0.099;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;['gpu'];IC_reconhecimento_emocoes_deep;Python notebook;341.0;1;;
2020-10-31 11:50:12;;Apache 2.0;https://www.kaggle.com/crucifierbladex/fer-challenge;1.0;['keras', 'sklearn'];['ai', 'nn', 'ml'];['predict', 'train', 'recognition', 'model', 'epoch', 'layer', 'loss'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.512;0.0;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;[];FER_challenge;Python notebook;119.0;0;;
2020-11-03 12:52:02;Libraries;Apache 2.0;https://www.kaggle.com/dolmangksun/facial-expression-ai;1.0;['keras'];['ai', 'dl', 'cnn', 'nn', 'ml'];['train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.569;0.0;2020-12-12 17:52:11;multiple data sources;['gpu'];Facial_expression_AI;Python notebook;274.0;0;;
2020-10-15 18:16:20;;Apache 2.0;https://www.kaggle.com/haneenabdelmaguid/facial-expression-detection-2;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'nn', 'ml'];['train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'predict', 'relu', 'resnet', 'classification'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.666;0.188;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;[];Facial Expression Detection 2;Python notebook;1502.0;3;;
2020-12-06 18:39:29;Get dataset;Apache 2.0;https://www.kaggle.com/huanhkv/test-dataset;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'ml'];['train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.542;0.0;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;['gpu'];Test dataset;Python notebook;182.0;0;;
2020-10-06 02:29:45;;Apache 2.0;https://www.kaggle.com/mariamboules/facial-expression-detection-2;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'nn', 'ml'];['train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.571;0.0;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;[];Facial Expression Detection 2;Python notebook;279.0;0;;
2020-02-28 13:24:26;;Apache 2.0;https://www.kaggle.com/mohammed94/facial-expression-model;0.5;[];['ai', 'nn', 'ml'];['train', 'recognition', 'label'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.662;0.099;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;[];Facial Expression Model;Python notebook;1369.0;1;;
2019-12-27 11:15:40;;Apache 2.0;https://www.kaggle.com/rkritika1508/fer-2013-emotion-recognition;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'cnn', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'recognition', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.664;0.099;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;['gpu'];kernel7061472c49;Python notebook;1436.0;1;;
2020-10-03 08:03:38;;Apache 2.0;https://www.kaggle.com/saurabhhulyal/ic-sentimentanalysis;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'ml', 'nn', 'ann'];['filter', 'train', 'recognition', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.585;0.099;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;['gpu'];IC_SentimentAnalysis;Python notebook;352.0;1;;
2020-10-26 16:31:14;;Apache 2.0;https://www.kaggle.com/shubhahuja/emotion-detection;1.0;['tensorflow', 'keras'];['ai', 'nn', 'ml'];['train', 'recognition', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.618;0.099;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;[];emotion_detection;Python notebook;611.0;1;;
2020-10-22 12:52:11;;Apache 2.0;https://www.kaggle.com/treamaniac/facial-exp;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.473;0.0;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;[];notebook5fc02bbf26;Python notebook;71.0;0;;
2020-12-01 14:02:43;HISTOGRAM EQUALIZER;Apache 2.0;https://www.kaggle.com/venkatavyshnav/minipjt;1.0;['keras', 'sklearn'];['ai', 'cv', 'ml', 'nn', 'ann'];['train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.522;0.0;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;[];minipjt;Python notebook;136.0;0;;
2020-02-16 06:30:19;;Apache 2.0;https://www.kaggle.com/ztaihong/minivgg13;1.0;['tensorflow', 'keras'];['ner', 'ai', 'cv', 'ml', 'nn', 'ann'];['train', 'recognition', 'model', 'epoch', 'layer', 'vgg', 'alexnet', 'label', 'loss', 'relu'];https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;0.598;0.099;2020-12-12 17:52:11;Challenges in Representation Learning: Facial Expression Recognition Challenge;['gpu'];MiniVGG13;Python notebook;437.0;1;;
2019-06-20 03:38:30;;Apache 2.0;https://www.kaggle.com/adrianoavelar/bond-calculaltion-lb-0-82;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gan', 'gbm', 'cv', 'rl', 'nn', 'ml'];['filter', 'training data', 'regression', 'generation', 'train', 'test data', 'model', 'loss', 'label', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/champs-scalar-coupling;0.724;0.526;2020-12-12 17:53:35;Predicting Molecular Properties;[];Bond Calculaltion LB -0.82;Python notebook;5283.0;145;-0.79980;-0.80369
2019-06-18 23:09:11;;Apache 2.0;https://www.kaggle.com/adrianoavelar/bond-calculation-lb-0-82;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml'];['filter', 'training data', 'regression', 'generation', 'train', 'test data', 'model', 'loss', 'label', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/champs-scalar-coupling;0.708;0.512;2020-12-12 17:53:35;Predicting Molecular Properties;[];Bond Calculation - LB -0.82 ðŸ˜±;Python notebook;3624.0;119;;
2019-06-20 13:36:05;;Apache 2.0;https://www.kaggle.com/adrianoavelar/eachtype;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gan', 'gbm', 'cv', 'rl', 'nn', 'ml'];['filter', 'training data', 'regression', 'generation', 'train', 'test data', 'model', 'loss', 'label', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/champs-scalar-coupling;0.737;0.545;2020-12-12 17:53:35;Predicting Molecular Properties;[];EachType;Python notebook;7283.0;190;-0.91977;-0.92402
2019-07-02 22:42:53;"SynopsisThe number of bonds between two atoms is an interresting feature to play with but is not provided with the competition data. This kernel builds a dataset with the reconstructed bonding scheme for each molecule:  The algorithm used to do so is to build a spanning tree conecting each atom of the molecule.  Connection are made based on the expecting bonding distance for each pair of nearby atoms.   A partial ionized group detection is performed.   Shortest cycles are detected.  Note: There is another alternative to build bonding data, which I've not tested yet, see post here: Helpful Information: How to obtain bonding data Changelog V4 : manual inspection of one molecule failing bond matching : dsgdb9nsd_000271 = alanine isomere V5 : added Ase 3D mouse visualization for alanine ""bug"" V6 : added support for ionized molecules and 4N bonds after discovering the zwitterionic form of alanine in dsgdb9nsd_000202. Changed type from 1CH to 1.0CH V9 : added cycle detection code with statistics and examples V11: save cycles in .csv output files  Table train_bonds and test_bondsThe Output Dataset will be updated following bond detction improvements.  It is built with the same keys as for the competition tables structure and train/test. Tables train_bonds and test_bonds have one row per bond : molecule_name |Â atom_index_0 | atom_index_1 | nbond | error | bond_type  nbond is the number of covalent bonds between atom_index_0 (relative to structure) and atom_index_1. It can take values 1.5 for COO- groups. error is 0 if the molecule has a  consistent reconstruction (meaning that each atom has the expected number ofcovalent bonds).  Otherwise it is set to 1 for each bond of the molecule bond_type is . Atom appear in lexicographic order to ensure unicity for 2 atoms combination.   Example: 1.0CH, 2.0CC,  1.5CO  Table train_charges and test_chargesIt has been found that some molecules are ionized after discovering the zwiterionic form of beta-alanine in molecule dsgdb9nsd_000202. Hence support for atom charge has been included in tables train_charges and test_charges, plus an optional 4th bond for N.  These tables have the very same key as structure and one row is: molecule_name |Â atom_index | charge  charge can be 0, -1, +0.5 (for O in carboxyle) or +1 (may evolve if needed)  Table train_cycles and test_cyclesCycles are detected and stored in a separated table because one atom can be a member of several cycles. These tables have the very same key as structure and one row is: molecule_name |Â cycle_index | cycle_seq | atom_index  cycle_index starts from zero and allow to select one cycle cycle_seq is the index of atom in the cycle sequence. It is guaranteed that two consecutive atoms in cycle sequence have a covalent bond. atom_index is the atom idex from table structure which can be member of several cycle.     General information This kernel uses data from Predicting Molecular Properties which is intended to predict interactions between atoms in the domain of Nuclar Magnetic Resonnance (NMR). More precisely it is the scalar coupling constant between atoms which is to be predicted. As this challenge is based uppon molecular topological properties, it can be useful to have an appropriate way of representing molecules. These visualizations can then be used to infer useful hints to understand the coupling properties, engineer appropriate features and debug prediction failures.";Apache 2.0;https://www.kaggle.com/asauve/dataset-with-number-of-bonds-between-atoms;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/champs-scalar-coupling;0.733;0.525;2020-12-12 17:53:35;Predicting Molecular Properties;['feature engineering'];Dataset with number of bonds between atoms;Python notebook;6607.0;142;;
2019-06-04 16:23:30;HOW TO: Easy Visualization of Molecules.Greetings everyone! I've seen many people ask for a simple yet elegant visualization tool for the Predicting Molecular Properties challenge. Therefore, I'm going to explain in this Kernel how to install and use ase, which is a python module that allows one to work with atoms and molecules. It is available on gitlab: ase. The first thing we need to do is to install ase on our Kernel. To do that, just click on the Settings tab on the right panel, then click on Install..., right next to Packages. In the pip package name entry, just write ase then hit Install Package. Kaggle is going to do its things then restart the Kernel. ase should then be installed! Let's check this:;Apache 2.0;https://www.kaggle.com/borisdee/how-to-easy-visualization-of-molecules;0.5;[];['ner', 'ai', 'dl', 'gan'];['filter', 'predict'];https://www.kaggle.com/c/champs-scalar-coupling;0.743;0.563;2020-12-12 17:53:35;Predicting Molecular Properties;['data visualization'];How To: Easy Visualization of Molecules.;Python notebook;8595.0;247;;
2019-08-02 20:36:46;Distance matrix between atoms of a single molecule are used in many public kernels to compute features, for instance Coulomb interaction, Van de Walls interaction, and Yukawa interactions, This kernel shows how to speed up the distance matrix computation tremendously compared to the already fast version used in coulomb_interaction - speed up!.  The code from that kernel takes 2 minutes for all molecules. Here we provide a code that runs in 3 seconds, i.e. 40 times faster. This speedup is nice, but the code optimization technique used in this kernel is rather generic and can be reused in other context. V4 update.  @jmtest has suggested a nice improvement using einssum.  I added his version.  This brings down time to about 1.2 second.  We can do even better using numba, which brings down time to about 0.4 second. This is more than 250 faster than original code.;Apache 2.0;https://www.kaggle.com/cpmpml/ultra-fast-distance-matrix-computation;0.5;[];['ner', 'ai', 'nn', 'ann'];['train', 'filter'];https://www.kaggle.com/c/champs-scalar-coupling;0.741;0.541;2020-12-12 17:53:35;Predicting Molecular Properties;[];Ultra Fast Distance Matrix Computation;Python notebook;8159.0;180;;
2019-05-29 18:55:28;;Apache 2.0;https://www.kaggle.com/inversion/atomic-distance-benchmark;1.0;['sklearn'];['ner', 'ai'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/champs-scalar-coupling;0.747;0.512;2020-12-12 17:53:35;Predicting Molecular Properties;[];Atomic Distance Benchmark;Python notebook;9653.0;118;;
2019-07-07 05:29:31;Merge atom positions;Apache 2.0;https://www.kaggle.com/titericz/giba-r-data-table-simple-features-1-17-lb;1.0;['lightgbm'];['ai', 'gbm', 'cv', 'rl', 'nn', 'ml'];['filter', 'regression', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/champs-scalar-coupling;0.752;0.547;2020-12-12 17:53:35;Predicting Molecular Properties;[];Giba R + data.table + Simple Features -1.012 LB;R notebook;10797.0;196;-1.17180;-1.17666
2020-07-29 19:08:18;;Apache 2.0;https://www.kaggle.com/aarooxx/object-recognition-90-88-accuracy;1.0;['keras'];['ner', 'ai', 'nn', 'cnn'];['train', 'recognition', 'model', 'neural network', 'layer', 'loss', 'label', 'predict', 'relu', 'convolutional neural network'];https://www.kaggle.com/c/cifar-10;0.644;0.268;2020-12-12 17:54:35;multiple data sources;[];object recognition 90.88% accuracy;Python notebook;972.0;7;;
2020-08-28 13:08:51;Download DataSet;Apache 2.0;https://www.kaggle.com/abhishekshaw21/cifar-10-using-pytorch;0.5;[];['ner', 'ai', 'dl', 'cnn', 'rl', 'nn'];['test data', 'train', 'model', 'epoch', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/cifar-10;0.62;0.253;2020-12-12 17:54:35;multiple data sources;['gpu, python'];CIFAR-10 using PyTorch;Python notebook;630.0;6;;
2020-05-08 04:58:06;;Apache 2.0;https://www.kaggle.com/amithasanshuvo/cifar-images-classification-using-cnn;1.0;['tensorflow', 'keras'];['ai'];['train', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/cifar-10;0.72;0.39;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;['classification, image data, cnn'];CIFAR Images classification using CNN;Python notebook;4902.0;26;;
2020-06-09 21:58:44;;Apache 2.0;https://www.kaggle.com/franckepeixoto/cifar-10-recognition-in-images-to-the-point;1.0;['tensorflow', 'keras'];['ner', 'ai'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/cifar-10;0.658;0.236;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;['beginner, deep learning, computer vision, +1 moretensorflow']; CIFAR 10 - Recognition in Images / to the point.;Python notebook;1266.0;5;0.71240;0.71240
2020-09-18 16:06:11;The Model - RESNET18;Apache 2.0;https://www.kaggle.com/greatcodes/pytorch-cnn-resnet18-cifar10;0.5;[];['ai', 'nn'];['training data', 'train', 'model', 'epoch', 'layer', 'label', 'loss', 'relu', 'resnet'];https://www.kaggle.com/c/cifar-10;0.586;0.214;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;['gpu, deep learning, cnn, +2 morepython, pytorch'];Pytorch-CNN_Resnet18-CIFAR10;Python notebook;356.0;4;;
2020-09-09 09:24:56;Import packages;Apache 2.0;https://www.kaggle.com/kalashnimov/keras-callbacks-with-91-acc;1.0;['tensorflow', 'keras'];['ner', 'ai', 'cnn', 'rl', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/cifar-10;0.641;0.302;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;['gpu, deep learning, cnn, +1 morecomputer vision'];Keras Callbacks with 91% acc;Python notebook;930.0;10;0.00000;0.00000
2020-09-24 12:54:11;;Apache 2.0;https://www.kaggle.com/lkatran/blank-cifar-10;1.0;['tensorflow', 'keras'];['ai'];['train'];https://www.kaggle.com/c/cifar-10;0.581;0.253;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;[];CIFAR_10;Python notebook;329.0;6;;
2020-11-12 23:11:11;;Apache 2.0;https://www.kaggle.com/olegmatsevych/notebook0d79dc2184;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/cifar-10;0.494;0.188;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;['gpu'];notebook0d79dc2184;Python notebook;93.0;3;;
2020-07-24 20:26:42;;Apache 2.0;https://www.kaggle.com/pg1007/starting-with-cnn;1.0;['keras', 'sklearn'];['ner', 'ai', 'nn', 'ann'];['image classification', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/cifar-10;0.583;0.281;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;['data visualization, image data, cnn'];Starting with cnn;Python notebook;342.0;8;;
2020-11-06 10:16:26;;Apache 2.0;https://www.kaggle.com/raghavjha858/cifar-10-image;1.0;['tensorflow', 'keras'];['ner', 'ai', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/cifar-10;0.509;0.188;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;['gpu, image data, cnn'];cifar 10 image ;Python notebook;114.0;3;;
2020-07-14 20:41:47;;Apache 2.0;https://www.kaggle.com/sahil2398/cifar-10-with-augmentation;1.0;['tensorflow', 'keras'];['ner', 'ai', 'nn', 'cv'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/cifar-10;0.555;0.188;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;['beginner, classification'];CIFAR-10 with augmentation;Python notebook;221.0;3;0.00000;0.00000
2020-09-12 16:56:02;importing all libraries;Apache 2.0;https://www.kaggle.com/shaurov/cifar-image-classification-using-cnn-for-beginner;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'rl', 'ml', 'nn', 'ann'];['training data', 'test data', 'train', 'model', 'neural network', 'epoch', 'validation data', 'layer', 'loss', 'label', 'predict', 'relu', 'classification', 'convolutional neural network'];https://www.kaggle.com/c/cifar-10;0.572;0.253;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;[];CIFAR image classification using CNN for beginner;Python notebook;286.0;6;;
2020-07-12 18:05:21;Loading the Dataset;Apache 2.0;https://www.kaggle.com/ujjwalsharma26/cifar10-classification;1.0;['keras'];['ai', 'nn', 'rl'];['train', 'model', 'epoch', 'layer', 'label', 'loss', 'relu', 'classification'];https://www.kaggle.com/c/cifar-10;0.587;0.281;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;[];CIFAR10-Classification;Python notebook;360.0;8;;
2020-07-04 16:05:49;MODEL TRAINING USING AUGEMENTED DATASET;Apache 2.0;https://www.kaggle.com/vivekhn/cifar-10-images-classification-using-cnns;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'fitting', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/cifar-10;0.603;0.188;2020-12-12 17:54:35;CIFAR-10 - Object Recognition in Images;['gpu, classification, feature engineering, +1 morecnn'];CiFAR-10 Images Classification Using CNNs;Python notebook;473.0;3;;
2020-07-23 06:56:20;;Apache 2.0;https://www.kaggle.com/yashvi/machine-learning-using-turi-create;1.0;['vocabulary', 'tensorflow', 'keras', 'pillow'];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'natural language processing', 'logistic regression', 'predict', 'sentiment analysis', 'machine learning', 'training data', 'object detection', 'train', 'text classification', 'recommend', 'linear regression', 'classification', 'image classification', 'model', 'loss', 'rank', 'resnet', 'supervised learning', 'regression', 'label', 'natural language'];https://www.kaggle.com/c/cifar-10;0.625;0.319;2020-12-12 17:54:35;multiple data sources;['gpu, tpu, beginner, +2 moredeep learning, classification'];Turicreate;Python notebook;693.0;12;;
2019-07-04 11:07:16;;Apache 2.0;https://www.kaggle.com/ahmedehsan1993/aiproject;0.5;[];['ner', 'ai', 'nn', 'rl'];['train'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.547;0.152;2020-12-12 17:55:04;multiple data sources;['beginner, deep learning'];AIProject;Python notebook;194.0;2;;
2019-04-09 14:27:13;"Notebook structureThis kernel explains how to crack the cipher algorithm of difficulty level 2. It also describes the plan of attack for further difficulty levels. Plan of attackBased on the competition description, a ""Cipher in the middle"" based attack approach was constructed (shown in the figure below). To find, for example, the cipher 2 encryption algorithm, we will need a cipher 1 texts (generated from a known plain text) and compare it with the corresponding cipher 2. Therefore, it will be necessary to get both the encryption and decryption algorithms for all algorithms.  Two subproblems have to be solved before cracking this code:  We will have to find both the encryption and decryption for the previous difficulty level (resued from Cracking the code: difficulty 1) We must search (in each difficulty level) for one text matching the Ciphertext in order to execute a known-plaintext attack  Reading in our data.All text files are stored in a pandas dataframe. We will store the length in a column as well. Which will turn out to be useful.";Apache 2.0;https://www.kaggle.com/bsteenwi/cracking-the-code-difficulty-2;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['train', 'training data'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.637;0.302;2020-12-12 17:55:04;Ciphertext Challenge II;[];Cracking the code: difficulty 2;Python notebook;859.0;10;;
2019-04-09 11:50:57;Cracking the code for difficulty 1Crypto-analysis: length & frequency analysisIn this notebook, we discuss two types of crypto-analysis, which could help in cracking multiple cipher algorithms. The first approach tries to find a matching cipher and plain text, to perform a so-called known-plain-text attack. The second approach analysis the occurrence of the most common characters in both the cipher and plaintext documents. Both techniques are visualised on the difficulty 1 ciphertexts, but can be used for other challenges as well (gathering more insights);Apache 2.0;https://www.kaggle.com/group16/cracking-the-code-difficulty-1;0.5;[];['ai', 'rl', 'nn', 'cv'];['train'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.696;0.371;2020-12-12 17:55:04;Ciphertext Challenge II;[];Cracking the code: difficulty 1;Python notebook;2802.0;21;;
2019-03-31 09:40:09;;Apache 2.0;https://www.kaggle.com/jazivxt/difficulty-2;0.5;[];['ner', 'ai', 'nn', 'rl'];['train'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.654;0.319;2020-12-12 17:55:04;multiple data sources;[];Difficulty 2;Python notebook;1174.0;12;0.41540;0.41540
2019-03-29 09:27:03;;Apache 2.0;https://www.kaggle.com/jazivxt/the-crypto-keeper;0.5;[];['ai', 'dl', 'rl'];['train'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.644;0.311;2020-12-12 17:55:04;Ciphertext Challenge II;['puzzles'];The Crypto Keeper;Python notebook;972.0;11;0.25036;0.25036
2019-03-28 08:34:34;Hey guys, this is just a very early version of a glance to this challenge. It will remain be a draft in such early stage. Apologize if it is difficult to read, I understand it looks messy now, but I'll modify it soon after. Version 1: Instant ideas. 2019-03-28;Apache 2.0;https://www.kaggle.com/jshen97/a-glance-to-ciphertext-level-4;0.5;[];['ai', 'dl', 'ml', 'rl'];['train', 'understanding', 'filter'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.656;0.281;2020-12-12 17:55:04;Ciphertext Challenge II;[];Ciphertext Challenge;Python notebook;1223.0;8;;
2019-04-03 17:58:42;In the folllowing I assume the first three ciphers are completely known so you can encrypt your plaintext to get the input to the cipher at level 4. Now we want to find a pair of ciphertext and plaintext - but is this possible and how can we do it.;Apache 2.0;https://www.kaggle.com/kaggleuser58/how-to-get-started-with-level-4;0.2;[];['ai'];[];https://www.kaggle.com/c/ciphertext-challenge-ii;0.605;0.236;2020-12-12 17:55:04;Ciphertext Challenge II;[];How to get started with level 4;Python notebook;488.0;5;;
2019-04-21 21:25:57;This notebook is based on https://www.kaggle.com/group16/cracking-the-code-difficulty-1. Here, I present an algorithm for acquiring the perfect substitution instead of doing tedious manual correction.;Apache 2.0;https://www.kaggle.com/lemonkoala/cipher-difficulty-1-solution;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nn', 'ml'];['train', 'recognition'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.599;0.253;2020-12-12 17:55:04;Ciphertext Challenge II;[];Cipher Difficulty 1 Solution;Python notebook;438.0;6;0.25034;0.25034
2019-12-01 01:33:42;"A chave Ã© a a string ""ID_"" concatenada com um valor hexadecimal de 8 caracteres aleatÃ³rios";Apache 2.0;https://www.kaggle.com/maxlincoln/circuitos-digitais-2;0.5;[];['ai', 'nn', 'gan'];['train'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.586;0.0;2020-12-12 17:55:04;Ciphertext Challenge II;[];Circuitos Digitais 2;Python notebook;358.0;0;;
2019-06-24 12:08:11;;Apache 2.0;https://www.kaggle.com/qaziahmed/qaziahmedpaper;0.5;[];['ner', 'ai', 'nn', 'rl'];['train'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.544;0.099;2020-12-12 17:55:04;multiple data sources;[];QaziAhmedPaper;Python notebook;186.0;1;0.41540;0.41540
2019-06-24 13:37:39;;Apache 2.0;https://www.kaggle.com/qaziahmed/the-crypto-keeper;0.5;[];['ai', 'dl', 'rl'];['train'];https://www.kaggle.com/c/ciphertext-challenge-ii;0.513;0.099;2020-12-12 17:55:04;Ciphertext Challenge II;[];The Crypto Keeper;Python notebook;120.0;1;0.25036;0.25036
2019-08-09 17:21:52;;Apache 2.0;https://www.kaggle.com/anuraglahon/ciferiii;0.5;[];['ai', 'nn', 'ml'];['train'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.547;0.214;2020-12-12 17:56:23;Ciphertext Challenge III;[];ciferIII;Python notebook;195.0;4;;
2019-08-10 20:59:12;Some More Hints for Difficulty 1This Notebook continiues on apporach by Paul Dnt in his kernel Something to begin with: a first hint I will update this as new ideas come;Apache 2.0;https://www.kaggle.com/ayush10004/some-more-hints;0.5;[];['ai', 'dl', 'ml', 'rl'];['train', 'test data'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.578;0.292;2020-12-12 17:56:23;Ciphertext Challenge III;[];Some More Hints ;Python notebook;315.0;9;;
2019-08-09 09:03:53;Cracking the code: difficulty 1 I used this as the reference.;Apache 2.0;https://www.kaggle.com/chizuchizu/difficulty-1-visualization;0.5;[];['ai'];['train', 'test data', 'training data'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.61;0.188;2020-12-12 17:56:23;Ciphertext Challenge III;['data visualization, exploratory data analysis'];Difficulty 1 EDA;Python notebook;530.0;3;;
2019-08-28 10:36:25;;Apache 2.0;https://www.kaggle.com/elvenmonk/ciphertext-challenge-iii-fast-level-3;1.0;['pattern', 'tensorflow'];['ai', 'rl', 'ml', 'gan'];['train'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.615;0.188;2020-12-12 17:56:23;Ciphertext Challenge III;[];Ciphertext Challenge III - Fast Level 3;Python notebook;580.0;3;1.00000;1.00000
2019-08-12 17:38:12;Libraries;Apache 2.0;https://www.kaggle.com/gullfaxi/a-few-exploration-on-difficulty-4;0.5;[];['ner', 'ai', 'nn', 'rl'];['train'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.566;0.214;2020-12-12 17:56:23;Ciphertext Challenge III;[];A few exploration on difficulty 4;Python notebook;261.0;4;;
2019-08-11 17:26:43;Time Efficient PairingWork smarter not harder;Apache 2.0;https://www.kaggle.com/julianb/time-efficient-pairing;0.5;[];['ai', 'dl', 'rl', 'nn', 'ann'];['train'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.591;0.302;2020-12-12 17:56:23;Ciphertext Challenge III;[];Time Efficient Pairing;Python notebook;387.0;10;;
2019-08-13 07:41:43;IntroductionTime to share solution of cipher level 1 so you can look at the next level.In the previous Cipher Challenge II one of the levels was a cipher with multiple substitutions generated from a key of length 8 if I remember correct. The level 1 of this Cipher Challenge III is the same kind but with a key of length 4, so only 4 substitutions are used for each character mapping. The cipher The cipher only apllies to UPPERCASE and LOWERCASE letters. The key only shifts every time an UPPERCASE or LOWERCASE letter is met.  PaddingFrom Cipher Challenge II it was found that padding could be done both up front and in the end. Number of padding characters in the end was always equal to or at most 1 character more (if number of characters to pad with was odd) than the number of padding characters up front. Level 1 solution;Apache 2.0;https://www.kaggle.com/kaggleuser58/cipher-challenge-iii-level-1;0.5;[];['ner', 'ai', 'nn', 'ml'];['train'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.653;0.34;2020-12-12 17:56:23;Ciphertext Challenge III;[];Cipher Challenge III Level 1;Python notebook;1166.0;15;;
2019-08-14 20:30:27;Hi everyone! This kernel aims at showing you a basic approach to our ciphering problem. It shows my way of exploring solutions and I hope it will help you through your discovery of our dataset.;Apache 2.0;https://www.kaggle.com/pednt9/something-to-begin-with-a-first-hint;0.5;[];['ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'test data', 'training data', 'filter'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.702;0.46;2020-12-12 17:56:23;Ciphertext Challenge III;['exploratory data analysis'];Something to begin with: a first hint;Python notebook;3235.0;60;;
2019-08-08 21:30:45;Import Packages;Apache 2.0;https://www.kaggle.com/sanikamal/ciphertext-challenge-iii;1.0;['nltk'];['ai', 'dl', 'cv', 'rl', 'nn', 'ml'];['train', 'filter'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.608;0.152;2020-12-12 17:56:23;Ciphertext Challenge III;[];Ciphertext Challenge III;Python notebook;512.0;2;;
2019-08-09 16:40:09;;Apache 2.0;https://www.kaggle.com/seriousran/only-length-0-00000;1.0;['sklearn'];['ai', 'nn', 'ml'];['train'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.586;0.188;2020-12-12 17:56:23;Ciphertext Challenge III;[];Only length? 0.00000;Python notebook;356.0;3;0.00000;0.00000
2019-08-28 13:59:49;Ciphertext Challenge III;Apache 2.0;https://www.kaggle.com/smlopezza/ciphertext-challenge-iii-v3;0.5;[];['dl', 'ai', 'nn', 'ml'];['training data', 'filter', 'test data', 'train', 'understanding'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.592;0.253;2020-12-12 17:56:23;Ciphertext Challenge III;[];Ciphertext Challenge III v2;Python notebook;392.0;6;;
2019-09-05 21:15:44;Preliminary Visualizations;Apache 2.0;https://www.kaggle.com/sunandosamaddar/scratch-8urykgfg;1.0;['nltk'];['ai', 'dl', 'cnn', 'nlg', 'cv', 'rl', 'nn', 'ml'];['train', 'gru', 'label', 'predict'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.508;0.099;2020-12-12 17:56:23;Ciphertext Challenge III;[];scratch_ 8URYKgFG;Python notebook;112.0;1;;
2019-08-09 21:28:58;;Apache 2.0;https://www.kaggle.com/tauffer/dumb-luck;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.629;0.292;2020-12-12 17:56:23;Ciphertext Challenge III;[];dumb luck;Python script;739.0;9;0.00000;0.00000
2019-08-10 15:31:50;A good start is from the previous work. Thanks for Chizuchizu's discussion.https://www.kaggle.com/c/ciphertext-challenge-iii/discussion/103414#latest-595317 20 Newsgroups Ciphertext Challenge:  https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge/overview https://www.kaggle.com/leflal/cipher-1-cipher-2-full-solutions https://www.kaggle.com/leflal/cipher-3-solution https://www.kaggle.com/rturley/a-first-crack-tools-tips-3-cipher-solutions https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge/discussion/77894  Ciphertext Challenge II  https://www.kaggle.com/c/ciphertext-challenge-ii https://www.kaggle.com/group16/cracking-the-code-difficulty-1 https://www.kaggle.com/jazivxt/difficulty-2 https://www.kaggle.com/group16/cracking-the-code-difficulty-3;Apache 2.0;https://www.kaggle.com/tenffe/ciphertext-eda-and-baseline;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['test data', 'train', 'vgg', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/ciphertext-challenge-iii;0.652;0.268;2020-12-12 17:56:23;Ciphertext Challenge III;[];Ciphertext: EDA and baseline;Python notebook;1136.0;7;;
2019-08-11 08:36:53;;Apache 2.0;https://www.kaggle.com/ptyshevs/cnn-for-reversing-game-of-life;1.0;['tensorflow', 'keras'];['ner', 'ai', 'cnn', 'rl', 'nn'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'predict', 'relu'];https://www.kaggle.com/c/conway-s-reverse-game-of-life;0.668;0.253;2020-12-12 17:56:32;Conway's Reverse Game of Life;['gpu'];cnn_v2;Python notebook;1560.0;6;0.10793;0.10691
2020-09-02 16:05:22;Conway's Game of LifeIn train.csv we are given the 50000 simulations of Conway's Game of Life. Each row of the dataframe has information about the start and stop boards. The columns start_0 to start_624 represent the 25 by 25 start board where a 1 represents a live cell and 0 represents a dead cell. Similarly, the columns stop_0 to stop_624 represent the start board's state after delta time steps. To determine the state of a board after 1 time step, each cell of the board undergoes 1 of the 4 transformations:  Overpopulation: if a living cell is surrounded by more than three living cells, it dies. Stasis: if a living cell is surrounded by two or three living cells, it survives. Underpopulation: if a living cell is surrounded by fewer than two living cells, it dies. Reproduction: if a dead cell is surrounded by exactly three cells, it becomes a live cell.  In test.csv we are only given delta and stop_0 to stop_624. Our goal is to predict the start board's original state, which is delta time steps before the stop board's state.;Apache 2.0;https://www.kaggle.com/candaceng/understanding-the-problem-and-eda;0.5;[];['ai', 'ml', 'rl'];['train', 'predict'];https://www.kaggle.com/c/conways-reverse-game-of-life-2020;0.629;0.352;2020-12-12 17:58:02;Conway's Reverse Game of Life 2020;['beginner, exploratory data analysis'];Understanding the Problem and EDA;Python notebook;741.0;17;;
2020-10-02 23:17:03;Game of Life - Genetic Algorithm (Spanish) V1-V2: Algoritmo GenÃ©tico - 15 iteraciones. V3: ComparaciÃ³n con otros mÃ©todos y mejora de predicciones. V4-V5: OptimizaciÃ³n de los resultados de V3.;Apache 2.0;https://www.kaggle.com/desareca/game-of-life-genetic-algorithm-spanish;0.5;[];['ner', 'ai', 'rl', 'gan'];['filter', 'generation', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/conways-reverse-game-of-life-2020;0.636;0.292;2020-12-12 17:58:02;multiple data sources;[];Game of Life - Genetic Algorithm (Spanish);R notebook;843.0;9;0.06189;0.06189
2020-09-06 10:40:11;First look on the data;Apache 2.0;https://www.kaggle.com/docxian/game-of-life-first-glance;0.5;[];['ai'];['train', 'test data', 'training data'];https://www.kaggle.com/c/conways-reverse-game-of-life-2020;0.595;0.292;2020-12-12 17:58:02;Conway's Reverse Game of Life 2020;['games, simulations'];Game of Life - First glance;Python notebook;412.0;9;;
2020-09-03 06:09:48;;Apache 2.0;https://www.kaggle.com/li325040229/simple-lgb-model-model-using-only-0-6-test-data;1.0;['lightgbm', 'sklearn', 'keras'];['ner', 'ai', 'gbm', 'cv', 'rl'];['train', 'model', 'layer', 'loss', 'label', 'lstm', 'predict'];https://www.kaggle.com/c/conways-reverse-game-of-life-2020;0.567;0.292;2020-12-12 17:58:02;Conway's Reverse Game of Life 2020;['lightgbm'];Simple lgb model model using only 0.6% test data;Python notebook;266.0;9;0.14469;0.14469
2020-09-02 17:31:54;The Game of LifeIf you like it, welcome to upvote~This is a simple version of EDA, and I will continue to refine it.;Apache 2.0;https://www.kaggle.com/li325040229/the-game-of-life-display-of-cell-changes;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/conways-reverse-game-of-life-2020;0.597;0.319;2020-12-12 17:58:02;Conway's Reverse Game of Life 2020;[];The Game of Life:Display of cell changes;Python notebook;425.0;12;;
2020-09-08 11:12:57;Loading...;Apache 2.0;https://www.kaggle.com/seraphwedd18/application-of-gan-for-predicting-initial-state;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nn', 'rl'];['test data', 'generation', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/conways-reverse-game-of-life-2020;0.604;0.319;2020-12-12 17:58:02;Conway's Reverse Game of Life 2020;['gpu, keras'];Application of GAN for Predicting Initial State;Python notebook;476.0;12;;
2020-09-11 13:32:22;V1-V3In this naive sample submission, we submit the last grid as the initial grid, naively assuming therefore that there was no change. The solely purpose was to get an idea of the worst score possibly achievable and an idea of performances range. After V3Since the private and public dataset are the same, we can save our submission in a dataset and submit them and even bag them. Moreover we can choose iterative optimization algorithms and process grid by chunks on our computer for days and combine all the chunks on one csv file (one notebook coming soon) Credits (link to original notebook)  sub_1111 sub_1245 sub_1274 sub_1333 sub_1402 sub_1334  Versions V4: sub_1245 V5-V6: Bagging V7: sub_1111;Apache 2.0;https://www.kaggle.com/ulrich07/sample-submission;0.2;[];['ai'];[];https://www.kaggle.com/c/conways-reverse-game-of-life-2020;0.67;0.393;2020-12-12 17:58:02;multiple data sources;['beginner'];sample_submission;Python notebook;1624.0;27;0.11117;0.11117
2020-09-10 23:47:16;Game of LifeThis notebook trying to solve True Target Problem  show some use cases for probabilistic extension of Game of Life provides some useful function  Paragraphs Task overview/Game Rules True Target Problem Probability Extension Correct Loss Function DataStream (useful function) CNN_Model Direct gradient optimization Result evaluation (useful function);Apache 2.0;https://www.kaggle.com/yakuben/crgl-probability-extension-true-target-problem;0.5;[];['ner', 'ai', 'cnn', 'nn', 'ann'];['predict', 'model', 'neural network', 'loss', 'relu'];https://www.kaggle.com/c/conways-reverse-game-of-life-2020;0.668;0.408;2020-12-12 17:58:02;Conway's Reverse Game of Life 2020;['gpu, beginner, exploratory data analysis, +1 moreneural networks'];[CRGL] Probability Extension | True Target Problem;Python notebook;1550.0;32;0.11117;0.11117
2018-07-23 21:57:18;Overall data statisics;Apache 2.0;https://www.kaggle.com/aditya1702/eda-of-all-142-variables;1.0;['pattern'];['ner', 'ai', 'nn'];['train', 'label', 'filter'];https://www.kaggle.com/c/costa-rican-household-poverty-prediction;0.66;0.357;2020-12-12 17:59:32;Costa Rican Household Poverty Level Prediction;[];EDA of all 142 variables!;R notebook;1333.0;18;;
2020-05-03 18:45:39;1. Importing the libraries;Apache 2.0;https://www.kaggle.com/ruslanmamedov/poverty-level-prediction-beginer-s-kernel;1.0;['lightgbm', 'sklearn', 'h2o'];['ner', 'ai', 'automl', 'gbm', 'rl', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/costa-rican-household-poverty-prediction;0.703;0.327;2020-12-12 17:59:32;Costa Rican Household Poverty Level Prediction;[];Poverty level prediction: Beginer's kernel;Python notebook;3256.0;13;;
2018-08-17 08:52:39;;Apache 2.0;https://www.kaggle.com/taindow/predicting-poverty-levels-with-r;1.0;['pattern', 'catboost', 'xgboost', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'k-nearest neighbor', 'predict', 'unsupervised learning', 'training data', 'train', 'clustering', 'classification', 'model', 'rank', 'understanding', 'supervised learning', 'regression', 'generation', 'fitting', 'gradient descent', 'label', 'gradient boosting', 'random forest'];https://www.kaggle.com/c/costa-rican-household-poverty-prediction;0.74;0.453;2020-12-12 17:59:32;Costa Rican Household Poverty Level Prediction;['gpu, data visualization, exploratory data analysis, +2 morefeature engineering, xgboost'];Predicting Poverty Levels with R;Rmarkdown script;7974.0;55;0.42671;0.42671
2018-09-19 09:05:22;1. Check datasets;Apache 2.0;https://www.kaggle.com/youhanlee/3250feats-532-feats-using-shap-lb-0-436;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nn'];['filter', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/costa-rican-household-poverty-prediction;0.709;0.416;2020-12-12 17:59:32;Costa Rican Household Poverty Level Prediction;['data visualization, exploratory data analysis'];3250feats->532 feats using shap[LB: 0.436];Python notebook;3781.0;35;0.43096;0.43096
2018-08-30 02:01:30;Part 1. EDA;Apache 2.0;https://www.kaggle.com/zhav1k/make-it-beautiful-with-seaborn-xgb-model;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nn', 'ann'];['regression', 'train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/costa-rican-household-poverty-prediction;0.651;0.39;2020-12-12 17:59:32;Costa Rican Household Poverty Level Prediction;['beginner, data visualization, exploratory data analysis'];(: make it beautiful with seaborn + XGB model;Python notebook;1112.0;26;;
2015-08-04 21:00:16;;Apache 2.0;https://www.kaggle.com/abhishek/modified-cosine-similarity-0-00644;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.66;0.214;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Modified Cosine Similarity (0.00644);R script;1339.0;4;;
2015-09-16 06:48:29;;Apache 2.0;https://www.kaggle.com/alexxanderlarko/furthermodifiedcosine-007600-al2;0.5;[];['nlp', 'ai', 'nn', 'ner'];['training data', 'test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.643;0.188;2020-12-12 18:10:21;Coupon Purchase Prediction;[];FurtherModifiedCosine(.007600)al2;R script;964.0;3;0.00575;0.00761
2015-08-25 07:10:40;;Apache 2.0;https://www.kaggle.com/anguyen/translate-everything-to-english-using-r;0.5;[];['ner', 'ai', 'gan', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/coupon-purchase-prediction;0.769;0.44;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Translate Everything to English Using R;R script;17237.0;47;;
2015-07-15 17:31:17;;Apache 2.0;https://www.kaggle.com/dchudz/python-template;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/coupon-purchase-prediction;0.682;0.236;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Python Template;Python script;2060.0;5;;
2015-09-12 19:32:07;;Apache 2.0;https://www.kaggle.com/fredhseymour/furthermodifiedcosine-007600;0.5;[];['nlp', 'ai', 'nn', 'ner'];['training data', 'test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.731;0.383;2020-12-12 18:10:21;Coupon Purchase Prediction;[];FurtherModifiedCosine(.007600);R script;6296.0;24;0.00571;0.00759
2015-08-16 01:31:54;;Apache 2.0;https://www.kaggle.com/fviktor/japanise-english-translation-table;0.5;[];['ner', 'ai', 'gan', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/coupon-purchase-prediction;0.701;0.268;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Japanese => English translation table;Python script;3144.0;7;;
2015-08-12 08:59:22;;Apache 2.0;https://www.kaggle.com/haisland0909/sample-machine-learning-script;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['test data', 'regression', 'train', 'model', 'deep learning', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.729;0.292;2020-12-12 18:10:21;Coupon Purchase Prediction;[];sample logistic regression script;Python script;6050.0;9;0.00027;0.00032
2015-07-21 03:19:13;;Apache 2.0;https://www.kaggle.com/jimenezluna/gower-distance-of-train-test-coupons;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.707;0.268;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Gower distance of train/test coupons;R script;3599.0;7;0.00264;0.00249
2015-08-02 17:14:08;;Apache 2.0;https://www.kaggle.com/linkanray/explore-data;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/coupon-purchase-prediction;0.69;0.236;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Explore data;R script;2439.0;5;;
2015-08-31 16:38:56;;Apache 2.0;https://www.kaggle.com/makotoinoue/coupon-list-train-data-exploration;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['test data', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.684;0.311;2020-12-12 18:10:21;Coupon Purchase Prediction;[];coupon_list_train data exploration;Rmarkdown script;2184.0;11;;
2015-07-17 15:42:58;;Apache 2.0;https://www.kaggle.com/mandalsubhajit/cosine-similarity-of-user-and-coupon;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.726;0.327;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Cosine Similarity of User and Coupon;R script;5648.0;13;0.00046;0.00046
2015-07-16 20:10:41;;Apache 2.0;https://www.kaggle.com/mandalsubhajit/match-by-ken-name;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/coupon-purchase-prediction;0.738;0.387;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Beating the Benchmark;R script;7641.0;25;0.00299;0.00310
2015-09-10 20:18:55;;Apache 2.0;https://www.kaggle.com/mandalsubhajit/modified-cosine-similarity-0-00556;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.779;0.435;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Modified Cosine Similarity (0.006349);R script;23197.0;44;0.00509;0.00645
2015-09-11 04:34:55;;Apache 2.0;https://www.kaggle.com/michaelpawlus/modified-cosine-similarity-0-006253;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.678;0.214;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Modified Cosine Similarity (0.006253);R script;1919.0;4;0.00443;0.00524
2015-09-06 19:35:59;;Apache 2.0;https://www.kaggle.com/mkurnikov/modified-cos-similarity-python-edition;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.737;0.334;2020-12-12 18:10:21;Coupon Purchase Prediction;[];modified cos similarity, python edition;Python script;7431.0;14;0.00507;0.00628
2015-09-06 17:23:34;;Apache 2.0;https://www.kaggle.com/prasanna23/translate-japanese-to-english-python;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/coupon-purchase-prediction;0.744;0.346;2020-12-12 18:10:21;Coupon Purchase Prediction;[];translate japanese to english - python;Python script;8732.0;16;;
2015-09-15 14:22:53;;Apache 2.0;https://www.kaggle.com/skiaaa/furthermodifiedcosine;0.5;[];['nlp', 'ai', 'nn', 'ner'];['training data', 'test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.569;0.188;2020-12-12 18:10:21;Coupon Purchase Prediction;[];FurtherModifiedCosine;R script;272.0;3;0.00571;0.00760
2015-09-24 15:55:25;;Apache 2.0;https://www.kaggle.com/sushize/furthermodifiedcosine-007603adj;0.5;[];['nlp', 'ai', 'nn', 'ner'];['training data', 'test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/coupon-purchase-prediction;0.721;0.292;2020-12-12 18:10:21;Coupon Purchase Prediction;[];FurtherModifiedCosine(.007603adj);R script;4963.0;9;0.00575;0.00759
2015-07-22 19:52:09;;Apache 2.0;https://www.kaggle.com/tobycheese/translate-capsule-text-and-genre-name;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/coupon-purchase-prediction;0.717;0.346;2020-12-12 18:10:21;Coupon Purchase Prediction;[];Translate capsule text and genre name;Python script;4468.0;16;0.00000;0.00000
2015-08-06 18:31:34;;Apache 2.0;https://www.kaggle.com/tonets/0-00664coupon;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/coupon-purchase-prediction;0.706;0.253;2020-12-12 18:10:21;Coupon Purchase Prediction;[];0.00644coupon [0.000146];Python script;3509.0;6;0.00014;0.00013
2020-03-24 14:09:55;Introduction (Although it doesn't need any)(The Visualizations are updated every 24 Hours);Apache 2.0;https://www.kaggle.com/abhinand05/covid-19-digging-a-bit-deeper;0.5;[];['ai', 'dl', 'gan', 'rl', 'nn'];['rank', 'label', 'train'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.809;0.604;2020-12-12 18:11:58;multiple data sources;[];COVID-19: Digging a Bit Deeper;Python notebook;63020.0;478;;
2020-04-12 19:17:38;In this notebook I add weather informations, such as temperature and precipitations, to the training set of the COVID-19 forecasting competition, in order to determine whether there is any correlation with the growth of confirmed cases. Weather data is imported from the NOAA GSOD dataset, continuously updated to include recent measurments. Data for this and previous weeks is available in dataset form here. Edit: now missing values are denoted with usual NaNs, and not with 9999s. Edit 2: information concerning humidity was added, following brennanmurphy's advice. More specifically, dewpoint temperature was added from the NOAA GSOD dataset, then absolute and relative humidity were computed.;Apache 2.0;https://www.kaggle.com/davidbnn92/weather-data;0.5;[];['ai', 'rl', 'nn', 'gan'];['train', 'training data'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.787;0.535;2020-12-12 18:11:58;multiple data sources;[];Weather data;Python notebook;29884.0;164;;
2020-06-26 18:43:01;The following project has been prepared using colab (used for extra RAM and GPU time) and kaggle, so the entire project is published on GitHub.Link: GitHub Link for the kernels on One-Stop-for-COVID-19-Infection-and-Lung-Segmentation-plus-Classification;Apache 2.0;https://www.kaggle.com/deadskull7/covid-19-inf-lung-segmentation-classification;0.5;[];['cv'];['classification'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.731;0.387;2020-12-12 18:11:58;multiple data sources;[];COVID-19 (Inf +Lung) Segmentation + Classification;Python notebook;6401.0;25;;
2020-03-29 15:49:08;Logistic Curve Fitting Confirmed Cases Please watch the following 9-minute video on exponential growth and the spread of disease...https://www.youtube.com/watch?v=Kas0tIxDvrg;Apache 2.0;https://www.kaggle.com/dferhadi/logistic-curve-fitting-global-covid-19-confirmed;0.5;[];['ai', 'dl', 'rl', 'nn', 'ml'];['train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.728;0.447;2020-12-12 18:11:58;multiple data sources;[];Logistic Curve Fitting - Global Covid-19 Confirmed;Python notebook;5939.0;51;;
2020-03-27 09:54:40;# Total Cases in China;Apache 2.0;https://www.kaggle.com/janvichokshi/covid-19-eda-using-plotly;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.597;0.367;2020-12-12 18:11:58;multiple data sources;[];COVID-19 EDA;Python notebook;430.0;20;;
2020-03-25 21:54:03;;Apache 2.0;https://www.kaggle.com/nonserial/covid19-prediction-with-restcountries-eu-api-data;1.0;['pattern', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nn', 'ml'];['training data', 'regression', 'train', 'model', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.703;0.371;2020-12-12 18:11:58;COVID19 Global Forecasting (Week 1);['feature engineering, covid19, gradient boosting'];COVID19: prediction with restcountries.eu API data;Rmarkdown script;3246.0;21;1.36359;1.11937
2020-03-19 23:00:22;Load and preview Data;Apache 2.0;https://www.kaggle.com/opanichev/covid19-global-simple-model;1.0;['sklearn'];['ai', 'nn'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.718;0.403;2020-12-12 18:11:58;COVID19 Global Forecasting (Week 1);[];COVID19 Global Simple Model;Python notebook;4586.0;30;3.48553;1.19022
2020-03-27 19:08:58;Data loading;Apache 2.0;https://www.kaggle.com/osciiart/covid19-lightgbm;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'gbm', 'rl'];['filter', 'test data', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.707;0.405;2020-12-12 18:11:58;multiple data sources;[];COVID19_LightGBM;Python notebook;3610.0;31;;
2020-04-09 09:42:31;;Apache 2.0;https://www.kaggle.com/sambitmukherjee/covid-19-forecasting-with-regression-trees;0.5;[];['ner', 'ai', 'rl', 'cv', 'ml'];['filter', 'machine learning', 'regression', 'train', 'model', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.744;0.449;2020-12-12 18:11:58;COVID19 Global Forecasting (Week 1);['beginner, covid19, decision tree'];COVID-19 Forecasting with Regression Trees;Rmarkdown script;8882.0;52;;
2020-03-25 17:53:45;COVID Global Forecast;Apache 2.0;https://www.kaggle.com/shakshisharma/kernelb454ab925d;1.0;['statsmodels', 'sklearn'];['ai', 'dl', 'rl', 'nn', 'ml'];['regression', 'train', 'fitting', 'model', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.629;0.421;2020-12-12 18:11:58;multiple data sources;[];kernelb454ab925d;Python notebook;748.0;37;2.86816;2.31144
2020-03-19 11:14:38;Load train/test dataset;Apache 2.0;https://www.kaggle.com/super13579/covid19-global-forcast-simple-eda-pr-model;1.0;['sklearn'];['ai', 'rl'];['test data', 'regression', 'train', 'model', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.715;0.423;2020-12-12 18:11:58;COVID19 Global Forecasting (Week 1);[];COVID19 Global forcast Simple EDA & PR model;Python notebook;4352.0;38;2.79344;0.91052
2020-03-19 16:09:07;;Apache 2.0;https://www.kaggle.com/tunguz/simple-covid-19-global-eda;0.5;[];['ai', 'nn', 'ml'];['train'];https://www.kaggle.com/c/covid19-global-forecasting-week-1;0.691;0.371;2020-12-12 18:11:58;COVID19 Global Forecasting (Week 1);[];Simple COVID-19 Global EDA;Python notebook;2505.0;21;;
2020-04-02 00:15:33;;Apache 2.0;https://www.kaggle.com/aerdem4/covid19-w2-final-v2;1.0;['tensorflow', 'xgboost', 'sklearn', 'keras'];['ai', 'nn'];['regression', 'train', 'model', 'epoch', 'layer', 'loss', 'predict', 'relu'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.765;0.411;2020-12-12 18:13:38;multiple data sources;[];covid19 w2 final v2;Python notebook;15765.0;33;0.99762;0.06168
2020-04-01 10:14:04;Prepare;Apache 2.0;https://www.kaggle.com/binhlc/sars-cov-2-exponential-model-week-2;1.0;['statsmodels', 'sklearn'];['ai', 'gan', 'rl', 'nn', 'ml'];['filter', 'regression', 'train', 'model', 'label', 'predict', 'bayesian'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.704;0.437;2020-12-12 18:13:38;COVID19 Global Forecasting (Week 2);[];SARS-COV-2 Exponential Model - Week 2;Python notebook;3314.0;45;0.88992;0.08319
2020-04-12 19:17:38;In this notebook I add weather informations, such as temperature and precipitations, to the training set of the COVID-19 forecasting competition, in order to determine whether there is any correlation with the growth of confirmed cases. Weather data is imported from the NOAA GSOD dataset, continuously updated to include recent measurments. Data for this and previous weeks is available in dataset form here. Edit: now missing values are denoted with usual NaNs, and not with 9999s. Edit 2: information concerning humidity was added, following brennanmurphy's advice. More specifically, dewpoint temperature was added from the NOAA GSOD dataset, then absolute and relative humidity were computed.;Apache 2.0;https://www.kaggle.com/davidbnn92/weather-data;0.5;[];['ai', 'rl', 'nn', 'gan'];['train', 'training data'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.787;0.535;2020-12-12 18:13:38;multiple data sources;[];Weather data;Python notebook;29885.0;164;;
2020-03-29 15:49:08;Logistic Curve Fitting Confirmed Cases Please watch the following 9-minute video on exponential growth and the spread of disease...https://www.youtube.com/watch?v=Kas0tIxDvrg;Apache 2.0;https://www.kaggle.com/dferhadi/logistic-curve-fitting-global-covid-19-confirmed;0.5;[];['ai', 'dl', 'rl', 'nn', 'ml'];['train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.728;0.447;2020-12-12 18:13:38;multiple data sources;[];Logistic Curve Fitting - Global Covid-19 Confirmed;Python notebook;5940.0;51;;
2020-04-01 22:01:49;;Apache 2.0;https://www.kaggle.com/dott1718/cv19w2-2-sub;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'gan', 'rl', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.617;0.34;2020-12-12 18:13:38;COVID19 Global Forecasting (Week 2);[];cv19w2 2 sub;Python notebook;604.0;15;1.04202;0.08813
2020-04-02 05:27:14;Introduction;Apache 2.0;https://www.kaggle.com/eswarchandt/timeseries-forecasting-of-covid-19-week-2-arima;1.0;['statsmodels', 'sklearn'];['ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.729;0.433;2020-12-12 18:13:38;COVID19 Global Forecasting (Week 2);['healthcare, time series analysis'];TimeSeries Forecasting of Covid-19(week-2) ARIMA;Python notebook;6051.0;43;;
2020-05-06 16:51:53;Week 3;Apache 2.0;https://www.kaggle.com/gaborfodor/covid19-global-forecasting-top-submissions;1.0;['lightgbm'];['ai', 'gbm', 'cv', 'nn', 'ann'];['train', 'model', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.677;0.362;2020-12-12 18:13:38;multiple data sources;[];COVID19 Global Forecasting - Top Submissions;Python notebook;1869.0;19;;
2020-04-05 12:57:28;;Apache 2.0;https://www.kaggle.com/mdmahmudferdous/covid-19-global-forecasting-2;1.0;['xgboost', 'sklearn'];['dl', 'ai', 'nn'];['train', 'model', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.602;0.371;2020-12-12 18:13:38;COVID19 Global Forecasting (Week 2);[];COVID-19 Global Forecasting-2;Python notebook;465.0;21;;
2020-03-26 10:00:34;About this notebookAdapted from week 1 to week 2 : original https://www.kaggle.com/optimo/covid19-enriched-dataset This is a pipeline to merge different data sources with the covid train dataset. I hope this can help people start with an enriched dataset and do useful modelling. Don't hesitate to fork from here and add more useful informations. Some merging are quite brutal and might be improved.;Apache 2.0;https://www.kaggle.com/optimo/covid19-enriched-dataset-week-2;0.5;[];['ai', 'nn', 'ml', 'rl'];['train', 'model', 'training data'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.693;0.367;2020-12-12 18:13:38;multiple data sources;[];Covid19_Enriched_Dataset_week_2;Python notebook;2597.0;20;;
2020-04-01 18:02:15;Load Dataset;Apache 2.0;https://www.kaggle.com/ranjithks/few-lines-of-code-without-data-leak;1.0;['xgboost', 'sklearn'];['ai', 'rl', 'cv', 'nn', 'ml'];['filter', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.713;0.379;2020-12-12 18:13:38;multiple data sources;['beginner, feature engineering, covid19'];Few lines of code without data leak;Python notebook;4102.0;23;;
2020-05-05 14:59:59;;Apache 2.0;https://www.kaggle.com/sambitmukherjee/covid-19-data-adding-world-development-indicators;0.5;[];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'filter'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.687;0.367;2020-12-12 18:13:38;multiple data sources;['data cleaning, covid19, tabular data, +1 moreglobal'];COVID-19 Data: Adding World Development Indicators;Rmarkdown script;2328.0;20;;
2020-04-20 16:40:06;Visualizations using PlotLyPlease upvote if you like this notebook.I would also appreciate any suggestions you might have.;Apache 2.0;https://www.kaggle.com/sanskrutipanda/covid-19-data-visualizations-with-plotly;0.5;[];['ai', 'rl', 'ml', 'gan'];['train', 'label'];https://www.kaggle.com/c/covid19-global-forecasting-week-2;0.569;0.352;2020-12-12 18:13:38;COVID19 Global Forecasting (Week 2);[];COVID-19: Data Visualizations with Plotly;Python notebook;272.0;17;;
2020-08-10 13:50:53;What is SIR Model;Apache 2.0;https://www.kaggle.com/abhijithchandradas/sir-model-don-t-understand-calculus-don-t-worry;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['model', 'label', 'filter'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.655;0.346;2020-12-12 18:15:07;multiple data sources;['data visualization'];SIR model: Don't understand calculus, don't worry!;Python notebook;1203.0;16;;
2020-04-02 21:25:48;;Apache 2.0;https://www.kaggle.com/aerdem4/covid-19-basic-model-not-leaky;1.0;['sklearn'];['ai'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.71;0.427;2020-12-12 18:15:06;COVID19 Global Forecasting (Week 3);[];COVID-19 Basic Model (Not Leaky);Python notebook;3861.0;40;1.93129;0.68442
2020-10-10 20:12:28;COVID-19 Projection using LSTM;Apache 2.0;https://www.kaggle.com/arpandas65/covid-19-projection-using-lstm;1.0;['tensorflow', 'xgboost', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'rnn', 'ann'];['filter', 'recurrent neural network', 'regression', 'training data', 'train', 'test data', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'ground truth'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.69;0.346;2020-12-12 18:15:07;multiple data sources;['data visualization, covid19, lstm'];Covid-19 Projection using LSTM;Python notebook;2449.0;16;;
2020-04-12 19:17:38;In this notebook I add weather informations, such as temperature and precipitations, to the training set of the COVID-19 forecasting competition, in order to determine whether there is any correlation with the growth of confirmed cases. Weather data is imported from the NOAA GSOD dataset, continuously updated to include recent measurments. Data for this and previous weeks is available in dataset form here. Edit: now missing values are denoted with usual NaNs, and not with 9999s. Edit 2: information concerning humidity was added, following brennanmurphy's advice. More specifically, dewpoint temperature was added from the NOAA GSOD dataset, then absolute and relative humidity were computed.;Apache 2.0;https://www.kaggle.com/davidbnn92/weather-data;0.5;[];['ai', 'rl', 'nn', 'gan'];['train', 'training data'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.787;0.535;2020-12-12 18:15:06;multiple data sources;[];Weather data;Python notebook;29886.0;164;;
2020-08-22 19:58:54;Covid-19 Growth Factor by Country Please watch the following 9-minute video on exponential growth and the spread of disease...https://www.youtube.com/watch?v=Kas0tIxDvrg The versions before Version 16 explored Savitzkyâ€“Golay filter (https://en.wikipedia.org/wiki/File:Lissage_sg3_anim.gif) for smoothing data however it seems that that filter is too good at retaining the original trends in data. So here we change to a different smoothing technique. Disclaimer: This notebook demonstrates a very simple mathematical model, a differential equation called the logistic equation which is a special case of the Bernoulli equation. The purpose of this notebook was to illustrate mathematical modeling with simple ordinary differential equations to my introductory mathematical modeling class. I am not a health expert, this notebook should not be taken too seriously.;Apache 2.0;https://www.kaggle.com/dferhadi/covid-19-predictions-growth-factor-and-calculus;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'train', 'fitting', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.765;0.502;2020-12-12 18:15:06;multiple data sources;['data visualization, exploratory data analysis, covid19'];Covid-19 Predictions, Growth Factor, and Calculus;Python notebook;15684.0;103;;
2020-04-26 12:15:53;Introduction;Apache 2.0;https://www.kaggle.com/eswarchandt/covid-19-forecasting-xgboost;1.0;['xgboost', 'sklearn'];['ai', 'rl', 'cv', 'nn', 'ml'];['filter', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.718;0.456;2020-12-12 18:15:06;COVID19 Global Forecasting (Week 3);['beginner, xgboost'];Covid-19 Forecasting XGBOOST;Python notebook;4628.0;57;;
2020-04-07 07:10:02;Introduction;Apache 2.0;https://www.kaggle.com/eswarchandt/timeseries-forecasting-of-covid-19-week-3-arima;1.0;['statsmodels', 'sklearn'];['ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.655;0.421;2020-12-12 18:15:06;COVID19 Global Forecasting (Week 3);['beginner, exploratory data analysis'];TimeSeries Forecasting of Covid-19(week-3) ARIMA;Python notebook;1205.0;37;;
2020-05-06 16:51:53;Week 3;Apache 2.0;https://www.kaggle.com/gaborfodor/covid19-global-forecasting-top-submissions;1.0;['lightgbm'];['ai', 'gbm', 'cv', 'nn', 'ann'];['train', 'model', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.677;0.362;2020-12-12 18:15:06;multiple data sources;[];COVID19 Global Forecasting - Top Submissions;Python notebook;1870.0;19;;
2020-04-03 16:01:24;;Apache 2.0;https://www.kaggle.com/madz2000/covid-19-week-3-analysis-prediction;1.0;['xgboost', 'nltk', 'sklearn', 'tensorflow', 'keras'];['ai', 'nn'];['regression', 'train', 'model', 'layer', 'label', 'predict', 'classification'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.657;0.416;2020-12-12 18:15:06;COVID19 Global Forecasting (Week 3);[];Covid-19 Week 3 Analysis & Prediction;Python notebook;1264.0;35;;
2020-04-23 14:55:37;COVID-19 Italy Confirmed Cases and Fatalities Forecasting;Apache 2.0;https://www.kaggle.com/mdmahmudferdous/covid-19-italy-forecasting-fb-prophet;0.5;[];['ai', 'nn'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.625;0.357;2020-12-12 18:15:07;COVID19 Global Forecasting (Week 3);[];COVID-19 Italy Forecasting-Fb Prophet;Python notebook;685.0;18;;
2020-04-03 07:10:40;COVID Global Forecast;Apache 2.0;https://www.kaggle.com/ritarana123/kernel6bb9d38623;1.0;['statsmodels', 'sklearn'];['ai', 'dl', 'gan', 'cv', 'rl', 'ml'];['regression', 'train', 'fitting', 'model', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.56;0.327;2020-12-12 18:15:07;multiple data sources;[];kernel6bb9d38623;Python notebook;238.0;13;3.00136;2.62365
2020-10-19 09:12:59;NOTICE;Apache 2.0;https://www.kaggle.com/yuanquan/covid-19-prediction-by-country-and-province;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['filter', 'predict', 'fitting', 'model', 'gradient descent', 'loss'];https://www.kaggle.com/c/covid19-global-forecasting-week-3;0.748;0.437;2020-12-12 18:15:06;multiple data sources;['data visualization, covid19, global'];COVID-19 Prediction by Country and Province;Python notebook;9833.0;45;;
2020-04-13 13:50:12;;Apache 2.0;https://www.kaggle.com/darshanjain29/bagging-regressor-solution-covid19-week-4;1.0;['sklearn'];['ai', 'dl', 'cv', 'nn', 'ml'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.568;0.367;2020-12-12 18:17:36;COVID19 Global Forecasting (Week 4);[];Bagging Regressor Solution - COVID19 Week 4;Python notebook;267.0;20;0.97644;0.46798
2020-04-15 14:02:11;;Apache 2.0;https://www.kaggle.com/darshanjain29/xgboost-regressor-solution-covid19-week-4;1.0;['xgboost', 'sklearn'];['ai', 'dl', 'cv', 'rl', 'nn', 'ml'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.595;0.383;2020-12-12 18:17:36;COVID19 Global Forecasting (Week 4);[];XGBoost Regressor Solution - COVID19 Week 4;Python notebook;411.0;24;;
2020-04-12 19:17:38;In this notebook I add weather informations, such as temperature and precipitations, to the training set of the COVID-19 forecasting competition, in order to determine whether there is any correlation with the growth of confirmed cases. Weather data is imported from the NOAA GSOD dataset, continuously updated to include recent measurments. Data for this and previous weeks is available in dataset form here. Edit: now missing values are denoted with usual NaNs, and not with 9999s. Edit 2: information concerning humidity was added, following brennanmurphy's advice. More specifically, dewpoint temperature was added from the NOAA GSOD dataset, then absolute and relative humidity were computed.;Apache 2.0;https://www.kaggle.com/davidbnn92/weather-data;0.5;[];['ai', 'rl', 'nn', 'gan'];['train', 'training data'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.787;0.535;2020-12-12 18:17:36;multiple data sources;[];Weather data;Python notebook;29887.0;164;;
2020-08-22 19:58:54;Covid-19 Growth Factor by Country Please watch the following 9-minute video on exponential growth and the spread of disease...https://www.youtube.com/watch?v=Kas0tIxDvrg The versions before Version 16 explored Savitzkyâ€“Golay filter (https://en.wikipedia.org/wiki/File:Lissage_sg3_anim.gif) for smoothing data however it seems that that filter is too good at retaining the original trends in data. So here we change to a different smoothing technique. Disclaimer: This notebook demonstrates a very simple mathematical model, a differential equation called the logistic equation which is a special case of the Bernoulli equation. The purpose of this notebook was to illustrate mathematical modeling with simple ordinary differential equations to my introductory mathematical modeling class. I am not a health expert, this notebook should not be taken too seriously.;Apache 2.0;https://www.kaggle.com/dferhadi/covid-19-predictions-growth-factor-and-calculus;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'train', 'fitting', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.765;0.502;2020-12-12 18:17:36;multiple data sources;['data visualization, exploratory data analysis, covid19'];Covid-19 Predictions, Growth Factor, and Calculus;Python notebook;15685.0;103;;
2020-04-11 22:54:23;COVID-19 Forecasting Prediction using Auto ARIMA ModelThe Autoregressive Integrated Moving Average Model, or ARIMA for short is a standard statistical model for time series forecast and analysis.;Apache 2.0;https://www.kaggle.com/dktalaicha/covid-19-forecasting-week-4-arima;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'regression', 'train', 'model', 'predict', 'classification'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.668;0.4;2020-12-12 18:17:36;COVID19 Global Forecasting (Week 4);[];Covid-19 Forecasting Week 4 - ARIMA;R notebook;1571.0;29;1.12599;0.72997
2020-05-14 18:06:58;Introduction;Apache 2.0;https://www.kaggle.com/eswarchandt/timeseries-forecasting-of-covid-19-arima;1.0;['statsmodels', 'sklearn'];['ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.723;0.458;2020-12-12 18:17:36;multiple data sources;['beginner, data visualization, exploratory data analysis'];Timeseries Forecasting of Covid-19 ARIMA;Python notebook;5242.0;58;;
2020-04-12 02:21:03;Covid-19 Forecasting using an RNN;Apache 2.0;https://www.kaggle.com/frlemarchand/covid-19-forecasting-with-an-rnn;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'nn', 'rnn', 'ann'];['training data', 'test data', 'generation', 'train', 'fitting', 'model', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.787;0.568;2020-12-12 18:17:35;multiple data sources;['covid19, lstm, rnn'];Covid-19 Forecasting with an RNN;Python notebook;29826.0;269;1.54353;0.76222
2020-05-11 20:30:27;Covid-19 AnalysisOuassim Adnane May 2020</h5>;Apache 2.0;https://www.kaggle.com/ishivinal/covid-19-analysis-visualizations-predictions;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'nlu', 'dl', 'rl'];['filter', 'regression', 'train', 'model', 'clustering', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.725;0.452;2020-12-12 18:17:36;multiple data sources;['covid19, healthcare'];ðŸ”¬Covid-19 Analysis Visualizations & Predictions ;Python notebook;5533.0;54;;
2020-04-15 07:29:27;;Apache 2.0;https://www.kaggle.com/madz2000/simple-covid19-week-4-prediction-with-xgbregressor;1.0;['xgboost', 'nltk', 'sklearn', 'tensorflow', 'keras'];['ai', 'nn'];['regression', 'train', 'model', 'layer', 'lstm', 'label', 'predict', 'classification'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.683;0.429;2020-12-12 18:17:36;COVID19 Global Forecasting (Week 4);[];Simple Covid19 Week 4 Prediction with XGBRegressor;Python notebook;2112.0;41;1.04153;0.04633
2020-06-15 19:19:42;Introduction;Apache 2.0;https://www.kaggle.com/soham1024/covid-19-india-visualization-forecasting;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['gru', 'filter', 'train', 'model', 'label', 'loss', 'classification'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.761;0.53;2020-12-12 18:17:36;multiple data sources;['beginner, data visualization, covid19'];Covid-19 India ðŸ‡®ðŸ‡³ Visualization & Forecasting;Python notebook;14080.0;152;;
2020-09-12 14:14:54;Introduction;Apache 2.0;https://www.kaggle.com/soham1024/covid-19-usa-visualization-forecasting;0.5;[];['ai', 'rl', 'nn', 'gan'];['train', 'model', 'label', 'filter'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.694;0.393;2020-12-12 18:17:36;multiple data sources;['gpu, beginner, data visualization'];Covid-19 USA ðŸ‡ºðŸ‡¸ Visualization & Forecasting;Python notebook;2690.0;27;;
2020-10-19 09:12:59;NOTICE;Apache 2.0;https://www.kaggle.com/yuanquan/covid-19-prediction-by-country-and-province;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['filter', 'predict', 'fitting', 'model', 'gradient descent', 'loss'];https://www.kaggle.com/c/covid19-global-forecasting-week-4;0.748;0.437;2020-12-12 18:17:36;multiple data sources;['data visualization, covid19, global'];COVID-19 Prediction by Country and Province;Python notebook;9834.0;45;;
2020-07-26 21:41:28;1. World Updates;Apache 2.0;https://www.kaggle.com/abhiparashar/eda-covid-19-notebook-1;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl'];['label', 'filter'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.585;0.319;2020-12-12 18:20:15;multiple data sources;[];EDA Notebook- 1;Python notebook;352.0;12;;
2020-05-11 23:01:14;;Apache 2.0;https://www.kaggle.com/aerdem4/covid-19-w5-pipeline;1.0;['tensorflow', 'keras'];['ai', 'nn', 'rl'];['predict', 'train', 'model', 'layer', 'relu'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.668;0.362;2020-12-12 18:20:15;multiple data sources;[];Covid-19 W5 Pipeline;Python notebook;1550.0;19;0.29097;0.00735
2020-05-26 20:56:34;SummaryThis notebook shows plots that benchmark COVID-19 forecasting model performance for April. The metric I'm using are:  RMSLE  RMSE normalized by population size  Each model forecasts slightly different things. This evaluation takes place using the common forecasts:  State level forecasts for 51 US states Evaluates forecasts for up to 29 days ahead  For the top Kaggle model, we're picking the leader from the previous week to avoid making an unfair comparison with a model chosen ex-post. TakeawayLANL, Kaggle and IHME models were strongest in April, followed by the Columbia University model and then IHME;Apache 2.0;https://www.kaggle.com/antgoldbloom/visualizing-covid-19-model-benchmarks;0.5;[];['ner', 'ai', 'gan', 'nn', 'ann'];['model', 'label', 'loss'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.692;0.357;2020-12-12 18:20:15;multiple data sources;[];Visualizing COVID-19 Model Benchmarks ;Python notebook;2568.0;18;;
2020-07-08 07:52:14;CORONA VIRUS PANDEMIC!ðŸ¦ ;Apache 2.0;https://www.kaggle.com/avnika22/covid19-global-forecasting;1.0;['sklearn'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['test data', 'random forest', 'regression', 'train', 'model', 'label', 'predict', 'relu', 'decision tree', 'classification'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.681;0.39;2020-12-12 18:20:14;COVID19 Global Forecasting (Week 5);['beginner, data visualization, covid19'];ðŸ¦ Covid19 Global Forecasting ðŸ˜·;Python notebook;2035.0;26;;
2020-07-08 21:12:46;Introduction;Apache 2.0;https://www.kaggle.com/eswarchandt/geospatial-analysis-on-covid-19-day-to-day-track;0.5;[];['ai', 'rl', 'ml', 'gan'];['filter'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.732;0.481;2020-12-12 18:20:14;multiple data sources;['exploratory data analysis, covid19, healthcare, +1 moregeospatial analysis'];Geospatial Analysis on Covid-19(Day to Day Track);Python notebook;6527.0;78;;
2020-05-14 18:06:58;Introduction;Apache 2.0;https://www.kaggle.com/eswarchandt/timeseries-forecasting-of-covid-19-arima;1.0;['statsmodels', 'sklearn'];['ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.723;0.458;2020-12-12 18:20:14;multiple data sources;['beginner, data visualization, exploratory data analysis'];Timeseries Forecasting of Covid-19 ARIMA;Python notebook;5243.0;58;;
2020-06-11 18:07:34;Read train test files;Apache 2.0;https://www.kaggle.com/gaborfodor/w5-top-submissions;0.5;[];['dl', 'ai', 'nn', 'gan'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.651;0.327;2020-12-12 18:20:15;multiple data sources;[];W5 Top Submissions;Python notebook;1110.0;13;;
2020-05-11 20:30:27;Covid-19 AnalysisOuassim Adnane May 2020</h5>;Apache 2.0;https://www.kaggle.com/ishivinal/covid-19-analysis-visualizations-predictions;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'nlu', 'dl', 'rl'];['filter', 'regression', 'train', 'model', 'clustering', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.726;0.452;2020-12-12 18:20:14;multiple data sources;['covid19, healthcare'];ðŸ”¬Covid-19 Analysis Visualizations & Predictions ;Python notebook;5534.0;54;;
2020-05-17 12:11:06;What is Novel Coronavirus?The novel coronavirus (provisionally named 2019-nCoV) is a contagious virus that causes respiratory infection. It has been identified as the causative agent of the ongoing 2019â€“20 Wuhan coronavirus outbreak. As many early cases were linked to a large seafood and animal market, the virus is thought to have a zoonotic origin, but this has not been confirmed. Comparisons of the genetic sequences of this virus and other virus samples have shown similarities to SARS-CoV (79.5%) and bat coronaviruses (96%), which makes an ultimate origin in bats likely. The first known human infection occurred in December 8, 2019. An outbreak of 2019-nCoV was first detected in Wuhan, China, in mid-December 2019.The virus subsequently spread to all other provinces of China and to more than twenty other countries in Asia, Europe, North America, and Oceania. Human-to-human spread of the virus has been confirmed in China, Germany, Thailand, Taiwan, Japan, and the United States. As of 1 February 2020, there were 12,024 confirmed cases of infection, of which 11,860 were within mainland China. Cases outside China, to date, were people who have either travelled from Wuhan, or were in direct contact with someone who travelled from the area. The number of deaths was 259 as of 1 February 2020. Source: https://en.wikipedia.org/wiki/Novel_coronavirus_(2019-nCoV);Apache 2.0;https://www.kaggle.com/kdnishanth/covid-19-forcasting;0.5;[];['ai', 'rl', 'ml', 'gan'];['label'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.662;0.367;2020-12-12 18:20:14;multiple data sources;['beginner, data visualization'];covid-19 forcasting;Python notebook;1390.0;20;;
2020-05-05 20:49:01;;Apache 2.0;https://www.kaggle.com/mathurinache/starter-code;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.623;0.311;2020-12-12 18:20:15;multiple data sources;[];Starter Code;Python notebook;670.0;11;0.58568;0.47934
2020-05-10 13:16:56;Thanks to Russ for providing code from his last scraper: https://www.kaggle.com/sasrdw/gbt5fx We adapted a few things and rather focused on non-US countries. This is just a first attempt, and we are happy for others to contribute. Please do not rely on these results as they are and some countries might be completely wrong.;Apache 2.0;https://www.kaggle.com/philippsinger/covid-w5-worldometer-scraper;0.5;[];['ai', 'ml', 'rl'];['train', 'filter'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.636;0.319;2020-12-12 18:20:15;COVID19 Global Forecasting (Week 5);[];Covid W5 Worldometer scraper;Python notebook;843.0;12;;
2020-05-18 15:00:51;;Apache 2.0;https://www.kaggle.com/pranshu29/medical-situation-covid-india-deep-analysis;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['fitting', 'label', 'classification', 'predict'];https://www.kaggle.com/c/covid19-global-forecasting-week-5;0.687;0.444;2020-12-12 18:20:14;multiple data sources;['beginner, data visualization, exploratory data analysis, +1 morecovid19'];Medical situation - COVID(india) deep_analysis;Python notebook;2285.0;49;;
2020-03-25 13:28:06;Import Libraries;Apache 2.0;https://www.kaggle.com/abhijithchandradas/caprediction-linearregression-multiple-growth-rate;1.0;['sklearn'];['ai', 'rl'];['filter', 'test data', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.623;0.319;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);[];CAprediction-LinearRegression-Multiple-Growth-Rate;Python notebook;669.0;12;;
2020-03-19 02:10:12;Sample Submission: 4-Day Doubling BaselineThis is for the COVID-19 forecasting challenge, Local-CA Week 1 edition. This will walk through making a sample submission based on the dataset. It makes the obviously flawed assumption that COVID-19 cases/fatalities double indefinitely every 4 days from the most recent data points. It will apply the most recent data point (prior to each of the public/private evaluation windows), and propagate those forward through time.;Apache 2.0;https://www.kaggle.com/benhamner/sample-submission-4-day-doubling-baseline;0.5;[];['ai', 'ml'];['train', 'predict'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.635;0.253;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);[];Sample Submission: 4-Day Doubling Baseline;Python notebook;833.0;6;1.16070;0.19313
2020-03-27 06:44:27;(Updated on March 27);Apache 2.0;https://www.kaggle.com/ceshine/plotly-eda-example;0.5;[];['ai', 'rl'];['train', 'predict'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.609;0.281;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);[];Plotly EDA Example;Python notebook;524.0;8;;
2020-08-22 19:58:54;Covid-19 Growth Factor by Country Please watch the following 9-minute video on exponential growth and the spread of disease...https://www.youtube.com/watch?v=Kas0tIxDvrg The versions before Version 16 explored Savitzkyâ€“Golay filter (https://en.wikipedia.org/wiki/File:Lissage_sg3_anim.gif) for smoothing data however it seems that that filter is too good at retaining the original trends in data. So here we change to a different smoothing technique. Disclaimer: This notebook demonstrates a very simple mathematical model, a differential equation called the logistic equation which is a special case of the Bernoulli equation. The purpose of this notebook was to illustrate mathematical modeling with simple ordinary differential equations to my introductory mathematical modeling class. I am not a health expert, this notebook should not be taken too seriously.;Apache 2.0;https://www.kaggle.com/dferhadi/covid-19-predictions-growth-factor-and-calculus;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'train', 'fitting', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.765;0.502;2020-12-12 18:21:16;multiple data sources;['data visualization, exploratory data analysis, covid19'];Covid-19 Predictions, Growth Factor, and Calculus;Python notebook;15686.0;103;;
2020-03-25 10:26:47;Feature engineering;Apache 2.0;https://www.kaggle.com/hugorosen/covid-ca-logarithm-interpolation;0.5;[];['ai', 'nn'];['train', 'model', 'predict'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.561;0.188;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);[];[COVID CA] Logarithm Interpolation;Python notebook;241.0;3;1.75959;0.05465
2020-03-25 18:00:30;IntroductionUsing SIR-F model, we will predict the number of confirmed cases and fatal cases with COVID-19 in CA, USA. SIR-F model was created in another notebook of mine. Please refer to the following notebook. Contents:  Arrangement of dataset Explanation of SIR-F model Parameter estimaition of the model Prediction and data submission  References:  COVID-19 - Growth of Virus in Specific Countries COVID-19 data with SIR model;Apache 2.0;https://www.kaggle.com/lisphilar/sir-f-model-in-california-usa;0.5;[];['ner', 'ai', 'dl', 'ml', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.673;0.302;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);[];SIR-F model in California, USA;Python notebook;1730.0;10;1.71776;0.24601
2020-03-26 02:11:42;Importing data;Apache 2.0;https://www.kaggle.com/nickteim/covid19-california-model-1;1.0;['xgboost', 'sklearn'];['dl', 'ai', 'nn', 'ml'];['training data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.592;0.253;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);['beginner'];COVID19 California model 1;Python notebook;393.0;6;;
2020-05-02 17:04:54;Load the Data;Apache 2.0;https://www.kaggle.com/pravinborate/covid-19-us-ca-forecasting;1.0;['sklearn'];['ai', 'nn'];['filter', 'test data', 'regression', 'train', 'fitting', 'model', 'loss', 'predict', 'bayesian'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.541;0.214;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);['data visualization, exploratory data analysis, covid19'];COVID-19 US-CA Forecasting;Python notebook;179.0;4;2.61595;0.16100
2020-03-22 17:23:00;Load the training data form the csv;Apache 2.0;https://www.kaggle.com/rnglol/simple-taylor-series-model;0.5;[];['ai', 'ml', 'rl'];['test data', 'training data', 'train', 'model', 'predict'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.6;0.188;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);[];Simple taylor series model;Python notebook;446.0;3;2.00893;0.42512
2020-03-26 00:56:24;INTRODUCTION;Apache 2.0;https://www.kaggle.com/rvadlam2/identifying-spread-of-coronavirus-in-california;0.5;[];['ner', 'ai', 'rl'];['filter', 'training data', 'regression', 'train', 'model', 'predict', 'rank'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.592;0.236;2020-12-12 18:21:16;multiple data sources;[];Identifying Spread of Coronavirus in California;R notebook;390.0;5;0.96450;0.01782
2020-06-20 11:58:59;;Apache 2.0;https://www.kaggle.com/sureshmecad/global-forecasting-week-1-california;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.449;0.152;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);[];Global Forecasting_Week 1_California;Python notebook;52.0;2;;
2020-03-19 16:09:34;;Apache 2.0;https://www.kaggle.com/tunguz/simple-covid-19-ca-eda;0.5;[];['ai', 'nn', 'ml'];['train'];https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1;0.664;0.334;2020-12-12 18:21:16;COVID19 Local US-CA Forecasting (Week 1);[];Simple COVID-19 CA EDA;Python notebook;1439.0;14;;
2020-11-22 16:07:55;NECESSARY IMPORTS;Apache 2.0;https://www.kaggle.com/nishantrock/dlrm-beginner-steps;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'nn', 'rnn', 'ml'];['filter', 'generation', 'train', 'model', 'neural network', 'validation data', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/criteo-display-ad-challenge;0.578;0.0;2020-12-12 18:21:23;Display Advertising Challenge;[];DLRM:Beginner steps;Python notebook;314.0;0;;
2015-06-06 10:10:55;;Apache 2.0;https://www.kaggle.com/abhishek/beating-the-benchmark;1.0;['pattern', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.786;0.524;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];"Beating the Benchmark ;) 0.57+";Python script;29182.0;140;0.59245;0.57985
2015-06-12 18:43:39;;Apache 2.0;https://www.kaggle.com/benhamner/exploring-the-crowdflower-data;1.0;['pillow'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['train', 'deep learning', 'layer', 'label', 'predict', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.76;0.459;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Exploring the Crowdflower Data;Rmarkdown script;13617.0;59;;
2015-05-11 22:16:38;;Apache 2.0;https://www.kaggle.com/benhamner/python-benchmark;1.0;['pattern', 'sklearn', 'nltk'];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/crowdflower-search-relevance;0.749;0.383;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Python Benchmark;Python script;9989.0;24;0.34124;0.32176
2015-05-14 03:41:56;;Apache 2.0;https://www.kaggle.com/benhamner/wordclouds;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'train', 'deep learning', 'label', 'classification', 'propagation'];https://www.kaggle.com/c/crowdflower-search-relevance;0.65;0.268;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Wordclouds;Rmarkdown script;1091.0;7;;
2015-06-13 00:58:18;;Apache 2.0;https://www.kaggle.com/chenglongchen/customized-softkappa-loss-in-xgboost;1.0;['xgboost'];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.749;0.387;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Customized softkappa loss in XGBoost ;Python script;10105.0;25;;
2015-06-05 12:32:59;;Apache 2.0;https://www.kaggle.com/domcastro/utility-write-svd-components-to-file;1.0;['pattern', 'sklearn', 'nltk', 'pillow'];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['training data', 'test data', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.69;0.334;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];UTILITY - Write SVD components to file;Python script;2455.0;14;0.00000;0.00000
2015-06-22 05:44:39;;Apache 2.0;https://www.kaggle.com/duttaroy/porter-stemmer;1.0;['pattern', 'sklearn', 'nltk'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['test data', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.759;0.383;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Porter Stemmer;Python script;13281.0;24;0.58174;0.56791
2015-06-06 18:39:55;;Apache 2.0;https://www.kaggle.com/elenacuoco/cf-nn-py;1.0;['pattern', 'sklearn', 'nltk', 'theano'];['ner', 'ai', 'rl', 'nlp', 'nn'];['filter', 'regression', 'train', 'output layer', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'classification', 'hidden layer'];https://www.kaggle.com/c/crowdflower-search-relevance;0.679;0.281;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];CF-NN.py ;Python script;1969.0;8;0.36009;0.34142
2015-06-16 10:33:56;;Apache 2.0;https://www.kaggle.com/gmilosev/r-version-of-benchmark-script;1.0;['vocabulary'];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.725;0.397;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];R version of benchmark script;R script;5458.0;28;0.15498;0.13637
2015-06-18 18:48:27;;Apache 2.0;https://www.kaggle.com/gshguru/clubbing-2-benchmarks;1.0;['pattern', 'sklearn', 'nltk'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['test data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.742;0.403;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Stacking BenchSVD-PorterStemmer 0.626;Python script;8375.0;30;0.63930;0.61870
2015-06-28 17:33:02;;Apache 2.0;https://www.kaggle.com/hiendang/auto-correct-query;1.0;['nltk'];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn', 'ann'];['train', 'classification', 'test data', 'deep learning'];https://www.kaggle.com/c/crowdflower-search-relevance;0.717;0.327;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];auto correct query;Python script;4464.0;13;;
2015-06-16 10:05:58;;Apache 2.0;https://www.kaggle.com/jigneshvyas/word-cloud-r;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['test data', 'training data', 'train', 'deep learning', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.654;0.236;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Word Cloud - R;R script;1191.0;5;;
2015-05-22 02:58:54;;Apache 2.0;https://www.kaggle.com/lrargerich/r-vector-space-model;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.725;0.352;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];R Vector Space Model ;R script;5531.0;17;0.03316;0.02017
2015-07-10 05:51:33;;Apache 2.0;https://www.kaggle.com/marknagelberg/feature-extraction-for-final-model;1.0;['pattern', 'sklearn', 'nltk'];['ner', 'ai', 'nlp', 'nn', 'ann'];['training data', 'test data', 'train', 'model', 'deep learning', 'label', 'loss', 'rank', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.695;0.311;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Feature Extraction for Final Model;Python script;2759.0;11;;
2015-06-14 12:52:13;;Apache 2.0;https://www.kaggle.com/nikitakis/multiprocessing-clubbing-2-benchmarks;1.0;['pattern', 'sklearn', 'nltk'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['test data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.666;0.253;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Multiprocessing:Clubbing 2 Benchmarks;Python script;1497.0;6;0.63512;0.62530
2015-06-26 18:59:31;;Apache 2.0;https://www.kaggle.com/solution/lda-visualization;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.773;0.439;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];LDA Visualization;R script;19689.0;46;;
2015-06-03 21:10:17;;Apache 2.0;https://www.kaggle.com/triskelion/kappa-intuition;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'predict', 'classification', 'ground truth'];https://www.kaggle.com/c/crowdflower-search-relevance;0.735;0.427;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Kappa intuition;Python script;6981.0;40;;
2015-05-15 05:06:34;;Apache 2.0;https://www.kaggle.com/triskelion/normalized-kaggle-distance;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['filter', 'train', 'model', 'deep learning', 'label', 'clustering', 'classification', 'labeled'];https://www.kaggle.com/c/crowdflower-search-relevance;0.759;0.487;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];Normalized Kaggle Distance;Python script;13315.0;85;;
2015-06-21 10:09:19;;Apache 2.0;https://www.kaggle.com/uioreanu/r-simple-numbers-cruncher;1.0;['pattern', 'pillow'];['ner', 'ai', 'dl', 'nlp', 'nn', 'ann'];['linear regression', 'filter', 'training data', 'regression', 'test data', 'train', 'random forest', 'model', 'deep learning', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.649;0.253;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];R simple numbers cruncher;R script;1067.0;6;0.24638;0.25318
2015-05-30 18:35:19;;Apache 2.0;https://www.kaggle.com/wliang88/extra-engineered-features-w-svm;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/crowdflower-search-relevance;0.721;0.387;2020-12-12 18:32:08;Crowdflower Search Results Relevance;[];SVM 0.6+ ;R script;4950.0;25;0.36364;0.32174
2020-05-07 18:10:33;;Apache 2.0;https://www.kaggle.com/mohitsital/0-16372-predict-the-weather;1.0;['sklearn', 'nltk'];['ner', 'ai', 'gan', 'ml', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'loss', 'label', 'k-means', 'predict', 'naive bayes'];https://www.kaggle.com/c/crowdflower-weather-twitter;0.573;0.152;2020-12-12 18:32:36;Partly Sunny with a Chance of Hashtags;[];0.16412 - Predict the Weather;Python notebook;291.0;2;0.16412;0.16372
2019-01-18 19:14:16;;Apache 2.0;https://www.kaggle.com/ruchibahl18/predict-the-weather-bag-of-words;1.0;['caffe', 'xgboost', 'nltk', 'sklearn', 'h2o', 'pillow', 'pattern', 'stanza'];['ner', 'ai', 'nlu', 'dl', 'cnn', 'gbm', 'gan', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['filter', 'recognition', 'reward', 'predict', 'relu', 'gru', 'train', 'recommend', 'labeled', 'model', 'layer', 'loss', 'rank', 'understanding', 'test data', 'regression', 'generation', 'fitting', 'label'];https://www.kaggle.com/c/crowdflower-weather-twitter;0.703;0.099;2020-12-12 18:32:36;Partly Sunny with a Chance of Hashtags;[];Predict the weather Bag of words;Python notebook;3289.0;1;0.23793;0.23708
2018-06-27 06:16:30;;Apache 2.0;https://www.kaggle.com/dession/first;1.0;['skimage'];['ai', 'nn', 'cv'];['train', 'label'];https://www.kaggle.com/c/cvpr-2018-autonomous-driving;0.553;0.0;2020-12-12 18:34:37;CVPR 2018 WAD Video Segmentation Challenge;['gpu'];First;Python notebook;213.0;0;;
2018-05-08 20:04:14;;Apache 2.0;https://www.kaggle.com/jihyeseo/video-seg-movable-objects;1.0;['sklearn'];['ai'];['train', 'label'];https://www.kaggle.com/c/cvpr-2018-autonomous-driving;0.584;0.0;2020-12-12 18:34:37;CVPR 2018 WAD Video Segmentation Challenge;[];video seg movable objects;Python notebook;343.0;0;;
2018-04-24 02:32:43;;Apache 2.0;https://www.kaggle.com/kjeanclaude/download-train-test-samples-for-local-prototype;1.0;['skimage'];['ner', 'ai', 'gan', 'cv', 'nlp', 'nn'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/cvpr-2018-autonomous-driving;0.64;0.188;2020-12-12 18:34:37;CVPR 2018 WAD Video Segmentation Challenge;[];Download Train-Test samples for local prototype;Python script;907.0;3;;
2018-04-28 22:48:07;Train and Test IDs;Apache 2.0;https://www.kaggle.com/kjeanclaude/wad-video-full-implementation-atoz-tfkeras;1.0;['tensorflow', 'skimage', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'u-net'];https://www.kaggle.com/c/cvpr-2018-autonomous-driving;0.67;0.188;2020-12-12 18:34:37;multiple data sources;[];WAD-Video-Full-Implementation-AtoZ-TFKeras;Python notebook;1634.0;3;;
2018-04-15 12:14:51;;Apache 2.0;https://www.kaggle.com/lantingguo/get-a-few-images-and-labels-for-local-prototype;1.0;['skimage'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/cvpr-2018-autonomous-driving;0.641;0.152;2020-12-12 18:34:37;CVPR 2018 WAD Video Segmentation Challenge;[];Get a few images and labels for local prototype ;Python script;919.0;2;;
2018-05-20 17:50:09;Intro In this kernel I parse the input text file in order to use all the available information in the dataset I propose the source code and the output result in this kernel, and will bind to the file as soon as they will be available;Apache 2.0;https://www.kaggle.com/lcantat/build-database-from-text-files;0.5;[];['ner', 'ai', 'nn', 'cv'];['train', 'label', 'training data'];https://www.kaggle.com/c/cvpr-2018-autonomous-driving;0.619;0.0;2020-12-12 18:34:37;multiple data sources;[];Build database from text files;Python notebook;617.0;0;;
2017-04-13 00:03:47;Fast IRM data analysis using R;Apache 2.0;https://www.kaggle.com/amorsili/fast-exploratory-data-analysis-in-r;0.5;[];['ai'];['label'];https://www.kaggle.com/c/data-science-bowl-2017;0.717;0.468;2020-12-12 18:38:03;Data Science Bowl 2017;[];Fast Exploratory Data Analysis in R;R notebook;4525.0;66;;
2017-01-28 13:42:39;;Apache 2.0;https://www.kaggle.com/bpavlyshenko/nodules-segmentation;1.0;['keras'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/data-science-bowl-2017;0.737;0.393;2020-12-12 18:38:03;Data Science Bowl 2017;[];Nodules Segmentation;Python script;7272.0;27;;
2017-01-20 14:04:19;;Apache 2.0;https://www.kaggle.com/drn01z3/mxnet-xgboost-baseline-lb-0-57;1.0;['xgboost', 'sklearn', 'mxnet'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/data-science-bowl-2017;0.782;0.536;2020-12-12 18:38:03;Data Science Bowl 2017;[];mxnet + xgboost baseline [LB: 0.57];Python script;25809.0;167;;
2017-04-07 21:19:14;"Perfect scores were seen on Kaggle's public leaderboards before, but with a different evaluation function, and the authors never published their approach. Someone else commented that a perfect score was obtainable  in this competition after 198 submissions, or, since 3 submissions are allowed per day, 66 days,  using a ""brute force"" method: ""The whole process takes about 198/3 = 66 days, which is shorter than the competition length.""  I wanted to see how quickly this could really be done, within the rules,  and to try to win the race to the top of the public leaderboard. I ended up using 14 submissions, or 5 days  (Additionally,  I needlessly wasted 1 submission on the ""All 0.5 Benchmark"",   and spent another to actually claim the 0.00000 score, both being technically uninformative)  My approach is informed by information theory. You see, when the Kaggle server gives your submission a  score, it emits up to 21.7  bits of information  about the test labels, but there are only 198 labels with even fewer bits of information in them,  so one could learn all there is to know about the labels in 8 submissions or so. Well, that's a  theoretical limit, and might not be achievable in practice. If you train any two models and choose the one that has a better leaderboard score, you are already using 1 bit of information from your public scores. A generalization of this to any number of models is the boosting attack. However, it would require 4 years to get to the perfect score here. My approach is fundamentally similar, but is much more effective, as it learns more bits from each score. The core algebraic insight needed here is that if we choose 15 probabilities to be sigmoid(- n * epsilon * 2 ** i) where n=198, 0 <= i < 15, and epsilon = 1.05e-5 for example, and choose the rest of the probabilities to be 0.5, then the 15 labels corresponding to those 15 probabilities are easily discoverable from the score we get, because all  32768 possible label combinations lead to different scores. Note that the final rankings are based on the private labels of the second stage. Discovering all public labels helps with those only indirectly, by effectively increasing your training set size by 14%. (I believe the extra 14% are likely critical, given how close Kaggle competitions tend to be) USAGE To use the script, create an empty file called ""scores.txt"", copy ""stage1_sample_submission.csv"" (used to read patient IDs) into the same directory, and create a subdirectory called ""submissions"".  The former should contain the scores the Kaggle server gives you, one per line. It should be empty in the beginning. For example, the first line should be the score corresponding to ""submission_00.csv"". Keep any trailing 0s. There should be 5 digits after the decimal point. You can rerun the script whenever you update ""scores.txt"", but it's not necessary. This will do some partial label inference. When that file contains 14 lines, rerunning the script should also generate ""submission_fin.csv"", which will have all the correct labels. (Don't submit it though. If you wish to verify the labels, you may want to submit 1-labels instead and get the worst score possible: 34.54)";Apache 2.0;https://www.kaggle.com/olegtrott/the-perfect-score-script;0.5;[];['dl', 'ner', 'ai', 'nn'];['rank', 'model', 'label', 'train'];https://www.kaggle.com/c/data-science-bowl-2017;0.811;0.582;2020-12-12 18:38:03;Data Science Bowl 2017;[];"The ""Perfect Score"" Script";Python notebook;65922.0;335;;
2017-01-13 18:41:24;;Apache 2.0;https://www.kaggle.com/rmchamberlain/dicom-to-3d-numpy-arrays;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/data-science-bowl-2017;0.763;0.408;2020-12-12 18:38:03;Data Science Bowl 2017;[];DICOM to 3D numpy arrays;Python script;14680.0;32;;
2017-02-22 02:38:12;;Apache 2.0;https://www.kaggle.com/yoshcakes/full-preprocessing-in-r-with-3d-visualizations;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['label', 'classification', 'deep learning'];https://www.kaggle.com/c/data-science-bowl-2017;0.709;0.39;2020-12-12 18:38:03;Data Science Bowl 2017;[];Full preprocessing in R with 3D visualizations;R script;3771.0;26;;
2017-01-15 10:06:54;;Apache 2.0;https://www.kaggle.com/zfturbo/keras-vs-cancer;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/data-science-bowl-2017;0.752;0.416;2020-12-12 18:38:03;Data Science Bowl 2017;[];Keras vs Cancer;Python script;11007.0;35;;
2018-03-22 17:20:19;Reading the image;Apache 2.0;https://www.kaggle.com/akshayt19nayak/getting-started-image-processing-basics;1.0;['skimage', 'sklearn', 'opencv-python'];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'train', 'fitting', 'model', 'layer', 'clustering', 'label', 'k-means', 'predict', 'computer vision', 'classification', 'labeled'];https://www.kaggle.com/c/data-science-bowl-2018;0.77;0.439;2020-12-12 18:38:54;2018 Data Science Bowl;[];Getting Started/Image Processing Basics;Python notebook;18184.0;46;;
2018-03-21 15:37:59;"RationaleI found the explanation for the scoring metric on this competition a little confusing, and I wanted to create a  guide for those who are just entering or haven't made it too far yet. The metric used for this competition is defined as the mean average precision at different intersection over union (IoU) thresholds. This tells us there are a few different steps to getting the score reported on the leaderboard. For each image...  For each submitted nuclei ""prediction"", calculate the Intersection of Union metric with each ""ground truth"" mask in the image. Calculate whether this mask fits at a range of IoU thresholds. At each threshold, calculate the precision across all your submitted masks.  Average the precision across thresholds.  Across the dataset...  Calculate the mean of the average precision for each image.";Apache 2.0;https://www.kaggle.com/stkbailey/step-by-step-explanation-of-scoring-metric;0.5;[];['ai', 'rl', 'ml', 'nn', 'ann'];['training data', 'test data', 'train', 'label', 'predict', 'ground truth'];https://www.kaggle.com/c/data-science-bowl-2018;0.757;0.517;2020-12-12 18:38:54;2018 Data Science Bowl;['beginner'];Step-By-Step Explanation of Scoring Metric;Python notebook;12605.0;127;;
2018-03-14 08:23:08;For everybody's convenient, I converted the annotated nuclei from https://nucleisegmentationbenchmark.weebly.com/ into a public dataset following the folder structure of the competition data. I hope that you find it usefull..;Apache 2.0;https://www.kaggle.com/voglinio/external-h-e-data-with-mask-annotations;0.5;[];['ai', 'rl', 'cv', 'nn', 'ann'];['train'];https://www.kaggle.com/c/data-science-bowl-2018;0.741;0.494;2020-12-12 18:38:54;multiple data sources;[];External H&E Data with Mask Annotations;Python notebook;8172.0;93;;
2020-01-21 07:45:29;2019 Data Science Bowl;Apache 2.0;https://www.kaggle.com/fatihbilgin/data-science-bowl-2019-data-visualization;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'test data', 'label', 'filter'];https://www.kaggle.com/c/data-science-bowl-2019;0.745;0.532;2020-12-12 18:40:29;2019 Data Science Bowl;['data visualization, exploratory data analysis'];ðŸƒData Science Bowl 2019- Data Visualization;Python notebook;9058.0;156;;
2019-12-20 08:44:45;2019 Data Science Bowl EDAContent Introduction  Prepare the data analysis   -Load the packages   -Load the data  Data exploration   -Glimpse the data   -Missing data   -Unique values   -Most frequent values   -Values distribution   -Extract features from train/event_data   -Extract features from specs/args   -Merged data distribution  Next step;Apache 2.0;https://www.kaggle.com/gpreda/2019-data-science-bowl-eda;0.5;[];['ner', 'ai', 'ml', 'rl'];['test data', 'train', 'model', 'layer', 'label'];https://www.kaggle.com/c/data-science-bowl-2019;0.768;0.584;2020-12-12 18:40:29;2019 Data Science Bowl;['beginner, data visualization, exploratory data analysis'];2019 Data Science Bowl EDA;Python notebook;16779.0;345;;
2019-10-29 10:03:57;;Apache 2.0;https://www.kaggle.com/gpreda/data-science-bowl-fast-compact-solution;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/data-science-bowl-2019;0.724;0.509;2020-12-12 18:40:29;2019 Data Science Bowl;[];data_science_bowl_fast_compact_solution;Python script;5325.0;113;0.370;0.323
2019-10-25 23:11:31;"SummaryI was confused as to what is it that we are trying to forecast in this competition. Apparently, if you group your test set by installation_id you will get a long list of the activities of a user. It contains games, clips, activities, Assessments, etc. We should use this history to predict the very last row of each installation_id. Below is an example for the first user in test set 00abaee7. This data shows, he/she started the app, watched the welcome to app clip, then magma peak - level 1 clip and then two other clips. Then he/she played the ""Chow Time"" game. As you scroll down you can see all of his/her acitivies. At the very end of the activities list you can see that he/she started to play ""Cauldron Filler"" Assessment task which only has 1 row. The test set is truncated there to indicate that we should predict this user's Assessment on this task.";Apache 2.0;https://www.kaggle.com/mhviraf/a-baseline-for-dsb-2019;0.5;[];['ai', 'rl'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/data-science-bowl-2019;0.742;0.553;2020-12-12 18:40:29;2019 Data Science Bowl;[];A baseline for DSB 2019;Python notebook;8387.0;212;;
2019-10-30 18:58:57;Introduction;Apache 2.0;https://www.kaggle.com/shahules/xgboost-feature-selection-dsbowl;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'ml', 'rl'];['training data', 'test data', 'train', 'model', 'loss', 'label', 'labeled', 'predict', 'rank', 'understanding', 'natural language'];https://www.kaggle.com/c/data-science-bowl-2019;0.781;0.571;2020-12-12 18:40:29;2019 Data Science Bowl;['feature engineering, xgboost'];XGBoost & Feature Selection DSBowl ðŸ¥£ ðŸ¥£;Python notebook;24951.0;281;;
2020-01-02 09:21:57;Introduction The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment. My Data Exploration would be towards how to view the dataset in terms of improve the game or pointing out the existing games pro's and con's. If you think, this kernel is useful, plz upvote.;Apache 2.0;https://www.kaggle.com/subbuvolvosekar/eda-visualization-story-dsb-2019;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['train', 'label', 'test data', 'random forest'];https://www.kaggle.com/c/data-science-bowl-2019;0.709;0.511;2020-12-12 18:40:29;2019 Data Science Bowl;['beginner, data visualization, exploratory data analysis'];EDA_Visualization_Story_DSB_2019;Python notebook;3742.0;117;;
2020-01-06 09:04:49;;Apache 2.0;https://www.kaggle.com/vipulgandhi/how-to-choose-right-metric-for-evaluating-ml-model;1.0;['statsmodels', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'recognition', 'machine translation', 'logistic regression', 'predict', 'machine learning', 'train', 'recommend', 'classification', 'naive bayes', 'model', 'support vector machines', 'loss', 'rank', 'test data', 'regression', 'gradient descent', 'label', 'gradient boosting', 'random forest'];https://www.kaggle.com/c/data-science-bowl-2019;0.771;0.531;2020-12-12 18:40:29;multiple data sources;['beginner, classification, regression, +1 moremodel comparison'];How to Choose Right Metric for Evaluating ML Model;Python notebook;18598.0;155;;
2019-04-23 13:56:22;;Apache 2.0;https://www.kaggle.com/arjundas/when-i-grow-up-i-want-to-be;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'fitting', 'model', 'clustering', 'label', 'k-means', 'predict', 'recommend'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.717;0.462;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;['beginner, data visualization, exploratory data analysis, +2 morefeature engineering, clustering'];'When I grow up I want to be ..   ';R notebook;4470.0;61;;
2019-04-14 17:02:15;The model returns 768-dimensional embeddings:;Apache 2.0;https://www.kaggle.com/brendanhasz/bert-in-kernels;0.5;[];['dl', 'ai', 'nn', 'ml'];['train', 'model'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.685;0.327;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;[];BERT in Kernels;Python notebook;2198.0;13;;
2019-02-27 01:11:22;HowdyYour job in this Data Science for Good competition is to develop a recommendation engine that will suggest relevant questions to professionals via email. Students ask questons on the CareerVillage.org platform and Professionals answer them. CareerVillage.org has a pretty good recommendation system inplace that's based on hard-coded rules. They would love to make it more efficient and improve it's performance. That's where you come in :) Since questions and answers are the main focus of this competition, I thought it would be helpful to join a few tables together to help get things started. This is totally not an exhaustive list of things to do and there's a lot more data to explore so go get wild!;Apache 2.0;https://www.kaggle.com/crawford/starter-kernel;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['recommend'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.692;0.423;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;[];Starter kernel;Python notebook;2547.0;38;;
2019-04-22 18:29:02;CareerVillage.org Recommendation EngineDaniel Becker;Apache 2.0;https://www.kaggle.com/danielbecker/careervillage-org-recommendation-engine;1.0;['spacy', 'sklearn', 'gensim'];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'generation', 'train', 'model', 'understanding', 'natural language processing', 'label', 'labeled', 'rank', 'recommend', 'natural language'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.759;0.534;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;['exploratory data analysis, data cleaning, nlp'];CareerVillage.org Recommendation Engine;Python notebook;13249.0;161;;
2019-04-24 01:34:23;Recommender Engine for CareerVillage;Apache 2.0;https://www.kaggle.com/ididur/nn-based-recommender-engine;1.0;['nltk', 'gensim', 'sklearn', 'tensorflow', 'pattern', 'keras'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'reward', 'predict', 'autoencoder', 'training data', 'train', 'epoch', 'reinforcement learning', 'activation function', 'recommend', 'classification', 'model', 'neural network', 'layer', 'loss', 'rank', 'generation', 'artificial intelligence', 'label'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.737;0.512;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;['exploratory data analysis, deep learning, feature engineering, +2 moredata cleaning, recommender systems'];NN Based Recommender Engine;Python notebook;7416.0;119;;
2019-03-24 07:13:05;;Apache 2.0;https://www.kaggle.com/infocusp/deepdive-into-careervillage;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['filter', 'train', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.689;0.447;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;['beginner, data visualization, exploratory data analysis'];Deepdive into careervillage;Python notebook;2387.0;51;;
2019-04-01 01:29:44;EDA of CareerVillage.org dataDataset used: Data Science for Good: CareerVillage.org  Content Introduction All imports necessary A bit of configuration Auxiliary methods List all files available Read the data Quickly go through the files professionals tag_users students tag_questions groups emails group_memberships answers comments matches tags questions school_memberships   Data quality Null-values, unique values counts across tables professionals tag_users students tag_questions groups emails group_memberships answers comments matches tags questions school_memberships   What columns in what tables? Tables mapping Students intersected with Questions by author_id Professionals intersected with Questions by author_id Professionals intersected with Answers by author_id Students intersected with Answers by author_id Questions intersected with Answers by question_id Questions intersected with Comments by question_id Answers intersected with Comments by answer_id Professionals intersected with Comments by author_id Students intersected with Comments by author_id Students intersected with Group_memberships by user_id Professionals intersected with Group_memberships by user_id Students intersected with School_memberships by user_id Professionals intersected with School_memberships by user_id Students intersected with Tag_users by user_id Professionals intersected with Tag_users by user_id Questions intersected with Tag_questions by question_id Students intersected with Emails by recipient_id Professionals intersected with Emails by recipient_id Emails intersected with Matches by email_id Questions intersected with Matches by question_id   ER-diagram of data   Deeper analysis Cumulative community growth Yearly Monthly Weekly Daily   Community growth dynamic Yearly Monthly Weekly Daily   Cumulative questions/answers/comments growth Yearly Monthly Weekly Daily   Questions/answers/comments growth dynamic Yearly Monthly Weekly Daily   Professionals Professionals by answered_questions/asked_questions/wrote_comments flags Distribution of professionals by questions/answers/comments count Professionals locations, industries, headlines   Students Students by answered_questions/asked_questions/wrote_comments flags Students locations   Tags Emails Matches;Apache 2.0;https://www.kaggle.com/ioohooi/eda-with-some-insights-data-er-diagram;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'train', 'understanding', 'label', 'recommend'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.719;0.453;2020-12-12 18:41:32;multiple data sources;['beginner, data visualization, exploratory data analysis'];EDA with some insights + data ER-diagram;Python notebook;4717.0;55;;
2019-03-05 18:25:22;;Apache 2.0;https://www.kaggle.com/mistermichael/careervillage-exploration;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['recommend', 'label', 'filter', 'reward'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.662;0.4;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;['data visualization, exploratory data analysis, data cleaning'];CareerVillage Exploration;Rmarkdown script;1388.0;29;;
2019-02-27 08:30:22;Student's with particular questions;Apache 2.0;https://www.kaggle.com/nasirislamsujan/eda-data-science-for-good;0.5;[];['dl', 'ner', 'nn'];['rank'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.638;0.379;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;[];EDA : Data Science for Good ðŸ‘¨â€ðŸŽ“ ðŸ‘©â€ðŸŽ“;Python notebook;871.0;23;;
2019-04-23 22:03:27;IntroductionNotebook Overview: This notebook contains a solution for Career village recommendation system competition. In this notebook, I build a hybrid recommendation system for recommending students questions to professionals for CareerVillage.org. The recommender system works by matching professionals with questions by tags they follow, their previous answers' question tags and similar tags. Also, it overcomes some of the most highest rated problem for CareerVillage recommender system like cold-start and others. Competition problem statements: CareerVillage.org is a non-profit organization helping underserved youth to provide information to build their career. Students can ask their questions in the CareerVillage.org and professionals(expert people who love to help students) answer their questions. The challenge is that CareerVillage has to recommend correct questions to the professionals so that the questions match with the professional's interests. This will increase the likelihood of a question to get an answer. So in this competition, we have to make a recommendation system that will correctly recommend questions that will match with professionals interest.;Apache 2.0;https://www.kaggle.com/niyamatalmass/lightfm-hybrid-recommendation-system;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nn', 'ml'];['filter', 'train', 'model', 'understanding', 'epoch', 'loss', 'predict', 'rank', 'recommend'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.775;0.485;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;['beginner, recommender systems'];LightFM Hybrid Recommendation system;Python notebook;20869.0;82;;
2019-03-02 17:19:45;Kernel Start;Apache 2.0;https://www.kaggle.com/spurryag/eda-n-gram-text-cleaning;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'nlp', 'nn', 'ml'];['training data', 'train', 'label', 'recommend', 'classification'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.683;0.352;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;['gpu, data visualization, exploratory data analysis, +1 moredata cleaning'];EDA + N-gram + Text cleaning;Python notebook;2124.0;17;;
2019-03-12 01:29:06;Given a professional, the goal is to return a list of questions sorted by the predicted likelihood of the professional answering the question. This can be treated like a recommendation problem with implicit feedback, where the professional is the user and the question is the item. A professional answering a question can be used as a form of implicit feedback. The  Implicit library is used  to implement a collaboritve filtering algorithm that is based on the method used in Collaborative Filtering for Implicit Feedback Datasets.;Apache 2.0;https://www.kaggle.com/tagboola/recommendations-with-collaborative-filtering;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/data-science-for-good-careervillage;0.656;0.334;2020-12-12 18:41:32;Data Science for Good: CareerVillage.org;[];Recommendations with Collaborative Filtering;Python notebook;1232.0;14;;
2019-06-21 21:53:38;;Apache 2.0;https://www.kaggle.com/ambarish/la-jobs-big-analysis;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['recommend', 'label', 'filter', 'understanding'];https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;0.662;0.334;2020-12-12 18:42:37;multiple data sources;['data cleaning, nlp, text mining'];LA Jobs Big Analysis;Rmarkdown script;1389.0;14;;
2019-06-11 04:23:29;Quick note about aiofiles Ordinary local file IO is blocking, and cannot easily and portably made asynchronous. This means doing file IO may interfere with asyncio applications, which shouldn't block the executing thread. aiofiles helps with this by introducing asynchronous versions of files that support delegating operations to a separate thread pool.;Apache 2.0;https://www.kaggle.com/arlen444/comprehensive-fast-processing-using-asyncio;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'label', 'training data'];https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;0.743;0.319;2020-12-12 18:42:37;multiple data sources;[];Comprehensive Fast processing using Asyncio;Python notebook;8633.0;12;;
2019-05-13 09:46:22;Method to generate the CSV file;Apache 2.0;https://www.kaggle.com/jazivxt/a-freakonomics-opportunity;0.5;[];['ner', 'ai', 'ml', 'nn', 'ann'];['recommend'];https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;0.66;0.34;2020-12-12 18:42:37;Data Science for Good: City of Los Angeles;['data cleaning'];A Freakonomics Opportunity;Python notebook;1323.0;15;;
2019-06-21 00:23:53;;Apache 2.0;https://www.kaggle.com/jmartindelasierra/structuring-and-analyzing-job-descriptions;1.0;['pattern', 'vocabulary'];['ner', 'ai', 'dl', 'nn', 'ann'];['label', 'filter'];https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;0.599;0.319;2020-12-12 18:42:37;Data Science for Good: City of Los Angeles;['data visualization, nlp, text data, +2 moretext mining, dimensionality reduction'];Structuring and analyzing job descriptions;Rmarkdown script;443.0;12;;
2019-05-13 11:05:43;In this notebook I can able to get  Title Class code Open data Annual salary Requirment  Where to apply Dead Line (Date) Duties;Apache 2.0;https://www.kaggle.com/karthickaravindan/preparing-dataset;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['train'];https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;0.641;0.334;2020-12-12 18:42:37;Data Science for Good: City of Los Angeles;['gpu, data cleaning, nlp'];Preparing dataset;Python notebook;930.0;14;;
2019-06-21 12:02:40;Data Science For Good : CoLA A Complete Pipeline for Structuring, Analysis and Recommendation Improve Hiring Process and Decisions  Key Objectives: Keeping these challenges in mind, an ideal solution for the City of Los Angeles has following key objectives: Develop an nlp framework to accurately structurize the job descriptions. Develop an analysis framework to identify the implict bias in the text and encourage diversity. Develop a system which can clearly identify the promotion pathways within the organization.My Submission: Following are parts of Kernels Submissions in order:  Part 1: Job Bulletin Structuring Engine - City of Los Angeles   Part 2: Encourage Diversity and Remove Unconsious Bias from Job Bulletins - A Deep Analysis  Part 3: Impact of Content, Tone, and Language : CTL Analysis for CoLA  Part 4: Increasing the Discoverability of Promotional Pathways (Explicit)  Part 5: Implicit Promotional Pathways DiscoverabilityPart 4: Increasing the Discoverability of Promotional Pathways  Other Parts: Part 1 | Part 2 | Part 3 | Part 4 | Part 5 The aim of this kernel is to provide an easy solution to identify the explict promotion pathways in City of Los Angeles. I developed a reusable python module in which the program identifies which job roles are required to fill another particular job class. Using this class, one can identify: What are the possible pathways of Job Class which are required to fill a particular Job Class?      What are the possible Job Classes an Employee can be promoted to?   Table of Contents1. Possible Pathways to fill a Job Class  2. Possible Promotion Pathways for a Job Class   1. Possible Pathways to fill a ClassSometimes, it is very hard to find qualified candidates to fill a particular job class. Infact, City of Los Angeles has mentioned that there are atleast 17 job classes int which it is very challenging to find the right qualified candidates. These include Accountant, Accounting Clerk, Applications Programmer, Assistant Street Lighting Electrician, Building Mechanical, Inspector, Detention Officer, Electrical Mechanic, Equipment Mechanic, Field Engineering Aide, Housing Inspector, Housing Investigator, Librarian, Security Officer, Senior Administrative Clerk, Senior Custodian, Senior Equipment Mechanic, Tree Surgeon. The following framework analyzes the structured data, specifically the requirements of a particular job and identifies which other job classes are required to fill the particular job class. In the end, City of LA can use this piece of code to generate all possible promotion pathways using a single line of code.;Apache 2.0;https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['recommend'];https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;0.638;0.346;2020-12-12 18:42:37;Data Science for Good: City of Los Angeles;['data visualization'];4. Promotional Pathway Discoverability - CoLA;Python notebook;878.0;16;;
2019-06-19 05:20:09;City of LA - Job postings should be an invitation, not a barrierIf you are having issues viewing the kernel due to size, just fork it and you'll be able to see it much clearer.;Apache 2.0;https://www.kaggle.com/silverfoxdss/city-of-la-readability-and-promotion-nudges;0.5;[];['ner', 'ai', 'nlu', 'dl', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'generation', 'train', 'fitting', 'model', 'understanding', 'layer', 'label', 'rank', 'recommend', 'classification'];https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;0.724;0.423;2020-12-12 18:42:37;multiple data sources;['gpu, nlp'];City of LA  - Readability and Promotion Nudges;Python notebook;5279.0;38;;
2019-05-08 17:55:07;City of LA - Converting the job bulletins to a pd dataframe;Apache 2.0;https://www.kaggle.com/sobrinomario/city-of-la-starter-kernel;0.5;[];['ner', 'ai', 'dl', 'gan', 'nn', 'ann'];['filter'];https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles;0.685;0.346;2020-12-12 18:42:37;Data Science for Good: City of Los Angeles;['data cleaning'];City of LA  - Starter Kernel;Python notebook;2212.0;16;;
2020-10-02 23:10:24;;Apache 2.0;https://www.kaggle.com/ahmedmurad1990/data-science-london-scikit-learn;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'ann', 'cv'];['regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'decision tree', 'random forest', 'naive bayes'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.334;0.099;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];notebookdcb5978746;Python notebook;14.0;1;0.99192;0.99143
2020-07-07 23:49:23;;Apache 2.0;https://www.kaggle.com/alexlichtenberg/scikit-learn-getting-started;1.0;['sklearn'];['ai'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.541;0.214;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];SciKit Learn Getting Started;Python notebook;178.0;4;0.84943;0.85096
2018-11-29 11:45:37;Use only kNN for classification;Apache 2.0;https://www.kaggle.com/aman9d/data-science-london-scikit;1.0;['sklearn'];['ai', 'nn', 'ann', 'cv'];['random forest', 'regression', 'train', 'fitting', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.737;0.427;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Data Science London + Scikit ;Python notebook;7381.0;40;0.99160;0.98956
2020-06-16 20:19:11;Data Mining;Apache 2.0;https://www.kaggle.com/benscaria/data-science-london-sklearn;1.0;['statsmodels', 'sklearn'];['ai', 'rl', 'nn', 'cv'];['random forest', 'regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.529;0.188;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Data Science London + sklearn;Python notebook;151.0;3;;
2019-03-06 16:04:23;PRE-PROCESSING;Apache 2.0;https://www.kaggle.com/chahat1/data-science-london-classification;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'ann', 'cv'];['random forest', 'regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'decision tree', 'classification', 'naive bayes'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.705;0.39;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Data Science London  Classification ;Python notebook;3417.0;26;0.99113;0.99254
2020-10-19 09:38:16;;Apache 2.0;https://www.kaggle.com/etoile33/kaggle01;1.0;['sklearn'];['ai', 'nn'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.439;0.0;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Data Science London + Scikit-learn;Python notebook;46.0;0;;
2020-08-02 11:08:21;;Apache 2.0;https://www.kaggle.com/gsethi2409/compare-performance-metrics-dt-knn-svc-rf-mlp;1.0;['sklearn'];['ai', 'nn', 'ml', 'cv'];['train', 'model', 'layer', 'label', 'predict'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.481;0.214;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Compare Performance Metrics -DT, KNN, SVC, RF, MLP;Python notebook;78.0;4;0.00000;0.00000
2018-12-30 11:39:51;;Apache 2.0;https://www.kaggle.com/intu290/classifer-london-scikit;1.0;['sklearn'];['ai', 'nn'];['train', 'model', 'label'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.614;0.099;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Classifer_London_scikit;Python notebook;566.0;1;;
2020-09-21 10:09:24;;Apache 2.0;https://www.kaggle.com/julienmihai/data-science-london-classification;1.0;['sklearn'];['ai', 'nn'];['test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.45;0.152;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Data Science London;Python notebook;53.0;2;;
2019-03-05 05:25:06;;Apache 2.0;https://www.kaggle.com/rishikoush/predicting-best-among-knn-dt-lr-svc-rf-mlp;1.0;['sklearn'];['ai', 'nn', 'ml'];['regression', 'train', 'model', 'layer', 'label', 'predict'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.612;0.099;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Predicting best among KNN, DT, LR, SVC, RF, MLP;Python notebook;549.0;1;0.90547;0.89642
2020-10-15 10:04:07;XGBOOST CLASSIFIER AND HYPERPARAMETER TUNING;Apache 2.0;https://www.kaggle.com/rizkioktafianto/london-sklearn-using-xgboost-fine-tuned;1.0;['xgboost', 'sklearn'];['ai', 'rl', 'nn', 'cv'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.427;0.0;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];London Sklearn using XGBoost Fine Tuned;Python notebook;40.0;0;;
2019-01-13 17:41:45;;Apache 2.0;https://www.kaggle.com/ruchibahl18/neural-network-version;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn'];['train', 'fitting', 'model', 'input layer', 'output layer', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'hidden layer'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.614;0.188;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Neural Network version;Python notebook;569.0;3;0.86510;0.86289
2020-09-29 09:02:37;;Apache 2.0;https://www.kaggle.com/shashankrajput9/data-science-london-scikit-learn;1.0;['xgboost', 'sklearn'];['ai', 'nn'];['regression', 'train', 'model', 'support vector machines', 'label', 'logistic regression', 'gradient boosting', 'predict', 'decision tree', 'random forest'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.469;0.099;2020-12-12 18:43:22;Data Science London + Scikit-learn;['beginner'];Data Science London + Scikit-learn;Python notebook;67.0;1;0.88948;0.89307
2020-10-01 11:01:42;;Apache 2.0;https://www.kaggle.com/vigneshprakash/data-science-london-scikit-learn-modeling;1.0;['statsmodels', 'xgboost', 'sklearn', 'lightgbm'];['ai', 'gbm', 'cv', 'nn', 'ann'];['random forest', 'regression', 'train', 'model', 'loss', 'label', 'predict', 'decision tree', 'classification', 'naive bayes'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.536;0.319;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];Data Science London + Scikit-learn Modeling;Python notebook;167.0;12;;
2020-09-02 12:55:30;;Apache 2.0;https://www.kaggle.com/vsevicky/data-science-london-scikit-learn;1.0;['sklearn'];['ai'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/data-science-london-scikit-learn;0.463;0.188;2020-12-12 18:43:22;Data Science London + Scikit-learn;[];data-science-london-scikit-learn;Python notebook;62.0;3;;
2019-12-22 05:50:42;DeepFake Introductory EDA;Apache 2.0;https://www.kaggle.com/aleksandradeis/deepfake-challenge-eda;1.0;['skimage'];['ner', 'ai', 'dl', 'cnn', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'neural network', 'deep learning', 'label', 'predict', 'computer vision', 'labeled', 'convolutional neural network'];https://www.kaggle.com/c/deepfake-detection-challenge;0.731;0.509;2020-12-12 18:44:56;multiple data sources;['data visualization, exploratory data analysis, deep learning'];DeepFake Challenge EDA;Python notebook;6386.0;113;;
2019-12-12 10:09:32;Update: My mistake, I thought we didn't have access to the train_sample labels, but we do and they are there hiding as a json file in the train_sample videos folder.;Apache 2.0;https://www.kaggle.com/brassmonkey381/a-quick-look-at-the-first-frame-of-each-video;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'nn'];['training data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/deepfake-detection-challenge;0.74;0.478;2020-12-12 18:44:56;Deepfake Detection Challenge;[];A quick look at the first frame of each video;Python notebook;7943.0;75;;
2020-02-11 15:00:40;;Apache 2.0;https://www.kaggle.com/hamditarek/deepfake-detection-challenge-kaggle;1.0;['pytorch', 'face_recognition', 'skimage', 'pillow'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ml'];['filter', 'train', 'recognition', 'model', 'vgg', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/deepfake-detection-challenge;0.727;0.477;2020-12-12 18:44:56;multiple data sources;['gpu'];Deepfake Detection ðŸ™‚ðŸ™ƒ Challenge Kaggle;Python notebook;5768.0;74;;
2020-01-28 20:29:21;;Apache 2.0;https://www.kaggle.com/hmendonca/kaggle-pytorch-utility-script;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'loss'];https://www.kaggle.com/c/deepfake-detection-challenge;0.733;0.497;2020-12-12 18:44:56;multiple data sources;['deep learning, utility script'];Kaggle pytorch utility script;Python script;6733.0;96;;
2019-12-24 23:56:58;First Impression about Deepfake Detection Challenge </div>  Deepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online. These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rightsâ€”especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion. Identifying manipulated media is a technically demanding and rapidly evolving challenge that requires collaborations across the entire tech industry and beyond.;Apache 2.0;https://www.kaggle.com/marcovasquez/basic-eda-face-detection-split-video-and-roi;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ml'];['train', 'label', 'generation'];https://www.kaggle.com/c/deepfake-detection-challenge;0.729;0.5;2020-12-12 18:44:56;multiple data sources;[];Basic EDA Face Detection, split video and ROI;Python notebook;6090.0;101;;
2020-02-26 08:38:24;Comparison of face detection packages;Apache 2.0;https://www.kaggle.com/timesler/comparison-of-face-detection-packages;1.0;['pytorch', 'tensorflow'];['ai', 'dl', 'cnn', 'cv', 'rl', 'nn'];['train', 'vgg', 'model', 'label'];https://www.kaggle.com/c/deepfake-detection-challenge;0.774;0.533;2020-12-12 18:44:56;multiple data sources;['gpu'];Comparison of face detection packages;Python notebook;20208.0;159;;
2019-12-29 01:11:28;Metadata is leaking targets;Apache 2.0;https://www.kaggle.com/zaharch/data-leak-in-metadata;0.5;[];['ai', 'nn', 'ml'];['filter', 'test data', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/deepfake-detection-challenge;0.743;0.5;2020-12-12 18:44:56;multiple data sources;[];Data leak in metadata;Python notebook;8518.0;101;0.82247;0.82247
2020-03-20 15:39:19;Reference: https://github.com/aswalin/Outlier-Impact-Treatment;Apache 2.0;https://www.kaggle.com/duccao/outlier-treatment;1.0;['sklearn'];['dl', 'ai', 'nn', 'ml'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/deloitte-western-australia-rental-prices;0.46;0.0;2020-12-12 18:45:03;multiple data sources;[];Outlier treatment;Python notebook;60.0;0;;
2018-09-24 11:31:10;Loading data;Apache 2.0;https://www.kaggle.com/abhilashawasthi/feature-engineering-lgb-model;1.0;['lightgbm', 'sklearn'];['ai', 'rl', 'ml', 'gbm'];['test data', 'regression', 'train', 'model', 'validation data', 'label', 'predict'];https://www.kaggle.com/c/demand-forecasting-kernels-only;0.776;0.486;2020-12-12 18:46:39;Store Item Demand Forecasting Challenge;[];Feature Engineering + LGB Model;Python notebook;21363.0;83;12.88772;14.17918
2018-08-17 13:45:49;;Apache 2.0;https://www.kaggle.com/ashishpatel26/store-item-demand-using-using-arima;1.0;['statsmodels', 'sklearn'];['ai'];['filter', 'training data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/demand-forecasting-kernels-only;0.692;0.34;2020-12-12 18:46:39;Store Item Demand Forecasting Challenge;['gpu'];Store Item Demand Using USING ARIMA;Python notebook;2575.0;15;;
2018-09-01 22:03:23;Backtesting - Cross-Validation for TimeSeries The goal of this kernel is to introduce forecasters to the concept of backtesting and provide a basic implementation. Although in this competition we can directly see the test data, as a learning opportunity it is still important to apply forecasting best practices in our attempts. Backtesting is the time series equivalent of cross-validation. It is an attempt to bootstrap the data in such a way that we can estimate the expected test error. We cannot use cross-validation directly since this is sequenced data. Order must be respected to avoid peeking. There are two major methods of backtesting: sliding window and expanding window. In Sliding Window, we keep the same training size and slide the window across the data to create multiple train-test pairs. This may be a good strategy if you have a fixed size model (some neural network implementations and other machine learning methods.). It's also good for initial model development as it will result in faster model builds.  In Expanding Window, we expand the training size from some starting size to a maximum size. This method provides a good balance between creating enough training-test pairs while maximizing the amount of data your models receive.   By using these strategies we can have better estimation of the test error of the various models. These methods can also be used for density forecasting (evaluating the full quantiles and prediction intervals of model forecasts rather than just the mean forecast) I hope it helps. This is my first Kernel, so let me know how I can improve it to help out the community. Cheers!  Calvin Credit to Uber's Engineering Blog for some of the photos and ideas.;Apache 2.0;https://www.kaggle.com/cworsnup/backtesting-cross-validation-for-timeseries;0.5;[];['ai', 'nn', 'ann'];['machine learning', 'test data', 'train', 'model', 'neural network', 'predict'];https://www.kaggle.com/c/demand-forecasting-kernels-only;0.72;0.367;2020-12-12 18:46:39;Store Item Demand Forecasting Challenge;[];Backtesting - Cross-Validation for TimeSeries;R notebook;4897.0;20;;
2018-09-23 01:57:01;;Apache 2.0;https://www.kaggle.com/moizzz/introduction-to-forecasting;1.0;['xgboost'];['ai', 'dl', 'cv', 'rl', 'nn', 'ml'];['regression', 'train', 'model', 'gradient boosting', 'predict', 'random forest'];https://www.kaggle.com/c/demand-forecasting-kernels-only;0.751;0.452;2020-12-12 18:46:39;Store Item Demand Forecasting Challenge;[];Introduction to Forecasting;Rmarkdown script;10575.0;54;;
2018-07-25 08:15:36;;Apache 2.0;https://www.kaggle.com/nafisur/store-item-demand-forecasting-challenge;1.0;['xgboost', 'sklearn'];['ai', 'rl', 'gbm'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/demand-forecasting-kernels-only;0.701;0.334;2020-12-12 18:46:39;Store Item Demand Forecasting Challenge;['gpu'];Store Item Demand Forecasting Challenge;Python notebook;3161.0;14;12.94486;14.14306
2018-09-12 10:17:54;Introduction;Apache 2.0;https://www.kaggle.com/poiupoiu/how-to-use-sarimax;1.0;['statsmodels', 'sklearn'];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['filter', 'test data', 'regression', 'train', 'fitting', 'model', 'predict'];https://www.kaggle.com/c/demand-forecasting-kernels-only;0.821;0.462;2020-12-12 18:46:39;Store Item Demand Forecasting Challenge;['beginner, statistical analysis'];How to use SARIMAX;Python notebook;94440.0;61;22.16887;26.89327
2018-09-20 18:44:36;;Apache 2.0;https://www.kaggle.com/toshinoue/lgbm-plus-average-sales;1.0;['lightgbm'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['filter', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/demand-forecasting-kernels-only;0.716;0.346;2020-12-12 18:46:39;Store Item Demand Forecasting Challenge;['gpu'];LGBM plus Average Sales;R script;4371.0;16;12.82041;13.94140
2016-12-25 11:48:57;Creating simple autoencoder;Apache 2.0;https://www.kaggle.com/ahmedpyarali/autoencoded-denoising;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv'];['autoencoder', 'train', 'model', 'layer', 'predict', 'relu'];https://www.kaggle.com/c/denoising-dirty-documents;0.721;0.311;2020-12-12 18:52:45;Denoising Dirty Documents;[];Autoencoded denoising;Python notebook;4926.0;11;;
2019-02-09 02:55:42;;Apache 2.0;https://www.kaggle.com/anmour/convolutional-autoencoder-with-keras;1.0;['tensorflow', 'keras'];['ai', 'rl', 'cv', 'nn', 'ann'];['autoencoder', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/denoising-dirty-documents;0.769;0.379;2020-12-12 18:52:45;Denoising Dirty Documents;['gpu, deep learning'];Convolutional Autoencoder with Keras;Python notebook;17418.0;23;0.08759;0.08759
2015-10-04 20:53:03;;Apache 2.0;https://www.kaggle.com/colinpriest/background-information-leakage-using-r;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'test data', 'deep learning'];https://www.kaggle.com/c/denoising-dirty-documents;0.685;0.292;2020-12-12 18:52:45;Denoising Dirty Documents;[];Background Information Leakage Using R;R script;2187.0;9;;
2015-08-01 10:23:22;;Apache 2.0;https://www.kaggle.com/colinpriest/denoising-with-r-part-1;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/denoising-dirty-documents;0.754;0.334;2020-12-12 18:52:45;Denoising Dirty Documents;[];Denoising with R: Part 1;R script;11480.0;14;0.13450;0.13450
2015-08-08 06:20:57;;Apache 2.0;https://www.kaggle.com/colinpriest/denoising-with-r-part-2;1.0;['pattern'];['ner', 'ai', 'dl', 'gbm', 'cv', 'nlp', 'nn'];['training data', 'test data', 'train', 'model', 'deep learning', 'clustering', 'k-means', 'predict', 'classification'];https://www.kaggle.com/c/denoising-dirty-documents;0.711;0.236;2020-12-12 18:52:45;Denoising Dirty Documents;[];Denoising with R: Part 2;R script;3902.0;5;0.10806;0.10806
2015-08-28 04:06:44;;Apache 2.0;https://www.kaggle.com/colinpriest/denoising-with-r-part-5;1.0;['pattern', 'xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'deep learning', 'clustering', 'label', 'k-means', 'predict', 'classification'];https://www.kaggle.com/c/denoising-dirty-documents;0.703;0.292;2020-12-12 18:52:45;Denoising Dirty Documents;[];Denoising with R : Part 5;R script;3293.0;9;;
2015-09-07 02:24:33;;Apache 2.0;https://www.kaggle.com/colinpriest/denoising-with-r-part-6;1.0;['pattern', 'xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'deep learning', 'clustering', 'label', 'k-means', 'predict', 'classification'];https://www.kaggle.com/c/denoising-dirty-documents;0.701;0.268;2020-12-12 18:52:45;Denoising Dirty Documents;[];Denoising with R: Part 6;R script;3108.0;7;;
2015-05-28 20:01:38;;Apache 2.0;https://www.kaggle.com/dchudz/clean-by-thresholding;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/denoising-dirty-documents;0.692;0.292;2020-12-12 18:52:45;Denoising Dirty Documents;[];Clean by Thresholding;Python script;2549.0;9;;
2015-06-08 07:26:33;;Apache 2.0;https://www.kaggle.com/ngutten/high-pass-filter;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/denoising-dirty-documents;0.701;0.281;2020-12-12 18:52:45;Denoising Dirty Documents;[];High pass filter;Python script;3140.0;8;0.09568;0.09568
2015-08-03 04:33:13;;Apache 2.0;https://www.kaggle.com/oliversherouse/denoising-with-ransom-forests;1.0;['skimage', 'sklearn'];['nlp', 'ai', 'nn', 'ner'];['random forest', 'train', 'fitting', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/denoising-dirty-documents;0.711;0.236;2020-12-12 18:52:45;Denoising Dirty Documents;[];Denoising with Random Forests;Python script;3946.0;5;;
2019-03-17 13:31:03;;Apache 2.0;https://www.kaggle.com/palaksood97/image-denoising;1.0;['tensorflow', 'keras', 'pillow'];['ai', 'rl', 'nn', 'cv'];['predict', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/denoising-dirty-documents;0.665;0.387;2020-12-12 18:52:45;Denoising Dirty Documents;['gpu'];Image Denoising;Python notebook;1457.0;25;;
2015-06-08 17:27:15;;Apache 2.0;https://www.kaggle.com/rdokov/background-removal;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/denoising-dirty-documents;0.773;0.447;2020-12-12 18:52:45;Denoising Dirty Documents;[];Background removal;Python script;19828.0;51;;
2015-06-14 20:39:58;;Apache 2.0;https://www.kaggle.com/rdokov/nn-starter-kit;1.0;['theano'];['ner', 'ai', 'cv', 'nlp', 'nn'];['activation function', 'train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'gradient descent', 'predict', 'classification', 'hidden layer'];https://www.kaggle.com/c/denoising-dirty-documents;0.726;0.302;2020-12-12 18:52:45;Denoising Dirty Documents;[];NN starter kit;Python script;5543.0;10;;
2020-01-27 15:41:24;Preparing Data;Apache 2.0;https://www.kaggle.com/uurdeep/document-cleaner-with-convolutional-autoencoder;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nn', 'ann'];['autoencoder', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'rank'];https://www.kaggle.com/c/denoising-dirty-documents;0.632;0.253;2020-12-12 18:52:45;multiple data sources;['gpu'];Document-Cleaner With Convolutional AutoEncoder;Python notebook;778.0;6;;
2016-12-10 21:37:30;Image de-noising with fully-connected autoencoder;Apache 2.0;https://www.kaggle.com/vsmolyakov/fc-autoencoder;1.0;['tensorflow', 'keras'];['ai', 'nn'];['autoencoder', 'predict', 'test data', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/denoising-dirty-documents;0.697;0.214;2020-12-12 18:52:45;Denoising Dirty Documents;[];FC Autoencoder;Python notebook;2857.0;4;;
2018-07-08 01:32:05;;Apache 2.0;https://www.kaggle.com/zhoulingyan0228/denoising-documents-w-cnn;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/denoising-dirty-documents;0.697;0.253;2020-12-12 18:52:45;Denoising Dirty Documents;['beginner, classification, neural networks, +2 morecomputer vision, binary classification'];Denoising Documents w/ CNN;Python notebook;2856.0;6;;
2020-03-10 00:54:13;;Apache 2.0;https://www.kaggle.com/rishabhgarg1023/detecting-insults-from-social-commentary;1.0;['pattern', 'vocabulary', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nn', 'ann'];['gru', 'filter', 'generation', 'train', 'model', 'layer', 'label', 'predict', 'rank', 'classification', 'naive bayes'];https://www.kaggle.com/c/detecting-insults-in-social-commentary;0.623;0.214;2020-12-12 18:52:53;Detecting Insults in Social Commentary;[];kernel61b6c7d573;Python notebook;665.0;4;;
2019-04-04 15:49:47;;Apache 2.0;https://www.kaggle.com/amitasr/diabetic-retinopathy-detection;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'label'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.714;0.281;2020-12-12 18:54:56;Diabetic Retinopathy Detection;['gpu'];diabetic-retinopathy-detection;Python notebook;4248.0;8;;
2019-05-31 17:32:04;;Apache 2.0;https://www.kaggle.com/amitasr/kernel4d37e6ad35;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.573;0.152;2020-12-12 18:54:56;Diabetic Retinopathy Detection;['gpu'];kernel4d37e6ad35;Python notebook;289.0;2;;
2017-05-02 10:45:40;;Apache 2.0;https://www.kaggle.com/ayank77/college-work;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'nn', 'ml'];['filter', 'training data', 'train', 'model', 'epoch', 'layer', 'label', 'loss', 'rank', 'relu'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.734;0.34;2020-12-12 18:54:56;Diabetic Retinopathy Detection;[];college work;Python notebook;6767.0;15;;
2015-07-31 19:47:19;;Apache 2.0;https://www.kaggle.com/benhamner/sample-images;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['test data', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.756;0.319;2020-12-12 18:54:56;Diabetic Retinopathy Detection;[];Scripts Enabled on Diabetic Retinopathy;Rmarkdown script;12181.0;12;;
2018-08-20 13:35:02;;Apache 2.0;https://www.kaggle.com/bhargavbhatt/project-work;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'cnn', 'cv', 'nn', 'ann'];['training data', 'test data', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.659;0.188;2020-12-12 18:54:56;multiple data sources;['gpu'];Project Work;Python notebook;1292.0;3;;
2017-11-18 15:13:24;;Apache 2.0;https://www.kaggle.com/gufranmirza/retinopathy;1.0;['skimage', 'sklearn'];['ai', 'ml'];['train', 'label'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.658;0.236;2020-12-12 18:54:56;Diabetic Retinopathy Detection;[];Retinopathy;Python notebook;1274.0;5;;
2019-09-04 14:38:14;;Apache 2.0;https://www.kaggle.com/hirokazu12/capsnet-dr;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.655;0.188;2020-12-12 18:54:56;multiple data sources;[];capsNet_DR;Python notebook;1205.0;3;0.00000;0.00000
2020-03-25 17:59:10;"Diabetic retinopathy (DR) is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment. What is Diabetic Retinopathy?Diabetic Retinopathy is an eye disease associated with long-standing diabetes. This happens when high blood sugar levels cause damage to blood vessels in the retina. These blood vessels can swell and leak. Or they can close, stopping blood from passing through. Sometimes abnormal new blood vessels grow on the retina. All of these changes can steal your vision. You can follow this link to find more about it.  How can we help?Currently, detecting Diabetic Retinopathy (DR) is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. Expertise and equipment required to diagnose the disease often lacks in the areas where rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient. We need a comprehensive and automated method for DR screening. With color fundus photography as input, we need to detect whether eye has DR or not automatically. If there is DR, we can go even further and rate how severe the disease is in the eye. In this blog post, I will write about how we participated in the Diabetic retinopathy detection challenge on Kaggle using Intelec AI and built an automated DR detector. Let's get started. DataWe were provided with a large set of high-resolution retina images taken under a variety of imaging conditions. A clinician had rated the presence of diabetic retinopathy in each image between 0 and 4, according to the following scale: 0 - No DR 1 - Mild 2 - Moderate 3 - Severe 4 - Proliferative DR Our task was to create an automated DR detector capable of assigning a score to a given image based on this scale. So we donwloaded the dataset from the Data page of the competition and extracted it:  The images in this dataset came from different models and types of cameras and featured very mixed quality. There was noise in both the images and labels. Some images contained artifacts - were out of focus, underexposed, or overexposed. A major aim of this competition was to develop robust algorithms that can function in the presence of noise and variation.  Install machine learning toolsWe used Intelec AI to train diabetic retinopathy detectors. You can download and install it for free from here. Training first modelWe decided to start from ""Simple image classifier"" to train a small neural network to classifiy given images into 5 categories (0 - 4). Small neural networks train fast, which is helpful in getting the first benchmark accuracy in short time. But there was a problem, images were very big (minimum resolution 2500x1900), which, if used as given, might have slowed down our training significantly. We solved this problem by shrinking all images 20 times. Shrinking the training images can decrease accuracy of final model but it is usually a good idea to start working on small images, before using the original images to get best accuracy. So we trained a small neural network on the training images, which were shrinked 20 times:  The training took 50 minutes and had 75% accuracy in the end. A nice start!  Increasing accuracy with deep neural networkOne way of increasing the detection accuracy is training a deeper network. It usually takes longer but produces a better accuracy. Hence we decided to train a deep neural network to see whether it would increase our detection accuracy.  The training took ~6.5hours and achieved 79% accuracy. Training time was 6-7 times more than our previous training but it improved the accuracy by 5%.  Increasing input image sizeOur previous models were trained with images, which were 20 times smaller than the original ones. Small input size allowed us to train our first models in relatively short time. However, by shrinking our training images so much we loose a lot of small details, especially in our case, because important decision factors, like hemorrhages, microaneurysms and even exudates, are usually small spots in the picture (see the image above). By shrinking our images a lot, we get rid of them. This results in images with DR not having any DR related features and hence been classified as ""No DR"". Therefore we decided to decrease the shrink factor hoping to get some of the important small details back. This time we decided to shrink the images only 10 times.   As you can already guess, it took significantly more time to train the model but it achieved higher accuracy.  Accuracy of our final model was 82%. Perhaps it could be further increased by decreasing the shrink factor even more. But we decided to stop here in order to keep our demo short. Test final modelNext, we deployed our model and ran it on some sample images. One of the results looked like the following:  You can see here the result (prediction) and the activation map. Activation map shows where the model gave more attention while calculating the result. In the above picture, the bright spots got more attention than the dark ones. Submit results to KaggleHow good is our final accuracy 82%? To answer this question, we needed to run our final model on the test images, submit the results to Kaggle and see where we stand on the leaderboard of the competition. So we created a task to classify all our test images in ""/diabetic retinopathy/test"" using the final model ""Deep DR classifier medRes"".  We then went ahead and downloaded the results,  converted them into the Kaggle submission format  and submitted to Kaggle.  Our score was ~0.58. If we compare it with another results on the leaderboard, we were at 57-58th place, which falls into bronze place range.  SummaryWow, that was a lot of work. We were able to train a diabetic retinopathy detector with 82% accuracy. The accuracy can be increased further  by training a model on even bigger images by training a separate model, which recognizes different details of DR (hemorrhages, microaneurysms and exudates) and combining its results with the results of our original DR detector to make final decision  But we decided to stop here in order to keep our demo short. I hope you enjoyed it. I have also prepared a video demo of this work, where I show eveything, which we did in this blog post, step-by-step. You can watch it here References Detect eye disease using Artificial Intelligence (step by step demo) What is Diabetic Retinopathy? Diabetic Retinopathy detection through integration of Deep Learning classification Diabetic retinopathy detection challenge on Kaggle";Apache 2.0;https://www.kaggle.com/intelecai/diabetic-retinopathy-detection-bronze-place;0.5;[];['ai', 'rl'];['machine learning', 'train', 'artificial intelligence', 'model', 'neural network', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.607;0.152;2020-12-12 18:54:56;Diabetic Retinopathy Detection;[];Diabetic retinopathy detection (bronze place);Python notebook;501.0;2;;
2018-07-27 19:45:08;;Apache 2.0;https://www.kaggle.com/meenavyas/diabetic-retinopathy-detection;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv'];['train', 'model', 'epoch', 'deep learning', 'layer', 'label', 'loss', 'relu', 'classification'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.793;0.512;2020-12-12 18:54:56;Diabetic Retinopathy Detection;[];diabetic-retinopathy-detection;Python notebook;36759.0;119;;
2019-06-03 13:14:06;;Apache 2.0;https://www.kaggle.com/mitramishra93/diabetic-retinopathy-detection-cnn;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'nn', 'ml'];['filter', 'training data', 'train', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.701;0.268;2020-12-12 18:54:56;Diabetic Retinopathy Detection;['gpu']; Diabetic Retinopathy Detection CNN;Python notebook;3110.0;7;;
2016-11-18 22:08:32;;Apache 2.0;https://www.kaggle.com/ralsaad/test-1;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.654;0.188;2020-12-12 18:54:56;Diabetic Retinopathy Detection;[];test 1;Python script;1187.0;3;;
2018-09-24 22:32:37;Classifying diabetic retinopathy;Apache 2.0;https://www.kaggle.com/simonandersen/fast-ai-lesson-1-diabetic-retinopathy;1.0;['pytorch', 'keras', 'sklearn'];['ner', 'ai', 'cnn', 'rl', 'ml', 'nn', 'ann'];['train', 'fitting', 'model', 'neural network', 'epoch', 'layer', 'gradient descent', 'loss', 'label', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.697;0.236;2020-12-12 18:54:56;multiple data sources;['gpu'];fast.ai lesson 1 + diabetic retinopathy;Python notebook;2834.0;5;;
2019-05-26 07:36:10;;Apache 2.0;https://www.kaggle.com/tahmina011/diabetic-retinopathy-detection-cnn;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'dl', 'ml', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/diabetic-retinopathy-detection;0.625;0.152;2020-12-12 18:54:56;Diabetic Retinopathy Detection;['gpu']; Diabetic Retinopathy Detection CNN;Python notebook;685.0;2;;
2017-10-04 16:55:27;Our goal is to check how many train images we have per breed;Apache 2.0;https://www.kaggle.com/johngull/breed-distribution;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/dog-breed-identification;0.694;0.362;2020-12-12 18:55:48;Dog Breed Identification;[];Breed distribution;Python notebook;2709.0;19;;
2018-01-04 16:57:17;Load CSV-File with pandas and print the first ten rows;Apache 2.0;https://www.kaggle.com/methindor/dogbreeddatavisualisation;0.5;[];['dl', 'ai', 'nn', 'cv'];['train', 'label'];https://www.kaggle.com/c/dog-breed-identification;0.677;0.327;2020-12-12 18:55:48;Dog Breed Identification;['data visualization'];DogBreedDataVisualisation;Python notebook;1888.0;13;;
2020-05-02 14:52:33;Fine Grained Image Classification;Apache 2.0;https://www.kaggle.com/snide713/fine-grained-image-classification-with-pytorch;1.0;['pytorch', 'sklearn'];['ai', 'nn', 'ann'];['image classification', 'train', 'model', 'epoch', 'deep learning', 'layer', 'label', 'loss', 'relu', 'resnet', 'classification'];https://www.kaggle.com/c/dog-breed-identification;0.569;0.319;2020-12-12 18:55:48;Dog Breed Identification;['gpu'];Fine Grained Image Classification with Pytorch;Python notebook;272.0;12;;
2018-02-14 11:39:26;;Apache 2.0;https://www.kaggle.com/stassl/displaying-inline-images-in-pandas-dataframe;0.5;[];['ai', 'dl', 'ml', 'gan'];['train', 'label'];https://www.kaggle.com/c/dog-breed-identification;0.784;0.411;2020-12-12 18:55:48;Dog Breed Identification;[];Displaying inline images in pandas DataFrame;Python notebook;27640.0;33;;
2017-10-05 10:14:07;è½½å…¥æ•°æ®é›†;Apache 2.0;https://www.kaggle.com/yangpeiwen/keras-inception-xception-0-47;1.0;['keras'];['ai', 'nn', 'cnn', 'cv'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/dog-breed-identification;0.75;0.463;2020-12-12 18:55:48;multiple data sources;[];Keras Inception + Xception (0.47);Python notebook;10279.0;62;;
2018-09-30 17:28:04;;Apache 2.0;https://www.kaggle.com/abhishekrock/cat-dog-try;1.0;['tensorflow', 'keras'];['ai', 'nn', 'cv'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/dogs-vs-cats;0.741;0.39;2020-12-12 18:56:58;Dogs vs. Cats;['beginner, deep learning, classification, +2 morecnn, binary classification'];cat_dog_Try;Python notebook;8088.0;26;;
2019-07-09 12:41:30;;Apache 2.0;https://www.kaggle.com/batibayburak/catsvsdogs;1.0;['tensorflow'];['ner', 'ai', 'nlu', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['regression', 'train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/dogs-vs-cats;0.662;0.352;2020-12-12 18:56:58;Dogs vs. Cats;[];CatsvsDogs;Python notebook;1391.0;17;;
2019-01-12 23:59:47;Import Library;Apache 2.0;https://www.kaggle.com/bulentsiyah/dogs-vs-cats-classification-vgg16-fine-tuning;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nn', 'ann'];['test data', 'train', 'model', 'output layer', 'epoch', 'validation data', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/dogs-vs-cats;0.768;0.498;2020-12-12 18:56:57;Dogs vs. Cats;['gpu, beginner, deep learning'];Dogs vs. Cats Classification (VGG16 Fine Tuning);Python notebook;17177.0;98;;
2020-07-24 11:03:17;Preprocessing Data;Apache 2.0;https://www.kaggle.com/ishaanaditya/dogs-vs-cats;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cnn', 'rl', 'nn', 'ann'];['filter', 'train', 'fitting', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/dogs-vs-cats;0.609;0.327;2020-12-12 18:56:58;Dogs vs. Cats;[];Dogs vs Cats;Python notebook;520.0;13;;
2019-05-19 09:29:02;;Apache 2.0;https://www.kaggle.com/jaeboklee/pytorch-cat-vs-dog;1.0;['pytorch'];['ai', 'nn', 'cv'];['train', 'model', 'epoch', 'label', 'loss'];https://www.kaggle.com/c/dogs-vs-cats;0.768;0.393;2020-12-12 18:56:58;Dogs vs. Cats;['gpu, beginner, classification'];[pytorch] cat vs dog;Python notebook;16798.0;27;;
2020-10-10 10:51:47;1. Importing necessary packages;Apache 2.0;https://www.kaggle.com/kamalkhumar/cat-or-dog-classification-keras-cnn;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nn'];['test data', 'neuron', 'train', 'model', 'output layer', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'hidden layer'];https://www.kaggle.com/c/dogs-vs-cats;0.583;0.302;2020-12-12 18:56:58;Dogs vs. Cats;['image data, cnn, keras'];Cat or Dog Classification - Keras CNN;Python notebook;341.0;10;;
2019-08-01 09:30:51;;Apache 2.0;https://www.kaggle.com/rohit1277/cat-dog-classifier-using-vgg16-transfer-learning;1.0;['tensorflow', 'keras'];['ner', 'ai', 'nn', 'rl'];['train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict'];https://www.kaggle.com/c/dogs-vs-cats;0.699;0.319;2020-12-12 18:56:58;Dogs vs. Cats;['beginner, deep learning, classification, +2 morecnn, transfer learning'];Cat & Dog Classifier Using VGG16 Transfer Learning;Python notebook;2966.0;12;;
2020-07-15 12:52:28;;Apache 2.0;https://www.kaggle.com/sahil2398/cats-vs-dogs-cnn;1.0;['tensorflow', 'keras'];['ai', 'nn', 'cv'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/dogs-vs-cats;0.607;0.292;2020-12-12 18:56:58;Dogs vs. Cats;['beginner, deep learning, cnn'];Cats vs Dogs CNN;Python notebook;505.0;9;;
2020-06-27 12:19:48;;Apache 2.0;https://www.kaggle.com/skbadhsm/cats-v-s-dogs-resnet50;1.0;['pytorch', 'sklearn'];['ai', 'nn', 'cv'];['predict', 'train', 'model', 'epoch', 'vgg', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/dogs-vs-cats;0.587;0.292;2020-12-12 18:56:58;Dogs vs. Cats;[];Cats v/s Dogs Resnet50;Python notebook;361.0;9;;
2019-11-20 10:22:19;Preparing data;Apache 2.0;https://www.kaggle.com/xiormeesh/cnn-cats-vs-dogs-classification;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'rl'];['test data', 'neuron', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'hidden layer'];https://www.kaggle.com/c/dogs-vs-cats;0.711;0.362;2020-12-12 18:56:58;Dogs vs. Cats;['gpu, cnn'];CNN: cats vs dogs classification;Python notebook;3924.0;19;;
2017-02-12 03:57:45;;Apache 2.0;https://www.kaggle.com/abnera/transfer-learning-keras-xception-cnn;1.0;['tensorflow', 'keras'];['ner', 'ai', 'cnn', 'cv', 'rl', 'nlp', 'nn', 'ml'];['image classification', 'train', 'model', 'epoch', 'deep learning', 'layer', 'vgg', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition;0.791;0.411;2020-12-12 18:58:25;Dogs vs. Cats Redux: Kernels Edition;[];Transfer Learning: Keras Xception  CNN;Python script;34532.0;33;;
2018-08-01 05:25:18;UpdatedNow using all 25000 images of the dataset since Kaggle increased available disk space to 5.2 GB. Thank you Kaggle! Previous Version (Score: 0.07092)I have used only 16000 of the available 25000 images of the training data due to kaggle kernel limitations of Disk space. Please make the necessary changes in the code if you want to train on the complete dataset. I realize this is not a well explained kernel, but I primarily wrote this to verify if it was possible to come up with an end-to-end solution using fastai inside kaggle kernels;Apache 2.0;https://www.kaggle.com/anshulrai/using-fastai-in-kaggle-kernel-updated;0.5;[];['ner', 'ai', 'nn', 'ml'];['predict', 'training data', 'test data', 'train', 'model', 'epoch', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition;0.731;0.411;2020-12-12 18:58:25;multiple data sources;['gpu, beginner, deep learning, +1 moretransfer learning'];Using fastai in Kaggle Kernel  (Updated);Python notebook;6278.0;33;0.07049;0.07049
2016-11-01 22:05:21;Preprocess ImagesNormalize the luminance values and resize the images to a standard shape. This is done because the training and test images come in a variety of shapes, sizes, and lighting.;Apache 2.0;https://www.kaggle.com/gauss256/image-preprocessing-exploration-2;0.5;[];['ai', 'nn', 'ann'];['train', 'model'];https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition;0.74;0.4;2020-12-12 18:58:25;Dogs vs. Cats Redux: Kernels Edition;[];Image preprocessing exploration 2;Python notebook;7997.0;29;;
2018-08-01 14:34:34;Multi-label classification;Apache 2.0;https://www.kaggle.com/hortonhearsafoo/fast-ai-lesson-2;1.0;['pytorch', 'sklearn', 'pattern'];['ner', 'ai', 'dl', 'cv'];['filter', 'train', 'model', 'epoch', 'label', 'loss', 'understanding', 'resnet', 'classification'];https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition;0.732;0.397;2020-12-12 18:58:25;multiple data sources;['gpu'];fast.ai lesson 2;Python notebook;6492.0;28;;
2018-04-07 11:48:59;;Apache 2.0;https://www.kaggle.com/ambarish/eda-fe-xgb-glm-maps-donors-choose;1.0;['vocabulary', 'xgboost'];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn', 'ml'];['filter', 'test data', 'train', 'model', 'layer', 'label', 'gradient boosting', 'predict', 'rank', 'sentiment analysis'];https://www.kaggle.com/c/donorschoose-application-screening;0.724;0.418;2020-12-12 19:01:17;multiple data sources;['data visualization, feature engineering, text mining'];EDA-FE-XGB-GLM-Maps-Donors Choose;Rmarkdown script;5322.0;36;;
2018-04-24 07:39:03;;Apache 2.0;https://www.kaggle.com/codename007/a-very-extensive-end-to-end-project-donorschoose;1.0;['xgboost', 'lightgbm', 'nltk', 'sklearn', 'textblob'];['ner', 'ai', 'dl', 'gbm', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'test data', 'train', 'model', 'layer', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/donorschoose-application-screening;0.749;0.514;2020-12-12 19:01:17;multiple data sources;['beginner, exploratory data analysis, feature engineering, +2 morenlp, model comparison'];A Very Extensive End to End Project : DonorsChoose;Python notebook;9922.0;121;;
2018-03-02 03:05:07;"Visual AnalysisIn this notebook I will try to prove some graphical analysis so get a better picture of the database, the goals of the project and how you can improve your score using interesting characteristics of the features. Fisrt lets talk shortly about the databases. train dataframeDonorsChose.org provide us a interesting dataset with a lot of string type variables. This means that the project and the submission that we made will depend conbsiderably on how we build categorical variables and create combinations of these categories to prdict with more accuracy the probability of each application of beeing aproved.  Variables:  id: ID of the application. For each ID application we need to calculate the probability of this application being pproved. teacher_id: ID of the teacher that is presenting the application.  teacher_prefix:  The title is important, it give uys information about the  gender and the title of the teacher presenting the project. ['Ms.', 'Mrs.', 'Mr.', 'Teacher', 'Dr.', nan].   school_state:  In which state the project will be develop in case it's accepted. I'm not sure but maybe there could be some budget restrictions per state so some states may accept more projects because they have more money. project_submitted_datetime:   The submitted datetime of the project (No way!!) . This feature it's important for many reasons: There could be some seasonality in the events. We have to check for specific months in which we found more accpeted projects or a more accpetance rate. Maybe the budget for all the projects it's assigned in a particular month , or every X months?? We should expect that during holydays and vacations there are less submitted projects (we should tottally include holyday information as external data!)   project_grade_category:  For which school grade the project it's oriented. Maybe most of the accepted projects are for small kids because they need special prgrams to learn better? Or maybe to older students that need special materials for science projects? project_subject_categories: For which academic area the project was proposed? Is it math related? Music related? This is feature is interesting beacuse one application may have multiple areas. For example:  -Math & Science-, -Math & Science, Applied Learning-,  -Math & Science, Warmth, Care & Hunger-, -History & Civics, Warmth, Care & Hunger- are 4 different categories that share at least one academic area so we may need to apply some string transformations to get a list of academic areas instead of a single-value string.  EX:  -Math & Science, Applied Learning-, => [Math, Science, Applied Learning] We have 51 different categories single-value.   project_subject_subcategories:  This is something more specific than the category feature. Then again we may need specific words or fields of the subcategory to identify more speciffic groups. Some examples of subcategories are:  'ESL, Performing Arts', 'Gym & Fitness, Visual Arts', 'Early Development, Health & Life Science', 'Foreign Languages, Special Needs', We have 407 different subcategories.   project_title:  Name of the project.  Just an interesting name:  ""Wiggle While We Work"" -> 149 different observations with the same name. 0.912751677852349 accpetance rate. Awesome name!!    project_essay_1:   When presenting the project the teachers have to make a descrption of their application in 4 paragraphs. 663.84 words average. 1st paragraph description.     project_essay_2:  833.552 words average  2nd paragraph description.  .   project_essay_3:   19.70 words average 3rd paragraph description.  96.4% of teacher didn't include a third description paragraph.   project_essay_4:  12.435 words average 4th paragraph description.  96.4% of teacher didn't include a third description paragraph.   project_resource_summary:  A description of the resources required for the project. Example:   ""My students need 6 Ipod Nano's to create and differentiated and engaging way to practice sight words during a literacy station.""    teacher_number_of_previously_posted_projects:  How many applications does the teacher had presented in the past. Teachers had presented 11.23 in average.   project_is_approved:   0.8476823374340949 accpetance rate. This value is really       test dataframeSame information of the train dataset without the  project_is_approved variable. sample_submision dataframeA datafarme with just two columns, id and project_is_approved. As mention in the details of the competition th order of the results doesn't matter. Also, you must predict a probability for the project_is_approved variable and the file should contain a header.";Apache 2.0;https://www.kaggle.com/coronate/donorschoose-exploratory-analysis;0.5;[];['ner', 'ai', 'dl', 'rl'];['test data', 'training data', 'train', 'label', 'predict'];https://www.kaggle.com/c/donorschoose-application-screening;0.699;0.371;2020-12-12 19:01:17;DonorsChoose.org Application Screening;[];DonorsChoose Exploratory Analysis;Python notebook;2994.0;21;;
2020-02-07 07:44:13;;Apache 2.0;https://www.kaggle.com/headsortails/an-educated-guess-update-feature-engineering;1.0;['pattern', 'vocabulary'];['ner', 'ai', 'dl', 'gan', 'rl', 'nlp', 'nn'];['filter', 'training data', 'test data', 'train', 'model', 'natural language processing', 'layer', 'relu', 'label', 'predict', 'rank', 'sentiment analysis', 'classification', 'natural language'];https://www.kaggle.com/c/donorschoose-application-screening;0.753;0.538;2020-12-12 19:01:17;DonorsChoose.org Application Screening;['beginner, data visualization, exploratory data analysis, +1 morenlp'];An Educated Guess - Update: Feature Engineering;Rmarkdown script;11311.0;170;;
2018-03-20 15:43:53;;Apache 2.0;https://www.kaggle.com/ibrahimaptlo10/probability-project-approved;1.0;['sklearn', 'nltk'];['ner', 'ai', 'rl', 'cv', 'ml'];['test data', 'train', 'model', 'predict', 'naive bayes'];https://www.kaggle.com/c/donorschoose-application-screening;0.647;0.383;2020-12-12 19:01:17;multiple data sources;['classification, feature engineering'];probability   project  approved ;Python notebook;1027.0;24;;
2018-04-19 23:18:01;;Apache 2.0;https://www.kaggle.com/matthewa313/ensembling-with-logistic-regression-lb-82-4;1.0;['vocabulary', 'xgboost', 'lightgbm'];['ner', 'ai', 'dl', 'cnn', 'gbm', 'rl', 'nlp', 'nn', 'rnn'];['gru', 'random forest', 'regression', 'train', 'model', 'neural network', 'deep learning', 'label', 'logistic regression', 'predict', 'classification', 'naive bayes', 'bayesian'];https://www.kaggle.com/c/donorschoose-application-screening;0.699;0.357;2020-12-12 19:01:17;multiple data sources;['deep learning, nlp, xgboost, +1 moreensembling'];Ensembling with Logistic Regression (LB 82.4%);Python script;2981.0;18;0.81751;0.82302
2018-03-27 15:38:11;;Apache 2.0;https://www.kaggle.com/opanichev/lightgbm-and-tf-idf-starter;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/donorschoose-application-screening;0.739;0.463;2020-12-12 19:01:17;DonorsChoose.org Application Screening;['feature engineering'];LightGBM and Tf-idf Starter;Python script;7815.0;62;0.78470;0.79516
2018-03-17 10:53:42;;Apache 2.0;https://www.kaggle.com/qinhui1999/deep-learning-is-all-you-need-lb-0-80x;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nlp', 'nn', 'rnn', 'ml'];['gru', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/donorschoose-application-screening;0.697;0.403;2020-12-12 19:01:17;multiple data sources;['beginner'];Deep learning is all you need! LB 0.80X;Python script;2841.0;30;0.79690;0.80121
2018-11-21 12:54:26;;Apache 2.0;https://www.kaggle.com/anikishaev/turkey-competition;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'rl'];['predict', 'train', 'model', 'epoch', 'layer', 'lstm', 'loss', 'relu'];https://www.kaggle.com/c/dont-call-me-turkey;0.648;0.281;2020-12-12 19:03:18;Don't call me turkey!;[];Turkey Competition;Python notebook;1053.0;8;0.94637;0.94637
2018-11-26 17:16:49;;Apache 2.0;https://www.kaggle.com/ceshine/pytorch-multilayer-perceptron-mlp-baseline;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/dont-call-me-turkey;0.751;0.236;2020-12-12 19:03:18;Don't call me turkey!;[];(PyTorch) Multilayer Perceptron (MLP) Baseline;Python script;10723.0;5;0.98987;0.98987
2018-11-24 16:49:33;;Apache 2.0;https://www.kaggle.com/ceshine/pytorch-temporal-convolutional-networks;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/dont-call-me-turkey;0.744;0.281;2020-12-12 19:03:18;Don't call me turkey!;[];(PyTorch) Temporal Convolutional Networks;Python script;8781.0;8;0.98893;0.98893
2018-11-24 20:36:17;;Apache 2.0;https://www.kaggle.com/diogomiguelribeiro/2layer-basic-turkeyflow;1.0;['tensorflow', 'sklearn'];['ai', 'nn', 'rl'];['test data', 'train', 'model', 'output layer', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'hidden layer'];https://www.kaggle.com/c/dont-call-me-turkey;0.543;0.152;2020-12-12 19:03:18;Don't call me turkey!;['gpu'];"2Layer Basic TurkeyFlow  ;)";Python notebook;183.0;2;;
2018-11-21 15:43:02;;Apache 2.0;https://www.kaggle.com/mkowoods/stacked-bidirectional-lstm-test-acc-95;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'rl'];['gru', 'predict', 'train', 'model', 'epoch', 'layer', 'lstm', 'loss', 'relu'];https://www.kaggle.com/c/dont-call-me-turkey;0.672;0.152;2020-12-12 19:03:18;Don't call me turkey!;[];Stacked BiDirectional LSTM - Test Acc: 95%;Python notebook;1680.0;2;;
2018-11-20 22:27:11;Data skewed so we need to consider f-beta score instead of accuracy;Apache 2.0;https://www.kaggle.com/moghazy/exploratory-data-analysis;0.5;[];['ai'];['train'];https://www.kaggle.com/c/dont-call-me-turkey;0.596;0.188;2020-12-12 19:03:18;Don't call me turkey!;[];Exploratory Data Analysis;Python notebook;417.0;3;;
2018-11-22 13:08:19;;Apache 2.0;https://www.kaggle.com/rishabh1294/turkey;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'rl'];['predict', 'training data', 'test data', 'train', 'model', 'epoch', 'validation data', 'layer', 'lstm', 'loss', 'relu'];https://www.kaggle.com/c/dont-call-me-turkey;0.541;0.152;2020-12-12 19:03:18;Don't call me turkey!;[];Turkey;Python notebook;180.0;2;;
2018-11-21 10:04:58;;Apache 2.0;https://www.kaggle.com/teemingyi/turkey-competition;1.0;['lightgbm', 'sklearn'];['ai', 'nn', 'gbm'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/dont-call-me-turkey;0.549;0.188;2020-12-12 19:03:18;Don't call me turkey!;[];Turkey Competition;Python notebook;201.0;3;0.97845;0.97845
2018-11-21 00:36:20;;Apache 2.0;https://www.kaggle.com/winstonvan/van-plan-for-kaggle-swaggle;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'rl'];['predict', 'training data', 'train', 'model', 'epoch', 'validation data', 'layer', 'lstm', 'loss', 'relu'];https://www.kaggle.com/c/dont-call-me-turkey;0.636;0.236;2020-12-12 19:03:18;Don't call me turkey!;[];The Van Plan for Kaggle Swaggle;Python notebook;842.0;5;;
2019-05-08 13:57:59;;Apache 2.0;https://www.kaggle.com/featureblind/robust-lasso-patches-with-rfe-gs;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['training data', 'test data', 'train', 'fitting', 'model', 'validation data', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/dont-overfit-ii;0.736;0.512;2020-12-12 19:05:08;Don't Overfit! II;[];Robust, Lasso, Patches with RFE & GS;Python script;7187.0;119;0.855;0.872
2019-03-02 22:37:30;;Apache 2.0;https://www.kaggle.com/gkoundry/bayesian-logistic-regression-with-pystan;0.5;[];['nlp', 'ai', 'nn', 'ner'];['regression', 'train', 'model', 'deep learning', 'logistic regression', 'predict', 'classification', 'bayesian'];https://www.kaggle.com/c/dont-overfit-ii;0.73;0.473;2020-12-12 19:05:08;Don't Overfit! II;[];Bayesian Logistic Regression with PyStan;Python script;6132.0;71;0.841;0.859
2019-05-08 19:35:16;Importing data;Apache 2.0;https://www.kaggle.com/iavinas/simple-short-solution-don-t-overfit-0-848;1.0;['sklearn'];['ai', 'cv'];['filter', 'regression', 'train', 'model', 'predict'];https://www.kaggle.com/c/dont-overfit-ii;0.693;0.397;2020-12-12 19:05:08;Don't Overfit! II;['beginner, logistic regression'];Simple & Short solution , Don't Overfit [0.848];Python notebook;2619.0;28;0.837;0.849
2019-02-09 05:57:43;;Apache 2.0;https://www.kaggle.com/jahaziel/simple-model-glmnet;0.5;[];['ai', 'cv'];['train', 'model', 'predict'];https://www.kaggle.com/c/dont-overfit-ii;0.709;0.439;2020-12-12 19:05:08;Don't Overfit! II;[];simple model - glmnet;R notebook;3753.0;46;0.842;0.851
2019-02-09 14:51:05;;Apache 2.0;https://www.kaggle.com/miroslavsabo/auc-0-844-in-11-loc;1.0;['sklearn'];['ai'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/dont-overfit-ii;0.675;0.393;2020-12-12 19:05:08;Don't Overfit! II;['logistic regression'];AUC 0.844 in 11 LOC;Python notebook;1803.0;27;;
2019-05-10 05:03:16;Load data;Apache 2.0;https://www.kaggle.com/plasticgrammer/don-t-overfit-i-try;1.0;['sklearn'];['ai', 'rl', 'cv'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/dont-overfit-ii;0.699;0.387;2020-12-12 19:05:09;Don't Overfit! II;[];Don't Overfit! I Try!!;Python notebook;2974.0;25;0.839;0.851
2019-05-09 03:37:55;Load the probed public LB AUCs;Apache 2.0;https://www.kaggle.com/zachmayer/first-place-solution;0.5;[];['ai', 'dl', 'ml'];['rank', 'model', 'training data', 'train'];https://www.kaggle.com/c/dont-overfit-ii;0.718;0.462;2020-12-12 19:05:08;multiple data sources;[];First Place Solution;R notebook;4585.0;61;0.886;0.923
2020-10-09 15:53:02;;Apache 2.0;https://www.kaggle.com/ayusheeagarwal/don-t-get-kicked;1.0;['sklearn'];['ai', 'nn', 'ann'];['train', 'model', 'test data', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.444;0.0;2020-12-12 19:05:42;Don't Get Kicked!;[];Don't Get Kicked;Python notebook;49.0;0;0.06269;0.05673
2018-11-08 22:07:13;;Apache 2.0;https://www.kaggle.com/cherednichenkoa/do-not-get-kicked-keras;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'label', 'loss'];https://www.kaggle.com/c/DontGetKicked;0.655;0.0;2020-12-12 19:05:42;Don't Get Kicked!;['gpu'];kernel6397fa9338;Python notebook;1209.0;0;;
2020-05-17 11:21:34;;Apache 2.0;https://www.kaggle.com/dhruvgupta2801/don-t-get-kicked;1.0;['sklearn'];['ai', 'nn', 'ann'];['train', 'model', 'test data', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.572;0.0;2020-12-12 19:05:42;Don't Get Kicked!;['gpu'];Don't get Kicked;Python notebook;285.0;0;;
2020-09-28 15:37:40;Import Libraries;Apache 2.0;https://www.kaggle.com/funxexcel/intermediate-code-k-fold-don-t-get-kicked;1.0;['sklearn'];['ai', 'dl', 'cv'];['regression', 'train', 'model', 'predict', 'random forest'];https://www.kaggle.com/c/DontGetKicked;0.439;0.099;2020-12-12 19:05:42;Don't Get Kicked!;[];Intermediate Code : k-Fold Don't Get Kicked;Python notebook;46.0;1;;
2020-07-30 16:19:11;Import Libraries;Apache 2.0;https://www.kaggle.com/funxexcel/starter-code-don-t-get-kicked-rf-model;1.0;['sklearn'];['dl', 'ai', 'nn', 'ann'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.574;0.302;2020-12-12 19:05:42;Don't Get Kicked!;[];Starter Code : Don't get Kicked RF Model;Python notebook;296.0;10;0.10239;0.08791
2020-07-07 17:42:00;;Apache 2.0;https://www.kaggle.com/julianbenny/don-t-get-kicked-knn;1.0;['sklearn'];['ai', 'nn', 'ann', 'rl'];['test data', 'training data', 'train', 'model', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.576;0.188;2020-12-12 19:05:42;Don't Get Kicked!;[];Don't get kicked;Python notebook;306.0;3;0.07230;0.06344
2020-07-07 10:12:53;;Apache 2.0;https://www.kaggle.com/pranavmittal88/kernel4b785e3973;1.0;['sklearn'];['ai', 'nn', 'ann'];['train', 'model', 'training data', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.442;0.0;2020-12-12 19:05:42;Don't Get Kicked!;[];kernel4b785e3973;Python notebook;48.0;0;;
2020-07-23 11:55:42;;Apache 2.0;https://www.kaggle.com/rahulkumar234/kernel7b2d75bf16;1.0;['sklearn'];['ai', 'nn', 'ann'];['train', 'model', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.421;0.0;2020-12-12 19:05:42;Don't Get Kicked!;[];kernel7b2d75bf16;Python notebook;37.0;0;0.06269;0.05673
2020-06-20 07:51:20;;Apache 2.0;https://www.kaggle.com/ramensingh/don-t-get-kicked;1.0;['sklearn'];['ai', 'nn', 'ann'];['train', 'model', 'test data', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.453;0.0;2020-12-12 19:05:42;Don't Get Kicked!;['gpu'];Don't get Kicked;Python notebook;55.0;0;0.06269;0.05673
2020-07-22 22:27:44;;Apache 2.0;https://www.kaggle.com/roohisharma/don-t-get-kicked;1.0;['sklearn'];['ai', 'nn', 'ann'];['test data', 'training data', 'train', 'model', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.511;0.152;2020-12-12 19:05:42;Don't Get Kicked!;[];Don't get kicked;Python notebook;117.0;2;;
2020-11-29 17:42:03;Importing the dataset;Apache 2.0;https://www.kaggle.com/semenedel/carbuy;1.0;['sklearn'];['ai', 'nn', 'ann'];['train', 'model', 'test data', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.423;0.0;2020-12-12 19:05:42;Don't Get Kicked!;[];CarBuy;Python notebook;38.0;0;0.05553;0.04954
2020-07-06 16:25:32;;Apache 2.0;https://www.kaggle.com/teramera/kernel782317bb70;1.0;['sklearn'];['ai', 'nn', 'ann'];['train', 'model', 'predict'];https://www.kaggle.com/c/DontGetKicked;0.447;0.0;2020-12-12 19:05:42;Don't Get Kicked!;[];kernel782317bb70;Python notebook;51.0;0;0.06269;0.05673
2019-08-03 17:46:54;;Apache 2.0;https://www.kaggle.com/yairhadad1/cars-bad-buy;0.5;[];['ai', 'nn', 'ann'];['train', 'model', 'filter'];https://www.kaggle.com/c/DontGetKicked;0.675;0.268;2020-12-12 19:05:42;Don't Get Kicked!;[];Cars bad buy -  prediction;Python notebook;1796.0;7;;
2016-04-29 16:29:05;;Apache 2.0;https://www.kaggle.com/anokas/naive-beat-the-benchmark;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.717;0.302;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Naive beat the benchmark! (0.09286);Python script;4492.0;10;0.02446;0.09285
2016-05-04 15:46:06;;Apache 2.0;https://www.kaggle.com/asymptote/homography-estimate-stitching-two-imag;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.734;0.188;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Homography Estimate + Stitching two imag;Python script;6790.0;3;;
2016-05-10 17:42:58;;Apache 2.0;https://www.kaggle.com/chabir/stitch-and-predict;1.0;['sklearn'];['ai', 'nn', 'cv'];['filter', 'train', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.639;0.152;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Stitch and Predict -- e001;Python notebook;897.0;2;0.00000;-0.00357
2016-04-30 18:15:56;;Apache 2.0;https://www.kaggle.com/chefele/plot-some-image-sets;0.5;[];['ai'];['train'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.626;0.152;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Plot Some Image Sets;Python notebook;700.0;2;;
2016-06-29 21:55:49;;Apache 2.0;https://www.kaggle.com/dogrishin/svm-based-on-zoom-and-rotations-0-7;1.0;['sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.675;0.214;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];SVM based on zoom and rotations ~ 0.7;Python script;1797.0;4;;
2016-04-29 18:05:50;;Apache 2.0;https://www.kaggle.com/khaledfayed/plot-images2;0.5;[];['ai'];['train'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.584;0.152;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Plot images2;Python notebook;348.0;2;;
2016-05-19 14:36:47;;Apache 2.0;https://www.kaggle.com/laurae2/imagej-pre-processing-for-deep-learning;0.5;[];['ai'];['train', 'deep learning', 'predict'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.732;0.319;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];ImageJ pre-processing for Deep Learning;Rmarkdown script;6478.0;12;;
2016-05-02 20:18:18;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/akaze-keypoint-detector;0.5;[];['ai', 'cv'];['train'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.668;0.214;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];AKAZE keypoint detector;Python notebook;1548.0;4;;
2016-05-18 15:51:39;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/akaze-stitching;0.5;[];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.74;0.367;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Stitching experiments;Python script;8012.0;20;;
2016-05-17 18:45:51;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/draper-notebook;0.5;[];['ai', 'rl', 'nn', 'cv'];['train', 'filter'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.622;0.214;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Draper notebook;Python notebook;654.0;4;;
2016-05-04 23:00:25;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/homography-estimate-stitching-two-imag;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.713;0.236;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Homography Estimate + Stitching two imag;Python script;4126.0;5;;
2016-04-30 02:47:49;;Apache 2.0;https://www.kaggle.com/stprior/mean-corner-detection;0.2;[];['ner', 'ai', 'cv'];[];https://www.kaggle.com/c/draper-satellite-image-chronology;0.656;0.152;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Mean Corner Detection;Python notebook;1231.0;2;0.09064;0.05357
2016-05-20 18:16:44;;Apache 2.0;https://www.kaggle.com/vicensgaitan/image-registration-the-r-way;0.5;[];['ner', 'ai', 'cv', 'ml', 'nn', 'ann'];['label', 'filter'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.76;0.444;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Image registration, the R way;Rmarkdown script;13580.0;49;;
2018-10-04 16:08:04;;Apache 2.0;https://www.kaggle.com/viswatejag/image-stiching-using-deeplearning;0.5;[];['ai', 'nn', 'cv'];['train', 'filter'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.595;0.099;2020-12-12 19:09:16;multiple data sources;['gpu'];image stiching using deeplearning;Python notebook;410.0;1;;
2016-04-29 16:36:47;;Apache 2.0;https://www.kaggle.com/wcukierski/plot-images;0.5;[];['ai'];['train'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.653;0.152;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Plot images;Python notebook;1166.0;2;;
2016-05-18 19:56:07;;Apache 2.0;https://www.kaggle.com/weedislove/andyafter-stitching;0.5;[];['ai', 'nn', 'cv'];['train', 'filter'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.544;0.099;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];andyafter_stitching;Python notebook;187.0;1;;
2016-05-18 19:48:51;;Apache 2.0;https://www.kaggle.com/weedislove/stitch-and-predict;1.0;['sklearn'];['ai', 'nn', 'cv'];['filter', 'train', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.618;0.099;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Stitch and Predict;Python notebook;608.0;1;-0.06187;0.05000
2016-05-16 11:28:33;There are many feature detectors in opencv. I choose ORB because of its speed and robust quality. See below for some function definition;Apache 2.0;https://www.kaggle.com/yourwanghao/align-images;0.5;[];['ner', 'ai', 'nn', 'cv'];['train'];https://www.kaggle.com/c/draper-satellite-image-chronology;0.683;0.292;2020-12-12 19:09:16;Draper Satellite Image Chronology;[];Align Images;Python notebook;2110.0;9;;
2020-03-12 12:07:50;"Summary  In order to get reasonable electricity generation values from the provided Power Plant dataset (gppd_120_pr.csv), the ""estimated_generation_gwh"" values should be fixed. Just the code in the following cell is necessary; the rest of the Notebook is the justification. Update: some data forensics The Kaggle Power Plant dataset for Puerto Rico is a subset of the GPPD (Global Power Plant Database), which is a wonderful effort to have information from as many Power Plants in the world as possible. The GPPD data for US Power Plants comes in turn from the EIA-923 reports, which seem very reliable; however, I have not seen information from Puerto Rico Power Plants in EIA-923 reports previous to 2017; thus, in case Puerto Rico's information in the GPPD was populated previous to 2017, it may be a combination of sources. The geopositioning data seems ok, and the ""capacity_mw"" and ""primary_fuel"" data match those in ""Tabla de datos"", from PREPA, Puerto Rico Electric Power Authority (Autoridad de EnergÃ­a ElÃ©ctrica, in Spanish). However, the ""estimated_generation_gwh"" values seem invented, as I justify below.";Apache 2.0;https://www.kaggle.com/ajulian/gppd-120-pr-csv-and-the-capacity-factor;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['understanding', 'generation'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.635;0.34;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;[];gppd_120_pr.csv and the Capacity Factor;Python notebook;824.0;15;;
2020-03-25 00:21:35;DS4G: Environmental Insights Explorer ðŸŒExploring alternatives for emissions factor calculations source;Apache 2.0;https://www.kaggle.com/caesarlupum/green-future-analysis-and-solution;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['anomaly detection', 'machine learning', 'training data', 'generation', 'train', 'model', 'deep learning', 'clustering', 'predict', 'recommend', 'ground truth'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.624;0.311;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;['geospatial analysis, survey analysis'];ðŸŒðŸŒ¿Green Future: Analysis and Solution ;Python notebook;674.0;11;;
2020-02-14 18:53:05;Objective This Data Science for Good Competition intends to use remote sensing techniques to understand Environmental Emissions. Since the whole concept of Satellite Imagery and can be a little overwhelming, this is just an introductory kernel, where I try to explain the various terms and datasets related to satellite Imagery.;Apache 2.0;https://www.kaggle.com/deepakdeepu8978/methodology-for-average-historical-emissions;0.5;[];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['generation', 'model', 'layer', 'label', 'predict'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.613;0.423;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;['data visualization, exploratory data analysis, data cleaning, +1 moreenvironment'];Methodology for Average Historical Emissions !!!;Python notebook;560.0;38;;
2020-03-20 20:34:11;Explore the Power Plants Data;Apache 2.0;https://www.kaggle.com/docxian/explore-power-plants-r;1.0;['pattern'];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['model', 'gru', 'filter', 'generation'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.656;0.334;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;['beginner, data visualization, exploratory data analysis'];Explore Power Plants (R);R notebook;1232.0;14;;
2020-02-22 12:02:58;Saving the power plants to a GeoJSONGeoJSONs are a great format to be able to open points very easily. They are basically just geo-located JSON files, which prevent us from having to retrieve the location info from each point with a self-made function each time we want to plot / access to points. In this short notebook, I'll show how this is done with a library called geopandas and how to then simply save this file. I also created a dataset here if you simply want to download the .geojson file directly.;Apache 2.0;https://www.kaggle.com/maxlenormand/saving-the-power-plants-csv-to-geojson;0.5;[];['ner', 'ai', 'dl', 'rl'];['generation'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.629;0.319;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;[];Saving the power plants CSV to GeoJSON;Python notebook;748.0;12;;
2020-02-17 20:14:24;;Apache 2.0;https://www.kaggle.com/mpwolke/dr-david-g-fearn-ds4g;1.0;['statsmodels', 'skimage'];['ner', 'ai', 'dl', 'gbm', 'gan', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['gru', 'generation', 'model', 'layer', 'vgg', 'predict'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.613;0.408;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;[];Dr David G. Fearn - DS4G;Python notebook;559.0;32;;
2020-05-17 04:57:03;ObjectiveThis Data Science for Good Competition intends to use remote sensing techniques to understand Environmental Emissions. Since the whole concept of Satellite Imagery and can be a little overwhelming, this is just an introductory kernel, where I try to explain the various terms and datasets related to satellite Imagery. Problem Statement: Measuring Emissions factors from Satellite DataAir Quality Management is an important area and influences a lot of decisions taken by countries. But how does one ascertain the Air quality of a place? This is done by calculating the Emissions Factor of that area. What is the EmissionÂ factor?Â  A lot of activities today results in the release of Green House Gases(GHG) in the atmosphere. There are various activities that contribute to the release of GHG like burning fuel, vehicles, Power Plants, etc. Therefore, in order to estimate GHG emissions per unit of available activity, we need to use a factor called emission factor (EF).[source] For example: how many kgs of GHG are emitted by 1 kWh of natural gas? Thus, an emission factor is a coefficient that converts any activity's data into GHG emissions. This factor attempts to relate the quantity of a pollutant released to the atmosphere with an activity associated with the release of that pollutant.   Source;Apache 2.0;https://www.kaggle.com/parulpandey/understanding-the-data;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['filter', 'generation', 'model', 'layer', 'label', 'predict'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.748;0.525;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;['beginner, data visualization, geospatial analysis'];Understanding theÂ Data ;Python notebook;9700.0;143;;
2020-02-21 18:50:44;Explore Image Metadata (S5P, GFS, GLDAS);Apache 2.0;https://www.kaggle.com/paultimothymooney/explore-image-metadata-s5p-gfs-gldas;0.5;[];['ai', 'nn', 'ann', 'rl'];['understanding', 'predict'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.652;0.334;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;[];Explore Image Metadata (S5P, GFS, GLDAS);Python notebook;1128.0;14;;
2020-07-14 17:14:51;Explore Earth Engine Data;Apache 2.0;https://www.kaggle.com/paultimothymooney/how-to-get-started-with-the-earth-engine-data;0.5;[];['ner', 'ai', 'nn', 'rl'];['layer', 'filter', 'generation'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.745;0.504;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;['geospatial analysis, environment'];How to get started with the Earth Engine data;Python notebook;9052.0;106;;
2020-02-21 18:51:25;Overview of the EIE Analytics ChallengeDS4G: Environmental Insights Explorer  Exploring alternatives for emissions factor calculations;Apache 2.0;https://www.kaggle.com/paultimothymooney/overview-of-the-eie-analytics-challenge;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['layer', 'recommend', 'model', 'generation'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.704;0.437;2020-12-12 19:11:13;DS4G - Environmental Insights Explorer;['geospatial analysis, environment'];Overview of the EIE Analytics Challenge;Python notebook;3374.0;45;;
2020-03-23 20:06:03;Project overviewDevelop a methodology to calculate an average historical emissions factor of electricity generated for a sub-national region, using remote sensing data and techniques. The Environmental Insights Explorer team at Google is keen to gather insights on ways to improve calculations of global emissions factors for sub-national regions. The ultimate goal of this challenge is to test if calculations of emissions factors using remote sensing techniques are possible and on par with calculations of emissions factors from current methodologies.;Apache 2.0;https://www.kaggle.com/vlarmet/an-r-notebook-for-no2-emission-factor;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['filter', 'generation', 'fitting', 'model', 'layer', 'label', 'predict'];https://www.kaggle.com/c/ds4g-environmental-insights-explorer;0.639;0.34;2020-12-12 19:11:13;multiple data sources;[];An R notebook for NO2 emission factor;R notebook;893.0;15;;
2019-05-25 14:24:25;Problem DescriptionThe EMC Data Science Global Hackathon dataset, or the â€˜Air Quality Predictionâ€˜ dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days.;Apache 2.0;https://www.kaggle.com/sanikamal/air-quality-prediction-eda;0.5;[];['ai', 'rl'];['train', 'label', 'predict'];https://www.kaggle.com/c/dsg-hackathon;0.605;0.0;2020-12-12 19:11:21;EMC Data Science Global Hackathon (Air Quality Prediction);[];Air Quality Prediction EDA;Python notebook;485.0;0;;
2017-01-06 21:03:22;;Apache 2.0;https://www.kaggle.com/aamaia/rgb-using-m-bands-example;0.2;[];['cv'];[];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.732;0.408;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];RGB using M bands (example);Python notebook;6512.0;32;;
2017-01-04 04:55:18;I started writing the notebook to check the effect of moving vehicles  vs the time gap in acquisition of sensor bands.  The plan was to:  choose samples where vehicles were along a road compare vehicle positions against different bands  But in the meantime I realized this is not very important. There are more challenging things to deal with. The images below speak for themselves.;Apache 2.0;https://www.kaggle.com/aamaia/small-vehicles;0.5;[];['ai', 'cv'];['train', 'filter'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.724;0.357;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];Small vehicles ;Python notebook;5395.0;18;;
2016-12-21 10:57:38;Polygon count can be misleading! The polygon count of trees is more but their area is not. Update: Kernel now uses geojson data instead of the WKT data.;Apache 2.0;https://www.kaggle.com/amanbh/eda-the-scouring-of-the-shire;0.5;[];['ner', 'ai', 'dl', 'rl'];['train', 'label', 'training data'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.661;0.334;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];[EDA] The Scouring of the Shire;Python notebook;1355.0;14;;
2016-12-22 19:30:43;;Apache 2.0;https://www.kaggle.com/amanbh/visualize-polygons-and-image-data;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ann'];['training data', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.758;0.455;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];Visualize Polygons and Image Data;Python script;12954.0;56;;
2017-03-05 18:11:59;;Apache 2.0;https://www.kaggle.com/ceperaang/lb-0-42-ultimate-full-solution-run-on-your-hw;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.77;0.487;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];[LB 0.42]Ultimate full solution (run on your HW);Python script;17996.0;85;;
2016-12-26 20:54:52;This is a quick demonstration of how to load a TIF, display it, and then overlay polygons of one of the feature types, POOR_DIRT_CART_TRACK. I used tifffile to load the image because it's easier to install than GDAL, and the files don't have headers anyway so there's no advantage to GDAL.  I used matplotlib to handle polygons, although I should mention that some of my other code uses openCV for some polygon operations.;Apache 2.0;https://www.kaggle.com/chatcat/load-a-3-band-tif-image-and-overlay-dirt-tracks;0.5;[];['ai', 'rl', 'dl', 'cv'];['train', 'label'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.699;0.334;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];Load a 3-band TIF image and overlay Dirt Tracks;Python notebook;2972.0;14;;
2017-01-30 22:25:33;;Apache 2.0;https://www.kaggle.com/drn01z3/end-to-end-baseline-with-u-net-keras;1.0;['keras', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification', 'u-net'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.81;0.551;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];End-to-end baseline with U-net (keras);Python script;63922.0;206;;
2017-02-28 14:30:52;This Python script shows training polygon classes (defined in Class variable) on corresponding satalite image. I used Small Veichles script as a template creating this script.  Image IDs defined in training data are commented out after ImageID variable To get closer look, edit and use commented out lines at the end of script;Apache 2.0;https://www.kaggle.com/mehmetbercan/display-a-polygon-class-on-16-band-m;0.5;[];['ai', 'cv'];['train', 'training data'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.723;0.34;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];Display a Polygon Class on 16-Band (M);Python notebook;5174.0;15;;
2017-01-04 23:37:32;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/25-images-with-polygon-overlay;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'layer'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.73;0.435;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];25 Images with polygon overlay;R script;6248.0;44;;
2016-12-20 23:28:54;;Apache 2.0;https://www.kaggle.com/randel/25-images-and-polygons-side-by-side-eda;1.0;['pattern'];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'layer'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.711;0.346;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];25 images and polygons side by side (EDA);R script;3957.0;16;;
2017-05-20 13:04:15;Panchromatic sharpening;Apache 2.0;https://www.kaggle.com/resolut/panchromatic-sharpening;0.5;['skimage'];[];[];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.771;0.429;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];Panchromatic sharpening ;Python notebook;18432.0;41;;
2016-12-31 03:11:39;;Apache 2.0;https://www.kaggle.com/shawn775/polygon-transformation-to-match-image;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.72;0.379;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];Polygon Transformation to match image;Python notebook;4872.0;23;;
2016-12-17 04:19:22;;Apache 2.0;https://www.kaggle.com/torrinos/exploration-and-plotting;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['training data', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.763;0.463;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];Exploration and Plotting;Python notebook;14593.0;62;;
2017-01-02 11:19:07;;Apache 2.0;https://www.kaggle.com/visoft/correct-image-missalignment-v2;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.737;0.387;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];Correct image missalignment v2;Python script;7448.0;25;;
2017-01-02 10:37:18;;Apache 2.0;https://www.kaggle.com/visoft/export-pixel-wise-mask;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'classification', 'training data', 'deep learning'];https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection;0.753;0.437;2020-12-12 19:16:58;Dstl Satellite Imagery Feature Detection;[];Export pixel-wise mask;Python script;11294.0;45;;
2019-02-05 12:20:38;Biggest Blending;Apache 2.0;https://www.kaggle.com/ashishpatel26/lb-3-687-truncated-mean;0.7;['lightgbm', 'sklearn'];['ner', 'gbm', 'rl', 'nn', 'ann'];[];https://www.kaggle.com/c/elo-merchant-category-recommendation;0.747;0.497;2020-12-12 19:19:32;multiple data sources;['gpu, ensembling'];[LB 3.687] Truncated Mean;Python notebook;9594.0;97;3.61423;3.68801
2018-12-11 07:21:15;Missing Value Exploration;Apache 2.0;https://www.kaggle.com/frtgnn/elo-eda-lgbm;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ai', 'gbm', 'rl', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/elo-merchant-category-recommendation;0.712;0.504;2020-12-12 19:19:32;Elo Merchant Category Recommendation;['beginner, exploratory data analysis, xgboost'];ELO eda + lgbm;Python notebook;3991.0;106;3.62924;3.70658
2019-01-25 00:36:12;;Apache 2.0;https://www.kaggle.com/gpreda/elo-world-high-score-without-blending;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['filter', 'regression', 'train', 'model', 'deep learning', 'label', 'gradient boosting', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/elo-merchant-category-recommendation;0.739;0.507;2020-12-12 19:19:32;Elo Merchant Category Recommendation;['feature engineering, data cleaning, regression, +1 moregradient boosting'];elo_world_high_score_without_blending;Python script;7752.0;110;3.62135;3.69682
2019-02-04 00:36:10;;Apache 2.0;https://www.kaggle.com/kailex/r-eda-for-elo-ensemble-learning;1.0;['pattern', 'xgboost', 'lightgbm', 'keras'];['ner', 'ai', 'gbm', 'rl', 'nn', 'ml'];['filter', 'regression', 'generation', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'recommend', 'random forest'];https://www.kaggle.com/c/elo-merchant-category-recommendation;0.738;0.522;2020-12-12 19:19:32;Elo Merchant Category Recommendation;['beginner, data visualization, exploratory data analysis, +2 morefeature engineering, regression'];R EDA for ELO + Ensemble learning;Rmarkdown script;7520.0;136;3.62007;3.69538
2018-12-12 01:48:38;;Apache 2.0;https://www.kaggle.com/kailex/tidy-elo-starter-3-70;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['filter', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/elo-merchant-category-recommendation;0.735;0.5;2020-12-12 19:19:32;Elo Merchant Category Recommendation;['feature engineering, data cleaning, regression, +1 morexgboost'];tidy_elo_starter [3.70];R script;6978.0;100;3.62008;3.70096
2019-01-26 11:24:09;;Apache 2.0;https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['filter', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/elo-merchant-category-recommendation;0.768;0.566;2020-12-12 19:19:31;Elo Merchant Category Recommendation;[];Simple LightGBM without blending;Python script;16976.0;262;3.61748;3.69199
2019-02-11 21:14:38;Different time points for card observations;Apache 2.0;https://www.kaggle.com/raddar/card-id-loyalty-different-points-in-time;0.5;[];['ai'];['train'];https://www.kaggle.com/c/elo-merchant-category-recommendation;0.706;0.507;2020-12-12 19:19:32;Elo Merchant Category Recommendation;[];card_id loyalty - different points in time;Python notebook;3502.0;111;;
2019-02-11 01:08:09;This is going to be epic... sit back, relax and enjoy!;Apache 2.0;https://www.kaggle.com/raddar/target-true-meaning-revealed;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['train', 'model', 'predict'];https://www.kaggle.com/c/elo-merchant-category-recommendation;0.764;0.603;2020-12-12 19:19:31;Elo Merchant Category Recommendation;[];target - true meaning revealed!;Python notebook;15159.0;472;;
2019-02-07 22:08:25;Digging deeper into anonymizationIt is clear that some features, and the target itself is somehow obfuscated. Well, guess what - I really like working with these kind of puzzles - lookup BNP Paribas competition in Kaggle :). And it was quite obvious for me that same tricks could be applied here as well (at least at some level). I am going to share what I found in this competition so far.;Apache 2.0;https://www.kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights;0.5;[];['ai'];['train', 'predict'];https://www.kaggle.com/c/elo-merchant-category-recommendation;0.75;0.569;2020-12-12 19:19:31;Elo Merchant Category Recommendation;[];Towards de-anonymizing the data! Some insights;Python notebook;10409.0;272;;
2020-10-05 16:55:47;"train.csv has six columns:  user, event, invited, timestamp, interested, and not_interested.  Test.csv contains the same columns as train.csv, except for interested and not_interested. Each row corresponds to an event that was shown to a user in our application.  event is an id identifying an event in a our system.  user is an id representing a user in our system.  invited is a binary variable indicated whether the user has been invited to the event. timestamp is a ISO-8601 UTC time string representing the approximate time (+/- 2 hours) when the user saw the event in our application. interested is a binary variable indicating whether a user clicked on the ""Interested"" button for this event; it is 1 if the user clicked Interested and 0 if the user did not click the button.  Similarly, not_interested is a binary variable indicating whether a user clicked on the ""Not Interested"" button for this event; it is 1 if the user clicked the button and 0 if not.  It is possible that the user saw an event and clicked neither Interested nor Not Interested, and hence there are rows that contain 0,0 as values for interested,not_interested.";Apache 2.0;https://www.kaggle.com/tombalu/event-recommendation-mldm-eda;0.5;[];['ai', 'nn', 'rl'];['train', 'recommend', 'label'];https://www.kaggle.com/c/event-recommendation-engine-challenge;0.551;0.0;2020-12-12 19:19:39;Event Recommendation Engine Challenge;[];event-recommendation-mldm-EDA;Python notebook;207.0;0;;
2018-09-22 15:53:12;Data Preprocessing on Expedia Hotel Dataset;Apache 2.0;https://www.kaggle.com/ajay1216/practical-guide-on-data-preprocessing-in-python;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'test data'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.737;0.367;2020-12-12 19:25:28;Expedia Hotel Recommendations;['data visualization, exploratory data analysis, feature engineering, +1 moredata cleaning'];Practical Guide on Data Preprocessing in Python;Python notebook;7359.0;20;;
2016-04-16 22:32:35;;Apache 2.0;https://www.kaggle.com/ccccat/r-version-of-most-popular-local-hotel;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.754;0.362;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];R version of most popular local hotel;R script;11557.0;19;0.30098;0.30087
2016-04-29 19:54:39;;Apache 2.0;https://www.kaggle.com/chipmonkey/channel-is-different-test-v-train;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.721;0.379;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Channel is different (test v train);R script;4920.0;23;;
2016-06-08 21:19:53;;Apache 2.0;https://www.kaggle.com/company/ehr-1;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.779;0.387;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];EHR_1;Python script;23504.0;25;;
2016-05-05 18:01:18;;Apache 2.0;https://www.kaggle.com/domesc/explore-data;0.5;[];['ai', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.757;0.455;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];explore_data;Python notebook;12324.0;56;;
2016-05-04 02:17:42;"This notebook shows a ""most popular local hotel"" benchmark implemented with pandas. Read the train dataRead in the train data using only the necessary columns.  Specifying dtypes helps reduce memory requirements. The file is read in chunks of 1 million rows each. In each chunk we count the number of rows and number of bookings for every destination-hotel cluster combination.";Apache 2.0;https://www.kaggle.com/dvasyukova/predict-hotel-type-with-pandas;0.5;[];['ai', 'nn'];['train', 'label', 'test data', 'predict'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.802;0.51;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Predict hotel type with pandas;Python notebook;48909.0;115;0.30325;0.30340
2016-05-14 18:35:34;Doing this type of analysis is against the competition rulesIt has been pointed out by the competition admin that incorporating external data about distances between cities is against the rules. So please don't use this in any way for building your models! The locations puzzleExpedia presented us with a dataset where countries and cities are hidden behind integer codes. Is it possible to find out which city is which? Let's grab our pandas and find out :). Read in a few lines to get a list of columns.;Apache 2.0;https://www.kaggle.com/dvasyukova/the-locations-puzzle;0.5;[];['ai', 'nn', 'ann', 'rl'];['train', 'fitting', 'model'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.737;0.403;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];The locations puzzle;Python notebook;7362.0;30;;
2016-06-06 16:51:16;;Apache 2.0;https://www.kaggle.com/gaborfodor/last-week-xkcd;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'label', 'recommend', 'classification'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.705;0.362;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Last week xkcd;Python script;3410.0;19;;
2016-05-05 15:53:01;Latent Destination FeaturesLet's have a look at the latent search region features. It won't help you to boost your score immediately although you might gain a few ideas how to apply dimensionality reduction. I just wanted to play with seaborn a bit.;Apache 2.0;https://www.kaggle.com/gaborfodor/latent-destination-features;0.5;[];['dl', 'ai', 'nn'];['label', 'clustering'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.714;0.281;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Latent Destination Features;Python notebook;4247.0;8;;
2016-04-22 11:44:52;Checking the Train & Test periods;Apache 2.0;https://www.kaggle.com/gaborfodor/time-dimension;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.695;0.292;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Time Dimension;Python notebook;2731.0;9;;
2016-04-15 22:34:33;;Apache 2.0;https://www.kaggle.com/jiweiliu/most-popular-local-hotels;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.72;0.281;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];most popular local hotels;Python script;4892.0;8;0.30025;0.30014
2016-04-23 00:47:32;;Apache 2.0;https://www.kaggle.com/josealberto/destination-clusters;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.719;0.311;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Destination clusters;R script;4786.0;11;;
2016-04-28 15:35:31;;Apache 2.0;https://www.kaggle.com/omarelgabry/explore-expedia-search-data;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'ann', 'cv'];['machine learning', 'regression', 'train', 'model', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.752;0.379;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Explore Expedia Search Data;Python notebook;10867.0;23;0.30348;0.30366
2016-04-24 09:53:37;;Apache 2.0;https://www.kaggle.com/signochastic/apr-23;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.737;0.281;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Apr_23;R script;7356.0;8;0.37783;0.37815
2016-05-20 19:28:59;;Apache 2.0;https://www.kaggle.com/sionek/simple-validation;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.714;0.281;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Simple Validation;Python script;4229.0;8;0.49577;0.49729
2016-04-16 00:01:11;;Apache 2.0;https://www.kaggle.com/triskelion/most-popular-local-hotels;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.729;0.371;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];most popular local hotels;Python script;5971.0;21;0.30096;0.30086
2016-05-27 01:56:15;This Notebook analyses trends of hotel bookings and clickswe will start with reading the data, leaving just necessary columns, aggregating it to the day level and dropping the original dataframe;Apache 2.0;https://www.kaggle.com/vykhand/untitled;0.5;[];['ai', 'dl'];['train', 'model', 'label'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.719;0.319;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Interactive booking trends;Python notebook;4762.0;12;;
2016-04-19 18:58:19;I am going to show how order matters in MAP@K when there is only 1 answer.This experiment is done by calculating AP@K, which gives 1 value. MAP@K is the average of AP@K.;Apache 2.0;https://www.kaggle.com/wendykan/map-k-demo;0.5;[];['ml', 'rl'];['predict'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.77;0.488;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];MAP@K demo;Python notebook;18147.0;86;;
2016-05-10 08:25:42;;Apache 2.0;https://www.kaggle.com/zfturbo/leakage-solution;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/expedia-hotel-recommendations;0.799;0.522;2020-12-12 19:25:28;Expedia Hotel Recommendations;[];Leakage solution;Python script;43534.0;136;0.49476;0.49654
2020-03-26 11:55:46;;Apache 2.0;https://www.kaggle.com/kari554/tfugday4;1.0;['sklearn'];['ai', 'nn'];['train', 'label', 'k-means', 'regression'];https://www.kaggle.com/c/expedia-personalized-sort;0.56;0.0;2020-12-12 19:25:53;Personalize Expedia Hotel Searches - ICDM 2013;[];tfugDay4;Python notebook;239.0;0;;
2019-10-14 12:02:18;Load Libraries;Apache 2.0;https://www.kaggle.com/nikitsoftweb/production-time-series-of-price-anomaly-detection;1.0;['sklearn'];['ner', 'ai', 'rl', 'nn', 'ann'];['anomaly detection', 'filter', 'train', 'model', 'clustering', 'label', 'k-means', 'predict', 'supervised learning'];https://www.kaggle.com/c/expedia-personalized-sort;0.698;0.214;2020-12-12 19:25:53;Personalize Expedia Hotel Searches - ICDM 2013;['gpu'];Production Time Series of Price Anomaly Detection ;Python notebook;2906.0;4;;
2020-07-18 05:45:54;;Apache 2.0;https://www.kaggle.com/raghavbang/feature-engineering-and-k-means;1.0;['sklearn'];['ai'];['train', 'label', 'k-means', 'predict'];https://www.kaggle.com/c/expedia-personalized-sort;0.587;0.311;2020-12-12 19:25:53;Personalize Expedia Hotel Searches - ICDM 2013;[];Feature Engineering and K means;Python notebook;361.0;11;;
2020-08-04 19:24:38;;Apache 2.0;https://www.kaggle.com/vishalkasa/feature-engineering-k-means;1.0;['sklearn'];['ai'];['train', 'label', 'predict'];https://www.kaggle.com/c/expedia-personalized-sort;0.523;0.236;2020-12-12 19:25:53;Personalize Expedia Hotel Searches - ICDM 2013;[];Feature Engineering[K means];Python notebook;139.0;5;;
2019-10-11 17:12:31;;Apache 2.0;https://www.kaggle.com/yoheiii/for-yota;1.0;['lightgbm', 'sklearn'];['ai', 'gbm'];['train', 'model', 'label', 'filter'];https://www.kaggle.com/c/expedia-personalized-sort;0.571;0.0;2020-12-12 19:25:53;Personalize Expedia Hotel Searches - ICDM 2013;[];for yota;Python notebook;283.0;0;;
2020-12-03 16:31:28;Data Preprocessing;Apache 2.0;https://www.kaggle.com/anonymous55/ml-project-final;1.0;['pattern', 'sklearn', 'nltk'];['ner', 'ai', 'rl', 'nn', 'ml'];['training data', 'regression', 'train', 'model', 'supervised learning', 'gradient descent', 'loss', 'label', 'k-means', 'clustering', 'logistic regression', 'predict', 'unsupervised learning', 'classification'];https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;0.502;0.0;2020-12-12 19:26:25;Facebook Recruiting III - Keyword Extraction;[];ML Project Final;Python notebook;103.0;0;;
2020-07-13 11:11:54;;Apache 2.0;https://www.kaggle.com/ashishkkumar/kernel3fae69652f;1.0;['sklearn'];['ai', 'nn', 'rl'];['train', 'model', 'predict'];https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;0.403;0.0;2020-12-12 19:26:25;Facebook Recruiting III - Keyword Extraction;[];kernel3fae69652f;Python notebook;30.0;0;;
2020-06-18 19:02:46;1) Libraries Used;Apache 2.0;https://www.kaggle.com/chandanmalla/stackoverflow-multi-label-tag-prediction;1.0;['sklearn', 'nltk'];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'training data', 'train', 'fitting', 'model', 'test data', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;0.618;0.099;2020-12-12 19:26:25;Facebook Recruiting III - Keyword Extraction;['beginner, exploratory data analysis, nlp, +1 moremulticlass classification'];StackOverFlow Multi label Tag Prediction;Python notebook;607.0;1;;
2020-06-14 09:01:01;Stack Overflow: Tag Prediction;Apache 2.0;https://www.kaggle.com/f13rc3/multi-label-classification-for-tag-prediction;1.0;['vocabulary', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'training data', 'train', 'test data', 'model', 'loss', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;0.579;0.099;2020-12-12 19:26:25;Facebook Recruiting III - Keyword Extraction;[];Multi_Label Classification For Tag prediction;Python notebook;319.0;1;;
2020-07-05 21:09:08;Stack Exchange - Tag Classifier;Apache 2.0;https://www.kaggle.com/l0new0lf/tag-classifier;1.0;['vocabulary', 'sklearn'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'train', 'fitting', 'model', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;0.565;0.214;2020-12-12 19:26:25;Facebook Recruiting III - Keyword Extraction;[];Tag-Classifier;Python notebook;257.0;4;;
2020-08-18 11:23:07;Removing Duplicates;Apache 2.0;https://www.kaggle.com/reintegrated/1-stackoverflow-eda-and-sampling;1.0;['pattern', 'sklearn'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'layer', 'label', 'random forest'];https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;0.527;0.0;2020-12-12 19:26:25;Facebook Recruiting III - Keyword Extraction;[];stackoverflow tag prediction;Python notebook;147.0;0;;
2020-08-24 10:03:31;;Apache 2.0;https://www.kaggle.com/reintegrated/4-stackoverflow-training;1.0;['sklearn'];['ai', 'dl', 'ml', 'nn', 'ann'];['train', 'model', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;0.49;0.0;2020-12-12 19:26:25;multiple data sources;[];4. StackOverflow training;Python notebook;88.0;0;;
2020-05-06 15:45:14;StackOverflow tag prediction;Apache 2.0;https://www.kaggle.com/sumantindurkhya/stackoverflow-tag-prediction-using-search-title;1.0;['pattern', 'sklearn', 'nltk'];['ner', 'ai', 'nn', 'rl'];['regression', 'train', 'model', 'loss', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction;0.622;0.236;2020-12-12 19:26:25;Facebook Recruiting III - Keyword Extraction;[];StackOverflow tag prediction using search title;Python notebook;652.0;5;;
2016-06-08 16:26:52;As you can see, there are a huge number of place ids. This means that any algorithm which trains using a one vs all approach won't work on this dataset (unless of course you're willing to train 100k models). That combined with the extremely low input dimensionality should make this an interesting competition, indeed :);Apache 2.0;https://www.kaggle.com/anokas/quick-look-at-the-data;0.5;[];['ai', 'nn', 'ml'];['train', 'model', 'label', 'training data'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.753;0.418;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Quick look at the data;Python notebook;11077.0;36;;
2016-06-11 00:52:41;;Apache 2.0;https://www.kaggle.com/apapiu/random-forest-on-a-few-blocks;1.0;['xgboost'];['ai', 'nn', 'ml', 'rl'];['filter', 'machine learning', 'random forest', 'train', 'model', 'supervised learning', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.79;0.538;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Random Forest And KNN on a few blocks;Rmarkdown script;33332.0;170;;
2016-05-13 07:22:07;Kaggle - Facebook recruiting;Apache 2.0;https://www.kaggle.com/beyondbeneath/data-exploration-and-visualisations;0.5;[];['ai', 'nn', 'ann'];['train', 'label', 'predict'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.757;0.486;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Data exploration and visualisations;Python notebook;12596.0;83;;
2016-05-25 22:27:15;;Apache 2.0;https://www.kaggle.com/chistyakov/simple-r;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['filter', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.722;0.327;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];R Script;R script;5069.0;13;0.50537;0.50568
2016-06-27 09:05:16;;Apache 2.0;https://www.kaggle.com/chopra/logistic-regression-to-find-knn-weights;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['regression', 'train', 'model', 'deep learning', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.722;0.319;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Logistic Regression to find KNN weights;Python script;5057.0;12;;
2016-05-13 01:20:00;;Apache 2.0;https://www.kaggle.com/jsab16/on-time;0.5;[];['ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.702;0.327;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];On Time;Python notebook;3181.0;13;;
2016-05-19 03:24:25;;Apache 2.0;https://www.kaggle.com/jturkewitz/mean-x-y-over-time-days;0.5;[];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.704;0.346;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Mean X,Y over time (days);R script;3342.0;16;;
2016-05-13 11:06:18;JSab and Michael Campos provided very detailed analysis on the time data. Here I would like to share my method, and I think it confirms their conclusion that the unit of the time is minute in this dataset. The method I used to figure out the time definition is through Fourier transform.;Apache 2.0;https://www.kaggle.com/leonlu/another-way-to-know-the-time-definition;0.5;[];['ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.732;0.449;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Another Way to Know the Time Definition;Python notebook;6570.0;52;;
2016-05-12 03:46:06;;Apache 2.0;https://www.kaggle.com/msjgriffiths/exploratory-data-analysis;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['filter', 'train', 'model', 'predict', 'rank'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.771;0.515;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Exploratory Data Analysis;Rmarkdown script;18704.0;123;;
2016-06-20 19:20:32;;Apache 2.0;https://www.kaggle.com/nigelhenry/accuracy-explained;0.5;[];['ai', 'nn'];['filter', 'train', 'label', 'k-means', 'clustering'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.698;0.334;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Accuracy Explained;Rmarkdown script;2931.0;14;;
2016-06-19 06:45:59;;Apache 2.0;https://www.kaggle.com/overfit/grid-knn;1.0;['sklearn'];['ner', 'ai', 'nlp', 'nn', 'ml'];['test data', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.733;0.334;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];grid_knn;Python script;6627.0;14;0.57125;0.57140
2016-05-19 08:39:15;;Apache 2.0;https://www.kaggle.com/peatle/visualise-check-ins-over-time-gif;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.738;0.429;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Visualise check-ins over time (gif);R script;7544.0;41;;
2017-05-19 02:25:33;;Apache 2.0;https://www.kaggle.com/rsakata/3rd-place-solution-simple-version;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.765;0.453;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];3rd Place Solution (simple version);R script;15790.0;55;0.07103;0.07111
2016-05-19 22:03:56;;Apache 2.0;https://www.kaggle.com/sakvaua/animated-check-ins;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.686;0.371;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Animated check-ins;Python script;2243.0;21;;
2016-06-08 17:52:25;;Apache 2.0;https://www.kaggle.com/svpons/grid-knn;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['test data', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.753;0.427;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];grid_knn;Python script;11293.0;40;0.56713;0.56735
2016-05-31 17:00:58;;Apache 2.0;https://www.kaggle.com/svpons/grid-plus-classifier;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['training data', 'test data', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.739;0.397;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];grid_plus_classifier;Python script;7675.0;28;0.02281;0.02281
2016-05-27 08:19:22;;Apache 2.0;https://www.kaggle.com/valeriur/python-starter-0-55;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['regression', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.704;0.311;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];python starter ~0.55;Python script;3370.0;11;;
2016-06-30 08:17:27;;Apache 2.0;https://www.kaggle.com/zeroblue/mad-scripts-battle-z;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['filter', 'train', 'validation data', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.73;0.375;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Mad Scripts Battle Z;Python script;6181.0;22;;
2016-05-13 13:47:21;;Apache 2.0;https://www.kaggle.com/zfturbo/mad-scripts-battle;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/facebook-v-predicting-check-ins;0.729;0.375;2020-12-12 19:33:25;Facebook V: Predicting Check Ins;[];Mad scripts battle;Python script;6100.0;22;0.45835;0.45843
2019-10-11 15:37:55;Stack Overflow: Tag Prediction;Apache 2.0;https://www.kaggle.com/ajaysh/stackoverflow-tag-prediction;1.0;['vocabulary', 'sklearn', 'nltk'];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'training data', 'train', 'test data', 'model', 'loss', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/FacebookRecruiting;0.634;0.236;2020-12-12 19:33:39;Facebook Recruiting Competition;[];Stackoverflow Tag Prediction;Python notebook;812.0;5;;
2020-02-01 19:43:23;;Apache 2.0;https://www.kaggle.com/curioso/link-prediction-facebook;0.5;[];['ner', 'ai', 'nn'];['train', 'label', 'predict'];https://www.kaggle.com/c/FacebookRecruiting;0.528;0.0;2020-12-12 19:33:39;Facebook Recruiting Competition;[];kernel2c5bafde5b;Python notebook;148.0;0;;
2019-03-26 15:27:55;Benchmark models;Apache 2.0;https://www.kaggle.com/aparajit0511/facial-keypoint-detection-udacity;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'output layer', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'convolutional neural network'];https://www.kaggle.com/c/facial-keypoints-detection;0.73;0.367;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu'];Facial Keypoint Detection Udacity;Python notebook;6158.0;20;;
2019-09-16 18:52:17;Data pipeline;Apache 2.0;https://www.kaggle.com/chaitanyagarikipati/facial-keypoints-detection-tensorflow-cnn;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/facial-keypoints-detection;0.792;0.253;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu'];Facial keypoints Detection Tensorflow CNN;Python notebook;34998.0;6;;
2018-10-30 14:30:07;;Apache 2.0;https://www.kaggle.com/gakshaygupta/real-time-cnn-architecture;1.0;['tensorflow', 'sklearn', 'keras', 'mxnet'];['ai', 'dl', 'rl', 'cv', 'nn'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'predict', 'relu'];https://www.kaggle.com/c/facial-keypoints-detection;0.671;0.268;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu, deep learning, cnn'];real time cnn architecture;Python notebook;1637.0;7;;
2019-11-06 16:12:17;Model;Apache 2.0;https://www.kaggle.com/liudmyla/easy-keras-facial-keypoint-detection;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'nn', 'ml'];['predict', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/facial-keypoints-detection;0.698;0.352;2020-12-12 19:34:30;multiple data sources;['gpu'];Easy keras facial keypoint detection;Python notebook;2938.0;17;2.76011;2.90900
2018-09-05 19:50:26;;Apache 2.0;https://www.kaggle.com/madhawav/basic-fully-connected-nn;1.0;['tensorflow', 'keras'];['ai'];['predict', 'training data', 'test data', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/facial-keypoints-detection;0.733;0.383;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu'];Basic Fully Connected NN;Python notebook;6681.0;24;;
2020-08-05 07:18:23;;Apache 2.0;https://www.kaggle.com/mannsingh/facial-keypoints;1.0;['tensorflow', 'keras'];['dl', 'ner', 'ai', 'nn'];['predict', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/facial-keypoints-detection;0.576;0.188;2020-12-12 19:34:30;Facial Keypoints Detection;[];facial_keypoints;Python notebook;305.0;3;;
2019-01-30 03:25:08;Test set;Apache 2.0;https://www.kaggle.com/mirodil/facial-keypoints-detection;1.0;['sklearn'];['ner', 'ai', 'dl', 'nn', 'ml'];['regression', 'train', 'model', 'epoch', 'label', 'loss', 'relu'];https://www.kaggle.com/c/facial-keypoints-detection;0.65;0.214;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu'];Facial Keypoints Detection;Python notebook;1097.0;4;3.51523;3.61818
2019-05-15 14:07:54;Load Data;Apache 2.0;https://www.kaggle.com/nitron/facial-keypoints-fastai-image-regression;1.0;['pytorch'];['ner', 'ai', 'dl', 'cnn', 'nn', 'ml'];['predict', 'train', 'model', 'epoch', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/facial-keypoints-detection;0.709;0.311;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu'];Facial Keypoints: Fastai image regression;Python notebook;3750.0;11;2.01369;2.17431
2020-08-17 16:54:14;1. Data extraction;Apache 2.0;https://www.kaggle.com/obione26/facial-keypoints-detection-keras-albumentations;1.0;['sklearn', 'albumentations', 'tensorflow', 'pattern', 'keras'];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['filter', 'training data', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/facial-keypoints-detection;0.608;0.311;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu, beginner, data visualization, +2 moredeep learning, data cleaning'];Facial Keypoints Detection - Keras+Albumentations;Python notebook;510.0;11;1.88670;2.05535
2020-08-14 20:30:12;Modeling;Apache 2.0;https://www.kaggle.com/sshikamaru/keras-cnn-starter;1.0;['tensorflow', 'keras'];['dl', 'ner', 'ai', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/facial-keypoints-detection;0.591;0.319;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu, beginner, cnn'];Keras CNN Starter;Python notebook;387.0;12;3.15420;3.43914
2019-04-16 00:40:03;Preparing the training data;Apache 2.0;https://www.kaggle.com/utkarsh4430/facial-keypoints-detection-basic-keras-model;1.0;['tensorflow', 'sklearn', 'keras'];['dl', 'ner', 'ai', 'nn'];['predict', 'training data', 'test data', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/facial-keypoints-detection;0.709;0.302;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu'];Facial Keypoints Detection- Basic Keras Model;Python notebook;3774.0;10;1.98535;2.19286
2020-07-26 08:22:48;Reading the input file;Apache 2.0;https://www.kaggle.com/vinodhb95/charlie-version2;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'nn'];['filter', 'train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/facial-keypoints-detection;0.525;0.214;2020-12-12 19:34:30;Facial Keypoints Detection;['gpu'];Charlie_version2;Python notebook;142.0;4;2.00888;2.27832
2017-12-09 06:08:42;;Apache 2.0;https://www.kaggle.com/ambarish/grocery-eda-dirty-xgboost-arima-ets-prophet;1.0;['xgboost'];['ner', 'ai', 'gbm', 'cv', 'nn', 'ml'];['filter', 'test data', 'train', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.746;0.449;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;['exploratory data analysis, xgboost'];Grocery EDA  Dirty XGBoost, Arima,ETS,Prophet;Rmarkdown script;9233.0;52;;
2017-11-12 18:32:12;;Apache 2.0;https://www.kaggle.com/captcalculator/a-very-extensive-favorita-exploratory-analysis;0.5;[];['ai', 'nn', 'rl'];['train', 'label', 'training data'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.75;0.502;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;['data visualization, exploratory data analysis'];A Very Extensive Favorita Exploratory Analysis;Rmarkdown script;10186.0;103;;
2017-12-02 17:08:05;;Apache 2.0;https://www.kaggle.com/ceshine/lgbm-starter;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.777;0.515;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];LGBM Starter;Python script;22442.0;124;0.54038;0.52993
2017-10-23 03:36:27;;Apache 2.0;https://www.kaggle.com/ceshine/mean-baseline-lb-59;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.735;0.452;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];Mean Baseline (LB ~ .59);Python script;7053.0;54;;
2019-01-15 23:10:09;;Apache 2.0;https://www.kaggle.com/headsortails/shopping-for-insights-favorita-eda;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'reward', 'label', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.807;0.612;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;['beginner, data visualization, exploratory data analysis'];Shopping for Insights - Favorita EDA;Rmarkdown script;58634.0;548;;
2017-11-20 09:58:39;The Problem:The training data, train.csv is a large file (~5 GB) - this can be problematic if you have relatively low RAM (8 GB).The Solution:Set low_memory=True in Pandas' read_csvOn a machine with relatively low RAM, attempting to load the entire file in a pandas DataFrame can lead to failure caused by running out of memory.  One way of fixing this issue is to make use of the low_memory=True argument of read_csv.  With this method, the csv file is processed in chunks requiring lower memory usage, while at the same time reading the csv's contents into a single DataFrame. But the Jupyter kernel still keeps restarting even with low_memory=True....why?The dtypes of the columns of the DataFrame must be specified in read_csv if we wish to set low_memory=True.  This is because not specifying dtypes forces pandas to guess column dtypes - which is a memory-intensive task.  Please see this Stack Overflow answer for a additional explanation: https://stackoverflow.com/a/27232309 The Complete SolutionWe first create a new file called small_train.csv using only the first row of data from train.csv:;Apache 2.0;https://www.kaggle.com/kunalkotian/easily-load-train-csv-w-o-crash-save-feather-file;0.5;[];['ai', 'dl', 'gan', 'nn', 'ann'];['train', 'recommend', 'training data', 'filter'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.75;0.472;2020-12-12 19:41:27;multiple data sources;[];Easily Load train.csv w/o Crash, Save Feather File;Python notebook;10253.0;70;;
2017-12-02 13:12:25;;Apache 2.0;https://www.kaggle.com/paulorzp/log-ma-and-days-of-week-means-lb-0-529;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.766;0.529;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];Log MA and  Days of Week Means (LB: 0.529);Python script;15935.0;151;;
2018-01-16 00:53:36;;Apache 2.0;https://www.kaggle.com/paulorzp/log-ma-with-special-days-lb-0-529;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.702;0.413;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];Log MA with special days (LB:0.529);Python script;3169.0;34;;
2017-11-04 20:09:06;;Apache 2.0;https://www.kaggle.com/paulorzp/log-moving-averages-forecasting-lb-0-546;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.736;0.486;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];Log Moving Averages Forecasting (LB=0.546);Python script;7111.0;83;;
2017-10-22 16:53:24;;Apache 2.0;https://www.kaggle.com/paulorzp/one-line-median-lb-0-650;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.691;0.416;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];One Line Median (LB 0.650);Python script;2490.0;35;;
2017-12-27 10:55:10;;Apache 2.0;https://www.kaggle.com/senkin13/lstm-starter;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'lstm', 'predict', 'classification'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.761;0.469;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];LSTM Starter;Python script;13885.0;67;;
2018-01-16 15:21:10;;Apache 2.0;https://www.kaggle.com/shixw125/1st-place-lgb-model-public-0-506-private-0-511;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.786;0.528;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];1st Place LGB Model(public:0.506, private:0.511);Python script;29231.0;148;;
2018-01-16 15:20:53;;Apache 2.0;https://www.kaggle.com/shixw125/1st-place-nn-model-public-0-507-private-0-513;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.774;0.473;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];1st Place NN Model(public:0.507, private:0.513);Python script;20299.0;71;;
2018-05-15 10:53:55;Table of Content:;Apache 2.0;https://www.kaggle.com/thepathofd/a-first-kaggle-part-1-forecasting-store-47;1.0;['statsmodels', 'sklearn', 'pattern'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'fitting', 'model', 'supervised learning', 'clustering', 'loss', 'label', 'predict', 'unsupervised learning', 'classification'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.739;0.469;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;['data visualization'];A first Kaggle - Part 1 - Forecasting store #47;Python notebook;7665.0;67;;
2018-01-06 18:16:28;;Apache 2.0;https://www.kaggle.com/tunguz/lgbm-one-step-ahead-c8de0f;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.732;0.427;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];LGBM One Step Ahead c8de0f;Python script;6539.0;40;0.52162;0.51354
2017-12-25 23:36:52;;Apache 2.0;https://www.kaggle.com/vrtjso/ensemble-of-public-kernels;0.5;[];['ner', 'ai', 'dl', 'gbm', 'nlp', 'nn'];['training data', 'test data', 'train', 'model', 'deep learning', 'label', 'k-means', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.712;0.433;2020-12-12 19:41:27;multiple data sources;[];Ensemble of Public Kernels;Python script;4032.0;43;0.52778;0.51427
2017-12-18 07:39:21;;Apache 2.0;https://www.kaggle.com/vrtjso/lgbm-one-step-ahead;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/favorita-grocery-sales-forecasting;0.753;0.503;2020-12-12 19:41:27;CorporaciÃ³n Favorita Grocery Sales Forecasting;[];LGBM One Step Ahead;Python script;11090.0;105;;
2020-10-22 17:04:24;;Apache 2.0;https://www.kaggle.com/stephenfenel/chess-elo-predictor;1.0;['xgboost', 'sklearn'];['ai', 'nn'];['test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/finding-elo;0.52;0.0;2020-12-12 19:41:35;Finding Elo;[];Chess ELO predictor ;Python notebook;133.0;0;;
2015-07-23 15:08:04;;Apache 2.0;https://www.kaggle.com/abhishek/rf-xgboost-example-0-982253;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'nlp', 'nn'];['test data', 'random forest', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.68;0.319;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];RF + XGBoost Example (0.982253);Python script;1977.0;12;0.98367;0.98325
2015-08-20 04:50:23;;Apache 2.0;https://www.kaggle.com/benhamner/exploring-the-cern-lhcb-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['test data', 'random forest', 'train', 'deep learning', 'layer', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.71;0.311;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Exploring the CERN LHCB Data;Rmarkdown script;3793.0;11;;
2015-07-30 00:12:26;;Apache 2.0;https://www.kaggle.com/benhamner/flatness-boosting-example;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['predict', 'test data', 'train', 'deep learning', 'label', 'loss', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.705;0.302;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Flatness Boosting Example;Python script;3421.0;10;0.98072;0.97951
2015-08-05 00:55:23;;Apache 2.0;https://www.kaggle.com/benhamner/rf-xgboost-example;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'nlp', 'nn'];['test data', 'random forest', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.763;0.411;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];RF + XGBoost Example (0.982253);Python script;14725.0;33;0.98270;0.98225
2015-07-29 16:08:55;;Apache 2.0;https://www.kaggle.com/domcastro/rf-xgboost-keras;1.0;['xgboost', 'sklearn', 'keras', 'theano'];['ner', 'ai', 'gbm', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'random forest', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.672;0.236;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];RF + XGBoost + keras .983402;Python script;1704.0;5;;
2015-08-29 02:09:50;;Apache 2.0;https://www.kaggle.com/domcastro/rf-xgboost-keras-flatline;1.0;['xgboost', 'sklearn', 'keras', 'theano'];['ner', 'ai', 'gbm', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'random forest', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.714;0.268;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];RF + XGBoost + keras + flatline;Python script;4242.0;7;0.98391;0.98307
2016-01-02 19:25:25;;Apache 2.0;https://www.kaggle.com/fchollet/keras-starter-code-deep-pyramidal-mlp;1.0;['keras', 'sklearn', 'theano'];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.745;0.357;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Keras starter code: deep pyramidal MLP;Python script;9090.0;18;0.00000;0.00000
2015-09-15 23:24:17;;Apache 2.0;https://www.kaggle.com/gramolin/histograms;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/flavours-of-physics;0.658;0.281;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Histograms;Python script;1271.0;8;;
2015-07-28 23:51:27;;Apache 2.0;https://www.kaggle.com/harshaneel/check-you-agreement-correlation-and-roc;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'nlp', 'nn', 'ann'];['test data', 'random forest', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.69;0.281;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Check you Agreement, Correlation and ROC;Python script;2455.0;8;;
2015-10-13 11:10:18;;Apache 2.0;https://www.kaggle.com/holzner/candidate-mass;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'label', 'loss', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.625;0.236;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];candidate mass distributions;Python script;687.0;5;;
2015-10-17 06:56:58;;Apache 2.0;https://www.kaggle.com/josefslavicek/simplified-version-of-my-solution;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'cv', 'ml', 'nlp', 'nn', 'ann'];['filter', 'test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.733;0.253;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Simplified version of 3rd place solution;Python script;6710.0;6;0.99969;0.99976
2015-10-13 00:18:11;;Apache 2.0;https://www.kaggle.com/justfor/gridsearchcv-with-feature-in-xgboost;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'ml', 'nlp', 'nn', 'ann'];['filter', 'test data', 'random forest', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.704;0.214;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Ensemble Boost;Python script;3343.0;4;;
2015-09-11 22:21:32;;Apache 2.0;https://www.kaggle.com/karma86/rf-xgboost-keras-flatline-v-2-0;1.0;['xgboost', 'sklearn', 'keras'];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.723;0.281;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];RF + XGBoost + keras + flatline v 2.0;Python script;5221.0;8;;
2015-10-14 22:06:38;;Apache 2.0;https://www.kaggle.com/phunter/gridsearchcv-with-feature-in-xgboost;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'ml', 'nlp', 'nn', 'ann'];['filter', 'test data', 'train', 'fitting', 'model', 'deep learning', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.769;0.375;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];GridSearchCV with feature in xgboost;Python script;17523.0;22;0.98325;0.98130
2015-10-17 23:42:01;;Apache 2.0;https://www.kaggle.com/rakhlin/abcde;1.0;['keras', 'sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.663;0.214;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Get to the top 5 with 75 lines of code;Python script;1402.0;4;;
2015-08-30 18:12:13;;Apache 2.0;https://www.kaggle.com/triskelion/testing-python-3;1.0;['sklearn'];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'recognition', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.727;0.421;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Mapping 'IsoBDT' to 'track_Chi2D';Python script;5704.0;37;;
2015-10-18 21:26:38;;Apache 2.0;https://www.kaggle.com/vicensgaitan/clipping-spreading;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'label', 'predict', 'recommend', 'classification', 'labeled'];https://www.kaggle.com/c/flavours-of-physics;0.706;0.281;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Clipping & Spreading...  ;Rmarkdown script;3540.0;8;;
2015-08-07 09:17:08;;Apache 2.0;https://www.kaggle.com/vicensgaitan/t-sne-embedding-of-tau-3-mu-events;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'fitting', 'classification', 'deep learning'];https://www.kaggle.com/c/flavours-of-physics;0.705;0.281;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];t-SNE Embedding of tau -> 3 mu events;R script;3389.0;8;;
2018-07-12 12:22:00;;Apache 2.0;https://www.kaggle.com/vicensgaitan/why-is-so-hard-to-learn-the-tau-mass;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'nn', 'ann'];['training data', 'random forest', 'train', 'model', 'label', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics;0.717;0.302;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Why is so hard to 'learn' the tau mass ;Rmarkdown script;4536.0;10;;
2015-08-27 19:52:45;;Apache 2.0;https://www.kaggle.com/yashpatel5400/particle-feature-importance;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/flavours-of-physics;0.632;0.214;2020-12-12 19:51:54;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼;[];Particle Feature Importance;Python script;784.0;4;;
2019-08-31 21:41:04;Outline;Apache 2.0;https://www.kaggle.com/abhinav2308/raredecaysearch;1.0;['tensorflow', 'xgboost', 'sklearn'];['ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'loss', 'label', 'predict', 'rank', 'recommend', 'classification', 'labeled'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.5;0.099;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];rareDecaySearch;Python notebook;101.0;1;;
2018-07-21 17:53:34;;Apache 2.0;https://www.kaggle.com/artgor/flavours-of-physics-basic-model;1.0;['lightgbm', 'sklearn'];['ai', 'gbm', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.64;0.236;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];Flavours of Physics: basic model;Python notebook;906.0;5;0.97374;0.97092
2018-08-11 20:32:03;1 . Read file;Apache 2.0;https://www.kaggle.com/ashishpatel26/think-diffrently-e-mc2;1.0;['xgboost'];['ner', 'ai', 'nn', 'ann'];['train', 'model', 'training data', 'predict'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.642;0.327;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);['gpu'];Think Diffrently E=MC2 ðŸ§ ðŸ§ ðŸ§ ;Python notebook;941.0;13;0.98194;0.97907
2019-06-16 15:53:46;;Apache 2.0;https://www.kaggle.com/maliabbas/five-line-model;0.5;[];['ai'];['train', 'predict'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.48;0.0;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];Five Line Model;Python notebook;77.0;0;0.97968;0.97801
2019-06-20 04:39:36;Importing Libraries and Data Loading  We are Using UGradientBoostingClassifier for Task of Classification;Apache 2.0;https://www.kaggle.com/maliabbas/flavours-of-physics;0.5;[];['ner', 'ai', 'dl', 'ml', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'fitting', 'model', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.479;0.0;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];Flavours of Physics;Python notebook;76.0;0;0.99245;0.99248
2019-06-16 15:59:06;;Apache 2.0;https://www.kaggle.com/maliabbas/ugbc-gs;0.5;[];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.509;0.152;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];UGBC GS;Python script;113.0;2;0.99165;0.99158
2018-08-22 15:45:11;;Apache 2.0;https://www.kaggle.com/mik3hall/hep-ml;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'label', 'predict', 'classification', 'labeled'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.633;0.152;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);['gpu'];hep_ml ;Python script;802.0;2;0.99799;0.99827
2018-07-24 13:29:54;;Apache 2.0;https://www.kaggle.com/nafisur/ugbc-gs;0.5;[];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.556;0.188;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);['gpu'];UGBC GS;Python script;223.0;3;0.99134;0.98976
2020-05-02 14:41:35;;Apache 2.0;https://www.kaggle.com/prashantmudgal/lhc-tau-to-muon-decay;1.0;['xgboost', 'sklearn'];['cv', 'ai', 'nn', 'gbm'];['train', 'model', 'random forest', 'predict'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.468;0.099;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];LHC_Tau_to_Muon_Decay;Python notebook;66.0;1;;
2019-06-23 12:36:12;;Apache 2.0;https://www.kaggle.com/rahulbagga/neural-networks-in-keras;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.431;0.0;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];kernel812d35055c;Python notebook;42.0;0;0.97623;0.97444
2020-02-01 15:13:52;;Apache 2.0;https://www.kaggle.com/scirpus/complex-math;1.0;['sklearn'];['ai', 'rl', 'cv', 'nn', 'ann'];['filter', 'train', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.556;0.214;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];Complex Math;Python notebook;224.0;4;;
2018-07-05 18:09:09;;Apache 2.0;https://www.kaggle.com/scirpus/five-line-model;0.5;[];['ai'];['train', 'predict'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.627;0.236;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];Five Line Model;Python notebook;718.0;5;0.97968;0.97801
2020-02-05 15:58:19;;Apache 2.0;https://www.kaggle.com/scirpus/fork-of-complex-math;1.0;['sklearn'];['ai', 'rl', 'cv', 'nn', 'ann'];['filter', 'train', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.522;0.0;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];Fork of Complex Math;Python notebook;136.0;0;0.99133;0.99070
2018-06-28 14:38:22;;Apache 2.0;https://www.kaggle.com/sionek/ugbc-gs;0.5;[];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.636;0.281;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];UGBC GS;Python script;842.0;8;0.99165;0.99158
2018-07-20 13:19:41;Five Lines Model;Apache 2.0;https://www.kaggle.com/skooch/ensemble-with-ugbc;1.0;['lightgbm', 'sklearn'];['ai', 'gbm', 'cv', 'rl', 'ml'];['train', 'model', 'loss', 'label', 'predict', 'rank', 'recommend', 'labeled'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.586;0.099;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];Ensemble with UGBC;Python notebook;355.0;1;0.98634;0.98565
2018-08-23 12:12:15;;Apache 2.0;https://www.kaggle.com/skooch/ugbc-es4;0.5;[];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/flavours-of-physics-kernels-only;0.586;0.099;2020-12-12 19:55:11;Flavours of Physics: Finding Ï„  â†’  Î¼Î¼Î¼ (Kernels Only);[];UGBC ES4;Python script;356.0;1;0.99245;0.99248
2020-11-03 22:28:49;TPU or GPU detection;Apache 2.0;https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['unlabeled', 'machine learning', 'training data', 'test data', 'train', 'model', 'epoch', 'validation data', 'layer', 'vgg', 'loss', 'label', 'predict', 'classification', 'labeled'];https://www.kaggle.com/c/flower-classification-with-tpus;0.786;0.584;2020-12-12 19:56:26;Flower Classification with TPUs;['tpu, deep learning, computer vision, +1 moretensorflow'];Getting started with 100+ flowers on TPU;Python notebook;28854.0;345;;
2018-06-23 08:33:03;;Apache 2.0;https://www.kaggle.com/ambarish/forest-cover-type-eda-and-modelling;1.0;['xgboost'];['ner', 'ai', 'nn'];['filter', 'train', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.667;0.311;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);['exploratory data analysis, xgboost'];Forest Cover Type EDA and Modelling;Rmarkdown script;1529.0;11;;
2018-09-24 23:24:55;;Apache 2.0;https://www.kaggle.com/arjundas/forest-run;0.5;[];['ner', 'ai', 'gbm', 'cv', 'nn'];['test data', 'random forest', 'regression', 'train', 'model', 'logistic regression', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.669;0.362;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);['data visualization, exploratory data analysis, data cleaning, +1 moremulticlass classification'];Forest Run !!!;Rmarkdown script;1586.0;19;0.54261;0.54261
2018-11-03 14:39:03;;Apache 2.0;https://www.kaggle.com/bigkirill/forest-cover-using-catboost-multiclass-classifier;1.0;['catboost', 'sklearn'];['ai'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.727;0.281;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);['gpu'];Forest cover using Catboost Multiclass Classifier;Python notebook;5699.0;8;0.74698;0.74698
2018-06-26 11:10:21;;Apache 2.0;https://www.kaggle.com/codename007/forest-cover-type-eda-baseline-model;1.0;['sklearn'];['ai', 'nn'];['test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.742;0.458;2020-12-12 19:58:01;Forest Cover Type (Kernels Only);['data visualization, exploratory data analysis, classification'];Forest Cover Type : EDA + Baseline Model âœ“âœ“;Python notebook;8462.0;58;0.81316;0.81316
2018-07-01 14:06:14;;Apache 2.0;https://www.kaggle.com/indranilbhattacharya/xgb-parameter-tuning-from-scratch-random-search;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.687;0.319;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);['beginner, xgboost, multiclass classification'];XGB parameter tuning from scratch <Random Search>;R script;2290.0;12;0.76757;0.76757
2018-10-03 10:00:21;summary;Apache 2.0;https://www.kaggle.com/nadare/eda-feature-engineering-and-modeling-4th-359;1.0;['tensorflow', 'lightgbm', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nn'];['filter', 'training data', 'regression', 'test data', 'train', 'random forest', 'model', 'layer', 'loss', 'label', 'logistic regression', 'k-nearest neighbor', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.706;0.403;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);[];EDA, Feature engineering and Modeling (4th/359) ;Python notebook;3489.0;30;;
2018-09-03 11:12:38;Histogram distribution of continuous features;Apache 2.0;https://www.kaggle.com/sassy19a/exploratory-data-analysis-forest-cover;1.0;['pattern'];['ai'];['train'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.628;0.302;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);['beginner, data visualization, exploratory data analysis'];Exploratory Data Analysis (forest cover);Python notebook;730.0;10;;
2018-07-02 19:54:38;;Apache 2.0;https://www.kaggle.com/skillsmuggler/forest-cover-feature-engineering-and-reduction;1.0;['lightgbm', 'sklearn'];['ai', 'rl', 'gbm'];['filter', 'training data', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.653;0.311;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);['beginner, exploratory data analysis, classification, +1 morefeature engineering'];Forest Cover | Feature Engineering and Reduction;Python notebook;1158.0;11;0.77220;0.77220
2018-09-04 08:46:24;;Apache 2.0;https://www.kaggle.com/suyashgulati/using-pyspark-randomforest-crossvalidatn-gridsrch;0.5;[];['ai', 'nn', 'ml', 'cv'];['train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.788;0.292;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);[];Pyspark with crossvalidation, gridsearch;Python notebook;30642.0;9;0.61278;0.61278
2018-06-27 13:42:32;;Apache 2.0;https://www.kaggle.com/thebrownviking20/ann-starter;1.0;['tensorflow', 'keras'];['ai'];['predict', 'train', 'model', 'epoch', 'layer', 'loss'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.673;0.362;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);['gpu, deep learning, forestry'];ANN starter;Python notebook;1713.0;19;0.71509;0.71509
2018-07-24 09:52:01;;Apache 2.0;https://www.kaggle.com/thebrownviking20/voting-classifier-for-victory;1.0;['xgboost', 'sklearn', 'lightgbm'];['ai', 'gbm'];['test data', 'predict', 'train', 'model', 'loss'];https://www.kaggle.com/c/forest-cover-type-kernels-only;0.662;0.371;2020-12-12 19:58:02;Forest Cover Type (Kernels Only);[];Voting Classifier for victory!;Python notebook;1393.0;21;0.81471;0.81471
2015-04-10 06:19:16;;Apache 2.0;https://www.kaggle.com/apurvadubey/r-starter-code;0.5;[];['ner', 'ai', 'gbm', 'nlp', 'nn'];['training data', 'test data', 'train', 'model', 'deep learning', 'label', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/forest-cover-type-prediction;0.69;0.236;2020-12-12 20:01:44;Forest Cover Type Prediction;[];R_Starter_Code;R script;2465.0;5;;
2016-04-05 10:42:36;;Apache 2.0;https://www.kaggle.com/briantc/forest-cover-training-data-viz;0.5;[];['ai'];['training data', 'train', 'model', 'clustering', 'label', 'predict'];https://www.kaggle.com/c/forest-cover-type-prediction;0.737;0.334;2020-12-12 20:01:44;Forest Cover Type Prediction;[];Forest Cover Training: Data Viz;Rmarkdown script;7308.0;14;;
2015-05-05 01:22:02;;Apache 2.0;https://www.kaggle.com/dchudz/rmarkdown-starter-script-3;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'predict', 'classification', 'propagation'];https://www.kaggle.com/c/forest-cover-type-prediction;0.689;0.253;2020-12-12 20:01:44;Forest Cover Type Prediction;[];RMarkdown Starter Script;Rmarkdown script;2385.0;6;;
2017-03-22 02:46:44;;Apache 2.0;https://www.kaggle.com/jianyu/my-first-submission;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/forest-cover-type-prediction;0.652;0.253;2020-12-12 20:01:44;Forest Cover Type Prediction;[];my_first_submission;Python script;1134.0;6;0.81285;0.81285
2018-10-04 09:08:02;;Apache 2.0;https://www.kaggle.com/joydeb28/forest-cover-type-prediction;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['test data', 'random forest', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/forest-cover-type-prediction;0.637;0.253;2020-12-12 20:01:44;Forest Cover Type Prediction;[];Forest Cover Type Prediction;Python script;852.0;6;0.71906;0.71906
2020-05-24 08:50:06;Forest cover types EDA;Apache 2.0;https://www.kaggle.com/saniaks/forest-cover-type;1.0;['pattern'];['ai', 'nn', 'ann', 'rl'];['train', 'label', 'filter', 'predict'];https://www.kaggle.com/c/forest-cover-type-prediction;0.584;0.302;2020-12-12 20:01:44;Forest Cover Type Prediction;['beginner, data visualization, exploratory data analysis'];Forest cover type;Python notebook;346.0;10;;
2020-05-16 13:20:52;Forest Cover Type Analysis and Prediction;Apache 2.0;https://www.kaggle.com/siddheshpujari/forest-cover-eda;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['filter', 'predict', 'training data', 'train', 'loss', 'understanding'];https://www.kaggle.com/c/forest-cover-type-prediction;0.632;0.357;2020-12-12 20:01:44;Forest Cover Type Prediction;['beginner, data visualization, exploratory data analysis'];Forest Cover EDA;Python notebook;786.0;18;;
2017-07-03 11:28:02;Predicting Forest Cover Type;Apache 2.0;https://www.kaggle.com/sociopath00/forest-type-prediction-using-python;1.0;['sklearn'];['ai', 'cv'];['train', 'model', 'random forest', 'predict'];https://www.kaggle.com/c/forest-cover-type-prediction;0.686;0.253;2020-12-12 20:01:44;Forest Cover Type Prediction;[];Forest Type Prediction using Python;Python notebook;2270.0;6;;
2015-06-12 05:21:58;;Apache 2.0;https://www.kaggle.com/triskelion/first-try-with-random-forests;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['predict', 'train', 'deep learning', 'random forest', 'classification'];https://www.kaggle.com/c/forest-cover-type-prediction;0.753;0.334;2020-12-12 20:01:44;Forest Cover Type Prediction;[];First try with Random Forests;Python script;11149.0;14;0.75121;0.75121
2020-06-26 17:59:54;Data Visualization.;Apache 2.0;https://www.kaggle.com/vibeeshk/forest-cover-type-prediction;1.0;['lightgbm', 'sklearn'];['cv', 'ai', 'nn', 'gbm'];['linear regression', 'machine learning', 'test data', 'regression', 'train', 'fitting', 'model', 'label', 'predict', 'decision tree', 'random forest'];https://www.kaggle.com/c/forest-cover-type-prediction;0.529;0.236;2020-12-12 20:01:44;Forest Cover Type Prediction;[];Forest Cover_Type Prediction;Python notebook;151.0;5;0.75044;0.75044
2018-06-17 09:28:33;Log-mel feature is widely used in the audio tagging task. In this notebook, we show log-mel features by category.  Let's take a glance at them.;Apache 2.0;https://www.kaggle.com/aadan2017/log-mel-features-shown-by-category;0.5;[];['ai', 'ml'];['train', 'label'];https://www.kaggle.com/c/freesound-audio-tagging;0.69;0.281;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;[];Log-mel features shown by category.;Python notebook;2459.0;8;;
2020-07-07 11:03:51;;Apache 2.0;https://www.kaggle.com/aditya48/sound-classifier-introduction;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'neural network', 'deep learning', 'label', 'predict', 'classification', 'convolutional neural network'];https://www.kaggle.com/c/freesound-audio-tagging;0.588;0.292;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;[];Sound Classifier Introduction;Python notebook;371.0;9;;
2018-03-31 19:40:41;;Apache 2.0;https://www.kaggle.com/amlanpraharaj/random-forest-using-mfcc-features;1.0;['lightgbm', 'sklearn'];['ai', 'nn', 'gbm'];['train', 'fitting', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/freesound-audio-tagging;0.704;0.302;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;[];Random Forest using MFCC features;Python notebook;3327.0;10;;
2018-04-16 17:42:31;;Apache 2.0;https://www.kaggle.com/amlanpraharaj/xgb-using-mfcc-opanichev-s-features-lb-0-811;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'nn', 'ml'];['train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/freesound-audio-tagging;0.72;0.4;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;['feature engineering'];XGB using MFCC & opanichev's features[LB 0.811];Python notebook;4876.0;29;0.57775;0.60963
2018-07-26 09:47:53;Distribution Category;Apache 2.0;https://www.kaggle.com/ashishpatel26/best-trick-for-audio-data;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn'];['generation', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/freesound-audio-tagging;0.723;0.397;2020-12-12 20:04:19;multiple data sources;['gpu'];Best Trick for audio data;Python notebook;5224.0;28;0.86476;0.89590
2019-12-08 16:50:27;More To Come. Stay Tuned. !!If there are any suggestions/changes you would like to see in the Kernel please let me know :). Appreciate every ounce of help! This notebook will always be a work in progress. Please leave any comments about further improvements to the notebook! Any feedback or constructive criticism is greatly appreciated!. If you like it or it helps you , you can upvote and/or leave a comment :).|;Apache 2.0;https://www.kaggle.com/codename007/a-very-extensive-freesound-exploratory-analysis;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'label', 'training data'];https://www.kaggle.com/c/freesound-audio-tagging;0.738;0.492;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;['beginner, data visualization, exploratory data analysis, +1 morefeature engineering'];A Very Extensive Freesound Exploratory Analysis âœ“âœ“;Python notebook;7602.0;90;;
2018-04-22 23:47:33;"MFCC theory and implementationTheoryMel Frequency Cepstral Coefficents (MFCCs) is a way of extracting features from an audio. The MFCC uses the MEL scale to divide the frequency band to sub-bands and then extracts the Cepstral Coefficents using Discrete Cosine Transform (DCT). MEL scale is based on the way humans distinguish between frequencies which makes it very convenient to process sounds. Lets first understand how humans perceive sounds. Human voice sound perceptionAdult humans fundamental voice frequency range is between 85Hz to 255Hz (85Hz to 180Hz for male and 165Hz to 255Hz for female). On top of the fundamental frequency there are harmonics of fundamental frequencys. Harmonics are whole multiplications of the fundamental frequency. If for instance the fundamental frequency is 100Hz then its second harmonic will be 200Hz, third harmonic is 300Hz and so on. You can see an example in the image below [1] which shows frequency vs. time of several pronounced words and color represents frequency power at that point (yellow strongest and black weakest): <img src=""https://static.scientificamerican.com/sciam/assets/media/sound/speechsep-audio/speechsep-2-spect.png"", width=600, height=200> Notice the first horizontal yellow line on the bottom of each segment. That is the fundamental frequency and its the strongest. Above that there are harmonics with the same frequncy distance from each other. Humans can hear roughly between 20Hz to 20KHz. The perception of sound is non-linear [2] and you can better distinguish between low frequency sounds than high frequency sounds e.g. humans can clearly hear the difference betwee 100Hz and 200Hz but not between 15kHz and 15.1kHz. You can try in usin a tone generator: http://www.szynalski.com/tone-generator/ MEL scaleA MEL scale is a unit of PITCH proposed by Stevens, Volkmann and Newmann in 1937. The MEL scale is a scale of pitches judged by listeners to be equal in distance one from another [3] [4]. Because of how humans perceive sound the MEL scale is a non-lenear scale and the distances between the pitches increeses with frequency. MFCC implementation [5] [6]Sample signalFirst, lets load a sample audio and start working with it:";Apache 2.0;https://www.kaggle.com/ilyamich/mfcc-implementation-and-tutorial;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['train', 'filter'];https://www.kaggle.com/c/freesound-audio-tagging;0.787;0.393;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;[];MFCC implementation and tutorial;Python notebook;29925.0;27;;
2018-04-08 02:53:23;Removing Uninformative Parts of the Audio FilesInitially forked from ILM. Updated and presented by Matthew. In this notebook, I will demonstrate the removal of empty or very quiet parts of audio files in the train and test audio set.  This new training corpus and testing set should hopefully increase accuracy and decrease the memory of models when employed.;Apache 2.0;https://www.kaggle.com/matthewa313/removing-uninformative-parts-of-audio-files;0.5;[];['ai', 'nn'];['train', 'model'];https://www.kaggle.com/c/freesound-audio-tagging;0.678;0.327;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;[];Removing Uninformative Parts of Audio Files;Python notebook;1907.0;13;;
2018-04-09 00:04:18;;Apache 2.0;https://www.kaggle.com/opanichev/lightgbm-baseline;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/freesound-audio-tagging;0.711;0.327;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;['beginner, feature engineering'];LightGBM Baseline;Python script;3933.0;13;0.62278;0.66002
2018-04-23 07:24:43;;Apache 2.0;https://www.kaggle.com/sunqpark/data-loader-for-pytorch-with-mfcc;1.0;['pytorch'];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/freesound-audio-tagging;0.719;0.334;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;[];Data loader for PyTorch with mfcc;Python script;4745.0;14;;
2020-08-31 08:32:20;V3 Added Cut-out;Apache 2.0;https://www.kaggle.com/tanulsingh077/audio-albumentations-transform-your-audio;1.0;['pytorch', 'albumentations'];['ner', 'ai', 'cv', 'nn', 'ann'];['train', 'recognition', 'model', 'label', 'computer vision'];https://www.kaggle.com/c/freesound-audio-tagging;0.687;0.484;2020-12-12 20:04:19;multiple data sources;['audio data'];Audio Albumentations : Transform Your Audio;Python notebook;2309.0;81;;
2018-05-08 19:26:53;;Apache 2.0;https://www.kaggle.com/thebrownviking20/xgb-using-lda-and-mfcc-opanichev-s-features;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'nn', 'ml'];['train', 'fitting', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/freesound-audio-tagging;0.676;0.334;2020-12-12 20:04:19;Freesound General-Purpose Audio Tagging Challenge;['gpu'];XGB using LDA and MFCC & opanichev's features;Python notebook;1844.0;14;0.77777;0.80011
2019-04-17 16:12:36;Reference https://www.kaggle.com/daisukelab/cnn-2d-basic-solution-powered-by-fast-ai. https://www.kaggle.com/daisukelab/fat2019_prep_mels1 https://www.kaggle.com/c/freesound-audio-tagging-2019/overview/timeline https://nbviewer.jupyter.org/github/fastai/course-v3/blob/master/nbs/dl1/lesson3-planet.ipynb  NewI tried training with more models and 2 models: vgg_16 , vgg_19 with the best results. Also, I changed the bz from 64 to 128 and also gave better results.;Apache 2.0;https://www.kaggle.com/hung96ad/naive-ensemble-model;0.5;[];['ner', 'ai', 'dl', 'cnn', 'nn'];['training data', 'test data', 'train', 'model', 'epoch', 'vgg', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/freesound-audio-tagging-2019;0.679;0.4;2020-12-12 20:05:18;multiple data sources;['gpu'];Naive ensemble model;Python notebook;1931.0;29;0.57174;0.57174
2019-04-23 01:06:00;imports;Apache 2.0;https://www.kaggle.com/mhiro2/simple-2d-cnn-classifier-with-pytorch;1.0;['sklearn'];['dl', 'ai', 'nn', 'ann'];['train', 'model', 'epoch', 'loss', 'label', 'predict', 'rank', 'relu'];https://www.kaggle.com/c/freesound-audio-tagging-2019;0.769;0.53;2020-12-12 20:05:18;multiple data sources;['gpu'];Simple 2d-CNN Classifier with PyTorch;Python notebook;17333.0;153;0.61073;0.61073
2019-04-26 09:31:23;imports;Apache 2.0;https://www.kaggle.com/peining/simple-cnn-classifier-with-pytorch;1.0;['sklearn'];['dl', 'ai', 'nn', 'ann'];['train', 'model', 'epoch', 'loss', 'label', 'predict', 'rank', 'relu'];https://www.kaggle.com/c/freesound-audio-tagging-2019;0.724;0.439;2020-12-12 20:05:18;multiple data sources;['gpu'];Simple CNN Classifier with PyTorch;Python notebook;5345.0;46;0.58584;0.58584
2019-05-25 23:54:48;utils;Apache 2.0;https://www.kaggle.com/tanlikesmath/2d-cnn-high-score;0.5;[];['ner', 'ai', 'dl', 'cnn', 'nn', 'ann'];['training data', 'test data', 'train', 'model', 'epoch', 'relu', 'loss', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/freesound-audio-tagging-2019;0.696;0.423;2020-12-12 20:05:18;multiple data sources;['gpu'];2D CNN high score;Python notebook;2789.0;38;0.63625;0.63625
2018-09-22 02:39:08;;Apache 2.0;https://www.kaggle.com/captcalculator/a-very-extensive-gstore-exploratory-analysis;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['training data', 'train', 'fitting', 'model', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/ga-customer-revenue-prediction;0.767;0.554;2020-12-12 20:08:17;Google Analytics Customer Revenue Prediction;[];A Very Extensive GStore Exploratory Analysis;Rmarkdown script;16420.0;218;;
2018-09-14 20:44:47;1. Quick start: read csv and flatten json fieldsHi! This notebook just loads the data and flattens the json fields.  I have put the code in one function so you can copy it easily. Also, I dumped the processed dataframes to disk so they are easily importable from within another kernel.;Apache 2.0;https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields;0.5;[];['ai'];['train'];https://www.kaggle.com/c/ga-customer-revenue-prediction;0.789;0.604;2020-12-12 20:08:16;Google Analytics Customer Revenue Prediction;[];1 - Quick start: read csv and flatten json fields;Python notebook;31970.0;480;;
2018-11-21 19:22:15;;Apache 2.0;https://www.kaggle.com/kailex/group-xgb-for-gstore-v2;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/ga-customer-revenue-prediction;0.743;0.494;2020-12-12 20:08:17;Google Analytics Customer Revenue Prediction;['beginner, feature engineering, data cleaning, +2 moreregression, xgboost'];Group  xgb for Gstore v2;R script;8555.0;93;2.58594;0.00000
2018-10-25 20:58:02;;Apache 2.0;https://www.kaggle.com/kailex/r-eda-for-gstore-glm-keras-xgb;1.0;['pattern', 'xgboost', 'h2o', 'keras'];['ner', 'ai', 'dl', 'automl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['autoencoder', 'filter', 'regression', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'rectifier', 'predict', 'rank', 'relu'];https://www.kaggle.com/c/ga-customer-revenue-prediction;0.792;0.596;2020-12-12 20:08:16;Google Analytics Customer Revenue Prediction;['beginner, data visualization, exploratory data analysis, +2 moredata cleaning, xgboost'];R EDA for GStore + GLM + KERAS + XGB;Rmarkdown script;35500.0;417;0.00000;0.00000
2018-09-16 00:39:47;;Apache 2.0;https://www.kaggle.com/mrlong/r-flatten-json-columns-to-make-single-data-frame;0.5;[];['ner', 'ai', 'dl', 'gan', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/ga-customer-revenue-prediction;0.759;0.54;2020-12-12 20:08:17;Google Analytics Customer Revenue Prediction;[];R - Flatten JSON columns to make single data frame;R script;13304.0;175;;
2018-10-05 19:12:49;;Apache 2.0;https://www.kaggle.com/ogrellier/user-level-lightgbm-lb-1-4480;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/ga-customer-revenue-prediction;0.748;0.521;2020-12-12 20:08:17;Google Analytics Customer Revenue Prediction;[];user_level_lightgbm LB 1.4480;Python script;9690.0;134;0.00000;0.00000
2018-11-11 14:11:40;;Apache 2.0;https://www.kaggle.com/paulorzp/perfect-score-one-line-without-semicolon;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/ga-customer-revenue-prediction;0.748;0.502;2020-12-12 20:08:17;Google Analytics Customer Revenue Prediction;[];Perfect Score (one line without  semicolon);Python script;9677.0;104;;
2019-04-17 03:37:40;;Apache 2.0;https://www.kaggle.com/plasticgrammer/customer-revenue-prediction-v2-playground;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'gan', 'rl', 'nn', 'ann'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/ga-customer-revenue-prediction;0.763;0.49;2020-12-12 20:08:17;Google Analytics Customer Revenue Prediction;['beginner'];Customer Revenue Prediction V2 : playground;Python notebook;14848.0;88;1.41090;0.00000
2019-06-23 12:33:22;Test Prediction Submission;Apache 2.0;https://www.kaggle.com/helmehelmuto/keras-cnn;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai'];['predict', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge;0.699;0.188;2020-12-12 20:08:34;Galaxy Zoo - The Galaxy Challenge;['gpu'];Keras CNN;Python notebook;2972.0;3;0.09946;0.09978
2019-08-26 05:28:04;ClassificaÃ§Ã£o morfolÃ³gica de galÃ¡xias;Apache 2.0;https://www.kaggle.com/mgambati/galaxy-challenge;1.0;['sklearn', 'pillow', 'tensorflow', 'keras', 'skimage'];['ner', 'ai', 'nn', 'ml'];['filter', 'test data', 'train', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge;0.639;0.099;2020-12-12 20:08:34;Galaxy Zoo - The Galaxy Challenge;['gpu'];Galaxy Challenge;Python notebook;888.0;1;;
2019-08-31 22:51:10;Filter;Apache 2.0;https://www.kaggle.com/mgambati/teste-classifica-o-de-gal-xias;1.0;['tensorflow', 'sklearn', 'keras', 'pillow'];['ner', 'ai', 'nn', 'ann'];['filter', 'test data', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge;0.63;0.0;2020-12-12 20:08:34;Galaxy Zoo - The Galaxy Challenge;['gpu'];teste classificaÃ§Ã£o de galÃ¡xias;Python notebook;760.0;0;;
2019-11-03 03:55:25;;Apache 2.0;https://www.kaggle.com/zhuangjw/compress-galaxy-test-data;1.0;['skimage'];['ai', 'nn', 'ann'];['train'];https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge;0.636;0.099;2020-12-12 20:08:34;Galaxy Zoo - The Galaxy Challenge;[];compress_galaxy_test_data;Python notebook;842.0;1;;
2019-11-02 20:26:47;;Apache 2.0;https://www.kaggle.com/zhuangjw/compress-galaxy-train-data;1.0;['keras', 'skimage'];['ai', 'nn', 'cnn', 'ann'];['train', 'label'];https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge;0.625;0.099;2020-12-12 20:08:34;Galaxy Zoo - The Galaxy Challenge;[];compress_galaxy_train_data;Python notebook;694.0;1;;
2019-02-07 22:41:56;This notebook will sample a few sentences, highlighting the pronoun with the positive (green) and negative (red) targets.;Apache 2.0;https://www.kaggle.com/bguberfain/sentences-with-highlight;0.5;[];['ner', 'ai', 'gan', 'rl', 'nn', 'ml'];['rank', 'gru', 'label', 'train'];https://www.kaggle.com/c/gendered-pronoun-resolution;0.648;0.367;2020-12-12 20:09:27;Gendered Pronoun Resolution;[];Sentences with highlight;Python notebook;1054.0;20;;
2019-02-06 09:53:22;;Apache 2.0;https://www.kaggle.com/eliseygusev/perfect-lb-score-in-5-lines-of-code-1;0.0;[];[];[];https://www.kaggle.com/c/gendered-pronoun-resolution;0.726;0.371;2020-12-12 20:09:27;multiple data sources;[];PERFECT LB SCORE IN 5 LINES OF CODE!!!1;Python notebook;5599.0;21;0.00000;0.00000
2019-04-17 07:27:54;;Apache 2.0;https://www.kaggle.com/jazivxt/ml-model-example-with-train-test;1.0;['spacy', 'sklearn', 'nltk'];['ai', 'nlp'];['test data', 'predict', 'train', 'model', 'loss'];https://www.kaggle.com/c/gendered-pronoun-resolution;0.67;0.371;2020-12-12 20:09:27;Gendered Pronoun Resolution;[];ML Model Example with Train & Test;Python notebook;1607.0;21;0.90000;0.00000
2019-02-09 19:11:42;;Apache 2.0;https://www.kaggle.com/jazivxt/pro-noun-pro-gram;1.0;['spacy', 'sklearn', 'nltk'];['nlp', 'nn'];['loss'];https://www.kaggle.com/c/gendered-pronoun-resolution;0.679;0.387;2020-12-12 20:09:27;Gendered Pronoun Resolution;[];Pro Noun Pro Gram;Python notebook;1957.0;25;0.00000;0.00000
2019-02-10 05:48:24;;Apache 2.0;https://www.kaggle.com/shujian/ml-model-example-with-train-test;1.0;['spacy', 'xgboost', 'sklearn', 'nltk'];['ai', 'nlp'];['filter', 'predict', 'train', 'model', 'loss'];https://www.kaggle.com/c/gendered-pronoun-resolution;0.702;0.442;2020-12-12 20:09:27;Gendered Pronoun Resolution;[];ML Model Example with Train & Test;Python notebook;3178.0;48;0.00000;0.00000
2019-08-07 18:27:30;;Apache 2.0;https://www.kaggle.com/bestfitting/i96-1e-4-6e-4-bs32-195;1.0;['pytorch', 'pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'activation function', 'relu', 'recommend', 'classification'];https://www.kaggle.com/c/generative-dog-images;0.751;0.496;2020-12-12 20:11:09;Generative Dog Images;['gpu'];i96_1e-4_6e_4_bs32_195;Python script;10524.0;95;77.80956;12.46328
2019-07-01 11:10:43;Generative Adversarial Networks (GANs);Apache 2.0;https://www.kaggle.com/jesucristo/gan-introduction;1.0;['tensorflow'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['generation', 'train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'generative adversarial network', 'convolutional neural network', 'propagation'];https://www.kaggle.com/c/generative-dog-images;0.811;0.607;2020-12-12 20:11:09;Generative Dog Images;['gpu, beginner, deep learning, +2 morecomputer vision, gan'];GAN Introduction;Python notebook;65894.0;508;;
2019-07-01 21:12:57;;Apache 2.0;https://www.kaggle.com/paulorzp/show-annotations-and-breeds;0.2;[];['ann', 'nn', 'ml'];[];https://www.kaggle.com/c/generative-dog-images;0.686;0.471;2020-12-12 20:11:09;Generative Dog Images;[];Show Annotations and Breeds;Python notebook;2277.0;69;;
2019-08-02 04:17:48;;Apache 2.0;https://www.kaggle.com/phoenix9032/gan-dogs-starter-24-jul-custom-layers;0.5;[];['ner', 'ai', 'gan', 'ml', 'nn', 'ann'];['train', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/generative-dog-images;0.738;0.516;2020-12-12 20:11:09;Generative Dog Images;['gpu'];GAN dogs starter 24-Jul -Custom Layers;Python notebook;7484.0;125;118.44737;66.38957
2019-07-13 21:20:48;Variational AutoEncoder;Apache 2.0;https://www.kaggle.com/speedwagon/variational-autoencoder-dogs-generation;1.0;['pytorch'];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['autoencoder', 'predict', 'generation', 'train', 'model', 'epoch', 'layer', 'loss', 'relu', 'understanding'];https://www.kaggle.com/c/generative-dog-images;0.718;0.431;2020-12-12 20:11:09;Generative Dog Images;['gpu'];Variational Autoencoder - dogs generation;Python notebook;4676.0;42;;
2019-08-04 08:18:54;Memorizer GAN - pyTorch;Apache 2.0;https://www.kaggle.com/timetraveller98/memorizer-cgan-pytorch-version;1.0;['pytorch', 'keras'];['ner', 'ai', 'gan', 'rl', 'ml', 'nn', 'ann'];['rank', 'epoch', 'train', 'loss'];https://www.kaggle.com/c/generative-dog-images;0.695;0.446;2020-12-12 20:11:09;Generative Dog Images;['gpu'];Memorizer CGAN - pyTorch version;Python notebook;2769.0;50;0.00000;0.00000
2020-09-26 23:08:22;;Apache 2.0;https://www.kaggle.com/wendykan/demo-mifid-metric-for-dog-image-generation-comp;1.0;['tensorflow'];['ner', 'ai', 'dl', 'rl', 'nn'];['generation', 'train', 'model', 'layer', 'predict'];https://www.kaggle.com/c/generative-dog-images;0.712;0.453;2020-12-12 20:11:09;multiple data sources;[];Demo MiFID metric for Dog image generation comp;Python notebook;4061.0;55;;
2019-06-21 23:56:56;;Apache 2.0;https://www.kaggle.com/wendykan/gan-dogs-starter;0.5;[];['ner', 'ai', 'nn', 'ann'];['train', 'epoch', 'loss', 'label', 'relu'];https://www.kaggle.com/c/generative-dog-images;0.749;0.514;2020-12-12 20:11:09;Generative Dog Images;[];GAN dogs starter;Python notebook;10139.0;122;242.05871;278.12283
2019-07-02 04:49:23;;Apache 2.0;https://www.kaggle.com/whizzkid/crop-images-using-bounding-box;0.2;[];['ann', 'nn', 'ml'];[];https://www.kaggle.com/c/generative-dog-images;0.744;0.455;2020-12-12 20:11:09;Generative Dog Images;[];Crop images using bounding box;Python notebook;8771.0;56;;
2016-11-10 08:38:48;;Apache 2.0;https://www.kaggle.com/amberthomas/ghosts-goblins-and-ghouls-oh-my;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nn'];['machine learning', 'training data', 'train', 'model', 'clustering', 'label', 'k-means', 'predict', 'rank', 'random forest'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.741;0.497;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Ghosts, Goblins, and Ghouls. Oh my!;Rmarkdown script;8097.0;97;;
2016-11-02 20:36:17;;Apache 2.0;https://www.kaggle.com/anki08/classifying-ghouls-goblins-and-ghosts-using-svm;1.0;['xgboost', 'sklearn'];['ai', 'cv'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.662;0.236;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Classifying ghouls goblins and ghosts using svm;Python notebook;1393.0;5;;
2016-11-16 19:19:01;First look at features.;Apache 2.0;https://www.kaggle.com/daavoo/tensorflow-1vs1;1.0;['statsmodels', 'tensorflow', 'sklearn'];['ai', 'dl', 'rl', 'nn', 'ann'];['regression', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.741;0.442;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Tensorflow 1vs1;Python notebook;8108.0;48;0.60868;0.60868
2016-11-25 09:54:52;;Apache 2.0;https://www.kaggle.com/donyoe/ghostbuster-data-exploration;0.5;[];['ai'];['train', 'test data'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.684;0.352;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Ghostbuster data exploration;Rmarkdown script;2167.0;17;;
2017-02-12 19:18:45;;Apache 2.0;https://www.kaggle.com/enrique1500/hyperparameter-random-search-with-mlr;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'machine learning', 'regression', 'training data', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.71;0.352;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Hyperparameter Random Search with `mlr`;Rmarkdown script;3879.0;17;;
2016-11-25 10:19:56;Data Vizualization;Apache 2.0;https://www.kaggle.com/gauravjoshi1986/ghostbuster-data;1.0;['statsmodels', 'tensorflow', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nn', 'ml'];['filter', 'regression', 'train', 'model', 'layer', 'loss', 'label', 'logistic regression', 'predict', 'relu', 'random forest'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.67;0.281;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Ghostbuster data;Python notebook;1622.0;8;0.72778;0.72778
2016-11-15 10:06:03;Imports;Apache 2.0;https://www.kaggle.com/hhllcks/comparison-between-classifiers;1.0;['sklearn'];['ai', 'cv', 'ml', 'nn', 'ann'];['regression', 'train', 'fitting', 'model', 'layer', 'predict', 'random forest'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.717;0.367;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Comparison between classifiers;Python notebook;4531.0;20;;
2016-11-18 12:59:10;Imports;Apache 2.0;https://www.kaggle.com/hhllcks/neural-net-with-gridsearch;1.0;['sklearn'];['ai', 'rl', 'ml', 'cv'];['train', 'fitting', 'model', 'layer', 'predict', 'relu'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.755;0.281;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Neural Net with GridSearch;Python notebook;11924.0;8;;
2020-08-16 16:05:03;Load data;Apache 2.0;https://www.kaggle.com/mikhailg0/monsters-classification-solution;1.0;['sklearn'];['ai', 'nn', 'ml'];['regression', 'train', 'model', 'layer', 'label', 'predict'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.507;0.214;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];ðŸ‘» Monsters Classification Solution;Python notebook;111.0;4;0.65406;0.65406
2016-11-11 08:40:48;Data loading and preparation;Apache 2.0;https://www.kaggle.com/oysteijo/ghosts-n-goblins-n-neural-networks-lb-0-74858;0.5;[];['ai', 'rl', 'nn', 'cv'];['train', 'fitting', 'neural network', 'epoch', 'layer', 'activation function', 'hidden layer', 'propagation'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.729;0.416;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Ghosts'n'Goblins'n'Neural Networks (LB: 0.74858);Python notebook;5966.0;35;;
2018-09-02 04:26:49;Loading datasets;Apache 2.0;https://www.kaggle.com/shahules/ghouls-goblins-and-ghosts;1.0;['sklearn'];['ai', 'nn', 'ann', 'cv'];['filter', 'random forest', 'train', 'model', 'label', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.572;0.268;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Ghouls, Goblins and Ghosts;Python notebook;286.0;7;0.71077;0.71077
2016-11-25 09:39:07;RandomForestClassifier GridSearch;Apache 2.0;https://www.kaggle.com/xingobar/ghost-data-visualization;1.0;['statsmodels', 'xgboost', 'sklearn'];['ai', 'nn', 'cv'];['train', 'model', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.633;0.268;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Ghost Data Visualization;Python notebook;799.0;7;;
2016-11-13 12:25:12;Yoann Boj8th November 2016;Apache 2.0;https://www.kaggle.com/yoyocm/let-s-explore-and-classify-monsters;1.0;['sklearn'];['ai', 'nn', 'ann'];['regression', 'train', 'model', 'predict', 'classification'];https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo;0.708;0.357;2020-12-12 20:12:33;Ghouls, Goblins, and Ghosts... Boo!;[];Let's explore and classify monsters;Python notebook;3692.0;18;;
2019-04-14 09:46:07;;Apache 2.0;https://www.kaggle.com/abhishekgiri/give-me-some-credit-logistic-regression;0.5;[];['ai', 'dl', 'rl'];['regression', 'train', 'fitting', 'model', 'logistic regression', 'predict'];https://www.kaggle.com/c/GiveMeSomeCredit;0.684;0.214;2020-12-12 20:13:55;Give Me Some Credit;['gpu'];Give Me Some Credit - Logistic Regression;R notebook;2160.0;4;;
2020-03-14 16:47:37;Imports;Apache 2.0;https://www.kaggle.com/caesarlupum/modeling-give-me-some-credit;1.0;['xgboost', 'sklearn', 'lightgbm'];['ai', 'dl', 'gbm', 'cv', 'rl', 'nn', 'ml'];['filter', 'regression', 'train', 'fitting', 'model', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/GiveMeSomeCredit;0.713;0.379;2020-12-12 20:13:55;Give Me Some Credit;['beginner, dailychallenge'];Modeling: Give Me Some Credit  ðŸ“ˆðŸ“‰;Python notebook;4149.0;23;0.84903;0.83945
2020-04-29 12:19:08;Check column type;Apache 2.0;https://www.kaggle.com/dnreddy/give-me-some-credit;1.0;['statsmodels', 'xgboost', 'sklearn', 'pattern'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/GiveMeSomeCredit;0.573;0.152;2020-12-12 20:13:55;Give Me Some Credit;[]; Give Me Some Credit;Python notebook;288.0;2;0.85533;0.84796
2020-10-25 15:01:36;Import library;Apache 2.0;https://www.kaggle.com/doufulai/give-credit-jasper;1.0;['tensorflow', 'xgboost', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/GiveMeSomeCredit;0.733;0.152;2020-12-12 20:13:55;Give Me Some Credit;[];give_credit_jasper;Python notebook;6661.0;2;;
2019-11-09 08:59:32;;Apache 2.0;https://www.kaggle.com/ejkim0121/loan-lgbmclassifer;1.0;['lightgbm'];['ai', 'dl', 'gbm', 'rl', 'nn'];['train', 'predict'];https://www.kaggle.com/c/GiveMeSomeCredit;0.658;0.188;2020-12-12 20:13:55;Give Me Some Credit;[];Loan_LGBMClassifer;Python notebook;1272.0;3;;
2018-12-07 16:04:28;simple model;Apache 2.0;https://www.kaggle.com/fastwalker/give-me-credit;0.5;[];['ai', 'rl', 'dl', 'gbm'];['train', 'model', 'predict'];https://www.kaggle.com/c/GiveMeSomeCredit;0.722;0.34;2020-12-12 20:13:55;Give Me Some Credit;['gpu'];give me credit;R notebook;5082.0;15;0.86199;0.85477
2019-05-09 11:26:18;Esercitazione su Credit Risk Analysis;Apache 2.0;https://www.kaggle.com/gclauser/credit-risk-analysis-gfc;1.0;['sklearn'];['ner', 'ai', 'dl', 'rl', 'nn'];['regression', 'train', 'model', 'label', 'logistic regression', 'predict'];https://www.kaggle.com/c/GiveMeSomeCredit;0.666;0.236;2020-12-12 20:13:55;Give Me Some Credit;[];Credit risk analysis - GFC;Python notebook;1490.0;5;;
2020-10-10 05:44:06;Label Distribution;Apache 2.0;https://www.kaggle.com/jasonwct/eda-simple-ml-for-beginners;1.0;['sklearn'];['ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['random forest', 'regression', 'train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/GiveMeSomeCredit;0.573;0.099;2020-12-12 20:13:55;Give Me Some Credit;['random forest, logistic regression'];EDA + Simple ML for Beginners;Python notebook;291.0;1;;
2019-08-01 08:42:03;;Apache 2.0;https://www.kaggle.com/midun18/credit-issue-classification;1.0;['xgboost', 'sklearn'];['ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/GiveMeSomeCredit;0.687;0.253;2020-12-12 20:13:55;Give Me Some Credit;['gpu'];Credit Issue Classification;Python notebook;2314.0;6;;
2018-09-15 09:31:25;;Apache 2.0;https://www.kaggle.com/poposkygyq/scorecard;1.0;['sklearn'];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['train', 'label', 'predict'];https://www.kaggle.com/c/GiveMeSomeCredit;0.743;0.302;2020-12-12 20:13:55;multiple data sources;[];scorecard;Python notebook;8649.0;10;;
2019-11-03 23:42:54;;Apache 2.0;https://www.kaggle.com/zhannabitiukova/ds-study-credit-desiciontree-impute-mean;1.0;['sklearn'];['ai', 'dl', 'rl', 'nn', 'ml'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/GiveMeSomeCredit;0.573;0.152;2020-12-12 20:13:55;Give Me Some Credit;[];DS_study_Credit_DesicionTree_Impute_mean;Python notebook;288.0;2;;
2020-05-26 07:58:55;;Apache 2.0;https://www.kaggle.com/aleksandradeis/globalwheatdetection-eda;1.0;['pytorch', 'albumentations'];['ner', 'ai', 'gan', 'cv', 'rl', 'nn', 'ann'];['filter', 'object detection', 'train', 'recognition', 'model', 'deep learning', 'label', 'predict', 'computer vision'];https://www.kaggle.com/c/global-wheat-detection;0.752;0.585;2020-12-12 20:14:55;Global Wheat Detection;[];GlobalWheatDetection EDA;Python notebook;11010.0;350;;
2020-07-17 17:03:25;YoloV5 Pseudo Labeling + OOF EvaluationThis notebook is a clean up of Yolov5 Pseudo Labeling, with OOF-evaluation to search the best score_threshold for final prediction. My pretrained checkpoint gives worse results compared to the original notebook, but to my surprise adding OOF-evaluation gives me a large boost about 1% LB. You can try your pretrained model on this notebook. update: fixed a bug in calculate_final_score, boxes should be sorted with discending conf score. References: Awesome original Pseudo Labeling notebook: https://www.kaggle.com/nvnnghia/yolov5-pseudo-labeling Evaluation Script: https://www.kaggle.com/pestipeti/competition-metric-details-script OOF-Evaluation: https://www.kaggle.com/shonenkov/oof-evaluation-mixup-efficientdet Bayesian Optimization (though failed to improve my results): https://www.kaggle.com/shonenkov/bayesian-optimization-wbf-efficientdet;Apache 2.0;https://www.kaggle.com/hawkey/yolov5-pseudo-labeling-oof-evaluation;0.5;[];['ai', 'rl', 'cv', 'nn', 'ml'];['train', 'model', 'epoch', 'label', 'predict', 'rank', 'bayesian'];https://www.kaggle.com/c/global-wheat-detection;0.76;0.532;2020-12-12 20:14:55;multiple data sources;['gpu'];YoloV5 Pseudo Labeling + OOF Evaluation;Python notebook;13646.0;158;0.6979;0.7647
2020-06-28 09:27:29;YOLOv5 Pseudo LabelingAccording to the results of this notebook FaterRCNN seems to work well with Pseudo Labeling. In this notebook I am going to test Pseudo labeling technique on Yolov5.;Apache 2.0;https://www.kaggle.com/nvnnghia/yolov5-pseudo-labeling;0.5;[];['ner', 'ai', 'cnn', 'cv', 'nn', 'ml'];['train', 'model', 'epoch', 'label', 'predict'];https://www.kaggle.com/c/global-wheat-detection;0.787;0.587;2020-12-12 20:14:55;multiple data sources;['gpu'];YoloV5 Pseudo Labeling;Python notebook;29861.0;361;0.7127;0.7644
2020-06-29 05:48:10;;Apache 2.0;https://www.kaggle.com/orkatz2/yolov5-fake-or-real-single-model-l-b-0-753;0.5;[];['ner', 'ai', 'nn', 'cv'];['model', 'label', 'predict'];https://www.kaggle.com/c/global-wheat-detection;0.756;0.512;2020-12-12 20:14:55;multiple data sources;['gpu'];YoloV5 fake or real [Single Model] L.B>0.753;Python notebook;12168.0;118;0.6770;0.7488
2020-06-22 17:42:53;;Apache 2.0;https://www.kaggle.com/orkatz2/yolov5-train;1.0;['tensorflow', 'spacy', 'opencv-python', 'pillow'];['ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'epoch', 'layer', 'label', 'recommend'];https://www.kaggle.com/c/global-wheat-detection;0.764;0.538;2020-12-12 20:14:55;multiple data sources;[];YoloV5 [train];Python notebook;15109.0;172;;
2020-05-11 17:55:39;Evaluation metricIn this notebook I'll try to explain and implement the competition metrics. If you find a bug please leave a comment. Thanks. And do not forget to vote :)  UPDATE v2 - BUG fix! Inside the calculate_precision function I did not pass the form param, it was hardcoded coco. This caused wrong calculations if you used the pascal_voc format. UPDATE v3 - Optimized with Numba!  I optimized the code, so with help of numba it will run much faster Added the tips of Alexander (see the comments below) Importnat change: In some of the methods the order of the gt_box, pred_box arguments were mixed. Please double check your code. The correct order is gt first, pred second.  UPDATE v4 - Another BUG fixed!;Apache 2.0;https://www.kaggle.com/pestipeti/competition-metric-details-script;0.5;[];['ner', 'ai', 'rl', 'cv', 'ml'];['test data', 'train', 'model', 'predict', 'ground truth'];https://www.kaggle.com/c/global-wheat-detection;0.748;0.576;2020-12-12 20:14:55;Global Wheat Detection;['beginner'];Competition metric details + script;Python notebook;9716.0;306;;
2020-08-02 09:47:23;Importing Libraries;Apache 2.0;https://www.kaggle.com/reighns/augmentations-data-cleaning-and-bounding-boxes;1.0;['albumentations'];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['object detection', 'train', 'model', 'label', 'classification'];https://www.kaggle.com/c/global-wheat-detection;0.763;0.569;2020-12-12 20:14:55;Global Wheat Detection;['data visualization'];Augmentations, Data Cleaning and Bounding Boxes;Python notebook;14758.0;273;;
2018-07-07 14:46:31;;Apache 2.0;https://www.kaggle.com/ashishpatel26/inception-resnet-comb-approach-on-google-image-ai;1.0;['tensorflow', 'keras', 'pillow'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'epoch', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/google-ai-open-images-object-detection-track;0.647;0.302;2020-12-12 20:15:53;multiple data sources;['gpu']; Inception Resnet Comb Approach on Google Image AI;Python notebook;1038.0;10;;
2018-08-24 09:45:28;;Apache 2.0;https://www.kaggle.com/ashishpatel26/xception-on-google-image-ai;1.0;['tensorflow', 'keras', 'pillow'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'predict'];https://www.kaggle.com/c/google-ai-open-images-object-detection-track;0.692;0.319;2020-12-12 20:15:53;multiple data sources;['gpu'];Xception on Google Image AI;Python notebook;2585.0;12;0.01275;0.01513
2018-07-21 06:53:02;;Apache 2.0;https://www.kaggle.com/dinuuu/using-keras-inceptionv3-0-016795;1.0;['tensorflow', 'keras', 'pillow'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'predict'];https://www.kaggle.com/c/google-ai-open-images-object-detection-track;0.712;0.327;2020-12-12 20:15:53;multiple data sources;['gpu, data visualization, exploratory data analysis, +1 moreclassification'];Using Keras InceptionV3  0.016795;Python notebook;3997.0;13;;
2019-04-20 17:21:00;Projet commencÃ© le 04/02/2019;Apache 2.0;https://www.kaggle.com/lefuret/train-ai-open-images-object-detection-track;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/google-ai-open-images-object-detection-track;0.646;0.099;2020-12-12 20:15:53;multiple data sources;['gpu'];[TRAIN] AI Open Images - Object Detection Track;Python notebook;1018.0;1;;
2018-07-24 13:28:01;"This kernel will take the Google tsv files that were designed for downloading the full dataset and remove all the lines that are not in the training set. Make sure you run it with ""internet connected"" Download the trimmed tsv from the output tab of the kernel";Apache 2.0;https://www.kaggle.com/moshel/filling-your-bucketlist-tsv-trimmer;0.5;[];['ai', 'nn', 'cv'];['train'];https://www.kaggle.com/c/google-ai-open-images-object-detection-track;0.594;0.188;2020-12-12 20:15:53;Google AI Open Images - Object Detection Track;[];Filling your bucketlist - tsv trimmer;Python notebook;403.0;3;;
2019-10-19 21:09:39;Environment Setup;Apache 2.0;https://www.kaggle.com/soumikrakshit/yolo-v3-using-tensorflow-2-0;1.0;['tensorflow', 'keras'];['ai', 'cv', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'layer', 'loss', 'label', 'predict', 'rank', 'relu'];https://www.kaggle.com/c/google-ai-open-images-object-detection-track;0.687;0.236;2020-12-12 20:15:53;Google AI Open Images - Object Detection Track;['gpu, deep learning, neural networks'];YOLO V3 using Tensorflow 2.0;Python notebook;2294.0;5;;
2018-08-31 07:55:03;;Apache 2.0;https://www.kaggle.com/vipul92/object-detection-fast-tinyyolov3-imageai;1.0;['tensorflow', 'keras'];['ner', 'ai'];['model', 'object detection', 'epoch', 'predict'];https://www.kaggle.com/c/google-ai-open-images-object-detection-track;0.723;0.268;2020-12-12 20:15:53;multiple data sources;['image data'];Object Detection Fast-TinyYOLOv3-ImageAI;Python notebook;5231.0;7;;
2018-07-11 19:23:53;;Apache 2.0;https://www.kaggle.com/dinuuu/no-script;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning', 'predict'];https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track;0.635;0.0;2020-12-12 20:16:35;Google AI Open Images - Visual Relationship Track;['gpu'];No Script;Python script;832.0;0;0.00000;0.00000
2018-08-01 14:29:57;Train Data - What is there?;Apache 2.0;https://www.kaggle.com/mkagenius/fork-of-explore-a-bit;0.5;[];['ai', 'nn', 'cv'];['train', 'label', 'predict'];https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track;0.698;0.334;2020-12-12 20:16:35;multiple data sources;[];Fork of Explore a bit ;Python notebook;2905.0;14;0.00000;0.00000
2018-08-31 17:04:50;;Apache 2.0;https://www.kaggle.com/titericz/beat-the-benchmark-lb-0-0032;0.5;[];['ai', 'dl', 'cv'];['train', 'label', 'predict'];https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track;0.681;0.362;2020-12-12 20:16:35;multiple data sources;[];Beat the Benchmark LB 0.00119;R notebook;2045.0;19;0.00320;0.00350
2020-10-02 11:11:41;;Apache 2.0;https://www.kaggle.com/anshumoudgil/basketball-2020-vectors-feature-engg-strategy;1.0;['pattern'];['ner', 'ai', 'gan', 'rl', 'nn', 'ann'];['rank', 'classification', 'layer'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.624;0.387;2020-12-12 20:18:13;multiple data sources;['data visualization, feature engineering, sports, +1 morecategorical data'];Basketball 2020: vectors-feature engg.-Strategy;Rmarkdown script;683.0;25;;
2020-05-13 07:30:17;Here is a function I made in order to delete leaked data from the training set. It removes all the matches from the training set that appear also in the test set. It inputs two DataFrame : df_train (input data set) and df_test (test data set).;Apache 2.0;https://www.kaggle.com/catadanna/delete-leaked-from-training-ncaam-ncaaw-stage1;0.5;[];['ai'];['train', 'test data'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.616;0.334;2020-12-12 20:18:13;Google Cloud & NCAAÂ® ML Competition 2020-NCAAM;[];Delete Leaked from Training NCAAM/NCAAW - Stage1;Python notebook;594.0;14;;
2020-03-13 12:52:28;;Apache 2.0;https://www.kaggle.com/gyanendradas/xgboost-train-0-44;1.0;['xgboost', 'sklearn'];['ai', 'rl', 'nn', 'cv'];['train', 'model', 'layer', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.596;0.334;2020-12-12 20:18:13;Google Cloud & NCAAÂ® ML Competition 2020-NCAAM;[];Xgboost Train 0.44;Python notebook;418.0;14;;
2020-03-08 21:09:25;;Apache 2.0;https://www.kaggle.com/headsortails/jump-shot-to-conclusions-march-madness-eda;1.0;['pattern', 'xgboost'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'fitting', 'model', 'layer', 'loss', 'label', 'predict', 'rank', 'understanding', 'classification', 'ground truth'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.764;0.556;2020-12-12 20:18:13;multiple data sources;['beginner, data visualization, exploratory data analysis'];Jump Shot to Conclusions - March Madness EDA;Rmarkdown script;15182.0;223;;
2020-02-16 06:36:54;First of all, I want to apologize for my English, and you will see the reason.;Apache 2.0;https://www.kaggle.com/holoong9291/eda-for-ncaa-march-madness-en-just-for-men;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn'];['rank', 'loss', 'filter', 'layer'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.64;0.34;2020-12-12 20:18:13;Google Cloud & NCAAÂ® ML Competition 2020-NCAAM;['beginner, data visualization, exploratory data analysis'];EDA for NCAAÂ® March Madness - EN/ä¸­æ–‡ just for Men;Python notebook;910.0;15;;
2020-04-13 11:04:28;;Apache 2.0;https://www.kaggle.com/jaseziv83/applying-pythagorean-expectation-to-major-sports;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'regression', 'fitting', 'model', 'layer', 'loss', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.721;0.435;2020-12-12 20:18:13;multiple data sources;['data visualization, exploratory data analysis, regression'];Applying Pythagorean Expectation to Major Sports;Rmarkdown script;5017.0;44;;
2020-03-14 22:49:02;;Apache 2.0;https://www.kaggle.com/jaseziv83/moreyball-in-the-college-game-a-full-ncaa-eda;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['filter', 'model', 'layer', 'loss', 'label', 'predict', 'rank'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.707;0.477;2020-12-12 20:18:13;multiple data sources;['beginner, data visualization, exploratory data analysis'];Moreyball in the College Game... A Full NCAA EDA;Rmarkdown script;3588.0;74;;
2020-02-25 16:45:28;;Apache 2.0;https://www.kaggle.com/nxrprime/right-left-shoot-march-madness-eda-and-analysis;1.0;['xgboost', 'lightgbm'];['ner', 'ai', 'gan', 'gbm', 'cv', 'rl', 'nn', 'ml'];['filter', 'regression', 'layer', 'loss', 'label', 'predict', 'rank'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.652;0.334;2020-12-12 20:18:13;multiple data sources;[];Right, Left, Shoot: March Madness EDA and Analysis;Rmarkdown script;1138.0;14;;
2020-02-20 21:26:02;;Apache 2.0;https://www.kaggle.com/paulorzp/kenpom-scraper-2020;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'test data', 'model', 'deep learning', 'layer', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.644;0.362;2020-12-12 20:18:13;multiple data sources;[];Kenpom scraper 2020;R script;980.0;19;;
2020-03-28 15:42:18;2020 March MadnessIn this notebook I explore the 2020 Men's and Women's NCAA basketball data. Hopefully you find the analysis and code helpful. Feel free to use any of the helper functions in your code but please reference this as the original source.;Apache 2.0;https://www.kaggle.com/robikscube/2020-march-madness-data-first-look-eda;0.5;[];['ner', 'ai', 'gan', 'rl', 'nn', 'ml'];['layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.765;0.565;2020-12-12 20:18:13;multiple data sources;[];ðŸ€ 2020 March Madness Data - First Look EDA;Python notebook;15513.0;255;;
2020-02-27 06:18:36;"Plotting The CourtKaggle and the NCAA have provided event day for seasons 2015-2020. In this notebook I provide some helper functions for plotting these events on a ""court"" using python. Please feel free to use these plots in your code, just reference this kernel when you do so.";Apache 2.0;https://www.kaggle.com/robikscube/ncaa-basketball-court-plot-helper-functions;0.5;[];['ner', 'ai', 'nn'];['layer', 'label'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament;0.681;0.379;2020-12-12 20:18:13;multiple data sources;[];ðŸŽŸï¸ NCAA Basketball Court Plot Helper Functions;Python notebook;2024.0;23;;
2020-10-02 11:11:41;;Apache 2.0;https://www.kaggle.com/anshumoudgil/basketball-2020-vectors-feature-engg-strategy;1.0;['pattern'];['ner', 'ai', 'gan', 'rl', 'nn', 'ann'];['rank', 'classification', 'layer'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament;0.624;0.387;2020-12-12 20:19:12;multiple data sources;['data visualization, feature engineering, sports, +1 morecategorical data'];Basketball 2020: vectors-feature engg.-Strategy;Rmarkdown script;684.0;25;;
2020-02-19 11:19:03;;Apache 2.0;https://www.kaggle.com/chariots17/using-xgboost-lgb-to-predict;1.0;['xgboost', 'sklearn', 'lightgbm'];['ai', 'dl', 'gbm', 'cv', 'rl', 'nn', 'ml'];['regression', 'train', 'model', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament;0.663;0.362;2020-12-12 20:19:12;Google Cloud & NCAAÂ® ML Competition 2020-NCAAW;['xgboost']; Using XGBOOST&lgb to predict;Python notebook;1420.0;19;0.26295;0.26295
2020-04-13 11:04:28;;Apache 2.0;https://www.kaggle.com/jaseziv83/applying-pythagorean-expectation-to-major-sports;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'regression', 'fitting', 'model', 'layer', 'loss', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament;0.721;0.435;2020-12-12 20:19:12;multiple data sources;['data visualization, exploratory data analysis, regression'];Applying Pythagorean Expectation to Major Sports;Rmarkdown script;5018.0;44;;
2020-03-28 15:42:18;2020 March MadnessIn this notebook I explore the 2020 Men's and Women's NCAA basketball data. Hopefully you find the analysis and code helpful. Feel free to use any of the helper functions in your code but please reference this as the original source.;Apache 2.0;https://www.kaggle.com/robikscube/2020-march-madness-data-first-look-eda;0.5;[];['ner', 'ai', 'gan', 'rl', 'nn', 'ml'];['layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament;0.765;0.565;2020-12-12 20:19:12;multiple data sources;[];ðŸ€ 2020 March Madness Data - First Look EDA;Python notebook;15514.0;255;;
2020-02-27 06:18:36;"Plotting The CourtKaggle and the NCAA have provided event day for seasons 2015-2020. In this notebook I provide some helper functions for plotting these events on a ""court"" using python. Please feel free to use these plots in your code, just reference this kernel when you do so.";Apache 2.0;https://www.kaggle.com/robikscube/ncaa-basketball-court-plot-helper-functions;0.5;[];['ner', 'ai', 'nn'];['layer', 'label'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament;0.681;0.379;2020-12-12 20:19:12;multiple data sources;[];ðŸŽŸï¸ NCAA Basketball Court Plot Helper Functions;Python notebook;2025.0;23;;
2020-02-23 12:33:04;https://www.kaggle.com/takaishikawa/no-ml-modeling;Apache 2.0;https://www.kaggle.com/takaishikawa/no-ml-modeling-ncaaw2020;1.0;['sklearn'];['ai', 'ml', 'cv'];['layer', 'model', 'filter', 'loss'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament;0.574;0.292;2020-12-12 20:19:12;Google Cloud & NCAAÂ® ML Competition 2020-NCAAW;[];No ML Modeling - NCAAW2020;Python notebook;293.0;9;;
2020-02-23 15:09:57;NCAAÂ® March Madness: Exploratory Analysis;Apache 2.0;https://www.kaggle.com/vikassingh1996/ncaa-march-madness-exploratory-analysis-fe;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['predict', 'model', 'label', 'loss', 'rank', 'understanding'];https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament;0.617;0.302;2020-12-12 20:19:12;multiple data sources;['beginner, exploratory data analysis, feature engineering, +1 moresports'];ðŸ€NCAAÂ® March Madness: Exploratory Analysis + FE;Python notebook;598.0;10;;
2019-11-27 14:40:16;Features;Apache 2.0;https://www.kaggle.com/abazdyrev/use-features-oof;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'rl'];['train', 'model', 'epoch', 'layer', 'loss', 'predict', 'rank', 'understanding'];https://www.kaggle.com/c/google-quest-challenge;0.74;0.529;2020-12-12 20:20:08;multiple data sources;['gpu, beginner, deep learning, +1 morenlp'];USE + Features + OOF;Python notebook;7988.0;150;0.28843;0.32170
2019-11-23 06:11:35;;Apache 2.0;https://www.kaggle.com/hamditarek/get-started-with-nlp-lda-lsa;1.0;['nltk', 'gensim', 'sklearn', 'tensorflow', 'textblob', 'vocabulary', 'keras'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'train', 'fitting', 'model', 'epoch', 'natural language processing', 'layer', 'clustering', 'loss', 'label', 'predict', 'rank', 'understanding', 'relu', 'natural language'];https://www.kaggle.com/c/google-quest-challenge;0.705;0.468;2020-12-12 20:20:08;Google QUEST Q&A Labeling;[];Get Started with NLP: LDA/LSA;Python notebook;3436.0;66;0.28022;0.28996
2019-12-15 19:03:28;Pytorch BERT baseline;Apache 2.0;https://www.kaggle.com/idv2005/pytorch-bert-baseline;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'model', 'layer', 'label', 'predict', 'understanding', 'labeled'];https://www.kaggle.com/c/google-quest-challenge;0.73;0.486;2020-12-12 20:20:08;multiple data sources;['gpu'];PyTorch BERT Baseline;Python notebook;6195.0;83;0.34198;0.36247
2019-12-15 16:57:34;AcknowledgementsOiginal kernel: https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic;Apache 2.0;https://www.kaggle.com/khoongweihao/bert-base-tf2-0-minimalistic-iii;1.0;['tensorflow', 'sklearn', 'keras'];['ai'];['predict', 'train', 'model', 'output layer', 'epoch', 'layer', 'loss', 'understanding'];https://www.kaggle.com/c/google-quest-challenge;0.769;0.517;2020-12-12 20:20:08;multiple data sources;['gpu'];Bert-base TF2.0 (minimalistic) III;Python notebook;17357.0;127;0.36173;0.38516
2019-11-24 23:07:32;;Apache 2.0;https://www.kaggle.com/ldm314/universal-sentence-encoder-keras-nn;1.0;['tensorflow', 'keras'];['ai', 'nn', 'rl'];['predict', 'training data', 'test data', 'train', 'model', 'epoch', 'layer', 'loss', 'understanding'];https://www.kaggle.com/c/google-quest-challenge;0.722;0.476;2020-12-12 20:20:08;multiple data sources;['gpu'];Universal Sentence Encoder Keras NN;Python notebook;5132.0;73;0.30588;0.32912
2020-02-08 09:47:17;;Apache 2.0;https://www.kaggle.com/lhohoz/bert-base-pytorch-inference;1.0;['pytorch', 'sklearn'];['ai', 'nn', 'rl'];['train', 'model', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/google-quest-challenge;0.735;0.472;2020-12-12 20:20:08;multiple data sources;['gpu'];BERT-Base Pytorch(inference);Python notebook;6946.0;70;0.33387;0.38896
2015-07-01 03:46:55;;Apache 2.0;https://www.kaggle.com/acshock/how-noisy-are-these-eegs;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.671;0.188;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];How noisy are these EEGs?;Python script;1663.0;3;;
2015-07-30 21:26:05;;Apache 2.0;https://www.kaggle.com/ajoo88/low-freq-0-80-refactored;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'test data', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.702;0.214;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Low freq 0.80+ refactored;Python script;3208.0;4;0.82193;0.82157
2015-07-23 02:28:43;;Apache 2.0;https://www.kaggle.com/alexandrebarachant/beat-the-benchmark-0-67;1.0;['pattern', 'sklearn'];['ner', 'ai', 'nlp', 'nn', 'ann'];['filter', 'test data', 'regression', 'train', 'model', 'epoch', 'deep learning', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.826;0.473;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Beat the Benchmark. 0.708;Python script;115593.0;71;0.71350;0.70885
2015-07-02 01:43:06;;Apache 2.0;https://www.kaggle.com/alexandrebarachant/common-spatial-pattern-with-mne;1.0;['pattern', 'sklearn'];['ner', 'ai', 'gan', 'cv', 'nlp', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'epoch', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.785;0.469;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Common Spatial Pattern with MNE;Python script;27858.0;67;0.00000;0.00000
2015-07-13 07:39:32;;Apache 2.0;https://www.kaggle.com/alexandrebarachant/simple-grasp-cross-validation;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['training data', 'test data', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.711;0.34;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Simple Grasp Cross-validation;Python script;3894.0;15;0.00000;0.00000
2015-08-21 18:35:50;;Apache 2.0;https://www.kaggle.com/alexandrebarachant/visual-evoked-potential-vep;0.5;[];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'epoch', 'deep learning', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.753;0.427;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Visual Evoked Potential (VEP);Python script;11065.0;40;;
2015-08-22 07:15:37;;Apache 2.0;https://www.kaggle.com/anlthms/convnet-0-89;1.0;['sklearn'];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'label', 'predict', 'classification', 'convolutional neural network'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.772;0.405;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Convnet: AUC 0.934;Python script;19327.0;31;;
2015-08-13 02:23:40;;Apache 2.0;https://www.kaggle.com/bitsofbits/naive-nnet;1.0;['sklearn', 'theano'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'regression', 'train', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.778;0.411;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Naive NNet 0.76+ / 0.90+;Python script;22601.0;33;0.79195;0.78074
2015-06-27 00:56:46;;Apache 2.0;https://www.kaggle.com/datacanary/what-do-these-things-look-like;0.5;[];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.69;0.34;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];What do these things look like?;R script;2466.0;15;;
2015-07-03 19:32:44;;Apache 2.0;https://www.kaggle.com/datenkieker/beat-the-benchmark-0-67;1.0;['pattern', 'sklearn'];['ner', 'ai', 'nlp', 'nn', 'ann'];['filter', 'test data', 'regression', 'train', 'model', 'epoch', 'deep learning', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.662;0.253;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Beat the Benchmark. 0.67+;Python script;1374.0;6;;
2015-07-31 23:18:35;;Apache 2.0;https://www.kaggle.com/deepcnn/rf-lda-lr;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'test data', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.682;0.253;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];RF + LDA + LR;Python script;2089.0;6;0.84662;0.84250
2015-08-13 11:06:05;;Apache 2.0;https://www.kaggle.com/elenacuoco/simple-grasp-with-sklearn;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.761;0.435;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Simple Grasp with Sklearn. 0.70+ ;Python script;13975.0;44;0.70384;0.71433
2015-07-20 01:37:39;;Apache 2.0;https://www.kaggle.com/karma86/neural-nets;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.676;0.214;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Neural nets;Python script;1818.0;4;;
2015-07-22 06:40:43;;Apache 2.0;https://www.kaggle.com/karthikmurali11/logistic-regression-with-r-0-65;1.0;['pattern'];['nlp', 'ai', 'nn', 'ner'];['training data', 'test data', 'regression', 'train', 'model', 'deep learning', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.694;0.268;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Logistic Regression with R (0.65+);R script;2677.0;7;0.00000;0.00000
2015-07-22 16:06:07;;Apache 2.0;https://www.kaggle.com/korowiow/simple-grasp-with-sklearn-0-70;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.681;0.188;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Simple low freq, 0.80+;Python script;2011.0;3;0.76331;0.76803
2015-07-23 06:04:09;;Apache 2.0;https://www.kaggle.com/kumareshd/simple-low-freq-0-80;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.695;0.253;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Simple low freq, 0.80+;Python script;2747.0;6;0.82588;0.82539
2015-07-22 19:48:52;;Apache 2.0;https://www.kaggle.com/lnicalo/simple-low-freq-0-80;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.633;0.188;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Simple low freq, 0.80+;Python script;804.0;3;0.82365;0.82348
2015-07-23 03:17:36;;Apache 2.0;https://www.kaggle.com/stefaneng/simple-python-pandas-plots;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.681;0.188;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Simple Python Pandas Plots;Python script;2030.0;3;;
2015-07-11 01:34:25;;Apache 2.0;https://www.kaggle.com/titericz/simple-grasp-with-sklearn-giba;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['training data', 'test data', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.645;0.188;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];Simple Grasp with Sklearn Giba;Python script;996.0;3;0.00000;0.00000
2015-07-14 20:34:30;;Apache 2.0;https://www.kaggle.com/xophe92/first-view-of-the-data;0.5;[];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/grasp-and-lift-eeg-detection;0.696;0.292;2020-12-12 20:31:12;Grasp-and-Lift EEG Detection;[];First view of the data;R script;2793.0;9;;
2016-06-10 14:34:59;It looks like we are given quite a few sets as an input! Let's take a look at each one, starting with train and test.;Apache 2.0;https://www.kaggle.com/anokas/exploratory-data-analysis;0.5;[];['ai'];['gru', 'training data', 'train', 'label', 'predict'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.783;0.486;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Exploratory Data Analysis;Python notebook;26311.0;83;0.97639;0.96080
2016-06-08 15:24:33;;Apache 2.0;https://www.kaggle.com/anokas/optimised-beat-the-benchmark;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'gru', 'classification', 'deep learning'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.73;0.302;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Naive beat-the-benchmark (0.82706);Python script;6116.0;10;0.83638;0.82706
2016-08-16 09:49:11;;Apache 2.0;https://www.kaggle.com/apapiu/mean-vs-medians-a-mathy-approach;0.5;[];['dl', 'ai', 'nn', 'ml'];['train', 'model', 'predict'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.746;0.48;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Mean vs. Medians - A mathy approach;Rmarkdown script;9253.0;77;;
2016-08-31 09:12:22;;Apache 2.0;https://www.kaggle.com/apapiu/poor-man-s-xgboost;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['gru', 'filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.69;0.292;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Poor Man's Xgboost;R script;2483.0;9;0.48082;0.46810
2016-06-11 03:07:30;;Apache 2.0;https://www.kaggle.com/armalali/benchmark-medians;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['gru', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.718;0.302;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];BenchmarkMedians(0.52718 on Leaderboard);Python script;4605.0;10;;
2016-08-30 20:17:26;;Apache 2.0;https://www.kaggle.com/ben519/visualize-predictions;0.5;[];['nlp', 'ai', 'nn', 'ner'];['gru', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.696;0.302;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Visualize Predictions;R script;2831.0;10;;
2016-08-04 14:24:16;;Apache 2.0;https://www.kaggle.com/bpavlyshenko/bimbo-xgboost-r-script-lb-0-457;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['gru', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.781;0.496;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Bimbo XGBoost R script LB:0.457;R script;25044.0;95;0.59311;0.49626
2016-07-30 05:46:51;;Apache 2.0;https://www.kaggle.com/ericcouto/using-82-less-memory;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'gru', 'classification', 'deep learning'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.725;0.367;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Using ~82% less memory;Python script;5462.0;20;;
2016-07-19 11:22:02;;Apache 2.0;https://www.kaggle.com/fabienvs/grupo-bimbo-data-analysis;1.0;['pattern'];['ner', 'ai', 'nn', 'ann'];['train', 'gru', 'label', 'filter'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.803;0.516;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Grupo Bimbo data analysis;Rmarkdown script;51134.0;125;;
2016-06-11 11:27:03;;Apache 2.0;https://www.kaggle.com/lyytinen/basic-preprocessing-for-products;1.0;['pattern'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'gru', 'classification', 'deep learning'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.766;0.473;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Basic Preprocessing for Products;R script;15937.0;71;;
2016-06-14 08:08:13;"The Kaggle wiki page on RMSLE states that ""RMSLE penalizes an under-predicted estimate greater than an over-predicted estimate."" This notebook plots RMSLE for over- vs under-predictions to visualize this relationship.";Apache 2.0;https://www.kaggle.com/maxpowerwastaken/plotting-rmsle-for-over-vs-under-preds;0.5;[];['nn', 'ml'];['label', 'predict'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.649;0.302;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Plotting RMSLE for Over- vs Under- Preds;Python notebook;1068.0;10;;
2016-06-13 22:26:11;;Apache 2.0;https://www.kaggle.com/mlandry/h2o-gbm;1.0;['h2o'];['ner', 'ai', 'gbm', 'rl', 'ml', 'nlp', 'nn', 'ann'];['gru', 'training data', 'test data', 'train', 'model', 'deep learning', 'layer', 'predict', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.764;0.405;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];H2O GBM;R script;15045.0;31;;
2016-06-13 14:04:14;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/r-medians;0.5;[];['nlp', 'ai', 'nn', 'ner'];['gru', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.735;0.371;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];R medians;R script;6957.0;21;0.52837;0.50758
2016-08-26 02:00:02;;Apache 2.0;https://www.kaggle.com/paulorzp/log-mean-plus-lb-0-47000;0.5;[];['nlp', 'ai', 'nn', 'ner'];['gru', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.778;0.408;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Log_Mean_Plus LB: 0.47000;R script;22491.0;32;0.48818;0.47000
2016-08-22 17:41:53;;Apache 2.0;https://www.kaggle.com/rlomascolo/xgboost-python-script-lb-0-46249;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['gru', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.724;0.334;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];XGBoost Python script LB: 0.46249;Python script;5274.0;14;;
2016-06-29 20:11:10;;Apache 2.0;https://www.kaggle.com/scirpus/ftlr-use-pypy;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['gru', 'training data', 'regression', 'train', 'model', 'epoch', 'deep learning', 'validation data', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.738;0.367;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];FTLR (Use PYPY!);Python script;7547.0;20;;
2016-06-27 19:30:08;;Apache 2.0;https://www.kaggle.com/ybabakhin/products-clustering;0.5;[];['nlp', 'ai', 'nn', 'ner'];['gru', 'train', 'deep learning', 'k-means', 'clustering', 'classification'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.766;0.352;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Products Clustering;R script;15983.0;17;;
2016-06-11 17:30:59;;Apache 2.0;https://www.kaggle.com/zfturbo/simple-bimbo-solution;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'gru', 'classification', 'deep learning'];https://www.kaggle.com/c/grupo-bimbo-inventory-demand;0.747;0.352;2020-12-12 20:39:03;Grupo Bimbo Inventory Demand;[];Simple Bimbo Solution;Python script;9569.0;17;0.57360;0.54620
2020-09-05 04:20:21;This is my first competition and shared notebook so please forgive any newbie mistake. As part of participating in the Halite competition, I've looked at what top players do from time to time. This morning I've noticed that Leukocyte has been winning against the runaway No. 1 mzotkiew so I took a closer look at what happened. In short, Leukocyte seems to have taken hunting strategy up a notch. I look at the number of ships with zero halite as an indication of the level of hunting. In this game (https://www.kaggle.com/c/halite/submissions?dialog=episodes-episode-2473070), Leukocyte had a very high percentage of his ships with zero halite from step 80 onward. It reaches the peak at around step 300 when all his ships were hunting and how efficient they were! His total halite jumped from 1000 to 8000 in about 70-80 steps. In comparison, mzotkiew does not dedicate as many ships to hunting.;Apache 2.0;https://www.kaggle.com/huiunited/game-commentary;0.5;[];['ai', 'nn', 'rl'];['layer', 'label', 'reward'];https://www.kaggle.com/c/halite;0.629;0.387;2020-12-12 20:40:18;multiple data sources;[];Game Commentary;Python notebook;738.0;25;;
2020-07-02 18:26:44;What's this Visualize battle results from leaderboard downloaded or use this notebook with any local result json file;Apache 2.0;https://www.kaggle.com/iicyan/halite-result-visualization-from-leaderboard;0.3;[];[];['layer', 'label'];https://www.kaggle.com/c/halite;0.633;0.397;2020-12-12 20:40:17;multiple data sources;[];[Halite] Result Visualization from leaderboard;Python notebook;795.0;28;;
2020-07-06 03:43:29;What's this ä¸»ã«ãƒãƒ¼ãƒ å†…ã§ã®æƒ…å ±å…±æœ‰ã‚’ç›®çš„ã¨ã—ãŸæ–‡æ›¸ã§ã™ãŒã›ã£ã‹ããªã®ã§publicã«ã—ã¦ã„ã¾ã™ ã“ã®Notebookã¯æœ¬ã‚³ãƒ³ãƒšã®ãƒ«ãƒ¼ãƒ«ã®ç¿»è¨³ãªã©ã€ã¨ã«ã‹ãå‚åŠ ã™ã‚‹å‰ã«èª­ã‚“ã§ãŠãã¨è‰¯ã•ãã†ãªã“ã¨ã‚’ã¾ã¨ã‚ã¦ã„ãã¾ã™;Apache 2.0;https://www.kaggle.com/iicyan/japanese-halite-introduction;0.5;[];['ner', 'rl'];['generation'];https://www.kaggle.com/c/halite;0.701;0.469;2020-12-12 20:40:17;Halite by Two Sigma;[];[Japanese] Halite ã‚³ãƒ³ãƒšã®æ¦‚è¦ introduction;Python notebook;3096.0;67;;
2020-08-24 11:38:40;;Apache 2.0;https://www.kaggle.com/kwabenantim/stable-baselines-starter;1.0;['tensorflow'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ml'];['train', 'model', 'epoch', 'reward', 'layer', 'label', 'predict', 'rank', 'recommend'];https://www.kaggle.com/c/halite;0.709;0.439;2020-12-12 20:40:17;multiple data sources;['simulations'];Stable-Baselines Starter;Python notebook;3720.0;46;;
2020-08-22 05:51:08;Swarm Intelligence with SDK;Apache 2.0;https://www.kaggle.com/kwabenantim/swarm-intelligence-with-sdk;0.5;[];['ai', 'dl'];['layer'];https://www.kaggle.com/c/halite;0.695;0.425;2020-12-12 20:40:17;Halite by Two Sigma;['simulations'];Swarm Intelligence with SDK;Python notebook;2716.0;39;;
2020-06-24 03:28:03;Halite Beginner's Notebook: Disecting the Swarm;Apache 2.0;https://www.kaggle.com/melissarajaram/swarm-dissected-halite-beginners;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'understanding', 'reward', 'layer'];https://www.kaggle.com/c/halite;0.654;0.387;2020-12-12 20:40:18;Halite by Two Sigma;['beginner'];Swarm Dissected: Halite Beginners;Python notebook;1191.0;25;714.0;714.0
2020-06-18 02:46:41;"Submit to Competition ""Save & Run All"" (commit) this Notebook Go to the notebook viewer Go to ""Data"" section and find submission.py file. Click ""Submit to Competition"" Go to My Submissions to view your score and episodes being played.";Apache 2.0;https://www.kaggle.com/mylesoneill/halite-template-bot;0.3;[];[];['layer'];https://www.kaggle.com/c/halite;0.717;0.421;2020-12-12 20:40:17;Halite by Two Sigma;[];Halite Template Bot;Python notebook;4544.0;37;;
2020-06-16 00:05:58;If you're new to Halite, check out Alexis' Getting Started With Halite Notebook;Apache 2.0;https://www.kaggle.com/sam/halite-sdk-overview;0.5;[];['ner', 'ai', 'nn', 'ann'];['layer'];https://www.kaggle.com/c/halite;0.784;0.527;2020-12-12 20:40:17;Halite by Two Sigma;[];Halite SDK Overview;Python notebook;27466.0;146;;
2020-07-04 00:00:25;We will work out how to compare going to various cells containing halite and determine which is the optimal one to go to. NOTE: update July 3, 2020:  This new notebook does the calculation including carried halite by ships:  https://www.kaggle.com/solverworld/optimal-mining-with-carried-halite;Apache 2.0;https://www.kaggle.com/solverworld/optimal-mining;0.5;[];['ai', 'ml', 'rl'];['label'];https://www.kaggle.com/c/halite;0.666;0.429;2020-12-12 20:40:17;Halite by Two Sigma;[];Optimal Mining;Python notebook;1500.0;41;;
2020-07-01 22:35:49;"PROBLEM 1If you mine a cell containing H halite for m turns, what is the total amount mined? ANSWER Since 25% is mined at each step, 75% remains.  Thus .75mH remains after m steps mining, and (1âˆ’0.75m)H is mined by the ship.  Note: since the halite increases by 2% each square, this can be modified to account for that; it is a small adjustment and we will ignore it for the time being. Total Mined=(1âˆ’.75m)H PROBLEM 2A ship can travel to a square in n1 steps, mine the halite there (at 25% each step) and then return to a shipyard that is n2 steps from the halite cell.  What is the correct number of steps to mine in order to maximize the overall halite per step?  NEW: Ship is carrying C halite.";Apache 2.0;https://www.kaggle.com/solverworld/optimal-mining-with-carried-halite;0.5;[];['ai'];['label'];https://www.kaggle.com/c/halite;0.664;0.427;2020-12-12 20:40:17;Halite by Two Sigma;[];Optimal Mining with Carried Halite;Python notebook;1434.0;40;;
2020-07-03 19:05:55;This agent uses an optimal assignment algorithm to select targets for each ship.  The value of targets is as described in  https://www.kaggle.com/solverworld/optimal-mining-with-carried-halite . An earlier version of this agorithm is discussed in notebook kaggle.com/solverworld/optimal-mining.;Apache 2.0;https://www.kaggle.com/solverworld/optimus-mine-agent;0.5;[];['ner', 'ai', 'rl', 'cv', 'nn'];['layer', 'filter'];https://www.kaggle.com/c/halite;0.701;0.465;2020-12-12 20:40:17;Halite by Two Sigma;['optimization, simulations'];Optimus Mine Agent;Python notebook;3109.0;64;891.9;891.9
2020-05-18 13:44:11;ðŸ¤– ðŸš€ ðŸ¤– ðŸš€ ðŸ¤–   BoilerBot v3.1 at your service   ðŸ¤– ðŸš€ ðŸ¤– ðŸš€ ðŸ¤– The following code provides a framework to write bot strategies easily. You write strategies, which emit so-called plans (e.g. to move to a distant location) and the framework does the resolution and bot control. I can provide more documentation or updates, if I see that people are interested.;Apache 2.0;https://www.kaggle.com/superant/halite-boilerbot;0.5;[];['ner', 'ai', 'dl', 'nn', 'ann'];['train', 'generation', 'reward', 'layer'];https://www.kaggle.com/c/halite;0.66;0.39;2020-12-12 20:40:17;Halite by Two Sigma;[];Halite BoilerBot;Python notebook;1332.0;26;;
2020-05-22 18:53:08;The purpose of this notebook is to answer some questions about how to approach making an RL Agent for Halite. An important note: great results have not been reached with this design, but this is mainly due to short training times, untuned hyperparameters, and a sparse reward system. The design that we will be looking at contains three parts: an agent, an agent helper, and an environment.   *Simulated Environment does not necessarily provide the reward;Apache 2.0;https://www.kaggle.com/tiger37/reinforcement-learning-meets-halite;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['predict', 'train', 'reward', 'layer', 'loss'];https://www.kaggle.com/c/halite;0.706;0.437;2020-12-12 20:40:17;Halite by Two Sigma;[];Reinforcement Learning meets Halite;Python notebook;3538.0;45;;
2020-04-14 23:32:32;"IntroductionThis notebook contains a couple of example rules-based agents for Halite. As you will see, neither of them is actually using any form of learning. They are intended to be examples of fairly intuitive strategies that mostly boil down to ""grab as much halite as you can, as quickly as you can"". I doubt either of them will earn a particularly high rank in the competition, but they may be useful test opponents for more complex strategies. I have tried to make the code relatively clear, but there may still be some rough edges or comments that don't quite match up, so please let me know if you find anything that's off.";Apache 2.0;https://www.kaggle.com/tmbond/halite-example-agents;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['rank', 'loss', 'reward', 'layer'];https://www.kaggle.com/c/halite;0.693;0.397;2020-12-12 20:40:17;Halite by Two Sigma;[];Halite Example Agents;Python notebook;2633.0;28;;
2020-09-03 00:58:53;Install kaggle-environments;Apache 2.0;https://www.kaggle.com/yegorbiryukov/halite-swarm-intelligence;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'reward', 'layer'];https://www.kaggle.com/c/halite;0.774;0.557;2020-12-12 20:40:17;Halite by Two Sigma;['beginner, games, simulations'];Halite Swarm Intelligence;Python notebook;20141.0;227;752.0;752.0
2020-07-11 16:47:31;Install kaggle-environments;Apache 2.0;https://www.kaggle.com/yegorbiryukov/pirate-haven;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'reward', 'layer'];https://www.kaggle.com/c/halite;0.656;0.379;2020-12-12 20:40:18;Halite by Two Sigma;[];ðŸ´â€â˜ ï¸ Pirate Haven ðŸ´â€â˜ ï¸;Python notebook;1234.0;23;725.3;725.3
2020-07-27 07:01:20;;Apache 2.0;https://www.kaggle.com/angelhenriquez1/circle-of-hash;0.5;[];['ai', 'nn', 'gan'];['filter'];https://www.kaggle.com/c/hashcode-photo-slideshow;0.475;0.099;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];Circle of Hash;R notebook;72.0;1;;
2020-06-15 11:22:15;;Apache 2.0;https://www.kaggle.com/ar89dsl/hash-optimisation-multiple-approaches-430k;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['model', 'filter'];https://www.kaggle.com/c/hashcode-photo-slideshow;0.578;0.188;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];Hash Optimisation: Multiple Approaches (427k);Rmarkdown script;315.0;3;430894;430894
2020-07-27 23:58:25;;Apache 2.0;https://www.kaggle.com/arianit1990/kernel47d85f423c;0.0;[];[];[];https://www.kaggle.com/c/hashcode-photo-slideshow;0.403;0.0;2020-12-12 20:40:57;multiple data sources;[];kernel47d85f423c;Python notebook;30.0;0;;
2020-06-15 14:16:55;Optimizing a photo album from Hash Code 2019I think it is not necessary to implement greedy search through all images. Instead, I tried to split all photos into several subsequences and optimized them individually. Stages: arrange photos post processing;Apache 2.0;https://www.kaggle.com/egrehbbt/442k-in-2-hours;0.5;[];['dl', 'ai', 'nn', 'rl'];['model', 'label', 'loss'];https://www.kaggle.com/c/hashcode-photo-slideshow;0.628;0.319;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;['optimization'];442k in 2 hours;Python notebook;724.0;12;442742;442742
2020-05-08 18:48:31;I think an EDA would be helpful in this competition so I decided to create an extremely basic EDA where you can estimate the optimal amount of candidates to use in a greedy solution.;Apache 2.0;https://www.kaggle.com/fatehaliyev/extremely-basic-eda;0.2;[];['ai', 'dl'];[];https://www.kaggle.com/c/hashcode-photo-slideshow;0.559;0.099;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];Extremely Basic EDA;Python notebook;232.0;1;;
2020-06-08 15:51:09;This code shows how to put the pictures in order they came in, or random order. And other useful functions;Apache 2.0;https://www.kaggle.com/gb00000/hash-opt;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/hashcode-photo-slideshow;0.546;0.0;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];hash opt;Python notebook;193.0;0;;
2020-04-30 09:32:37;Greedy solution to the Hashcode problemI fell in love with optimization thanks to the Kaggle Christmas competitions. As a result, I participated in the Google Hashcode for the first time this year. As a preparation, I also solved the one of 2019, which is exactly this problem! Here is my greedy solution. Note that this can be improved A LOT by integer programming (it's basically a Traveling Salesman Problem but with the extra challenge of having vertical and horizontal photos)! I.e., you basically want to visit all the photos (cities) and maximize a score. My greedy solution is mainly based on this blog post.;Apache 2.0;https://www.kaggle.com/group16/greedy-solution-lb-400k;0.5;[];['dl', 'ai', 'nn', 'ann'];['filter'];https://www.kaggle.com/c/hashcode-photo-slideshow;0.706;0.393;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];Greedy solution (LB: 340K-350K);Python notebook;3473.0;27;415376;415376
2020-05-01 15:01:11;Local neighborhood search (swapping) by solving the Traveling Salesman ProblemIn this notebook, I show how we can iteratively improve our solution by taking subsequences and then solving the TSP (using Google OR-Tools) on that subsequence. This already improves a strong greedy solution, but is still not yet optimal due to the fact that I only swap the order of (subsequences of) slides around. I will not look for better pairs of vertical photos to put on a slide. You can play around with the hyper-parameters SUBSEQ_LEN and TIME_LIMIT to get a better score. The initial greedy solution was generated in this notebook. If you like the notebook, or you use it for your submission, please do not forget to upvote it! I do not like to ask for this, but apparently it is needed in this competition... (My previous notebook has more forks than upvotes);Apache 2.0;https://www.kaggle.com/group16/tsp-swapper;0.5;[];['ner', 'ai', 'dl', 'nn', 'ann'];['train', 'model', 'filter'];https://www.kaggle.com/c/hashcode-photo-slideshow;0.647;0.34;2020-12-12 20:40:57;multiple data sources;[];TSP Swapper;Python notebook;1043.0;15;416492;416492
2020-06-19 06:00:25;We could also do some mergesort alike thing to crate pairs of slides. But here I will avoid using my brain and just do it as greedily(?) as possible TODO:  Fix the slide picking so that if there is no possible slide to add (that would improve score), it does one of the two I mentioned down there  (preferably creating a new slide and merging it to the existing one from the optimal point.) Remove the hardcoded slide count and switch to the while len lenghts >0 loop (Maybe add a debug true thing to limit runtime to make it easier to see if everything runs fine) Implement greedy swapping (Existing random pimpMySlide is horrible and never improves Implement vertical picture swapping Make the vertical picture-> slide part of the code a bit smarter (Some runtime improvement would be great too);Apache 2.0;https://www.kaggle.com/hasimsait/greedy;0.2;[];['dl', 'ai', 'nn', 'rl'];[];https://www.kaggle.com/c/hashcode-photo-slideshow;0.551;0.152;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];Greedy;Python notebook;207.0;2;407077;407077
2020-05-31 12:43:24;"EditorialPlease refer to my Exploratory Data Analysis for my summary of the problem and some insights on the dataset. For any approach, the steps can be classified into two stages, both of which should be improved  You need to match vertical photos into ""combined"" photos. You need to arrange the combined/horizontal photos for maximum score.";Apache 2.0;https://www.kaggle.com/huikang/441k-in-11-mins;0.5;[];['ai', 'dl', 'rl', 'nn', 'ann'];['label'];https://www.kaggle.com/c/hashcode-photo-slideshow;0.67;0.379;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];441k in 10 mins;Python notebook;1608.0;23;441278;441278
2020-05-31 12:25:50;"Please refer to the problem statement for the full elaboration of the problem. To summarise the problem statement:  You are given a list photos. Each photo is either ""horizontal"" or ""vertical"". Each photo has a list of tags. If the photo is vertical, you will need to merge it with another vertical photo to form a combined photo. The combined photo will contain all the tags from both photos. You will need to provide a sequence of combined and horizontal photos to that maximises the score of your solution. The score of your solution is the sum of scores between all pairs of neighbours in your produced sequence. The score between neighbouring combined/horizontal photos is the minimum of  tags found in both neighbours tags found only in the first neighbour tags found only in the second neighbour    In this notebook, I present a baseline solution that only takes in information of the tag length and not the tags, along with some data analysis and visualisations. This should be sufficient for you to write an algorithm that is improved from this baseline.";Apache 2.0;https://www.kaggle.com/huikang/hc-2019q-eda-and-baseline-soln;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'recommend', 'label', 'understanding'];https://www.kaggle.com/c/hashcode-photo-slideshow;0.634;0.281;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];hc-2019q-eda-and-baseline-soln;Python notebook;805.0;8;;
2020-06-10 20:30:12;Store the data in the variables.;Apache 2.0;https://www.kaggle.com/lsiddiqsunny/simple-hashing-solution-for-hash-code-2019;0.2;[];['dl'];[];https://www.kaggle.com/c/hashcode-photo-slideshow;0.59;0.099;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];Simple Hashing Solution for Hash Code 2019;Python notebook;379.0;1;;
2020-07-05 15:26:32;"About the CompetitionNote: Put your heads together to solve programming challenges. Google's coding competition, Hash Code, has just finished for 2020. Use this online qualifier from 2019 to keep your skills sharp for future competitions! As the saying goes, ""a picture is worth a thousand words."" We agree â€“ photos are an important part of contemporary digital and cultural life. How we experience photos largely depends on the story theyâ€™re arranged to tell. The same shots could be a monotonous series of snaps or form a narrative masterpiece. Approximately 2.5 billion people around the world carry a camera â€“ in the form of a smartphone â€“ in their pocket every day. We tend to make good use of it, too, taking more photos than ever (back in 2017, Google Photos announced it was backing up more than 1.2 billion photos and videos per day)! The rise of digital photography creates an interesting challenge: what should we do with all of these photos? In this competition, you will compose a slideshow out of a photo collection. Given a list of photos and the tags associated with each photo, you are challenged to arrange the photos into a slideshow that is as interesting as possible (the evaluation section explains what we mean by â€œinterestingâ€) Will your slideshow tell a good story or be a major snoozefest?";Apache 2.0;https://www.kaggle.com/mahmudds/hash-code-archive-photo-slideshow-optimization;0.5;[];['ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['model', 'label', 'loss'];https://www.kaggle.com/c/hashcode-photo-slideshow;0.593;0.292;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];Hash Code Archive - Photo Slideshow Optimization;Python notebook;400.0;9;442742;442742
2020-04-28 17:41:33;HashCode2019;Apache 2.0;https://www.kaggle.com/mathurinache/googlehashcode2019-starter-code;0.2;[];['ai', 'dl'];[];https://www.kaggle.com/c/hashcode-photo-slideshow;0.681;0.346;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;[];GoogleHashCode2019 Starter Code;Python notebook;2036.0;16;173344;173344
2020-06-20 12:55:18;Greedy solution to the Hashcode problemI fell in love with optimization thanks to the Kaggle Christmas competitions. As a result, I participated in the Google Hashcode for the first time this year. As a preparation, I also solved the one of 2019, which is exactly this problem! Here is my greedy solution. Note that this can be improved A LOT by integer programming (it's basically a Traveling Salesman Problem but with the extra challenge of having vertical and horizontal photos)! I.e., you basically want to visit all the photos (cities) and maximize a score. The gist of what we do is the following:  In each iteration, we try to assign either 1 horizontal or 2 vertical photos such that the gain in score is maximized. Brute-forcing all options (all possible horizontal & combinations of 2 out of the vertical photos) would take way too long (and would still not give     you the optimal solution). As such, we create a list of 100 candidates (candidate = 1 horizontal or 2 vertical photos) in each iteration. The candidates are chosen such that they have as many tags in common with the previously assigned slide, as this heuristically improved the score.;Apache 2.0;https://www.kaggle.com/razamh/hash-code2019;0.5;[];['dl', 'ai', 'nn', 'ann'];['filter'];https://www.kaggle.com/c/hashcode-photo-slideshow;0.501;0.214;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;['gpu'];Hash code2019;Python notebook;102.0;4;;
2020-06-08 14:51:21;;Apache 2.0;https://www.kaggle.com/saraswatitiwari/kernel33f8061544;0.2;[];['dl', 'ai', 'nn'];[];https://www.kaggle.com/c/hashcode-photo-slideshow;0.488;0.152;2020-12-12 20:40:57;Hash Code Archive - Photo Slideshow Optimization;['gpu'];kernel33f8061544;Python notebook;86.0;2;;
2020-05-27 22:23:58;Import modules;Apache 2.0;https://www.kaggle.com/aliaksandr960/herbarium-data-exploration;1.0;['sklearn'];['ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['training data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/herbarium-2020-fgvc7;0.467;0.0;2020-12-12 20:41:48;Herbarium 2020 - FGVC7;[];herbarium data exploration;Python notebook;65.0;0;;
2020-05-20 12:09:14;;Apache 2.0;https://www.kaggle.com/gb00000/herb-nn;1.0;['pytorch', 'keras', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/herbarium-2020-fgvc7;0.555;0.099;2020-12-12 20:41:48;Herbarium 2020 - FGVC7;['gpu'];herb nn;Python notebook;219.0;1;0.00000;0.00000
2020-03-12 07:00:25;PeekThis notebook is here to just unify the dataset into one. I will perform further analysis and the Deep Learning algorithm in a future kernel. If you like this kernel, or forked this version, please upvote. First step, we peek at the data paths:;Apache 2.0;https://www.kaggle.com/jagannathrk/herbarium-2020;0.5;[];['ner', 'ai', 'dl', 'nn', 'ann'];['train', 'test data', 'deep learning', 'predict'];https://www.kaggle.com/c/herbarium-2020-fgvc7;0.624;0.253;2020-12-12 20:41:48;Herbarium 2020 - FGVC7;[];Herbarium 2020;Python notebook;675.0;6;0.00001;0.00002
2020-05-03 07:28:22;Merge;Apache 2.0;https://www.kaggle.com/khotijahs1/identify-plant-species-from-herbarium-specimens;1.0;['sklearn'];['ai', 'dl', 'rl', 'nn', 'ann'];['train', 'model', 'filter', 'predict'];https://www.kaggle.com/c/herbarium-2020-fgvc7;0.523;0.236;2020-12-12 20:41:48;Herbarium 2020 - FGVC7;[];Identify plant species from herbarium specimens. ;Python notebook;139.0;5;0.00002;0.00000
2020-06-05 23:31:13;Data preparation;Apache 2.0;https://www.kaggle.com/michaelschastlivcev/herbarium-2020-pytorch;1.0;['pytorch', 'albumentations', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ann'];['test data', 'train', 'recognition', 'model', 'epoch', 'validation data', 'layer', 'loss', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/herbarium-2020-fgvc7;0.46;0.0;2020-12-12 20:41:48;multiple data sources;[];Herbarium 2020 PyTorch;Python notebook;60.0;0;;
2020-06-08 07:29:14;Import modules;Apache 2.0;https://www.kaggle.com/riabovanderew/h-2fc-ce-d;1.0;['albumentations', 'sklearn'];['ai', 'nn', 'ann', 'cv'];['filter', 'training data', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/herbarium-2020-fgvc7;0.442;0.0;2020-12-12 20:41:48;multiple data sources;[];h_2fc_ce_d;Python notebook;48.0;0;;
2020-06-30 06:58:50;Herbarium - EfficientNetB3;Apache 2.0;https://www.kaggle.com/rivilcan/herbarium-efficientnetb3;1.0;['tensorflow', 'sklearn', 'keras', 'pillow'];['ner', 'ai', 'ml', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss'];https://www.kaggle.com/c/herbarium-2020-fgvc7;0.507;0.0;2020-12-12 20:41:48;multiple data sources;[];Herbarium - EfficientNetB3;Python notebook;111.0;0;;
2020-12-12 13:37:52;Importing and Setting up;Apache 2.0;https://www.kaggle.com/vishnuvardhanvm/herbarium-2020;1.0;['sklearn'];['ner', 'ai', 'dl', 'cnn', 'nn', 'ann'];['training data', 'train', 'model', 'label', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/herbarium-2020-fgvc7;0.526;0.236;2020-12-12 20:41:48;multiple data sources;[];Herbarium 2020;Python notebook;144.0;5;;
2020-03-28 13:42:09;To see what we have here I decided to just use code from my other notebooks.  Let's see what we can find out.;Apache 2.0;https://www.kaggle.com/wojciech1103/herbarium-2020-a-little-bit-fun-with-images;0.5;[];['ai', 'nn', 'ann', 'rl'];['train', 'label'];https://www.kaggle.com/c/herbarium-2020-fgvc7;0.581;0.099;2020-12-12 20:41:48;Herbarium 2020 - FGVC7;[];Herbarium 2020 - a little bit fun with images;Python notebook;330.0;1;;
2020-09-10 20:58:36;;Apache 2.0;https://www.kaggle.com/jiahuali/jiahua-li-higgs-bosons;1.0;['sklearn'];['ai', 'nn', 'ann'];['training data', 'regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'naive bayes'];https://www.kaggle.com/c/higgs-boson;0.519;0.099;2020-12-12 20:42:09;Higgs Boson Machine Learning Challenge;[];Jiahua Li Higgs Bosons;Python notebook;131.0;1;;
2020-07-19 21:11:41;Read data;Apache 2.0;https://www.kaggle.com/makhloufsabir/higgs-boson-classification-physics;1.0;['pytorch', 'tensorflow', 'xgboost', 'sklearn'];['ann', 'ai', 'nn', 'ml'];['filter', 'random forest', 'regression', 'train', 'model', 'gradient descent', 'loss', 'label', 'logistic regression', 'gradient boosting', 'predict', 'rank', 'decision tree', 'classification', 'naive bayes'];https://www.kaggle.com/c/higgs-boson;0.631;0.214;2020-12-12 20:42:09;Higgs Boson Machine Learning Challenge;['classification, xgboost, astronomy, +1 morephysics'];Higgs Boson Classification - Physics;Python notebook;772.0;4;2.77083;2.75018
2020-07-24 20:19:19;1-DNN;Apache 2.0;https://www.kaggle.com/makhloufsabir/higgs-boson-classification-physics-rnn;1.0;['xgboost', 'sklearn', 'pytorch', 'tensorflow', 'keras'];['ai', 'dl', 'cv', 'rl', 'nn', 'rnn'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'rank', 'relu'];https://www.kaggle.com/c/higgs-boson;0.618;0.188;2020-12-12 20:42:09;Higgs Boson Machine Learning Challenge;['rnn, astronomy, physics, +1 morednn'];Higgs Boson Classification - Physics - xgboost;Python notebook;609.0;3;2.99516;2.97492
2019-04-22 21:22:18;Higgs Boson Analysis;Apache 2.0;https://www.kaggle.com/shubhendra7/higgs-boson-analysis;0.5;[];['ai'];['train', 'label', 'training data'];https://www.kaggle.com/c/higgs-boson;0.723;0.253;2020-12-12 20:42:09;Higgs Boson Machine Learning Challenge;[];Higgs Boson Analysis;Python notebook;5151.0;6;;
2019-03-04 09:57:20;;Apache 2.0;https://www.kaggle.com/ssismasterchief/identifying-higgs-boson-atlas-h2o;1.0;['xgboost', 'h2o'];['ner', 'ai', 'automl', 'rl', 'nn', 'ml'];['train', 'model', 'epoch', 'deep learning', 'validation data', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/higgs-boson;0.687;0.214;2020-12-12 20:42:09;Higgs Boson Machine Learning Challenge;[];Identifying.Higgs.Boson-ATLAS-h2o;Python notebook;2325.0;4;;
2019-03-21 23:15:52;Simple custom generator;Apache 2.0;https://www.kaggle.com/bonhart/pytorch-cnn-from-scratch;1.0;['keras', 'sklearn'];['ner', 'ai', 'cnn', 'cv', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/histopathologic-cancer-detection;0.706;0.397;2020-12-12 20:43:30;Histopathologic Cancer Detection;['gpu, beginner'];PyTorch CNN from scratch;Python notebook;3484.0;28;0.9161;0.9563
2018-11-19 21:27:59;;Apache 2.0;https://www.kaggle.com/CVxTz/cnn-starter-nasnet-mobile-0-9709-lb;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'labeled'];https://www.kaggle.com/c/histopathologic-cancer-detection;0.782;0.528;2020-12-12 20:43:30;Histopathologic Cancer Detection;['gpu'];CNN Starter - NasNet Mobile ;Python notebook;25945.0;149;0.9584;0.9709
2019-05-07 13:15:37;;Apache 2.0;https://www.kaggle.com/mpalermo/keras-pipeline-custom-generator-imgaug;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['training data', 'train', 'model', 'output layer', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/histopathologic-cancer-detection;0.744;0.362;2020-12-12 20:43:30;Histopathologic Cancer Detection;['gpu'];Keras Pipeline(custom generator + imgaug);Python script;8877.0;19;0.9506;0.9557
2019-01-01 20:44:04;CNN with Keras;Apache 2.0;https://www.kaggle.com/sdelecourt/cnn-with-keras;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'rl', 'nn'];['filter', 'train', 'model', 'epoch', 'validation data', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/histopathologic-cancer-detection;0.698;0.371;2020-12-12 20:43:30;Histopathologic Cancer Detection;['gpu'];CNN with Keras;Python notebook;2951.0;21;;
2019-02-19 14:43:46;STEP 1 - DATASETS;Apache 2.0;https://www.kaggle.com/sermakarevich/complete-handcrafted-pipeline-in-pytorch-resnet9;1.0;['sklearn'];['ner', 'ai', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/histopathologic-cancer-detection;0.724;0.427;2020-12-12 20:43:30;Histopathologic Cancer Detection;['data visualization, deep learning, classification'];Complete handcrafted pipeline in PyTorch (Resnet9);Python notebook;5396.0;40;;
2020-10-07 21:56:46;HIV Progression;Apache 2.0;https://www.kaggle.com/cristianfat/hiv-progression;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'ann', 'cv'];['train', 'fitting', 'model', 'predict', 'classification'];https://www.kaggle.com/c/hivprogression;0.529;0.152;2020-12-12 20:43:57;Predict HIV Progression;['classification, xgboost, binary classification'];HIV Progression;Python notebook;150.0;2;;
2020-10-15 15:26:06;;Apache 2.0;https://www.kaggle.com/lingyuxiong/predict-hiv-progression;1.0;['xgboost', 'sklearn'];['ai', 'nn'];['train', 'model', 'random forest', 'predict'];https://www.kaggle.com/c/hivprogression;0.486;0.0;2020-12-12 20:43:57;Predict HIV Progression;['xgboost, svm, randomForest'];Predict HIV Progression;Python notebook;84.0;0;;
2019-08-21 06:59:08;No missing Values;Apache 2.0;https://www.kaggle.com/tiwaris436/predict-hiv-progression;1.0;['tensorflow', 'sklearn'];['ai', 'nn'];['regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/hivprogression;0.557;0.0;2020-12-12 20:43:57;Predict HIV Progression;[];Predict HIV Progression;Python notebook;225.0;0;;
2018-06-08 13:56:21;;Apache 2.0;https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance;1.0;['sklearn'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'train', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/home-credit-default-risk;0.825;0.634;2020-12-12 20:47:32;Home Credit Default Risk;['beginner, data visualization, exploratory data analysis, +1 morebinary classification'];Home Credit : Complete EDA + Feature Importance âœ“âœ“;Python notebook;113258.0;806;;
2018-05-30 23:28:19;;Apache 2.0;https://www.kaggle.com/eliotbarr/stacking-test-sklearn-xgboost-catboost-lightgbm;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/home-credit-default-risk;0.776;0.534;2020-12-12 20:47:32;Home Credit Default Risk;['beginner, xgboost'];Stacking Test-Sklearn, XGBoost, CatBoost, LightGBM;Python script;21220.0;162;0.75792;0.75787
2018-05-20 03:41:06;Here's how I load the data and reduce the memory usage of each dataframe.  I can save from 60% to 75% of memory usage on each dataframe. This method is inspired from this kernel. I don't handle NANs at this point. Hope it helps.;Apache 2.0;https://www.kaggle.com/gemartin/load-data-reduce-memory-usage;0.5;[];['ai', 'dl'];['train'];https://www.kaggle.com/c/home-credit-default-risk;0.778;0.569;2020-12-12 20:47:32;Home Credit Default Risk;[];load data (reduce memory usage);Python notebook;22969.0;271;;
2018-09-27 07:52:12;;Apache 2.0;https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'test data', 'train', 'fitting', 'model', 'deep learning', 'gradient boosting', 'predict', 'classification', 'bayesian'];https://www.kaggle.com/c/home-credit-default-risk;0.814;0.616;2020-12-12 20:47:32;Home Credit Default Risk;['finance, gradient boosting'];LightGBM with Simple Features;Python script;74240.0;590;0.79051;0.79070
2018-07-04 01:11:52;;Apache 2.0;https://www.kaggle.com/kailex/tidy-xgb-all-tables-0-796;1.0;['xgboost'];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/home-credit-default-risk;0.782;0.538;2020-12-12 20:47:32;Home Credit Default Risk;['beginner, classification, feature engineering, +2 morefinance, xgboost'];tidy_xgb - all_tables [0.796];R script;25700.0;172;;
2018-05-30 13:34:47;;Apache 2.0;https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/home-credit-default-risk;0.789;0.584;2020-12-12 20:47:32;Home Credit Default Risk;[];Good_fun_with_LigthGBM;Python script;31480.0;343;;
2018-07-02 10:41:57;;Apache 2.0;https://www.kaggle.com/ogrellier/lighgbm-with-selected-features;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification', 'bayesian'];https://www.kaggle.com/c/home-credit-default-risk;0.784;0.558;2020-12-12 20:47:32;Home Credit Default Risk;[];LighGBM_with_Selected_Features;Python script;27718.0;229;0.79029;0.79599
2016-02-18 07:41:19;;Apache 2.0;https://www.kaggle.com/achinjindall/exploratory-analysis-train-and-test;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.691;0.253;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Exploratory Analysis - Train and Test;Rmarkdown script;2536.0;6;;
2016-02-19 17:09:45;;Apache 2.0;https://www.kaggle.com/briantc/homedepot-first-dataexploreation-k;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.789;0.51;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];HomeDepot Data Ex;Rmarkdown script;32471.0;116;;
2016-04-17 21:16:32;;Apache 2.0;https://www.kaggle.com/buinyi/disclosing-external-data;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.673;0.188;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Disclosing external data;Python script;1729.0;3;;
2016-01-18 16:50:05;;Apache 2.0;https://www.kaggle.com/dsoreo/benchmark-score-script;1.0;['pattern'];['ner', 'ai', 'rl', 'nlp', 'nn'];['test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.741;0.34;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Benchmark Score Script;R script;8071.0;15;0.51064;0.51147
2016-01-19 20:21:36;;Apache 2.0;https://www.kaggle.com/dsoreo/testing-r;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.754;0.403;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Beginner Data Analysis;Rmarkdown script;11538.0;30;;
2016-03-10 04:44:36;;Apache 2.0;https://www.kaggle.com/hellozeyu/test-script-1;1.0;['sklearn', 'nltk'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ann'];['gru', 'regression', 'train', 'fitting', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.725;0.236;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];test_script_1;Python script;5450.0;5;0.47368;0.47385
2016-01-23 01:03:38;;Apache 2.0;https://www.kaggle.com/keithtrnka/data-exploration;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.727;0.346;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];data exploration;Python notebook;5741.0;16;;
2016-01-23 20:17:18;;Apache 2.0;https://www.kaggle.com/kelexu/word-cloud;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.677;0.268;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Search Word Cloud;R script;1859.0;7;;
2016-01-25 21:06:37;;Apache 2.0;https://www.kaggle.com/khaledfayed/rf-mean-squared-error;1.0;['pattern', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['train', 'fitting', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.679;0.214;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];RF Mean_Squared_Error;Python script;1958.0;4;0.48535;0.48553
2016-03-31 20:48:21;;Apache 2.0;https://www.kaggle.com/khaoticmind/deep-learning-regression;1.0;['keras', 'nltk', 'theano'];['ner', 'ai', 'dl', 'nlp', 'nn'];['regression', 'train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.721;0.214;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Deep Learning Regression;Python script;4991.0;4;;
2016-02-13 18:57:59;;Apache 2.0;https://www.kaggle.com/olest1980/clustering-of-product-descriptions;1.0;['sklearn', 'nltk'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'deep learning', 'k-means', 'clustering', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.723;0.253;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Clustering of product descriptions;Python script;5190.0;6;;
2016-01-27 12:32:30;;Apache 2.0;https://www.kaggle.com/remap1/get-train-validation-indices;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.691;0.319;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Get Train Validation Indices;Python script;2506.0;12;;
2016-02-08 02:47:17;;Apache 2.0;https://www.kaggle.com/rlkuhn/modified-benchark;1.0;['pattern'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'deep learning', 'predict', 'linear regression', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.672;0.214;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Modified Benchark;R script;1696.0;4;0.51424;0.51532
2016-04-25 23:18:27;;Apache 2.0;https://www.kaggle.com/ryabokonroman/thehomedepot;1.0;['sklearn', 'nltk'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['filter', 'regression', 'train', 'fitting', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.653;0.236;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];TheHomeDepot;Python script;1158.0;5;;
2016-03-13 18:22:05;Fixing Typos with Google;Apache 2.0;https://www.kaggle.com/steubk/fixing-typos;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'rnn', 'ann'];['filter', 'fitting', 'model', 'layer', 'label', 'loss'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.781;0.463;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Fixing Typos;Python notebook;24966.0;62;;
2016-01-20 11:37:56;;Apache 2.0;https://www.kaggle.com/thakurrajanand/gbm-beat-the-benchmark;1.0;['pattern'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.721;0.319;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];GBM Beat the Benchmark;R script;4992.0;12;0.50269;0.50310
2016-01-19 15:21:52;;Apache 2.0;https://www.kaggle.com/uditsaini/exploring-the-home-depot-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.724;0.302;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];Exploring the Home Depot Data;R notebook;5274.0;10;;
2016-01-21 19:32:25;;Apache 2.0;https://www.kaggle.com/wenxuanchen/sklearn-random-forest;1.0;['sklearn', 'nltk'];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/home-depot-product-search-relevance;0.793;0.465;2020-12-12 20:56:51;Home Depot Product Search Relevance;[];sklearn_random_forest;Python script;36912.0;64;0.48666;0.48721
2015-11-20 06:00:44;;Apache 2.0;https://www.kaggle.com/alexxanderlarko/extratreesclassifier;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.719;0.188;2020-12-12 21:07:09;Homesite Quote Conversion;[];ExtraTreesClassifier;Python script;4727.0;3;0.96010;0.95983
2015-12-29 23:27:36;;Apache 2.0;https://www.kaggle.com/beckeroo/summarize-variables;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/homesite-quote-conversion;0.644;0.214;2020-12-12 21:07:09;Homesite Quote Conversion;[];Summarize Variables;Python script;973.0;4;;
2015-12-29 04:31:05;;Apache 2.0;https://www.kaggle.com/brandenkmurray/beat-the-benchmark-in-7-lines;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/homesite-quote-conversion;0.738;0.367;2020-12-12 21:07:09;Homesite Quote Conversion;[];Beat the Benchmark in 7 lines;R script;7597.0;20;0.94492;0.94569
2015-12-30 15:26:54;;Apache 2.0;https://www.kaggle.com/darraghdog/homesite-with-lasagne;1.0;['sklearn', 'theano'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.706;0.253;2020-12-12 21:07:09;Homesite Quote Conversion;[];Homesite with Lasagne;Python notebook;3509.0;6;;
2016-02-01 19:02:34;;Apache 2.0;https://www.kaggle.com/director/xgb-0-9674;1.0;['xgboost', 'sklearn', 'keras'];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.703;0.268;2020-12-12 21:07:09;Homesite Quote Conversion;[];XGB (0.9674);Python script;3298.0;7;0.94959;0.94970
2017-09-23 08:53:55;;Apache 2.0;https://www.kaggle.com/longnguyen/histogram-of-important-features-vs-label;0.5;[];['ner', 'ai'];['train', 'model', 'layer'];https://www.kaggle.com/c/homesite-quote-conversion;0.69;0.253;2020-12-12 21:07:09;Homesite Quote Conversion;[];Histogram of important features vs label;R notebook;2458.0;6;;
2015-11-10 01:28:03;;Apache 2.0;https://www.kaggle.com/mpearmain/xgboost-benchmark;1.0;['xgboost', 'sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.761;0.393;2020-12-12 21:07:09;Homesite Quote Conversion;[];Xgboost_benchmark;Python script;13938.0;27;0.95906;0.95971
2015-11-18 08:29:31;;Apache 2.0;https://www.kaggle.com/omarelgabry/homesite-customer-predictions;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'ml', 'nlp', 'nn', 'ann'];['machine learning', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.729;0.34;2020-12-12 21:07:09;Homesite Quote Conversion;[];Homesite Customer Predictions;Python notebook;6102.0;15;0.96068;0.96140
2015-12-07 12:01:12;;Apache 2.0;https://www.kaggle.com/paso84/xgboost-bayesian-optimization;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'classification', 'bayesian'];https://www.kaggle.com/c/homesite-quote-conversion;0.728;0.188;2020-12-12 21:07:09;Homesite Quote Conversion;[];XGBoost Bayesian Optimization;Python script;5864.0;3;;
2016-01-28 22:29:35;;Apache 2.0;https://www.kaggle.com/phunter/xgboost-with-gridsearchcv;1.0;['xgboost', 'sklearn', 'mxnet'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.814;0.452;2020-12-12 21:07:09;Homesite Quote Conversion;[];xgboost with GridSearchCV;Python script;74240.0;54;;
2015-12-01 16:51:49;;Apache 2.0;https://www.kaggle.com/rejulien/why-is-everything-working-so-well;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/homesite-quote-conversion;0.706;0.311;2020-12-12 21:07:09;Homesite Quote Conversion;[];Why is everything working so well...;R script;3539.0;11;;
2016-01-18 15:40:31;;Apache 2.0;https://www.kaggle.com/selimraboudi/visualizing-the-leaderboard-scores;1.0;['pattern'];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/homesite-quote-conversion;0.706;0.236;2020-12-12 21:07:09;Homesite Quote Conversion;[];visualizing the leaderboard scores;R script;3482.0;5;;
2015-11-12 16:05:35;;Apache 2.0;https://www.kaggle.com/skylord/digging-deeper-and-deeper;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.76;0.418;2020-12-12 21:07:09;Homesite Quote Conversion;[];Digging deeper and deeper!;R notebook;13617.0;36;;
2016-01-30 19:46:09;;Apache 2.0;https://www.kaggle.com/sushize/xgb-stop;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['test data', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.779;0.352;2020-12-12 21:07:09;Homesite Quote Conversion;[];XGB_stop;R script;23241.0;17;0.94273;0.94336
2016-01-25 10:41:27;;Apache 2.0;https://www.kaggle.com/tcaneva/xgboost-and-h2o-grid-search-parameters;1.0;['xgboost', 'h2o'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'test data', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.731;0.253;2020-12-12 21:07:09;Homesite Quote Conversion;[];XGBoost and H2O grid search parameters;R script;6359.0;6;;
2016-01-03 17:28:12;;Apache 2.0;https://www.kaggle.com/tharindudr/xgb-r-starter-script-with-early-stopping;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['test data', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.735;0.236;2020-12-12 21:07:09;Homesite Quote Conversion;[];XGB R Starter Script with Early Stopping;R script;6917.0;5;;
2016-01-17 16:04:03;;Apache 2.0;https://www.kaggle.com/tmjiang/stats;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.712;0.352;2020-12-12 21:07:09;Homesite Quote Conversion;[];stats;Python notebook;4042.0;17;;
2015-11-10 04:34:18;;Apache 2.0;https://www.kaggle.com/tunguz/xgboost-benchmark;1.0;['xgboost', 'sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.636;0.188;2020-12-12 21:07:09;Homesite Quote Conversion;[];Xgboost_benchmark;Python script;844.0;3;0.96196;0.96256
2016-01-15 01:29:03;;Apache 2.0;https://www.kaggle.com/yangnanhai/keras-around-0-9633;1.0;['keras', 'sklearn', 'theano'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['autoencoder', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.764;0.383;2020-12-12 21:07:09;Homesite Quote Conversion;[];Keras around 0.9633*;Python script;15063.0;24;0.78080;0.78247
2015-12-02 16:06:52;;Apache 2.0;https://www.kaggle.com/yonidahan/example-using-random-forest;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['filter', 'random forest', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/homesite-quote-conversion;0.704;0.268;2020-12-12 21:07:09;Homesite Quote Conversion;[];Example using random forest;R script;3381.0;7;;
2015-06-08 23:21:43;;Apache 2.0;https://www.kaggle.com/annavictoria/no-rain-no-gain-s-code;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.589;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];Test code for blog;Python script;373.0;0;;
2015-10-27 14:54:33;;Apache 2.0;https://www.kaggle.com/avijeetsingh1608/11122222;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.581;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];11122222;R script;327.0;0;;
2015-10-27 15:05:56;;Apache 2.0;https://www.kaggle.com/avijeetsingh1608/clean-display-of-individual-records1;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.588;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];Clean Display of Individual Records1;R script;368.0;0;;
2015-10-27 11:41:49;;Apache 2.0;https://www.kaggle.com/avijeetsingh1608/sorry-don-t-run-running-out-of-ram;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.645;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];Sorry, don't run :-( Running out of RAM;R script;992.0;0;;
2015-05-13 02:13:03;;Apache 2.0;https://www.kaggle.com/benhamner/default-r-text;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.627;0.099;2020-12-12 21:16:25;How Much Did It Rain?;[];Default R Text;R script;718.0;1;;
2015-06-23 04:11:52;;Apache 2.0;https://www.kaggle.com/devinanzelmo/component-cdf-s-and-sample-predictions;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain;0.689;0.236;2020-12-12 21:16:25;How Much Did It Rain?;[];Component cdf's and sample predictions;Python script;2420.0;5;;
2015-06-08 07:08:27;;Apache 2.0;https://www.kaggle.com/devinanzelmo/fiddling-with-xgb;1.0;['xgboost'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain;0.675;0.236;2020-12-12 21:16:25;How Much Did It Rain?;[];Fiddling with XGB;Python script;1806.0;5;;
2015-06-24 06:57:59;;Apache 2.0;https://www.kaggle.com/devinanzelmo/kde-and-scatter-plot;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.722;0.319;2020-12-12 21:16:25;How Much Did It Rain?;[];Kde and Scatter plot;Python script;5088.0;12;;
2015-06-23 04:12:03;;Apache 2.0;https://www.kaggle.com/devinanzelmo/log-histogram-of-label-values-version1;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.683;0.099;2020-12-12 21:16:25;How Much Did It Rain?;[];Histograms of label values ;Python script;2141.0;1;;
2015-05-14 12:38:13;;Apache 2.0;https://www.kaggle.com/elenacuoco/splitmeandata-py;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.666;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];SplitMeanData.py;Python script;1492.0;0;;
2015-05-14 17:02:27;;Apache 2.0;https://www.kaggle.com/jamesallen1/test-r;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.583;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];test.R;R script;341.0;0;;
2015-10-12 01:00:54;;Apache 2.0;https://www.kaggle.com/jetheurer/liz-python;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.613;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];liz_python;Python script;564.0;0;;
2015-10-12 00:31:10;;Apache 2.0;https://www.kaggle.com/jetheurer/liz-test;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.58;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];liz_test;R script;325.0;0;;
2018-05-08 15:37:05;;Apache 2.0;https://www.kaggle.com/jihyeseo/rain-eda-need-to-unzip;1.0;['sklearn'];['ai', 'nn'];['train'];https://www.kaggle.com/c/how-much-did-it-rain;0.56;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];rain eda - need to unzip;Python notebook;238.0;0;;
2016-11-07 04:03:04;;Apache 2.0;https://www.kaggle.com/mcf171/notebookb0a53957ff;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/how-much-did-it-rain;0.568;0.0;2020-12-12 21:16:25;How Much Did It Rain?;[];Notebook98b947b14f;R notebook;270.0;0;;
2015-05-16 09:09:35;;Apache 2.0;https://www.kaggle.com/mlandry/clean-display-of-individual-records;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.678;0.214;2020-12-12 21:16:25;How Much Did It Rain?;[];Clean Display of Individual Records;R script;1894.0;4;;
2015-05-13 17:57:03;;Apache 2.0;https://www.kaggle.com/mlandry/simple-benchmark-improvements;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/how-much-did-it-rain;0.657;0.188;2020-12-12 21:16:25;How Much Did It Rain?;[];Sorry, don't run :-( Running out of RAM;R script;1254.0;3;;
2015-06-09 20:53:33;;Apache 2.0;https://www.kaggle.com/sudalairajkumar/rainfall-histogram-plot;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.723;0.319;2020-12-12 21:16:25;How Much Did It Rain?;[];Rainfall Histogram Plot;Python script;5156.0;12;;
2015-06-09 21:09:31;;Apache 2.0;https://www.kaggle.com/sudalairajkumar/rainfall-of-rr1-percentile-bins;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain;0.721;0.319;2020-12-12 21:16:25;How Much Did It Rain?;[];Rainfall of RR1 percentile bins ;Python script;5021.0;12;;
2019-02-10 14:06:41;;Apache 2.0;https://www.kaggle.com/adolgushev/rnn-rain;1.0;['tensorflow', 'keras'];['ai', 'nn', 'rnn'];['predict', 'training data', 'test data', 'train', 'model', 'epoch', 'layer', 'lstm', 'loss'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.627;0.152;2020-12-12 21:25:17;How Much Did It Rain? II;['gpu'];RNN-Rain;Python notebook;713.0;2;24.94242;23.92961
2015-10-20 12:52:58;;Apache 2.0;https://www.kaggle.com/amhchiu/pattern-in-distribution-of-outcomes;1.0;['pattern'];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'model', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.699;0.236;2020-12-12 21:25:17;How Much Did It Rain? II;[];Pattern in Distribution of Outcomes;R script;3000.0;5;;
2015-12-08 17:01:59;;Apache 2.0;https://www.kaggle.com/aminkh/features-gmm-xgboost;1.0;['pattern', 'xgboost', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn'];['train', 'fitting', 'model', 'neural network', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.695;0.214;2020-12-12 21:25:17;How Much Did It Rain? II;[];Features + GMM + XGBoost;Python script;2733.0;4;;
2015-11-19 19:40:26;;Apache 2.0;https://www.kaggle.com/antgoldbloom/exploring-the-how-much-does-it-rain-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.672;0.188;2020-12-12 21:25:17;How Much Did It Rain? II;[];Exploring the how much does it rain data;Python notebook;1671.0;3;;
2015-10-04 01:54:53;;Apache 2.0;https://www.kaggle.com/captcalculator/marshall-palmer-in-r;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.71;0.319;2020-12-12 21:25:17;How Much Did It Rain? II;[];Marshall-Palmer in R;R script;3855.0;12;25.09832;24.06968
2015-12-01 17:27:09;;Apache 2.0;https://www.kaggle.com/davidl6/rain-rate-estimation-from-kdp;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.696;0.188;2020-12-12 21:25:17;How Much Did It Rain? II;[];Rain Rate estimation from KDP Cut f;R script;2829.0;3;25.45000;24.44287
2015-10-26 16:00:38;;Apache 2.0;https://www.kaggle.com/denistsitko/xgb2110;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['training data', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.646;0.188;2020-12-12 21:25:17;How Much Did It Rain? II;[];xgb2110;R script;1015.0;3;;
2015-09-21 00:53:21;;Apache 2.0;https://www.kaggle.com/jeffhebert/exploring-na-values-in-rain2;1.0;['pattern'];['nlp', 'ai', 'nn', 'ner'];['training data', 'filter', 'train', 'deep learning', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.682;0.236;2020-12-12 21:25:17;How Much Did It Rain? II;[];Exploring NA Values in Rain2;R script;2059.0;5;;
2015-09-25 22:05:53;;Apache 2.0;https://www.kaggle.com/mlandry/h2o-starter-1;1.0;['h2o'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn', 'ml'];['training data', 'random forest', 'regression', 'train', 'fitting', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.741;0.311;2020-12-12 21:25:17;How Much Did It Rain? II;[];H2O Starter Random Forest;R script;8258.0;11;25.06357;24.05371
2015-09-25 01:32:40;;Apache 2.0;https://www.kaggle.com/mlandry/median-benchmark;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.709;0.236;2020-12-12 21:25:17;How Much Did It Rain? II;[];Median Benchmark (24.15781);R script;3781.0;5;25.16660;24.15781
2015-09-18 03:38:17;;Apache 2.0;https://www.kaggle.com/sudalairajkumar/rainfall-test;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.763;0.439;2020-12-12 21:25:17;How Much Did It Rain? II;[];Beware of Outliers !!;Python script;14574.0;46;;
2015-11-07 03:12:00;;Apache 2.0;https://www.kaggle.com/titericz/h2orf-1;1.0;['h2o'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn', 'ml'];['training data', 'random forest', 'regression', 'train', 'fitting', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.651;0.188;2020-12-12 21:25:17;How Much Did It Rain? II;[];H2ORF 1;R script;1106.0;3;;
2015-10-08 17:04:58;;Apache 2.0;https://www.kaggle.com/tmfukui/faster-marshall-palmer-in-r;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.613;0.152;2020-12-12 21:25:17;How Much Did It Rain? II;[];Faster Marshall-Palmer in R;R script;555.0;2;25.09832;24.06968
2015-10-14 15:29:55;;Apache 2.0;https://www.kaggle.com/tunguz/gbm-inches-only;1.0;['h2o'];['ner', 'ai', 'gbm', 'nlp', 'nn'];['training data', 'test data', 'regression', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.724;0.319;2020-12-12 21:25:17;How Much Did It Rain? II;[];gbm_inches_only;R script;5327.0;12;24.84483;23.82825
2015-10-09 16:49:06;;Apache 2.0;https://www.kaggle.com/tunguz/h2o-starter-gbm-lr-015-cut-70;1.0;['h2o'];['ner', 'ai', 'gbm', 'nlp', 'nn', 'ml'];['training data', 'regression', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.673;0.188;2020-12-12 21:25:17;How Much Did It Rain? II;[];H2O Starter GBM lr .015 cut 70;R script;1732.0;3;24.89752;23.87941
2015-09-18 08:26:34;;Apache 2.0;https://www.kaggle.com/uditsaini/look-at-data;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.634;0.152;2020-12-12 21:25:17;How Much Did It Rain? II;[];Look_At_Data;R script;810.0;2;;
2015-09-22 04:16:20;;Apache 2.0;https://www.kaggle.com/ys19931006/ohohoh;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.658;0.152;2020-12-12 21:25:17;How Much Did It Rain? II;[];ohohoh;R script;1281.0;2;28.93362;27.86395
2019-02-10 14:41:19;;Apache 2.0;https://www.kaggle.com/zytfoo/how-much-did-it-rain-kernel;1.0;['tensorflow', 'keras'];['ai', 'nn'];['predict', 'training data', 'test data', 'train', 'model', 'epoch', 'layer', 'lstm', 'loss'];https://www.kaggle.com/c/how-much-did-it-rain-ii;0.591;0.152;2020-12-12 21:25:17;How Much Did It Rain? II;['gpu'];kernelcd60210f82;Python notebook;387.0;2;24.99313;23.98215
2018-11-26 21:09:21;;Apache 2.0;https://www.kaggle.com/artemtprv/load-external-data;0.5;[];['ai', 'dl', 'gan', 'rl', 'ml'];['label'];https://www.kaggle.com/c/human-protein-atlas-image-classification;0.716;0.447;2020-12-12 21:26:51;multiple data sources;[];Load external data;Python notebook;4426.0;51;;
2018-11-21 15:01:35;Load dataset info;Apache 2.0;https://www.kaggle.com/byrachonok/pretrained-inceptionresnetv2-base-classifier;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/human-protein-atlas-image-classification;0.828;0.521;2020-12-12 21:26:50;Human Protein Atlas Image Classification;['gpu, deep learning, image data, +2 moremulticlass classification, medicine'];Pretrained InceptionResNetV2 base classifier;Python notebook;126987.0;134;0.27084;0.26916
2018-10-20 11:53:13;Keras Macro F1-Score ImplementationHi all! Here there is my implementation of the Macro-F1-Score in keras/tensorflow that gives the same results of the sklearn implementation on 'macro' mode and has very similar values to the LB scores. Both y_true and y_pred are matrices of size (batch_size, 28).;Apache 2.0;https://www.kaggle.com/guglielmocamporese/macro-f1-score-keras;0.5;['tensorflow', 'sklearn', 'keras'];[];[];https://www.kaggle.com/c/human-protein-atlas-image-classification;0.775;0.475;2020-12-12 21:26:50;Human Protein Atlas Image Classification;['beginner, multiclass classification'];Macro F1-Score Keras;Python notebook;20733.0;72;;
2018-10-10 14:53:24;Exploratory data analysis of the human protein atlas image datasetupdate 5/10/2018: beginning of cell segmentation algorithm update 5/10/2018: add red + blue channels stack and whole cell identification (does not give a clean result, though) This kernel is just the beginning of a work in progress and will be updated very often. We will explore the dataset available for the human protein atlas image competition. Questions we would like to answer include:  what channels of the image contain the relevant information how much can we reduce dimensionality of data while retaining important information;Apache 2.0;https://www.kaggle.com/jschnab/exploring-the-human-protein-atlas-images;0.5;[];['ai', 'dl', 'gan', 'cv', 'rl', 'nn', 'ann'];['image segmentation', 'training data', 'train', 'label', 'labeled'];https://www.kaggle.com/c/human-protein-atlas-image-classification;0.748;0.5;2020-12-12 21:26:50;Human Protein Atlas Image Classification;['beginner, data visualization'];Exploring the human protein atlas images;Python notebook;9686.0;101;;
2018-10-14 05:08:34;;Apache 2.0;https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['image classification', 'filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/human-protein-atlas-image-classification;0.734;0.473;2020-12-12 21:26:51;Human Protein Atlas Image Classification;['gpu, beginner, deep learning'];InceptionV3 baseline;Python script;6752.0;71;0.35491;0.37930
2018-12-09 01:44:19;Whales recognition ResNext baseline;Apache 2.0;https://www.kaggle.com/ateplyuk/resnext50-no-0-crop-sz384-0-651;1.0;['sklearn'];['ner', 'ai', 'cv', 'nn', 'ml'];['training data', 'train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/humpback-whale-identification;0.673;0.413;2020-12-12 21:28:20;multiple data sources;['gpu'];Resnext50 no 0 crop sz384;Python notebook;1736.0;34;0.66051;0.65142
2018-12-18 23:48:03;Ensembling can be a valuable tool for combining the insights of many different machine learning models.  Given the nature of this problem (rank the 5 most likely), a different ensembling algorithm than mere averaging must be used.  Here, I employ an ensembling algorithm that was recently very successful in a recent image recognition competition, Quick! Draw! Doodle Recognition Challenge.;Apache 2.0;https://www.kaggle.com/matthewa313/ensembling-algorithm-for-average-precision-metric;0.5;[];['dl', 'cv'];['machine learning', 'recognition', 'model', 'label', 'rank'];https://www.kaggle.com/c/humpback-whale-identification;0.707;0.418;2020-12-12 21:28:20;multiple data sources;[];Ensembling Algorithm for Average Precision Metric;Python notebook;3597.0;36;;
2018-12-02 13:59:14;In this notebook I'll try to implement the scoring metric. I am still not 100% sure about my interpretation, so please leave me a comment if you find something wrong.;Apache 2.0;https://www.kaggle.com/pestipeti/explanation-of-map5-scoring-metric;0.5;[];['ai'];['train', 'label', 'predict', 'rank', 'classification', 'labeled'];https://www.kaggle.com/c/humpback-whale-identification;0.74;0.523;2020-12-12 21:28:20;Humpback Whale Identification;[];Explanation of MAP5 scoring metric;Python notebook;8001.0;139;;
2018-12-06 16:46:35;Creating Labels;Apache 2.0;https://www.kaggle.com/satian/seresnext101-pytorch-starter;1.0;['pytorch', 'caffe', 'keras', 'sklearn'];['ai', 'cnn', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'loss', 'relu', 'resnet'];https://www.kaggle.com/c/humpback-whale-identification;0.739;0.452;2020-12-12 21:28:20;Humpback Whale Identification;['gpu, deep learning, neural networks'];SEResNeXt101 PyTorch Starter;Python notebook;7754.0;54;0.52008;0.49294
2018-12-02 14:47:27;Training;Apache 2.0;https://www.kaggle.com/suicaokhoailang/wip-resnet18-baseline-with-fastai-0-392-lb;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ml'];['predict', 'train', 'model', 'epoch', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/humpback-whale-identification;0.716;0.427;2020-12-12 21:28:20;Humpback Whale Identification;['gpu, cnn'];[WIP] Resnet18 baseline with Fastai [0.375 LB];Python notebook;4369.0;40;0.41219;0.39271
2019-01-14 08:22:14;IntroductionI found out that image size is related to the rate (probability) of new_whale. Check below example if you are interested. ExampleImport Packagescv2 is faster, but PIL is easy to distinguish gray scale images and RGB color images.;Apache 2.0;https://www.kaggle.com/toshik/image-size-and-rate-of-new-whale;0.5;[];['ai', 'nn', 'cv'];['train'];https://www.kaggle.com/c/humpback-whale-identification;0.684;0.411;2020-12-12 21:28:20;Humpback Whale Identification;[];Image Size and Rate of new_whale;Python notebook;2150.0;33;;
2019-03-01 02:08:56;Initialization;Apache 2.0;https://www.kaggle.com/zfturbo/visualisation-of-siamese-net;1.0;['tensorflow', 'keras'];['ner', 'ai', 'nn', 'cv'];['train', 'model', 'neural network', 'layer', 'loss', 'predict', 'relu'];https://www.kaggle.com/c/humpback-whale-identification;0.762;0.487;2020-12-12 21:28:20;multiple data sources;['gpu'];Visualisation of Siamese Net;Python notebook;14268.0;85;;
2015-06-17 02:46:37;;Apache 2.0;https://www.kaggle.com/benhamner/accessing-sqlite-from-python;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.668;0.152;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];Accessing SQLite from Python;Python script;1551.0;2;;
2015-06-15 20:48:23;;Apache 2.0;https://www.kaggle.com/benhamner/exploring-the-drawbridge-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['test data', 'train', 'deep learning', 'layer', 'label', 'predict', 'classification'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.752;0.39;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];Exploring the Drawbridge Data;Rmarkdown script;10922.0;26;0.00000;0.00000
2015-06-16 02:16:26;;Apache 2.0;https://www.kaggle.com/benhamner/fixing-bad-csv-files-with-download;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.715;0.268;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];Fixing Bad CSV Files (With Download);Python script;4327.0;7;0.00000;0.00000
2015-06-13 02:18:20;;Apache 2.0;https://www.kaggle.com/benhamner/reading-bad-csv-files;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['label', 'classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.714;0.327;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];Reading Bad CSV Files;Rmarkdown script;4167.0;13;;
2015-06-24 08:48:35;;Apache 2.0;https://www.kaggle.com/benhamner/sample-rows-from-each-sqlite-table;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.679;0.214;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];Sample Rows From Each SQLite Table;Sqlite script;1963.0;4;0.00000;0.00000
2015-06-17 02:30:42;;Apache 2.0;https://www.kaggle.com/benhamner/sqlite-database-schema;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.653;0.188;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];SQLite Database Schema;Sqlite script;1166.0;3;;
2015-06-14 05:17:38;;Apache 2.0;https://www.kaggle.com/benhamner/t-sne-visualization-of-devices;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.698;0.302;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];t-SNE Visualization of Devices;R script;2953.0;10;;
2015-08-07 03:56:59;;Apache 2.0;https://www.kaggle.com/cmorton/most-popular-cookie-in-country;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.607;0.099;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];Most Popular Cookie in Country;R script;506.0;1;;
2015-08-01 14:37:05;;Apache 2.0;https://www.kaggle.com/datayo/first-commit;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.641;0.188;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];First Commit;Python script;916.0;3;0.00000;0.00004
2015-07-18 02:37:58;;Apache 2.0;https://www.kaggle.com/dowakin/sql-structure;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.662;0.152;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];SQL structure;Sqlite script;1369.0;2;;
2015-07-23 16:48:47;;Apache 2.0;https://www.kaggle.com/jayjay75/summary;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.713;0.34;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];device/cookie top 10 correlated feat.;R script;4104.0;15;;
2015-08-02 21:10:13;;Apache 2.0;https://www.kaggle.com/jayjay75/unsupervised-to-supervised;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['training data', 'random forest', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.68;0.281;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];random forest features importance;R script;1994.0;8;;
2018-05-08 19:06:39;;Apache 2.0;https://www.kaggle.com/jihyeseo/unzip-eda-dataset-removed;1.0;['sklearn'];['ai'];['train'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.476;0.0;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];unzip eda (dataset removed??);Python notebook;73.0;0;;
2015-06-17 03:05:53;;Apache 2.0;https://www.kaggle.com/maddarwin/accessing-sqlite-from-python;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.626;0.099;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];Accessing SQLite from Python;Python script;700.0;1;;
2015-06-17 15:44:09;;Apache 2.0;https://www.kaggle.com/olivetti/ip-join;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.623;0.152;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];IP JOIN;Python script;670.0;2;;
2015-08-13 14:30:32;;Apache 2.0;https://www.kaggle.com/pbkaran/data-exploration;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.631;0.152;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];First Cut;R script;767.0;2;0.00005;0.00005
2015-08-17 16:32:14;;Apache 2.0;https://www.kaggle.com/pbkaran/sqlite-data-exploration;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.657;0.152;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];SQLite Data Exploration;Sqlite script;1259.0;2;;
2015-07-17 08:10:53;;Apache 2.0;https://www.kaggle.com/senbong/repair-csv-file;1.0;['pattern'];['ner', 'ai', 'dl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.654;0.099;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];repair bad csv;Python script;1176.0;1;;
2015-07-30 16:43:14;;Apache 2.0;https://www.kaggle.com/sujeeth/icdm-2015;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.626;0.099;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];ICDM 2015;Sqlite script;707.0;1;;
2015-08-14 06:33:51;;Apache 2.0;https://www.kaggle.com/willieliao/random-sample-by-country;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['rank', 'classification', 'deep learning'];https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections;0.718;0.327;2020-12-12 21:38:37;ICDM 2015: Drawbridge Cross-Device Connections;[];Random Sample by Country;R script;4640.0;13;0.00002;0.00005
2019-07-18 19:35:05;IEEE Fraud Detection transactions columns reference;Apache 2.0;https://www.kaggle.com/alijs1/ieee-transaction-columns-reference;0.5;[];['ai', 'ml'];['train', 'label', 'filter'];https://www.kaggle.com/c/ieee-fraud-detection;0.767;0.569;2020-12-12 21:40:28;IEEE-CIS Fraud Detection;[];IEEE Transaction columns Reference;Python notebook;16693.0;271;;
2019-09-18 05:54:23;;Apache 2.0;https://www.kaggle.com/jazivxt/safe-box;1.0;['tensorflow', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nn', 'ml'];['train', 'model', 'label'];https://www.kaggle.com/c/ieee-fraud-detection;0.765;0.54;2020-12-12 21:40:28;multiple data sources;[];Safe Box;Python notebook;15640.0;176;0.926283;0.951786
2019-08-28 18:29:27;;Apache 2.0;https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/ieee-fraud-detection;0.755;0.547;2020-12-12 21:40:28;multiple data sources;[];IEEE - FE with some EDA;Python notebook;11942.0;195;;
2019-08-23 10:13:50;;Apache 2.0;https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/ieee-fraud-detection;0.77;0.565;2020-12-12 21:40:28;multiple data sources;[];IEEE - GB-2 (make Amount useful again);Python notebook;17963.0;256;0.922083;0.946803
2019-08-30 11:49:47;;Apache 2.0;https://www.kaggle.com/kyakovlev/ieee-lgbm-with-groupkfold-cv;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'rl', 'gbm'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/ieee-fraud-detection;0.766;0.543;2020-12-12 21:40:28;multiple data sources;[]; IEEE - Lgbm with GroupKFold CV;Python notebook;16030.0;183;0.925307;0.948557
2019-08-07 13:31:32;;Apache 2.0;https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419;1.0;['lightgbm', 'sklearn'];['ai', 'rl', 'gbm'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/ieee-fraud-detection;0.772;0.565;2020-12-12 21:40:28;IEEE-CIS Fraud Detection;[];LGB Single model [LB 0.9419];Python notebook;19108.0;257;0.914288;0.941915
2019-09-19 03:22:32;Credits to the Experts (Please like their kernels) Ashish Gupta: 24+ top lgbm models outputs Konstantin: ieee-internal-blend;Apache 2.0;https://www.kaggle.com/paulorzp/gmean-of-low-correlation-lb-0-952x;0.5;[];['nn', 'ann', 'gbm'];['rank', 'model', 'label', 'filter'];https://www.kaggle.com/c/ieee-fraud-detection;0.776;0.556;2020-12-12 21:40:28;multiple data sources;[];GMEAN of low correlation (LB: 0.952x);Python notebook;21541.0;223;0.927892;0.952590
2019-07-17 03:32:21;IEEE Fraud Detection Competition In this kernel I do some basic exploritory data analysis on the IEEE Fraud Detection dataset. Please upvote if you find this kernel helpful. I will continue to update as I find more discoveries. I suggest you also read the complete competition overview and data description found in the competition page. I purposefully show all of my code. The intention is to not only show the results, but also have clear code that shows how similar analysis can be done on any dataset. From the competition overview: In this competition, youâ€™ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results. If successful, youâ€™ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.;Apache 2.0;https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda;0.5;[];['ner', 'ai', 'rl'];['filter', 'machine learning', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/ieee-fraud-detection;0.782;0.589;2020-12-12 21:40:28;IEEE-CIS Fraud Detection;[];ðŸ•µï¸ IEEE Fraud Detection - First Look and EDA;Python notebook;25441.0;377;;
2019-07-19 18:07:56;;Apache 2.0;https://www.kaggle.com/shahules/tackling-class-imbalance;1.0;['tensorflow', 'xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml'];['test data', 'regression', 'train', 'fitting', 'model', 'clustering', 'loss', 'label', 'logistic regression', 'gradient boosting', 'k-nearest neighbor', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/ieee-fraud-detection;0.776;0.6;2020-12-12 21:40:27;IEEE-CIS Fraud Detection;['gpu'];Tackling  Class imbalance ;Python notebook;21640.0;447;;
2020-03-12 13:50:21;;Apache 2.0;https://www.kaggle.com/neerajnair/kernel76721cc377;1.0;['tensorflow', 'keras'];['ann', 'ai', 'nn', 'ml'];['train', 'model', 'layer', 'label', 'predict'];https://www.kaggle.com/c/imagenet-object-localization-challenge;0.611;0.0;2020-12-12 21:40:36;multiple data sources;[];kernel76721cc377;Python notebook;540.0;0;;
2018-04-11 13:49:56;;Apache 2.0;https://www.kaggle.com/aguyhasnoname/clean-img-rmv-face-bg-detect-chge-skin;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['train', 'model', 'classification', 'deep learning'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.616;0.188;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];Clean Img-Rmv face , BG ,detect, chge skin ;Python script;592.0;3;;
2018-04-04 15:17:15;;Apache 2.0;https://www.kaggle.com/am1to2/data-exploration-and-analysis;0.5;[];['ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['test data', 'training data', 'train', 'validation data', 'label'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.718;0.408;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];Data Exploration And Analysis;Python notebook;4619.0;32;;
2018-04-07 21:28:49;I find some data leakage from the validation set. Though I am not sure whether it is intentional for it being called 'validation' set. I am actually not sure about the validation set purpose.;Apache 2.0;https://www.kaggle.com/anqitu/data-leakage-findings-from-validation;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['test data', 'filter', 'train', 'validation data', 'label'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.64;0.292;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];Data Leakage Findings from Validation;Python notebook;900.0;9;;
2018-04-05 09:18:13;Loading Json Object Into File;Apache 2.0;https://www.kaggle.com/badalgupta/simple-data-exploration;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['train', 'label', 'test data', 'validation data'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.691;0.387;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;['beginner, exploratory data analysis'];Simple Data Exploration;Python notebook;2519.0;25;;
2018-05-15 23:44:04;IntroductionIn this competition, FGVC workshop organizers with Wish and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product detection â€“ to accurately assign attribute labels for fashion images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop.  Let's fish some features and make a prediction;Apache 2.0;https://www.kaggle.com/blastchar/imaterialist-challenge-r-or-not-r;0.5;[];['ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['train', 'label', 'filter', 'predict'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.598;0.188;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];iMaterialist Challenge - R or not R;R notebook;431.0;3;;
2018-04-06 20:35:51;;Apache 2.0;https://www.kaggle.com/dfanghu/gui-for-further-annotating-and-viewing-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ann'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.66;0.099;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];GUI for further annotating (and viewing) data;Python script;1340.0;1;;
2018-05-06 17:38:25;;Apache 2.0;https://www.kaggle.com/frenchie4111/imaterialist-downloader-util-w-max-argument;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.615;0.214;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];iMaterialist downloader util w/max argument;Python script;578.0;4;;
2018-04-05 22:19:01;;Apache 2.0;https://www.kaggle.com/jahaziel/download-images-using-r;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.647;0.152;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];Download images using R;R script;1025.0;2;;
2018-04-17 05:04:01;;Apache 2.0;https://www.kaggle.com/kenluck2001/image-download-using-multiprocessing;0.5;[];['ner', 'ai', 'nn', 'rl'];['train'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.619;0.188;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];Image download using multiprocessing;Python notebook;616.0;3;;
2018-04-23 06:08:33;;[];https://www.kaggle.com/lecasax/imaterialist-async-downloader;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.55;0.099;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];iMaterialist Async Downloader;Python script;204.0;1;;
2018-04-04 17:26:28;;Apache 2.0;https://www.kaggle.com/nlecoy/imaterialist-downloader-util;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.732;0.447;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];iMaterialist downloader util;Python script;6445.0;51;;
2018-04-11 23:10:57;;Apache 2.0;https://www.kaggle.com/rahulbiswas0/data-utils-script-py;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.597;0.152;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];data_utils_script.py;Python script;426.0;2;;
2018-05-20 19:49:56;This notebook is to analyse the labels and their relationships, without downloading the images.;Apache 2.0;https://www.kaggle.com/shaswatrungta/count-based-analysis-of-dataset;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'label', 'filter', 'predict'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.636;0.188;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;['beginner, data visualization, exploratory data analysis'];Count based analysis of dataset;Python notebook;843.0;3;;
2018-04-05 16:23:42;;Apache 2.0;https://www.kaggle.com/sshekhar/download-image-progress-resume-multiprocessing;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.72;0.334;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];Download image : progress,resume,multiprocessing;Python script;4879.0;14;;
2018-04-05 11:51:24;;Apache 2.0;https://www.kaggle.com/suraj2596/download-datasets-to-your-google-drive;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['test data', 'train', 'validation data', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.752;0.327;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];Download datasets to your Google Drive!;Python notebook;10983.0;13;;
2018-04-05 02:01:58;;Apache 2.0;https://www.kaggle.com/zexihan/simplest-multithreading-downloader;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/imaterialist-challenge-fashion-2018;0.675;0.268;2020-12-12 21:47:00;iMaterialist Challenge (Fashion) at FGVC5;[];Simplest multithreading downloader;Python script;1790.0;7;;
2019-06-10 09:55:56;get torchvision utils for mask-rcnn;Apache 2.0;https://www.kaggle.com/abhishek/mask-rcnn-using-torchvision-0-17;1.0;['pytorch', 'pillow'];['ai', 'cnn', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'epoch', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.737;0.485;2020-12-13 11:35:56;multiple data sources;['gpu'];mask-rcnn using torchvision [0.17+];Python notebook;7360.0;82;;
2019-04-26 14:05:02;;Apache 2.0;https://www.kaggle.com/aerdem4/fashion-some-short-sanity-checks;0.5;[];['ai'];['train'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.62;0.236;2020-12-13 11:35:56;iMaterialist (Fashion) 2019 at FGVC6;[];Fashion Some Short Sanity Checks;Python notebook;632.0;5;;
2019-04-25 19:25:47;;Apache 2.0;https://www.kaggle.com/jmourad100/eda-imaterialist-fashion-2019-at-fgvc6;1.0;['pattern', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'gan', 'cv', 'rl'];['filter', 'train', 'model', 'layer', 'vgg', 'label', 'relu', 'resnet', 'classification'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.647;0.268;2020-12-13 11:35:56;iMaterialist (Fashion) 2019 at FGVC6;['gpu'];EDA - iMaterialist (Fashion) 2019 at FGVC6;Python notebook;1040.0;7;;
2019-06-11 19:55:05;Data Description;Apache 2.0;https://www.kaggle.com/kimwoojeong/simple-eda-imaterialist-fashion-2019-at-fgvc6;0.5;[];['cv', 'ai', 'rl', 'gan'];['train', 'label'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.605;0.188;2020-12-13 11:35:56;iMaterialist (Fashion) 2019 at FGVC6;[];Simple EDA - iMaterialist (Fashion) 2019 at FGVC6;Python notebook;491.0;3;;
2019-06-06 20:30:36;"This kernel implements the evaluation score. The evaluation scores are calculated with some pseudo data which is almost the same format of iMaterialist competition. I hope this kernel helps kagglers;)";Apache 2.0;https://www.kaggle.com/kyazuki/calculate-evaluation-score;0.5;[];['ai', 'ml', 'rl'];['train', 'ground truth', 'predict'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.6;0.188;2020-12-13 11:35:56;iMaterialist (Fashion) 2019 at FGVC6;['beginner'];Calculate Evaluation Score;Python notebook;445.0;3;;
2019-05-06 14:15:42;Both attributes and categories contain additional supercategory column, which might be the source of insights related to our data. Cloths from the same supercategory are similar in some sense. Attribute's supercategory denotes that it describes some specific property (I.e., length, style).;Apache 2.0;https://www.kaggle.com/latticetower/eda-supercategories-attributes-correctness;0.5;[];['ai', 'nn', 'ann', 'gan'];['filter', 'train', 'model', 'label', 'labeled'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.667;0.352;2020-12-13 11:35:56;iMaterialist (Fashion) 2019 at FGVC6;[];EDA: supercategories, attributes, correctness;Python notebook;1521.0;17;;
2019-04-27 23:17:41;Get the unique image ids;Apache 2.0;https://www.kaggle.com/mariammohamed/image-resizing-without-losing-the-thin-edges;1.0;['tensorflow', 'skimage', 'keras'];['ai', 'nn', 'ann', 'cv'];['predict', 'train', 'model', 'layer', 'loss'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.675;0.375;2020-12-13 11:35:56;iMaterialist (Fashion) 2019 at FGVC6;['gpu'];Image resizing without losing the thin edges;Python notebook;1796.0;22;;
2020-08-29 06:46:50;no null values;Apache 2.0;https://www.kaggle.com/nirupamkumar/rcnn-model;1.0;['pattern', 'skimage'];['ner', 'ai', 'cnn', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'epoch', 'loss', 'label', 'predict', 'understanding', 'resnet'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.54;0.214;2020-12-13 11:35:56;multiple data sources;[];ML Project Final version;Python notebook;176.0;4;;
2019-05-05 13:57:38;;Apache 2.0;https://www.kaggle.com/rblcoder/kernel-fashion;1.0;['pattern'];['ai', 'dl', 'cnn', 'gan', 'rl', 'nn'];['train', 'r-cnn', 'label'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.643;0.327;2020-12-13 11:35:56;iMaterialist (Fashion) 2019 at FGVC6;['gpu, beginner, exploratory data analysis, +1 moreimage data'];kernel-fashion;Python notebook;954.0;13;;
2019-07-08 20:36:07;Loading all needed modules;Apache 2.0;https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading;1.0;['skimage'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model'];https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6;0.751;0.496;2020-12-13 11:35:56;multiple data sources;[];Benchmark 2019: Speed of image reading;Python notebook;10684.0;95;;
2020-05-18 22:53:48;Making Dataset;Apache 2.0;https://www.kaggle.com/ajinkya1214/kernel6a5fd81147;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ai', 'cnn', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'validation data', 'layer', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;0.442;0.0;2020-12-13 11:37:42;iMaterialist (Fashion) 2020 at FGVC7;[];kernel6a5fd81147;Python notebook;48.0;0;;
2020-09-07 18:08:07;;Apache 2.0;https://www.kaggle.com/ashish2001/imaterialist-2020-starter-eda;0.5;[];['ai', 'gan'];['train'];https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;0.531;0.188;2020-12-13 11:37:41;iMaterialist (Fashion) 2020 at FGVC7;[];iMaterialist 2020: Starter EDA;Python notebook;154.0;3;;
2020-05-27 10:08:18;github: https://github.com/PaulChongPeng/darknet/blob/master/tools/k_means_yolo.py;Apache 2.0;https://www.kaggle.com/fanhaobei/ifashion-2020-anchors-kmeans;0.5;[];['ai', 'rl'];['train', 'label', 'k-means', 'loss'];https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;0.471;0.0;2020-12-13 11:37:42;multiple data sources;[];iFashion_anchors_KMEANS;Python notebook;69.0;0;;
2020-05-27 09:53:02;;Apache 2.0;https://www.kaggle.com/fanhaobei/ifashion-2020-evaluate-cocoapi;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'gan', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;0.516;0.0;2020-12-13 11:37:42;multiple data sources;[];iMaterialist(Fashion)2020-evaluate(5.17);Python notebook;125.0;0;;
2020-05-27 11:20:40;most object is small;Apache 2.0;https://www.kaggle.com/fanhaobei/ifashion-2020-masks-scale-analysis;0.5;[];['ai', 'rl'];['train'];https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;0.507;0.0;2020-12-13 11:37:42;iMaterialist (Fashion) 2020 at FGVC7;[];iFashion_2020_Mask_scale_analysis;Python notebook;111.0;0;;
2020-05-11 18:55:52;Installing Detectron and dependencies;Apache 2.0;https://www.kaggle.com/julienbeaulieu/imaterialist-detectron2;1.0;['pytorch', 'spacy', 'detectron', 'pillow'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;0.718;0.367;2020-12-13 11:37:41;iMaterialist (Fashion) 2020 at FGVC7;['gpu'];iMaterialist Detectron2;Python notebook;4628.0;20;;
2020-03-24 14:54:47;Data Overview;Apache 2.0;https://www.kaggle.com/nayuts/imaterialist-fashion-2020-data-overview;0.5;[];['ai'];['train', 'test data'];https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;0.62;0.099;2020-12-13 11:37:42;iMaterialist (Fashion) 2020 at FGVC7;[];iMaterialist (Fashion) 2020 Data Overview;Python notebook;631.0;1;;
2020-04-20 05:14:05;Import Packages;Apache 2.0;https://www.kaggle.com/tanreinama/lb0-1213-models-training-code;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7;0.674;0.302;2020-12-13 11:37:41;multiple data sources;['gpu'];LB0.1213 models + training code;Python notebook;1756.0;10;0.1240;0.1213
2019-04-21 16:44:36;Some images have a background that represents shades of the same color.;Apache 2.0;https://www.kaggle.com/ateplyuk/remove-background;0.5;[];['ai', 'cv'];['train'];https://www.kaggle.com/c/imet-2019-fgvc6;0.675;0.39;2020-12-13 11:40:08;iMet Collection 2019 - FGVC6;[];Remove background;Python notebook;1811.0;26;;
2019-04-02 05:47:52;iMet Collection 2019 - FGVC6Simple baseline for iMet Collection 2019 competition using fastai v1  Model: densenet201 Loss: Focal loss Metric: F2F2 score  What to try next?  Different models Optimize hyperparameter choice Few-shot learning to improve score on classes with very few samples;Apache 2.0;https://www.kaggle.com/backaggle/imet-fastai-starter-focal-and-fbeta-loss;0.5;[];['ner', 'ai', 'dl', 'cnn', 'rl', 'nn'];['train', 'model', 'epoch', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/imet-2019-fgvc6;0.698;0.418;2020-12-13 11:40:08;multiple data sources;['gpu'];iMet fastai starter - focal and fbeta loss;Python notebook;2899.0;36;0.541;0.541
2019-06-19 15:20:27;The leaderboard-sheet was downloaded in 2019-06-04 13:37:01;Apache 2.0;https://www.kaggle.com/h4211819/leaderboard-analysis;0.2;[];['ai', 'dl', 'rl', 'nn', 'ml'];[];https://www.kaggle.com/c/imet-2019-fgvc6;0.636;0.379;2020-12-13 11:40:08;multiple data sources;[];Leaderboard_Analysis;Python notebook;840.0;23;;
2019-04-13 15:18:50;iMet Collection 2019 - FGVC6Simple baseline for iMet Collection 2019 competition using fastai v1;Apache 2.0;https://www.kaggle.com/itslek/fastai-resnet50-imet-v4-2;0.5;[];['ner', 'ai', 'dl', 'cnn', 'rl', 'nn'];['train', 'model', 'epoch', 'loss', 'label', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/imet-2019-fgvc6;0.717;0.435;2020-12-13 11:40:08;multiple data sources;['gpu'];fastai resnet50 {iMet} v4.2;Python notebook;4557.0;44;0.596;0.596
2020-05-02 20:01:29;Just simple visualization and quick insights ... Based on last year: https://www.kaggle.com/ttahara/eda-compare-number-of-culture-and-tag-attributes I will hash the images and detect the repeated images between 2019 and 2020 data so we can identify the new images :) In this dataset, you are presented with a large number of artwork images and associated attributes of the art.  The dataset has been expanded from the 2019 edition of this competition.  Multiple modalities can be expected and the camera sources are unknown.  The photographs are often centered for objects, and in the case where the museum artifact is an entire room, the images are scenic in nature.;Apache 2.0;https://www.kaggle.com/jesucristo/imet2020-visualization;0.5;[];['ai', 'rl'];['train', 'label', 'filter'];https://www.kaggle.com/c/imet-2019-fgvc6;0.677;0.371;2020-12-13 11:40:08;multiple data sources;[];iMet2020 Visualization;Python notebook;1861.0;21;;
2019-04-02 19:26:44;Data Loading;Apache 2.0;https://www.kaggle.com/kenmatsu4/first-eda-label-coocurrence-matrix;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['rank', 'label', 'train'];https://www.kaggle.com/c/imet-2019-fgvc6;0.652;0.362;2020-12-13 11:40:08;iMet Collection 2019 - FGVC6;[];First EDA (label coocurrence matrix);Python notebook;1127.0;19;;
2019-05-27 23:32:18;;Apache 2.0;https://www.kaggle.com/konradb/really-ugly-clone;1.0;['pytorch'];['ner', 'ai', 'dl', 'cnn', 'gbm', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'fitting', 'model', 'epoch', 'deep learning', 'label', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/imet-2019-fgvc6;0.729;0.447;2020-12-13 11:40:08;multiple data sources;['gpu'];really ugly clone;Python script;6080.0;51;;
2019-04-01 13:05:17;;Apache 2.0;https://www.kaggle.com/lopuhin/imet-2019-submission;0.5;[];['ner', 'ai', 'nlu', 'dl', 'gan', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['neuron', 'train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/imet-2019-fgvc6;0.753;0.514;2020-12-13 11:40:08;multiple data sources;['gpu'];imet-2019-submission;Python script;11220.0;121;0.592;0.000
2019-07-29 04:17:04;;Apache 2.0;https://www.kaggle.com/mathormad/resnet50-v2-keras-focal-loss-mix-up;1.0;['sklearn', 'opencv-python', 'pillow', 'pytorch', 'albumentations', 'tensorflow', 'keras', 'skimage'];['ner', 'ai', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/imet-2019-fgvc6;0.832;0.494;2020-12-13 11:40:08;multiple data sources;['gpu, deep learning'];Resnet50_v2 keras + focal loss + mix_up;Python notebook;147258.0;93;;
2019-03-31 21:10:47;iMet Collection 2019 - FGVC6Simple baseline for iMet Collection 2019 competition using fastai v1  Model: densenet201 Loss: Focal loss Metric: F2F2 score  What to try next?  Different models Optimize hyperparameter choice Few-shot learning to improve score on classes with very few samples;Apache 2.0;https://www.kaggle.com/mnpinto/imet-fastai-starter;0.5;[];['ner', 'ai', 'dl', 'cnn', 'rl', 'nn'];['train', 'model', 'epoch', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/imet-2019-fgvc6;0.728;0.465;2020-12-13 11:40:08;multiple data sources;['gpu, deep learning, image data, +1 moremulticlass classification'];iMet fastai starter ;Python notebook;5917.0;64;0.545;0.545
2019-04-02 08:27:45;check labels;Apache 2.0;https://www.kaggle.com/ttahara/eda-compare-number-of-culture-and-tag-attributes;0.5;[];['ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'train', 'label', 'predict', 'classification'];https://www.kaggle.com/c/imet-2019-fgvc6;0.672;0.413;2020-12-13 11:40:08;iMet Collection 2019 - FGVC6;[];[EDA] Compare number of culture and tag attributes;Python notebook;1687.0;34;;
2019-04-14 05:58:28;;Apache 2.0;https://www.kaggle.com/xiuchengwang/keras-xception-fine-turning-facol-loss;1.0;['sklearn', 'pytorch', 'tensorflow', 'keras', 'skimage'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/imet-2019-fgvc6;0.714;0.421;2020-12-13 11:40:08;multiple data sources;['gpu, beginner, deep learning, +1 moreclassification'];Keras Xception Fine-Turning Facol-loss;Python notebook;4241.0;37;;
2019-07-08 20:36:07;Loading all needed modules;Apache 2.0;https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading;1.0;['skimage'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model'];https://www.kaggle.com/c/imet-2019-fgvc6;0.751;0.496;2020-12-13 11:40:08;multiple data sources;[];Benchmark 2019: Speed of image reading;Python notebook;10685.0;95;;
2020-05-21 01:12:00;"iMet 2020 â€” FGVC7 datasetA small test using fastaiHello World! This is my second submission for Kaggle's iMet 2020 â€” FGVC7 competition. Here we are checking some approaches taught on fastai's classes. There are a lot of inspiration from other places to this; you can find the references at the bottom, ok?  Notes: in this third iteration, we increased the batch size a little bit more, and also number of epochs on the first and second training rounds.  We start importing everything from fastai.vision. Hey, not very PEP8-ish, but stay with me.";Apache 2.0;https://www.kaggle.com/alexdesiqueira/imet-collection-2020-fgvc7-dataset;0.5;[];['ner', 'ai', 'cnn', 'rl', 'ml', 'nn', 'ann'];['test data', 'train', 'fitting', 'model', 'neural network', 'epoch', 'validation data', 'layer', 'loss', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/imet-2020-fgvc7;0.609;0.214;2020-12-13 11:41:37;multiple data sources;['gpu'];iMet Collection 2020 â€” FGVC7 dataset;Python notebook;526.0;4;0.631;0.684
2020-05-03 15:47:32;;Apache 2.0;https://www.kaggle.com/ashkhagan/imet2020;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/imet-2020-fgvc7;0.471;0.152;2020-12-13 11:41:37;iMet Collection 2020 - FGVC7;[];iMET2020;Python notebook;69.0;2;0.000;0.000
2020-04-06 17:34:04;Constants;Apache 2.0;https://www.kaggle.com/dimakyn/multi-label-keras;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv'];['train', 'fitting', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/imet-2020-fgvc7;0.646;0.334;2020-12-13 11:41:37;iMet Collection 2020 - FGVC7;['gpu'];Multi Label Keras;Python notebook;1015.0;14;0.006;0.005
2020-03-29 04:53:09;;Apache 2.0;https://www.kaggle.com/grapestone5321/imet-collection-2020-sample-submission;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/imet-2020-fgvc7;0.572;0.099;2020-12-13 11:41:37;iMet Collection 2020 - FGVC7;[];iMet Collection 2020-sample_submission;Python notebook;284.0;1;0.000;0.000
2020-05-02 20:01:29;Just simple visualization and quick insights ... Based on last year: https://www.kaggle.com/ttahara/eda-compare-number-of-culture-and-tag-attributes I will hash the images and detect the repeated images between 2019 and 2020 data so we can identify the new images :) In this dataset, you are presented with a large number of artwork images and associated attributes of the art.  The dataset has been expanded from the 2019 edition of this competition.  Multiple modalities can be expected and the camera sources are unknown.  The photographs are often centered for objects, and in the case where the museum artifact is an entire room, the images are scenic in nature.;Apache 2.0;https://www.kaggle.com/jesucristo/imet2020-visualization;0.5;[];['ai', 'rl'];['train', 'label', 'filter'];https://www.kaggle.com/c/imet-2020-fgvc7;0.677;0.371;2020-12-13 11:41:37;multiple data sources;[];iMet2020 Visualization;Python notebook;1862.0;21;;
2020-05-26 15:51:13;"Key insights: There are 3474 labels in total that are organized into following subclasses:  Countries: labels 0 to 99 Cultures: labels 99 to 780 Dimension: labels 781 to 785 Medium: labels 786 to 2705 Tag: 2706 to 3473  Of these 3474 labels 3 aren't used on any of the training data  Not every artwork was assigned a country or culture by the experts and some artworks were assigned 2-3 cultures or countries  The occurence of individual labels out of the subclasses grows exponentially towards some individual labels like ""female"" and ""male"" within Tags. However, the individual labels of the Dimension subclass were used with a similar frequency";Apache 2.0;https://www.kaggle.com/khofschen/imet-explore-visualization;0.5;[];['cv', 'ai', 'nn', 'gan'];['train', 'label', 'training data'];https://www.kaggle.com/c/imet-2020-fgvc7;0.563;0.253;2020-12-13 11:41:37;iMet Collection 2020 - FGVC7;[];iMet_explore_visualization;Python notebook;247.0;6;;
2020-05-16 13:54:54;;Apache 2.0;https://www.kaggle.com/leosabraham/imet-using-vgg16;1.0;['caffe', 'tensorflow', 'keras'];['ner', 'ai', 'nn'];['train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/imet-2020-fgvc7;0.439;0.099;2020-12-13 11:41:37;iMet Collection 2020 - FGVC7;['gpu'];kernel507ff0a823;Python notebook;46.0;1;0.086;0.161
2020-05-28 08:01:04;iMet 2020 Multilabel Artwork Image ClassificationHello! So here we have images of famous, and not so famous but still museum worthy, pieces of art in the Metropolitan Museum of Art's collection. Labeled, presumably by experts, with varied amounts of labels which have varied types of meanings. Our challenge is to create a multilabel image classification model. Lets explore!;Apache 2.0;https://www.kaggle.com/matthewallenfisher/imet-fastai;0.5;[];['ner', 'ai', 'nn', 'cnn'];['image classification', 'test data', 'train', 'model', 'label', 'predict', 'resnet', 'classification', 'labeled'];https://www.kaggle.com/c/imet-2020-fgvc7;0.598;0.214;2020-12-13 11:41:37;multiple data sources;['gpu'];iMet FastAI;Python notebook;433.0;4;;
2020-04-07 15:00:39;About this notebook;Apache 2.0;https://www.kaggle.com/nxrprime/imet-2020-resnet18-inference;1.0;['pytorch', 'albumentations', 'sklearn'];['ai', 'dl', 'cv', 'nn', 'ann'];['train', 'model', 'epoch', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/imet-2020-fgvc7;0.631;0.214;2020-12-13 11:41:37;multiple data sources;['gpu'];iMet 2020 ResNet18 Inference;Python notebook;774.0;4;0.582;0.635
2020-05-25 08:12:14;Hello everyone. This is a simple data visualization notebook for this competition. If you somehow find it useful, please upvote.  I will keep updating this notebook gradually..;Apache 2.0;https://www.kaggle.com/redwankarimsony/imet-collection-2020-data-visualization;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/imet-2020-fgvc7;0.531;0.311;2020-12-13 11:41:37;iMet Collection 2020 - FGVC7;[];iMet Collection 2020 (Data Visualization);Python notebook;155.0;11;;
2020-03-28 14:09:03;About this notebook;Apache 2.0;https://www.kaggle.com/yasufuminakama/imet-2020-pytorch-resnet18-inference;1.0;['pytorch', 'albumentations', 'sklearn'];['ai', 'dl', 'cv', 'nn', 'ann'];['predict', 'train', 'model', 'epoch', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/imet-2020-fgvc7;0.645;0.327;2020-12-13 11:41:37;multiple data sources;['gpu']; iMet 2020 PyTorch Resnet18 inference;Python notebook;1003.0;13;0.559;0.613
2020-03-28 01:42:25;About this notebook;Apache 2.0;https://www.kaggle.com/yasufuminakama/imet-2020-pytorch-resnet18-starter;1.0;['pytorch', 'albumentations', 'sklearn'];['ai', 'dl', 'cv', 'nn', 'ann'];['predict', 'train', 'model', 'epoch', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/imet-2020-fgvc7;0.672;0.379;2020-12-13 11:41:37;multiple data sources;['gpu'];iMet 2020 PyTorch Resnet18 starter;Python notebook;1696.0;23;0.000;0.000
2019-05-29 17:42:02;;Apache 2.0;https://www.kaggle.com/alainminda/kernel11ab6576ac;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'nn', 'ann'];['train', 'model', 'output layer', 'epoch', 'validation data', 'layer', 'label', 'loss', 'relu', 'classification'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.527;0.099;2020-12-13 11:42:48;multiple data sources;['gpu'];kernel11ab6576ac;Python notebook;146.0;1;;
2019-09-01 15:02:11;;Apache 2.0;https://www.kaggle.com/byhwdy/inat-utils;1.0;['tensorflow', 'keras'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ann'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.509;0.0;2020-12-13 11:42:48;iNaturalist 2019 at FGVC6;['utility script'];iNat_utils;Python script;113.0;0;;
2019-06-15 19:29:35;;Apache 2.0;https://www.kaggle.com/cedriclacrambe/inaturalist-xception-512;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.639;0.0;2020-12-13 11:42:48;multiple data sources;['gpu'];inaturalist xception 512;Python notebook;887.0;0;;
2019-05-11 22:55:49;;Apache 2.0;https://www.kaggle.com/chrisevans/kernelfea8793d3e;1.0;['pillow'];['dl', 'ai', 'nn', 'ann'];['train'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.514;0.0;2020-12-13 11:42:48;iNaturalist 2019 at FGVC6;[];kernelfea8793d3e;Python notebook;122.0;0;;
2019-05-08 00:20:20;;Apache 2.0;https://www.kaggle.com/hsinwenchang/keras-data-augmentation-visualize;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cv', 'nn', 'ann'];['activation function', 'test data', 'train', 'model', 'epoch', 'validation data', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.714;0.311;2020-12-13 11:42:48;iNaturalist 2019 at FGVC6;['deep learning, feature engineering, image data, +1 moremulticlass classification'];Keras Data Augmentation & Visualize;Python notebook;4221.0;11;0.96600;0.96409
2019-05-08 10:58:26;;Apache 2.0;https://www.kaggle.com/hsinwenchang/keras-mobilenet-data-augmentation-visualize;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cv', 'nn', 'ann'];['test data', 'train', 'model', 'epoch', 'validation data', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.679;0.214;2020-12-13 11:42:48;iNaturalist 2019 at FGVC6;['gpu'];Keras MobileNet Data Augmentation & Visualize;Python notebook;1957.0;4;0.97657;0.97509
2019-06-11 21:21:37;CNN HyperNetwork;Apache 2.0;https://www.kaggle.com/jas10022/cnn-hypernetwork;1.0;['skimage', 'tensorflow', 'sklearn'];['ai', 'cnn', 'gan', 'nn', 'rnn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'neural network', 'epoch', 'validation data', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.587;0.099;2020-12-13 11:42:48;multiple data sources;[];CNN HyperNetwork;Python notebook;362.0;1;;
2019-06-07 17:39:07;Reference  https://www.kaggle.com/sujoykg/xception-keras;Apache 2.0;https://www.kaggle.com/khursani8/fast-ai-ootb-cutout-efficientnet;1.0;['pytorch', 'keras'];['ner', 'ai', 'dl', 'cnn', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.724;0.429;2020-12-13 11:42:48;iNaturalist 2019 at FGVC6;['gpu'];fast.ai ootb (cutout+efficientnet);Python notebook;5324.0;41;;
2019-05-04 20:15:12;See: https://www.kaggle.com/c/inaturalist-2019-fgvc6/kernels;Apache 2.0;https://www.kaggle.com/lowecoryr/learn-from-other-kernels-fork-from-me;0.7;['tensorflow', 'keras'];['ner', 'rl'];[];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.57;0.099;2020-12-13 11:42:48;iNaturalist 2019 at FGVC6;[];Learn From Other Kernels - Fork From Me;Python notebook;278.0;1;;
2019-04-02 02:53:44;OverviewThis script will load the iNat2019 dataset, print a summary, and display a random image.;Apache 2.0;https://www.kaggle.com/macaodha/basic-inat2019-data-exploration;0.5;[];['ai', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.661;0.319;2020-12-13 11:42:48;iNaturalist 2019 at FGVC6;[];Basic iNat2019 data exploration;Python notebook;1365.0;12;;
2019-06-03 19:39:06;;Apache 2.0;https://www.kaggle.com/s3chwartz/inaturalist-2019-at-fgvc6;1.0;['pytorch'];['ner', 'ai', 'dl', 'cnn', 'nn', 'ann'];['train', 'model', 'epoch', 'loss', 'label', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.622;0.099;2020-12-13 11:42:48;iNaturalist 2019 at FGVC6;['gpu'];iNaturalist 2019 at FGVC6;Python notebook;649.0;1;0.88527;0.88152
2019-07-08 20:36:07;Loading all needed modules;Apache 2.0;https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading;1.0;['skimage'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model'];https://www.kaggle.com/c/inaturalist-2019-fgvc6;0.751;0.496;2020-12-13 11:42:48;multiple data sources;[];Benchmark 2019: Speed of image reading;Python notebook;10686.0;95;;
2018-05-08 20:15:52;;Apache 2.0;https://www.kaggle.com/jihyeseo/image-jpeg;1.0;['sklearn'];['ai'];['train'];https://www.kaggle.com/c/inaturalist-challenge-at-fgvc-2017;0.641;0.0;2020-12-13 11:42:54;iNaturalist Challenge at FGVC 2017;[];image jpeg ;Python notebook;923.0;0;;
2018-09-07 21:42:17;;Apache 2.0;https://www.kaggle.com/abosol/understanding-a-little-what-are-the-inputs;0.5;[];['dl', 'ai', 'nn', 'cv'];['train', 'label'];https://www.kaggle.com/c/inclusive-images-challenge;0.619;0.253;2020-12-13 11:44:51;Inclusive Images Challenge;['exploratory data analysis'];understanding a little what are the inputs;Python notebook;624.0;6;;
2018-11-07 21:16:56;;Apache 2.0;https://www.kaggle.com/alexanderliao/inclusive-images-stage-2;0.5;[];['ai', 'nn'];['label'];https://www.kaggle.com/c/inclusive-images-challenge;0.608;0.152;2020-12-13 11:44:51;multiple data sources;[];Inclusive Images Stage 2;Python notebook;511.0;2;0.13583;0.08840
2018-09-07 10:02:12;;Apache 2.0;https://www.kaggle.com/daikinban/baby-step;0.3;[];[];['label'];https://www.kaggle.com/c/inclusive-images-challenge;0.62;0.268;2020-12-13 11:44:51;Inclusive Images Challenge;['gpu'];Baby step;Python notebook;627.0;7;;
2020-04-26 16:48:12;UPDATEMy current project requires annotation of images, and I needed a refresher on the code. I found this kernel from last year and remembered how fun this competition was! Also, I don't like seeing a kernel of mine with 102 votes and only a silver medal:);Apache 2.0;https://www.kaggle.com/duboviy/basic-eda-with-images;0.5;[];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'label', 'clustering'];https://www.kaggle.com/c/inclusive-images-challenge;0.505;0.253;2020-12-13 11:44:51;multiple data sources;[];Basic EDA with Images;Python notebook;107.0;6;;
2018-11-05 13:33:06;;Apache 2.0;https://www.kaggle.com/gpreda/last-day-6-lines-baseline-compact-anton-petrov-s;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/inclusive-images-challenge;0.614;0.188;2020-12-13 11:44:51;Inclusive Images Challenge;['beginner'];last day 6 lines baseline (compact Anton Petrov's);Python script;569.0;3;;
2018-09-06 21:18:42;Images with more than 6 labels;Apache 2.0;https://www.kaggle.com/hanriver0618/test-image-visualization;0.5;[];['cv'];['label'];https://www.kaggle.com/c/inclusive-images-challenge;0.691;0.387;2020-12-13 11:44:51;Inclusive Images Challenge;['gpu'];Test Image Visualization;Python notebook;2516.0;25;;
2020-03-03 00:37:13;UPDATEMy current project requires annotation of images, and I needed a refresher on the code. I found this kernel from last year and remembered how fun this competition was! Also, I don't like seeing a kernel of mine with 102 votes and only a silver medal:);Apache 2.0;https://www.kaggle.com/jpmiller/basic-eda-with-images;0.5;[];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'label', 'clustering'];https://www.kaggle.com/c/inclusive-images-challenge;0.769;0.512;2020-12-13 11:44:51;multiple data sources;['exploratory data analysis, image data, bigquery'];Basic EDA with Images;Python notebook;17246.0;119;;
2018-10-02 09:31:07;Contents Loading and Cleaning Data Plots Interactive Graphs;Apache 2.0;https://www.kaggle.com/jtlowery/plots-and-interactive-graphs-eda;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'labeled'];https://www.kaggle.com/c/inclusive-images-challenge;0.661;0.319;2020-12-13 11:44:51;Inclusive Images Challenge;['data visualization, exploratory data analysis, data cleaning'];Plots and Interactive Graphs - EDA;Python notebook;1350.0;12;;
2018-09-29 14:01:12;;Apache 2.0;https://www.kaggle.com/malyutins/keras-f2-score;1.0;['tensorflow', 'keras'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'epoch', 'deep learning', 'label', 'loss', 'classification'];https://www.kaggle.com/c/inclusive-images-challenge;0.692;0.34;2020-12-13 11:44:51;Inclusive Images Challenge;[];Keras F2 score;Python script;2544.0;15;;
2018-09-08 17:45:22;;Apache 2.0;https://www.kaggle.com/mengtianjian/f2-score-implementation-in-pytorch;0.3;[];[];['label'];https://www.kaggle.com/c/inclusive-images-challenge;0.687;0.281;2020-12-13 11:44:51;Inclusive Images Challenge;[];F2 Score Implementation in PyTorch;Python notebook;2313.0;8;;
2018-11-08 17:40:19;"updated for Stage 2 tl;dr  tuning table might have similar distribution of labels to submission.csv tuning table is part of submission.csv";Apache 2.0;https://www.kaggle.com/petrov/naive-submission-updated-for-stage-2-0-088;0.5;[];['ner', 'ai'];['label', 'predict'];https://www.kaggle.com/c/inclusive-images-challenge;0.709;0.45;2020-12-13 11:44:51;Inclusive Images Challenge;[];Naive submission - updated for Stage 2 (0.088);Python notebook;3776.0;53;;
2018-09-11 16:13:47;classes-trainable.csv;Apache 2.0;https://www.kaggle.com/rezwan249/datasets-description-and-test-images-display;1.0;['caffe'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'layer', 'label', 'rank'];https://www.kaggle.com/c/inclusive-images-challenge;0.674;0.281;2020-12-13 11:44:51;Inclusive Images Challenge;[];Datasets Description and Test Images display;Python notebook;1768.0;8;;
2018-09-11 15:39:54;;Apache 2.0;https://www.kaggle.com/victorhz/just-guess;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/inclusive-images-challenge;0.625;0.214;2020-12-13 11:44:51;Inclusive Images Challenge;[];Just Guess;Python notebook;686.0;4;0.00000;0.00000
2018-09-27 23:38:10;;Apache 2.0;https://www.kaggle.com/victorkuzn1986/quick-draft-in-keras;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'rl', 'nn', 'ann'];['train', 'model', 'output layer', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/inclusive-images-challenge;0.61;0.152;2020-12-13 11:44:51;Inclusive Images Challenge;['gpu'];quick draft in keras;Python notebook;533.0;2;;
2018-10-17 05:49:48;;Apache 2.0;https://www.kaggle.com/yyqing/strange-chars-in-class-descriptions;0.5;[];['ai', 'dl', 'rl', 'cv', 'nn'];['train', 'label'];https://www.kaggle.com/c/inclusive-images-challenge;0.572;0.188;2020-12-13 11:44:51;Inclusive Images Challenge;[];strange chars in class descriptions;Python notebook;284.0;3;;
2017-10-18 07:50:19;Customer Segments;Apache 2.0;https://www.kaggle.com/asindico/customer-segments-with-pca;1.0;['pattern', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'ml', 'nn', 'ann'];['predict', 'train', 'model', 'label', 'clustering'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.8;0.545;2020-12-13 11:49:56;Instacart Market Basket Analysis;['data analytics'];Customer Segments with PCA;Python notebook;45410.0;189;;
2017-09-20 20:59:40;;Apache 2.0;https://www.kaggle.com/cjansen/instacart-xgboost-starter-lb-0-3808482;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['filter', 'test data', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.744;0.423;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Instacart XGBoost Starter [LB 0.3808482];R script;8740.0;38;;
2017-07-30 14:14:54;;Apache 2.0;https://www.kaggle.com/cpmpml/f1-score-expectation-maximization-in-o-n;0.5;[];['ner', 'ai', 'dl', 'gan', 'ml', 'nlp', 'nn', 'ann'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.711;0.429;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];F1-Score Expectation Maximization in O(nÂ²);Python script;3963.0;41;;
2017-09-18 20:23:11;Association Rules Mining Using Python Generators to Handle Large Datasets;Apache 2.0;https://www.kaggle.com/datatheque/association-rules-mining-market-basket-analysis;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl'];['filter', 'machine learning', 'understanding', 'layer', 'recommend'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.823;0.552;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Association Rules Mining/Market Basket Analysis;Python notebook;104408.0;210;;
2017-06-15 08:57:16;;Apache 2.0;https://www.kaggle.com/fabienvs/instacart-xgboost-starter-lb-0-3791;1.0;['xgboost'];['ner', 'ai', 'dl', 'gan', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.782;0.535;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Instacart XGBoost Starter - LB 0.3791;R script;25728.0;164;;
2017-08-09 16:20:08;;Apache 2.0;https://www.kaggle.com/jhamilton415/cultural-diversity-of-products-purchases-updated;0.5;[];['ner', 'ai', 'nn'];['train', 'label', 'filter'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.704;0.421;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Cultural Diversity of Products/Purchases (updated);Rmarkdown script;3339.0;37;;
2017-08-05 08:53:14;;Apache 2.0;https://www.kaggle.com/lukeeee/fast-r-port-of-faron-s-f1-maximization;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'nlp', 'nn', 'ml'];['train', 'model', 'classification', 'deep learning'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.7;0.444;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Fast R port of Faron's F1 Maximization;R script;3083.0;49;;
2017-07-30 04:57:35;;Apache 2.0;https://www.kaggle.com/mmotoki/markov-chain-tutorial;0.5;[];['ai', 'rl'];['rank', 'model'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.77;0.464;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Markov Chain Tutorial;Rmarkdown script;18084.0;63;;
2017-07-26 11:52:07;;Apache 2.0;https://www.kaggle.com/mmueller/f1-score-expectation-maximization-in-o-n;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.77;0.525;2020-12-13 11:49:56;Instacart Market Basket Analysis;['optimization'];F1-Score Expectation Maximization in O(nÂ²);Python script;18001.0;142;;
2017-07-26 09:02:00;;Apache 2.0;https://www.kaggle.com/mmueller/order-streaks-feature;1.0;['pattern'];['ner', 'ai', 'gan', 'nlp', 'nn', 'ann'];['train', 'model', 'classification', 'deep learning'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.723;0.423;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Order Streaks Feature;Python script;5166.0;38;;
2017-05-19 22:46:20;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/recreating-the-products-by-hour-chart;0.5;[];['ai', 'rl', 'nn', 'gan'];['train', 'filter'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.717;0.462;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Recreating the Products by Hour Chart;Rmarkdown script;4487.0;61;;
2017-05-16 22:20:21;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/simple-beat-the-benchmark;0.5;[];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'predict', 'understanding', 'classification', 'bayesian'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.708;0.453;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Simple beat the benchmark;R script;3625.0;55;;
2017-06-12 08:30:13;;Apache 2.0;https://www.kaggle.com/paulantoine/light-gbm-benchmark-0-3692;1.0;['xgboost', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ann'];['generation', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.788;0.528;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];light GBM benchmark 0.3692;Python script;30873.0;148;;
2017-06-30 08:29:53;;Apache 2.0;https://www.kaggle.com/philippsp/exploratory-analysis-instacart;0.5;[];['ner', 'ai', 'nn', 'gan'];['filter', 'train', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.822;0.635;2020-12-13 11:49:56;Instacart Market Basket Analysis;['data visualization, exploratory data analysis, intermediate'];Exploratory Analysis - Instacart;Rmarkdown script;100946.0;832;;
2017-07-05 21:00:21;Introduction Here is my first Kernel where I try to explore some basic information about Instacart dataset. As said in the description , the Dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users. The goal  is then to predict which previously purchased products will be in a userâ€™s next order.  Now  let's jump straight into the data and do some  exploratory analysis !!;Apache 2.0;https://www.kaggle.com/serigne/instacart-simple-data-exploration;0.5;[];['ner', 'ai', 'gan', 'ml', 'nn', 'ann'];['train', 'label', 'filter', 'predict'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.761;0.518;2020-12-13 11:49:56;Instacart Market Basket Analysis;[];Instacart Simple Data Exploration;Python notebook;13858.0;129;;
2017-06-02 06:20:55;In this notebook, we will try and explore the basic information about the dataset given. The dataset for this competition is a relational set of files describing customers' orders over time. Objective: The goal of the competition is to predict which products will be in a user's next order. The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users. For each user, 4 and 100 of their orders are given, with the sequence of products purchased in each order Let us start by importing the necessary modules.;Apache 2.0;https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-instacart;0.5;[];['ner', 'ai', 'gan', 'rl', 'nn', 'ann'];['train', 'label', 'predict'];https://www.kaggle.com/c/instacart-market-basket-analysis;0.806;0.613;2020-12-13 11:49:56;Instacart Market Basket Analysis;['data visualization, exploratory data analysis'];Simple Exploration Notebook - Instacart;Python notebook;56645.0;556;;
2019-06-05 02:58:26;;Apache 2.0;https://www.kaggle.com/paulorzp/nusvc-qda-4-lines-0-96;1.0;['sklearn'];['ner', 'ai', 'cv', 'ml', 'nlp', 'nn', 'ann'];['machine learning', 'regression', 'train', 'model', 'deep learning', 'layer', 'predict', 'classification'];https://www.kaggle.com/c/instant-gratification;0.72;0.486;2020-12-13 11:53:26;Instant Gratification;[];NuSVC+QDA [4 lines] 0.96+;Python script;4803.0;83;0.96045;0.95974
2019-05-28 04:07:54;;Apache 2.0;https://www.kaggle.com/prashantkikani/ig-pca-nusvc-knn-lr-stack;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['regression', 'train', 'model', 'deep learning', 'layer', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/instant-gratification;0.702;0.482;2020-12-13 11:53:26;Instant Gratification;[];IG PCA + NuSVC + KNN + LR  stack;Python script;3194.0;79;0.96654;0.96569
2019-06-19 19:18:51;;Apache 2.0;https://www.kaggle.com/rohandeysarkar/instant-gratification-qda;1.0;['sklearn'];['ai'];['train', 'model', 'filter', 'predict'];https://www.kaggle.com/c/instant-gratification;0.716;0.479;2020-12-13 11:53:26;Instant Gratification;[];tune_hyperparameter_and_add_standardscalar;Python notebook;4370.0;76;;
2019-06-17 06:46:38;;Apache 2.0;https://www.kaggle.com/rsakata/gmm-with-target-perfect-pred-random-shuffle;1.0;['sklearn'];['ner', 'ai', 'nlp', 'nn', 'ml'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/instant-gratification;0.702;0.49;2020-12-13 11:53:26;Instant Gratification;[];GMM with target (perfect pred.) + random shuffle;Python script;3182.0;88;0.97596;0.97445
2019-06-01 12:14:32;;Apache 2.0;https://www.kaggle.com/tunguz/ig-pca-nusvc-knn-qda-lr-stack;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['regression', 'train', 'model', 'deep learning', 'layer', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/instant-gratification;0.723;0.497;2020-12-13 11:53:26;Instant Gratification;[];IG PCA + NuSVC + KNN + +QDA + LR  stack;Python script;5166.0;97;0.96888;0.96800
2019-05-27 02:35:11;;Apache 2.0;https://www.kaggle.com/tunguz/pca-nusvc-knn;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['test data', 'regression', 'train', 'model', 'deep learning', 'predict', 'linear regression', 'classification'];https://www.kaggle.com/c/instant-gratification;0.734;0.494;2020-12-13 11:53:26;Instant Gratification;[];PCA + NuSVC + KNN;Python script;6768.0;93;0.96166;0.96119
2016-06-04 00:09:41;SequenceSize;Apache 2.0;https://www.kaggle.com/abevieiramota/sequences-features;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/integer-sequence-learning;0.673;0.281;2020-12-13 11:57:13;Integer Sequence Learning;[];Sequences features;Python notebook;1735.0;8;;
2016-06-09 02:29:38;;Apache 2.0;https://www.kaggle.com/chaithanya1996/applying-gam;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/integer-sequence-learning;0.639;0.188;2020-12-13 11:57:13;Integer Sequence Learning;[];Applying gam;R script;897.0;3;;
2016-09-02 19:34:18;"I very briefly had the #1 spot on the leaderboard with a score of nearly 80%, because I'm a big, stinky cheater. Leading up to this minor accomplishment, I felt that I had ""hacked"" this problem, and the effort I put into making things fast and efficient seemed data science-y enough. However, I immediately became racked with guilt and contacted the admins to have my submission removed. In the interest of open knowledge and for the sake of curiosity, however, here is how I destroyed this problem in 20 lines of code and in under a minute. First of all, you can go download the entire OEIS here: http://oeis.org/wiki/JSON_Format,_Compressed_Files There's a little bit of parsing to be done, then we very simply create a tree with the classic defaultdict-of-defaultdicts structure. Create the tree by iterating through each sequence, extending the node associated with a number by the next number. For example, consider the Fibonacci sequence 1, 1, 2, 3, 5, 8, ... The root node will have a key '1', the value for which is another node that has a key '1', the value of which is a node that has a key '2', and so on. If there was another sequence, say, 1, 1, 3, ... then that second node mentioned would also have a key '3'. Once you have this tree built from the data, all ~275,000 of them, go through the test set, split the sequence strings, and for each one, traverse through the tree. When you get to the end of a test sequence, the next number in the sequence is whatever key the current node has. Hopefully there's only one, otherwise there is ambiguity. Not all test sequences are in the OEIS, it seems, and this method only matches the beginning of sequences, thus the ~20% inaccuracy. I hope people find this interesting or at least amusing!";Apache 2.0;https://www.kaggle.com/drhunter/how-to-cheat;0.2;[];['nn', 'rl'];[];https://www.kaggle.com/c/integer-sequence-learning;0.735;0.346;2020-12-13 11:57:13;Integer Sequence Learning;[];How to cheat;Python notebook;6972.0;16;;
2016-06-08 00:10:14;;Apache 2.0;https://www.kaggle.com/endintears/linear-models;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['generation', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/integer-sequence-learning;0.716;0.302;2020-12-13 11:57:13;Integer Sequence Learning;[];Linear Models (0.13972+);R script;4377.0;10;0.16691;0.16691
2016-07-02 09:40:13;;Apache 2.0;https://www.kaggle.com/endintears/real-machine-learning;0.5;[];['ai'];['test data', 'regression', 'train', 'label', 'logistic regression', 'predict', 'random forest'];https://www.kaggle.com/c/integer-sequence-learning;0.747;0.393;2020-12-13 11:57:13;Integer Sequence Learning;[];Real Machine Learning;Rmarkdown script;9529.0;27;;
2016-09-06 15:49:18;;Apache 2.0;https://www.kaggle.com/enrique1500/oeis-combining-trivial-models;0.5;[];['ai', 'cv'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/integer-sequence-learning;0.585;0.214;2020-12-13 11:57:13;Integer Sequence Learning;[];OEIS Combining Trivial Models;Rmarkdown script;351.0;4;;
2016-10-13 15:47:08;;Apache 2.0;https://www.kaggle.com/enrique1500/predict-oeis-with-markov-chains;1.0;['pattern'];['ai', 'dl'];['train', 'model', 'training data', 'predict'];https://www.kaggle.com/c/integer-sequence-learning;0.741;0.379;2020-12-13 11:57:13;Integer Sequence Learning;[];Predict OEIS with Markov Chains;Rmarkdown script;8131.0;23;;
2016-08-16 21:38:03;;Apache 2.0;https://www.kaggle.com/enrique1500/predict-oeis-with-ngrams-2;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['training data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/integer-sequence-learning;0.681;0.188;2020-12-13 11:57:13;Integer Sequence Learning;[];Predict OEIS with Ngrams: (2);R script;2012.0;3;0.12738;0.12738
2016-08-30 00:24:36;"Classifying sequencesThis Kaggle competition is unusual from a machine learning perspective as it doesn't necessarily require any learning; each sequence is independent, and some can be solved directly. Others can be estimated using ML techniques that may or may not be appropriate for finding their generating function. ML techniques used here face a problem typical to ML in general:  Just because a function or method appears to solve a sequence, it doesn't necessarily follow that it's the correct function. Being able to describe the existing data doesn't necessarily mean it's possible to accurately predict new data. Claiming the latter without adequate cross-validation and testing would amount to circular reasoning: ""If a sequence is solved by x then it's of type x and should be solved by x."" One approach to help mitigate the false positives and circular reasoning problems would be to apply a priori knowledge of the properties of a sequence to make a more informed decision about appropriate approaches to try, rather than blindly attempting all possible approaches and scoring/comparing them afterwards. In order to do this in an efficient automated fashion, it is presumably necessary to tag the basic properties of each sequence beforehand. Sequences on the OEIS are tagged with keywords, but the majority of these appear either subjective or require knowledge of  the generating function (http://oeis.org/wiki/Clear-cut_examples_of_keywords). Using them here would obviously be cheating. The purpose of this notebook is to suggest a few (possibly trivial) functions for automatically tagging fundamental properties of sequences without any reference the generating function. There are also a few examples at the end where the tags might inform decisions. Suggestions for other tags and their potential usefulness are welcomed. A few possible types of tags:  ""Basic"" - A description of what the sequence looks like, these are what we'll focus on here. ""Comparative"" -  Tagging a sequence as superficially similar to another (not included here). ""OEIS Keywords"" - Either similar to ""basic"" and already applied by OEIS automatically (eg. sign, nonn, short), or the manually applied OEIS keywords. ""Solvers"" - ""This function appears to be solved by this method"" (not included here to avoid circular reasoning).";Apache 2.0;https://www.kaggle.com/garethjns/classifying-tagging-sequences;0.5;[];['ner', 'ai', 'dl', 'nn', 'ml'];['fitting', 'machine learning', 'clustering', 'predict'];https://www.kaggle.com/c/integer-sequence-learning;0.704;0.311;2020-12-13 11:57:13;Integer Sequence Learning;[];Classifying/tagging sequences;R notebook;3352.0;11;;
2016-10-19 19:33:48;Intro;Apache 2.0;https://www.kaggle.com/javiervlab/integer-sequences-and-machine-learning;1.0;['sklearn'];['ai'];['train', 'model', 'predict'];https://www.kaggle.com/c/integer-sequence-learning;0.631;0.214;2020-12-13 11:57:13;Integer Sequence Learning;[];integer Sequences and Machine Learning?;Python notebook;775.0;4;;
2016-06-25 22:45:58;Integer Sequence Learning;Apache 2.0;https://www.kaggle.com/juliojaavier/sequence-learning;1.0;['sklearn'];['ai'];['regression', 'train', 'model', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/integer-sequence-learning;0.707;0.319;2020-12-13 11:57:13;Integer Sequence Learning;[];Sequence learning;Python notebook;3591.0;12;;
2016-06-17 18:22:11;;Apache 2.0;https://www.kaggle.com/lukeaanderso/lstm-integer-sequence-testing;1.0;['keras', 'sklearn', 'theano'];['ner', 'ai', 'cv', 'nlp', 'nn'];['gru', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'lstm', 'predict', 'classification'];https://www.kaggle.com/c/integer-sequence-learning;0.715;0.188;2020-12-13 11:57:13;Integer Sequence Learning;[];LSTM Integer Sequence Testing;Python script;4299.0;3;;
2016-07-28 19:08:34;;Apache 2.0;https://www.kaggle.com/megaherz/naive-polinomical-prediction;0.5;[];['ai'];['train', 'predict'];https://www.kaggle.com/c/integer-sequence-learning;0.592;0.188;2020-12-13 11:57:13;Integer Sequence Learning;[];Naive polinomical prediction;R notebook;395.0;3;;
2016-08-02 20:09:58;;Apache 2.0;https://www.kaggle.com/mlanier/predict-oeis-with-markov-chains;1.0;['pattern'];['ai', 'dl'];['train', 'model', 'training data', 'predict'];https://www.kaggle.com/c/integer-sequence-learning;0.668;0.253;2020-12-13 11:57:13;Integer Sequence Learning;[];Predict OEIS with Markov Chains;Rmarkdown script;1542.0;6;;
2016-07-12 00:02:24;"SummaryHow the training set and test set are related to each other? How many sequences are identical up to scaling and translation? The goal of this notebook is to find pairs of test and train sequences that are related by a linear equation, y=mx+b. The challenge is, due to the large number of sequences, it will be time-consuming to compare all pairs of (test,train) sequences. To reduce the computation time, we group sequences by their ""signatures"" so that it is only necessary to compare pairs in the same group. In less than 5 minutes, we have found 4579 match pairs; these amount to 4% of the 113,845 test sequences. (Note: We require m,b to be integers, mâ‰ 0, and we ignore those sequences with length less than 10.)";Apache 2.0;https://www.kaggle.com/ncchen/match-test-set-to-training-set;0.5;[];['ai', 'dl'];['train', 'predict'];https://www.kaggle.com/c/integer-sequence-learning;0.696;0.327;2020-12-13 11:57:13;Integer Sequence Learning;[];Match test set to training set;Python notebook;2825.0;13;;
2016-06-20 15:40:42;Summary:This notebook uses elementary math methods to detect linear recurrence sequences.  Machine learning methods are not used here. Among the 113,849 sequenes in the test set, we found more than 5000 of them satify recurrence relations of order 2,3 or 4. For those sequences, we computed the recurrence relations and predict the next terms. Moreover, we found recurrence relations that are not described in the OEIS. Linear Recurrence Relations(Remark: This notebook only considers homogeneous linear recurrence relations with constant coefficients. Nonlinear or non-homogeneous relations are not investigated here.) 2nd order recurrence relationA second order recurrence relation is of the form:  an+2=c0an+c1an+1, where the coefficients c0 and c1 are constant. For example, the Fibonacci sequence an+2=an+an+1 is a second order recurrence sequence with coefficients (1,1). 3rd order recurrence relationA second order recurrence relation is of the form:  an+3=c0an+c1an+1+c2an+2, where the coefficients c0,c1,c2 are constant. Detect Recurrence RelationsGiven a sequence an, let's say we want to verify whether it's given by a 3rd order recurrence relation. In other words, we check if it's possible to find constants c0,c1,c2 so that an+3=c0an+c1an+1+c2an+2 is satified. To find possible c0,c1,c2, since there are 3 unknowns, we need at least 3 equations. Let's set the equations using a3,a4,a5 as follows: a3=c0a0+c1a1+c2a2a4=c0a1+c1a2+c2a3a5=c0a2+c1a3+c2a4.Writting these equations in matrix form, we obtain [a0a1a2a1a2a3a2a3a4][c0c1c2]=[a3a4a5], then we solve for (c0,c1,c2). Once the coefficients (c0,c1,c2) are found, we check whether the next terms a6,a7,â‹¯ satisfy the recurrence relation.;Apache 2.0;https://www.kaggle.com/ncchen/recurrence-relation;0.5;[];['ai', 'dl', 'ml'];['machine learning', 'predict'];https://www.kaggle.com/c/integer-sequence-learning;0.756;0.447;2020-12-13 11:57:13;Integer Sequence Learning;[];Recurrence relation;Python notebook;11992.0;51;;
2016-07-14 16:13:37;Classification of SeriesI am not a math expert. I am trying to find clever tricks to solve this problem. We need to divide the series into bins for ML to work better on them. There are different kind of series. We will try to categorize them into groups. Lets start with simple classification: oscillating (OS) or non oscillating (NOS). Other way to classify is to check if it is increasing (I), decreasing (D), or range bound (RB). We can further classify the a series as arthematic series (AS) or exponential series (ES). We can trian a different machine learning algorithm for each type of series and hopefully be more successful. Let us read the data and examine it.;Apache 2.0;https://www.kaggle.com/shivaramkrs/classification-of-series;0.5;[];['ai', 'ml', 'rl'];['train', 'classification', 'machine learning'];https://www.kaggle.com/c/integer-sequence-learning;0.667;0.188;2020-12-13 11:57:13;Integer Sequence Learning;[];Classification of Series;Python notebook;1515.0;3;;
2016-09-29 02:40:09;;Apache 2.0;https://www.kaggle.com/suruili/simple-ngram-with-python-and-no-lib;1.0;['nltk'];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/integer-sequence-learning;0.693;0.236;2020-12-13 11:57:13;Integer Sequence Learning;[];simple ngram with python and no lib;Python script;2648.0;5;;
2016-06-06 07:05:38;;Apache 2.0;https://www.kaggle.com/uioreanu/notebook-7e93afd402a4301e1421;0.5;[];['nlp', 'ai', 'nn', 'ner'];['test data', 'train', 'model', 'deep learning', 'classification'];https://www.kaggle.com/c/integer-sequence-learning;0.689;0.236;2020-12-13 11:57:13;Integer Sequence Learning;[];explorer Integer Sequence Learning;R script;2405.0;5;;
2017-03-24 15:05:29;In the forum efg2 commented about test images present in the training data set. Here's the confirmation.;Apache 2.0;https://www.kaggle.com/aamaia/leeak;0.5;[];['ai'];['train', 'label', 'training data', 'labeled'];https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;0.724;0.433;2020-12-13 11:58:48;Intel & MobileODT Cervical Cancer Screening;[];Leeak :-);Python notebook;5324.0;43;;
2017-03-22 12:56:30;;Apache 2.0;https://www.kaggle.com/aamaia/three-empty-images-in-additional-7z;1.0;['tensorflow', 'keras', 'theano'];['ai'];['train'];https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;0.674;0.352;2020-12-13 11:58:48;Intel & MobileODT Cervical Cancer Screening;[];Three empty images in additional.7z;Python notebook;1755.0;17;;
2017-03-15 20:26:15;Intel & MobileODT Cervical Cancer ScreeningWelcome to another image processing competition! This time it's a competition from Intel, where we are trying to predict the cervix type from cervical images, before they might develop into cancer. The goal is to predict one of three classes, so this seems like a straightforward image classification challenge, with the mlogloss evaluation metric.;Apache 2.0;https://www.kaggle.com/anokas/experimental-data-analysis;0.5;[];['ai', 'ml'];['loss', 'classification', 'image classification', 'predict'];https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;0.67;0.268;2020-12-13 11:58:48;Intel & MobileODT Cervical Cancer Screening;[];Experimental Data Analysis;Python notebook;1621.0;7;;
2017-03-24 10:18:46;In the forum efg2 commented about test images present in the training data set. Here's the confirmation.;Apache 2.0;https://www.kaggle.com/chiszpanski/non-cervix-images;0.5;[];['ai', 'nn', 'ann'];['train', 'label', 'training data'];https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;0.677;0.39;2020-12-13 11:58:48;Intel & MobileODT Cervical Cancer Screening;[];Non-cervix images;Python notebook;1869.0;26;;
2017-05-06 15:17:02;First part: display bounding boxesThis kernel is linked to the discussion thread: https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening/discussion/31565#174995 where I give three files with bounding boxes. I merged all files into one called annot.tsv I said that there could be a need to account for window resizing, so I show here what is working. Imports;Apache 2.0;https://www.kaggle.com/deveaup/checking-bounding-boxes-and-additional-dataset;0.5;[];['ai', 'nn', 'ann'];['train'];https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;0.652;0.268;2020-12-13 11:58:48;Intel & MobileODT Cervical Cancer Screening;[];Checking bounding boxes and additional dataset;Python notebook;1135.0;7;;
2017-05-11 20:36:38;;Apache 2.0;https://www.kaggle.com/mathurinache/notebook0fc3287d0d;0.0;[];[];[];https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;0.549;0.236;2020-12-13 11:58:48;Intel & MobileODT Cervical Cancer Screening;[];Notebook0fc3287d0d;Python notebook;200.0;5;;
2020-07-26 12:55:13;Poonam Ligade 16th March 2017 Hi Kagglers, In this notebook I am trying to capture various aspects of data which are images. We have to create an algorithm which  accurately classifies women's cervix type based on images. So its clearly multiclass classification problem where 3 classes are  Type_1  Type_2  Type_3  This will eventually prevent ineffective treatments for wrongly identified cervix types.;Apache 2.0;https://www.kaggle.com/poonaml/intel-cervical-cancer-eda;0.5;[];['ai', 'nn', 'ann', 'rl'];['test data', 'training data', 'train', 'label', 'classification'];https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;0.709;0.418;2020-12-13 11:58:48;Intel & MobileODT Cervical Cancer Screening;[];Intel cervical cancer EDA;Python notebook;3733.0;36;;
2017-05-06 18:24:49;There are many non-trivial samples, I listed a few below. Appreciate if anyone could shed some lights on How/Why they were classified as current type.;Apache 2.0;https://www.kaggle.com/scottykwok/making-sense-out-of-some-difficult-samples;0.5;[];['ai', 'cv'];['train'];https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;0.62;0.311;2020-12-13 11:58:48;Intel & MobileODT Cervical Cancer Screening;[];Making sense out of some difficult samples;Python notebook;629.0;11;;
2017-04-13 22:21:36;;Apache 2.0;https://www.kaggle.com/zfturbo/cervix-rectangles-generator;1.0;['skimage', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ann'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening;0.679;0.319;2020-12-13 11:58:48;Intel & MobileODT Cervical Cancer Screening;[];Cervix Rectangles Generator;Python script;1964.0;12;;
2018-02-07 01:39:45;;Apache 2.0;https://www.kaggle.com/arthurtok/fractal-dimensions-of-world-coastlines;1.0;['pattern', 'h2o'];['ner', 'ai', 'nlu', 'dl', 'cnn', 'gbm', 'gan', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['vgg', 'model', 'gru', 'predict'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.728;0.492;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Fractal Dimensions of World Coastlines;Python notebook;5876.0;90;;
2017-05-27 06:50:11;;Apache 2.0;https://www.kaggle.com/bilokij/run-1-script;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.665;0.311;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Run 1 Script;Python script;1460.0;11;;
2015-11-02 22:39:58;;Apache 2.0;https://www.kaggle.com/carloshuertas/kaggle-users-by-location-data-only;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.705;0.334;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Kaggle Users by Location (data only);R script;3432.0;14;;
2015-07-21 03:17:48;;Apache 2.0;https://www.kaggle.com/chefele/scripts-first-try;0.5;[];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn'];['filter', 'deep learning', 'label', 'loss', 'classification'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.763;0.403;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Python Sieve: 48M Primes in 4 sec;Python script;14833.0;30;;
2015-07-15 06:42:30;;Apache 2.0;https://www.kaggle.com/codebender/which-version-of-python-is-installed;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.708;0.302;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];which version of python is installed?;Python script;3646.0;10;;
2016-01-05 23:36:55;;Apache 2.0;https://www.kaggle.com/datacanary/another-prime-sieve-in-python;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.683;0.311;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Prime sieve in almost pure Python;Python script;2101.0;11;;
2015-07-10 15:22:51;;Apache 2.0;https://www.kaggle.com/domcastro/hello-world-the-data-science-way;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['label', 'classification', 'deep learning'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.718;0.393;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Hello World - the Data Science way;R script;4593.0;27;;
2015-08-10 03:07:37;;Apache 2.0;https://www.kaggle.com/jimthompson/kaggle-competition-medal-count-analysis;0.5;[];['ner', 'ai', 'cnn', 'gan', 'rl', 'nlp', 'nn', 'ml'];['machine learning', 'recognition', 'deep learning', 'layer', 'loss', 'label', 'text classification', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.732;0.357;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Kaggle Competition Medal Count Analysis;Rmarkdown script;6552.0;18;;
2015-08-21 01:45:53;;Apache 2.0;https://www.kaggle.com/jimthompson/visualizing-kaggle-team-structures;1.0;['h2o'];['ner', 'ai', 'nlu', 'dl', 'cnn', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['machine learning', 'fitting', 'model', 'recognition', 'deep learning', 'layer', 'loss', 'label', 'text classification', 'predict', 'rank', 'recommend', 'classification'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.735;0.371;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Kaggle Team Structures and Performance;Rmarkdown script;6935.0;21;;
2015-06-25 17:24:38;;Apache 2.0;https://www.kaggle.com/justmarkham/introduction-to-dplyr-part-1;0.5;[];['ner', 'ai', 'nlu', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'deep learning', 'label', 'rank', 'classification'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.753;0.375;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Introduction to dplyr (Part 1);Rmarkdown script;11123.0;22;;
2015-06-25 17:29:54;;Apache 2.0;https://www.kaggle.com/justmarkham/introduction-to-dplyr-part-2;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'model', 'deep learning', 'label', 'recommend', 'classification'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.732;0.327;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Introduction to dplyr (Part 2);Rmarkdown script;6459.0;13;;
2015-09-02 15:22:04;;Apache 2.0;https://www.kaggle.com/khyh00/xkcd-style-test;0.5;[];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['label', 'deep learning', 'classification', 'predict'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.72;0.357;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];xkcd style test;Python script;4887.0;18;;
2015-08-24 17:23:56;;Apache 2.0;https://www.kaggle.com/liubenyuan/time-series-and-anomaly-detection;0.5;[];['nlp', 'ai', 'nn', 'ner'];['anomaly detection', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.761;0.375;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Time Series and anomaly detection;Python script;14007.0;22;;
2015-07-09 23:09:25;;Apache 2.0;https://www.kaggle.com/michalthedude/kaggle-users-by-location;0.5;[];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn', 'ml'];['rank', 'classification', 'deep learning'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.715;0.34;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Kaggle Users by Location;R script;4286.0;15;;
2015-10-23 10:06:44;;Apache 2.0;https://www.kaggle.com/miniushkin/jitter-test-for-overfitting-notebook;1.0;['sklearn'];['ner', 'ai', 'nlu', 'dl', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'train', 'fitting', 'model', 'supervised learning', 'deep learning', 'support vector machines', 'label', 'predict', 'classification'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.732;0.319;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Jitter Test for Overfitting notebook;Python notebook;6438.0;12;;
2016-11-02 20:26:50;;Apache 2.0;https://www.kaggle.com/mylesoneill/normalized-kaggle-medal-count-by-country;0.2;[];['ai', 'rl'];[];https://www.kaggle.com/c/introducing-kaggle-scripts;0.713;0.362;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Normalized Kaggle Medal Count By Country;Rmarkdown script;4143.0;19;;
2018-03-15 15:26:13;;Apache 2.0;https://www.kaggle.com/nagadomi/list-of-installed-packages;1.0;['statsmodels', 'xgboost', 'h2o', 'simpleitk', 'nltk', 'catboost', 'tensorflow', 'spacy', 'keras', 'lightgbm', 'theano', 'mxnet', 'keras-rl', 'tpot', 'gensim', 'sklearn', 'opencv-python', 'pillow', 'textblob'];['ner', 'ai', 'cnn', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'model', 'deep learning', 'resnet', 'classification', 'bayesian'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.744;0.427;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];List of installed packages;Python script;8715.0;40;;
2015-07-10 05:30:24;;Apache 2.0;https://www.kaggle.com/tanitter/grid-search-xgboost-with-scikit-learn;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gan', 'cv', 'ml', 'nlp', 'nn', 'ann'];['predict', 'train', 'deep learning', 'label', 'loss', 'classification'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.803;0.429;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Grid search xgboost with scikit-learn;Python script;49733.0;41;;
2015-07-14 17:36:11;;Apache 2.0;https://www.kaggle.com/toshik/splines-with-r;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.723;0.34;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Splines with R;R script;5190.0;15;;
2015-07-11 01:10:17;;Apache 2.0;https://www.kaggle.com/triskelion/connected-particles-iii-bl-ocks;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['classification', 'deep learning'];https://www.kaggle.com/c/introducing-kaggle-scripts;0.698;0.408;2020-12-13 12:08:52;Introducing Kaggle Scripts;[];Connected Particles III (bl.ocks);Python script;2951.0;32;;
2017-08-06 02:14:37;;Apache 2.0;https://www.kaggle.com/algila/inception-v3-and-k-fold-in-python-0-98996;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ann'];['training data', 'test data', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/invasive-species-monitoring;0.693;0.292;2020-12-13 12:11:50;Invasive Species Monitoring;[];Inception v3 and k-fold in Python (0.98996);Python script;2619.0;9;;
2018-09-07 11:57:48;Acknowledgements;Apache 2.0;https://www.kaggle.com/ambarish/invasive-species-monitoring-analysis;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'rl', 'cv', 'nn'];['test data', 'train', 'model', 'epoch', 'validation data', 'layer', 'vgg', 'loss', 'label', 'predict'];https://www.kaggle.com/c/invasive-species-monitoring;0.609;0.188;2020-12-13 12:11:50;Invasive Species Monitoring;['gpu'];Invasive Species Monitoring Analysis;Python notebook;522.0;3;;
2017-05-26 09:53:52;;Apache 2.0;https://www.kaggle.com/finlay/naive-bagging-cnn-pb0-985;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/invasive-species-monitoring;0.684;0.327;2020-12-13 12:11:50;Invasive Species Monitoring;[];Naive Bagging CNN(PB0.985);Python script;2154.0;13;;
2017-08-01 19:47:27;;Apache 2.0;https://www.kaggle.com/ogurtsov/0-99-with-r-and-keras-inception-v3-fine-tune;1.0;['pattern', 'keras'];['ner', 'ai'];['test data', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/invasive-species-monitoring;0.719;0.397;2020-12-13 12:11:50;Invasive Species Monitoring;[];0.99 with R and Keras (Inception V3 fine-tune);Rmarkdown script;4738.0;28;;
2017-05-12 22:26:36;;Apache 2.0;https://www.kaggle.com/paulorzp/xgboost-starter;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['filter', 'test data', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/invasive-species-monitoring;0.676;0.319;2020-12-13 12:11:50;Invasive Species Monitoring;[];XGBoost Starter;Python script;1818.0;12;;
2017-08-10 01:41:08;;Apache 2.0;https://www.kaggle.com/shuyuan00/resnet50-with-0-985lb;1.0;['tensorflow', 'keras'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['test data', 'train', 'model', 'epoch', 'deep learning', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'resnet', 'classification'];https://www.kaggle.com/c/invasive-species-monitoring;0.697;0.236;2020-12-13 12:11:50;Invasive Species Monitoring;[];ResNet50 with 0.985LB;Python script;2873.0;5;;
2017-07-30 12:55:00;Keras based model LB 0.98ish;Apache 2.0;https://www.kaggle.com/suhasmallya/keras-based-model-0-98lb;1.0;['keras', 'sklearn'];['ner', 'ai', 'rl', 'cv', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/invasive-species-monitoring;0.612;0.188;2020-12-13 12:11:50;Invasive Species Monitoring;[];Keras based model - 0.98LB;Python notebook;550.0;3;;
2019-03-27 07:12:21;Let's have a quick look at the 'iWildCam 2019 - FGVC6' data...Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to nearby areas we are faced with an interesting probem: how do you classify a species in a new region that you may not have seen in previous training data?;Apache 2.0;https://www.kaggle.com/a45632/eda-iwildcam-2019-v1-1;0.5;[];['ai', 'nn', 'rl'];['filter', 'training data', 'test data', 'train', 'model', 'label', 'classification'];https://www.kaggle.com/c/iwildcam-2019-fgvc6;0.594;0.292;2020-12-13 12:12:46;iWildCam 2019 - FGVC6;['gpu, beginner, data visualization, +1 moreexploratory data analysis'];EDA iWildCam 2019 v1.1;Python notebook;405.0;9;;
2019-03-27 06:53:58;Prediction;Apache 2.0;https://www.kaggle.com/ateplyuk/iwildcam2019-keras-efficientnet;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cv', 'nn'];['train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/iwildcam-2019-fgvc6;0.662;0.393;2020-12-13 12:12:46;iWildCam 2019 - FGVC6;['gpu'];Keras Starter VGG 10epc;Python notebook;1371.0;27;0.084;0.094
2019-06-01 13:48:11;learn.lr_find();Apache 2.0;https://www.kaggle.com/benjibb/fastai-with-senet154;1.0;['sklearn', 'pillow'];['ner', 'ai', 'cnn', 'rl', 'nn', 'ml'];['train', 'model', 'layer', 'loss', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/iwildcam-2019-fgvc6;0.659;0.253;2020-12-13 12:12:46;iWildCam 2019 - FGVC6;['gpu'];fastai with senet154;Python notebook;1312.0;6;;
2019-05-08 18:23:51;EDA  (Exploratory Data Analysis);Apache 2.0;https://www.kaggle.com/bonhart/pytorch-eda-and-resnet;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'nn', 'ann'];['training data', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/iwildcam-2019-fgvc6;0.627;0.281;2020-12-13 12:12:46;iWildCam 2019 - FGVC6;['gpu, beginner, data visualization'];[PyTorch] EDA and ResNet;Python notebook;717.0;8;0.086;0.106
2019-04-06 18:35:39;;Apache 2.0;https://www.kaggle.com/hrush777/data-processing-iwildcam-2019;1.0;['tensorflow', 'keras'];['ai', 'nn', 'cv'];['train'];https://www.kaggle.com/c/iwildcam-2019-fgvc6;0.611;0.188;2020-12-13 12:12:46;iWildCam 2019 - FGVC6;[];Data_Processing (iWildCam 2019);Python notebook;537.0;3;;
2019-05-11 04:51:09;;Apache 2.0;https://www.kaggle.com/joocheol/train-on-pretrained-model;1.0;['tensorflow', 'keras'];['dl', 'ai', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/iwildcam-2019-fgvc6;0.664;0.281;2020-12-13 12:12:46;multiple data sources;['gpu'];Train on pretrained model;Python notebook;1450.0;8;;
2019-03-25 23:12:32;A Brief Data ExplorationSummary:  There are 23 classes but about 9 of them are not in the training set.  Highly unbalanced data;Apache 2.0;https://www.kaggle.com/kokecacao/a-brief-data-exploration-for-iwildcam;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/iwildcam-2019-fgvc6;0.6;0.302;2020-12-13 12:12:46;iWildCam 2019 - FGVC6;['data visualization, classification'];A Brief Data Exploration for iWildCam;Python notebook;445.0;10;;
2019-03-27 03:01:12;CLAHE(Contrast Limited Adaptive Histogram Equalization)ref: https://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html;Apache 2.0;https://www.kaggle.com/seriousran/image-pre-processing-for-wild-images;0.5;[];['ai', 'rl', 'ml', 'cv'];['train'];https://www.kaggle.com/c/iwildcam-2019-fgvc6;0.737;0.489;2020-12-13 12:12:46;iWildCam 2019 - FGVC6;[];Image Pre-processing for Wild Images;Python notebook;7376.0;87;;
2019-07-08 20:36:07;Loading all needed modules;Apache 2.0;https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading;1.0;['skimage'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model'];https://www.kaggle.com/c/iwildcam-2019-fgvc6;0.751;0.496;2020-12-13 12:12:46;multiple data sources;[];Benchmark 2019: Speed of image reading;Python notebook;10687.0;95;;
2020-03-21 17:25:00;;Apache 2.0;https://www.kaggle.com/aleksandradeis/iwildcam-eda;0.5;[];['ner', 'ai', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'neural network', 'label', 'k-nearest neighbor', 'classification'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.606;0.188;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;['data visualization, exploratory data analysis, deep learning'];iWildCam EDA;Python notebook;497.0;3;;
2020-04-30 18:32:21;;Apache 2.0;https://www.kaggle.com/boltcoder/iwildcam-2020-demo-kernel;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.59;0.188;2020-12-13 12:13:39;multiple data sources;['data visualization, classification, image data, +1 moretransfer learning'];iWildCam 2020 Demo Kernel;Python notebook;378.0;3;;
2020-05-05 09:00:06;iWildCam 2020 - FGVC7 Categorize animals in the wild;Apache 2.0;https://www.kaggle.com/bsridatta/eda-and-object-extraction-for-classifier-training;0.5;[];['ai', 'nn', 'ann'];['train', 'label', 'test data'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.66;0.393;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;['beginner, data visualization, classification'];EDA and Object Extraction for Classifier Trainingâš¡;Python notebook;1332.0;27;;
2020-03-21 01:38:32;Part â„–1 create tfrecords;Apache 2.0;https://www.kaggle.com/drobchak1988/iwildcam-2020-fgvc7-tensorflow-create-tfrecords;1.0;['pattern', 'tensorflow'];['ai', 'dl', 'rl', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.564;0.236;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;[];iWildCam 2020-FGVC7 Tensorflow create tfrecords;Python notebook;252.0;5;;
2020-03-23 04:19:36;;Apache 2.0;https://www.kaggle.com/drobchak1988/iwildcam-2020-fgvc7-tensorflow-tpu-cpu-gpu;1.0;['tensorflow', 'keras'];['ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.59;0.236;2020-12-13 12:13:39;multiple data sources;['gpu'];iWildCam 2020-FGVC7 Tensorflow (TPU,CPU,GPU);Python notebook;378.0;5;;
2020-03-10 04:52:03;;Apache 2.0;https://www.kaggle.com/grapestone5321/iwildcam-2020-sample-submission;0.5;[];['ai', 'nn', 'ann'];['train'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.614;0.214;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;[];iWildCam 2020-sample_submission;Python notebook;571.0;4;0.001;0.001
2020-04-03 09:00:03;To see if creating Duplicates improves the modelI dont think this will improve the model by a lot. But I'm placing this here anyway;Apache 2.0;https://www.kaggle.com/manojacharya/fastai-implementation-after-pre-processing-images;0.5;[];['ner', 'ai', 'dl', 'cnn', 'nn', 'ann'];['train', 'model', 'epoch', 'loss', 'label', 'predict', 'relu', 'resnet', 'classification'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.585;0.236;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;[];Fastai Implementation After Pre-Processing Images ;Python notebook;353.0;5;;
2020-06-12 16:54:43;;Apache 2.0;https://www.kaggle.com/nihalusman/kernel76;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'ann', 'cv'];['train', 'model', 'epoch', 'validation data', 'layer', 'label', 'loss', 'relu', 'resnet'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.49;0.099;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;[];kernel76;Python notebook;88.0;1;;
2020-03-13 03:13:42;Show how to draw bbox and crop them as train datas.;Apache 2.0;https://www.kaggle.com/qinhui1999/how-to-use-bbox-for-iwildcam-2020;0.5;[];['ai', 'nn', 'ann'];['train'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.603;0.292;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;[];how to use bbox for iWildCam 2020 ;Python notebook;474.0;9;;
2020-04-18 21:59:38;;Apache 2.0;https://www.kaggle.com/rogerterrazas/trailcam-models;1.0;['tensorflow', 'sklearn', 'keras', 'pillow'];['ner', 'ai', 'cv', 'rl', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.525;0.099;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;[];Trailcam_Models;Python notebook;142.0;1;;
2020-03-11 06:03:36;This kernel based on the this kernerl It introduce image pre-precessing for wild images using CLAHE and AWB;Apache 2.0;https://www.kaggle.com/seriousran/image-pre-processing-for-iwild-2020;0.5;[];['ner', 'ai', 'rl', 'cv', 'ml'];['train'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.665;0.371;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;[];Image Pre-processing for iWild 2020;Python notebook;1463.0;21;;
2020-03-10 04:06:54;to be continue...;Apache 2.0;https://www.kaggle.com/seshurajup/fastai-starter-code;0.5;[];['ai', 'nn', 'ann'];['train'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.597;0.152;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;[];fastai starter code;Python notebook;426.0;2;;
2020-04-10 20:03:28;** [MONK](https://github.com/Tessellate-Imaging/monk_v1)**;Apache 2.0;https://www.kaggle.com/sinchubhat/iwildcam2020usingmonk;1.0;['pytorch', 'keras', 'mxnet', 'pillow'];['ner', 'ai', 'cv', 'nn', 'ann'];['train', 'model', 'epoch', 'deep learning', 'predict', 'computer vision', 'resnet'];https://www.kaggle.com/c/iwildcam-2020-fgvc7;0.532;0.152;2020-12-13 12:13:39;iWildCam 2020 - FGVC7;['beginner'];iWildCam2020UsingMonk;Python notebook;157.0;2;;
2020-04-15 14:22:46;Install torch_xla;Apache 2.0;https://www.kaggle.com/abhishek/i-like-clean-tpu-training-kernels-i-can-not-lie;1.0;['pytorch', 'spacy', 'sklearn', 'pillow'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'rank', 'classification'];https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification;0.717;0.497;2020-12-13 12:15:35;multiple data sources;['tpu'];I Like Clean TPU Training Kernels & I Can Not Lie;Python notebook;4539.0;97;;
2020-03-26 10:20:59;;Apache 2.0;https://www.kaggle.com/abhishek/inference-of-bert-tpu-model-ml-w-validation;0.5;[];['ai', 'nn'];['train', 'model', 'classification', 'layer'];https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification;0.744;0.517;2020-12-13 12:15:35;multiple data sources;['gpu'];inference of bert tpu model ml w/ validation;Python notebook;8756.0;127;0.9162;0.9139
2020-05-03 05:51:02;This notebook is currently a test site for augmenting data by looking at previous ideas and possibly trying new ones. Here are three ideas, one for adding translations, one for creating synthetic data, and one for interjecting noise/variations. TranslationsThis idea originally came from the first Toxic challenge in which translation was used as an encoder-decoder. Pavel Ostyakov's A simple technique for extending dataset explains how translating an english comment to another language, and then back to english, can improve model accuracy. With this being a multilingual competition, there have been similar ideas implemented but maybe not in this exact way. In this notebook I'm using Google Translator via the googletrans package.;Apache 2.0;https://www.kaggle.com/jpmiller/augmenting-the-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'classification'];https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification;0.736;0.501;2020-12-13 12:15:35;multiple data sources;['nlp, linguistics'];Augmenting the Data;Python notebook;7242.0;102;;
2020-04-19 12:05:15;Introduction;Apache 2.0;https://www.kaggle.com/mobassir/understanding-cross-lingual-models;1.0;['nltk', 'gensim', 'sklearn', 'pytorch', 'tensorflow', 'textblob', 'vocabulary', 'keras'];['ner', 'ai', 'dl', 'cnn', 'gan', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['filter', 'natural language processing', 'machine translation', 'predict', 'gru', 'training data', 'train', 'epoch', 'lstm', 'classification', 'model', 'neural network', 'layer', 'loss', 'rank', 'understanding', 'validation data', 'natural language', 'convolutional neural network'];https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification;0.754;0.512;2020-12-13 12:15:35;multiple data sources;['tpu'];Understanding cross lingual models;Python notebook;11527.0;118;0.9346;0.9361
2020-09-03 17:47:59;"Introduction While working in python jupyter notebooks using pandas with small datasets (100MB), performance degradation is not seen however when we work on large datasets (1 GB or above), performance issue of the notebook is clearly seen as it takes longer time for the cell to execute resulting in either longer execution time of the cell or the notebook failing due to insufficient memory. Tools like Spark can handle large datasets (100's of GB to TB), taking full advantage of their capabilities usually requires more expensive hardware which is practically not possible while we try to work POC's or doing research. And unlike pandas, they lack rich feature sets for high quality data cleaning, exploration, and analysis. For medium-sized data, weâ€™re better off trying to get more out of pandas, rather than switching to a different tool. DatasetI have choosen a existing kaggle dataset from competition data namely Jigsaw Multilingual Toxic Comment Classification which has sizable data to demonstrate what I am trying to convey in this notebook. Brief about the data:Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results ""translate"" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages. As our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential. Files jigsaw-toxic-comment-train.csv - data from our first competition. The dataset is made up of English comments from Wikipediaâ€™s talk page edits. jigsaw-unintended-bias-train.csv - data from our second competition. This is an expanded version of the Civil Comments dataset with a range of additional labels. sample_submission.csv - a sample submission file in the correct format test.csv - comments from Wikipedia talk pages in different non-English languages. validation.csv - comments from Wikipedia talk pages in different non-English languages. jigsaw-toxic-comment-train-processed-seqlen128.csv - training data preprocessed for BERT jigsaw-unintended-bias-train-processed-seqlen128.csv - training data preprocessed for BERT validation-processed-seqlen128.csv - validation data preprocessed for BERT test-processed-seqlen128.csv - test data preprocessed for BERT  Letâ€™s start by importing both pandas and our data in Python and taking a look at the first five rows.";Apache 2.0;https://www.kaggle.com/pavansanagapati/14-simple-tips-to-save-ram-memory-for-1-gb-dataset;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'understanding', 'validation data', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification;0.75;0.544;2020-12-13 12:15:35;multiple data sources;[];14 Simple Tips to save RAM memory for 1+GB dataset;Python notebook;10306.0;187;;
2020-06-17 13:20:11;NLP Cheatsheet: Master NLP;Apache 2.0;https://www.kaggle.com/rftexas/nlp-cheatsheet-master-nlp;1.0;['pattern', 'vocabulary'];['ner', 'ai', 'dl', 'cnn', 'gan', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['unlabeled', 'filter', 'recurrent neural network', 'machine translation', 'predict', 'unsupervised learning', 'relu', 'gru', 'training data', 'neuron', 'train', 'lstm', 'activation function', 'recommend', 'classification', 'labeled', 'propagation', 'model', 'neural network', 'layer', 'loss', 'hidden layer', 'supervised learning', 'generation', 'deep learning', 'label', 'computer vision', 'natural language'];https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification;0.724;0.501;2020-12-13 12:15:35;multiple data sources;[];NLP Cheatsheet: Master NLP;Python notebook;5344.0;102;;
2020-06-22 06:23:10;;Apache 2.0;https://www.kaggle.com/shahules/tackle-with-label-smoothing-proved;1.0;['tensorflow', 'sklearn', 'keras'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification', 'ground truth'];https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification;0.722;0.496;2020-12-13 12:15:35;Jigsaw Multilingual Toxic Comment Classification;[];Tackle with Label Smoothing : Proved ðŸ”¥ðŸ”¥;Python notebook;5138.0;95;;
2018-02-23 14:50:36;CountVectorizer -- Brief Tutorial;Apache 2.0;https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments;1.0;['pattern', 'vocabulary', 'sklearn', 'nltk'];['ai', 'dl', 'ml', 'cv'];['filter', 'training data', 'regression', 'train', 'model', 'layer', 'loss', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.82;0.505;2020-12-13 12:20:07;Toxic Comment Classification Challenge;['classification'];CountVectorizer, TfidfVectorizer, Predict Comments;Python notebook;92128.0;108;;
2018-03-17 02:56:51;;Apache 2.0;https://www.kaggle.com/chongjiujjin/capsule-net-with-gru;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'rl'];['gru', 'train', 'model', 'epoch', 'layer', 'loss', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.775;0.543;2020-12-13 12:20:07;multiple data sources;[];Capsule net with GRU;Python notebook;20910.0;184;0.98077;0.98214
2018-01-19 22:26:51;;Apache 2.0;https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-069;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cnn', 'gan', 'cv', 'rl', 'nlp', 'nn', 'rnn', 'ann'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.788;0.559;2020-12-13 12:20:07;Toxic Comment Classification Challenge;[];Keras - Bidirectional LSTM baseline ( lb 0.069);Python script;31026.0;233;;
2018-03-06 20:36:41;;Apache 2.0;https://www.kaggle.com/eashish/bidirectional-gru-with-convolution;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'rl'];['gru', 'train', 'model', 'epoch', 'layer', 'loss', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.782;0.515;2020-12-13 12:20:07;multiple data sources;[];Bidirectional LSTM with Convolution;Python notebook;25623.0;124;0.98262;0.98393
2018-01-18 15:55:35;Feature engineeringIn this notebook i want try hand-crafting some features that could help to create a model. I want to see what creative ideas i can come up with - and if they indeed seem to work.;Apache 2.0;https://www.kaggle.com/eikedehling/feature-engineering;0.5;[];['ai'];['train', 'model', 'predict'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.755;0.507;2020-12-13 12:20:07;Toxic Comment Classification Challenge;[];Feature engineering;Python notebook;11782.0;111;;
2018-03-09 21:03:08;;Apache 2.0;https://www.kaggle.com/hhstrand/oof-stacking-regime;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'test data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'lstm', 'label', 'logistic regression', 'predict', 'rank', 'classification', 'labeled'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.761;0.526;2020-12-13 12:20:07;multiple data sources;[];OOF stacking regime;Python script;13801.0;145;0.98540;0.98582
2018-02-03 20:50:58;Introduction;Apache 2.0;https://www.kaggle.com/jhoward/minimal-lstm-nb-svm-baseline-ensemble;0.5;[];['ai'];['recurrent neural network', 'model', 'neural network', 'lstm', 'label', 'naive bayes'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.784;0.55;2020-12-13 12:20:07;multiple data sources;[];Minimal LSTM + NB-SVM baseline ensemble;Python notebook;27026.0;204;0.98103;0.98117
2020-05-03 05:51:02;This notebook is currently a test site for augmenting data by looking at previous ideas and possibly trying new ones. Here are three ideas, one for adding translations, one for creating synthetic data, and one for interjecting noise/variations. TranslationsThis idea originally came from the first Toxic challenge in which translation was used as an encoder-decoder. Pavel Ostyakov's A simple technique for extending dataset explains how translating an english comment to another language, and then back to english, can improve model accuracy. With this being a multilingual competition, there have been similar ideas implemented but maybe not in this exact way. In this notebook I'm using Google Translator via the googletrans package.;Apache 2.0;https://www.kaggle.com/jpmiller/augmenting-the-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'classification'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.736;0.501;2020-12-13 12:20:07;multiple data sources;['nlp, linguistics'];Augmenting the Data;Python notebook;7243.0;102;;
2018-03-04 14:25:44;;Apache 2.0;https://www.kaggle.com/ogrellier/lgbm-with-words-and-chars-n-gram;1.0;['pattern', 'vocabulary', 'lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ml'];['training data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'logistic regression', 'gradient boosting', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.748;0.504;2020-12-13 12:20:07;Toxic Comment Classification Challenge;['nlp, gradient boosting'];LGBM with words and chars n_gram ;Python script;9739.0;106;0.97818;0.97926
2017-12-24 06:46:12;;Apache 2.0;https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043;1.0;['nltk', 'gensim', 'theano', 'tensorflow', 'vocabulary', 'keras'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'rnn'];['gru', 'train', 'model', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.782;0.513;2020-12-13 12:20:07;Toxic Comment Classification Challenge;[];keras lstm attention glove840b,lb 0.043;Python script;25818.0;120;;
2018-02-22 19:24:21;;Apache 2.0;https://www.kaggle.com/thousandvoices/logistic-regression-with-words-and-char-n-grams;1.0;['pattern', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'rnn'];['training data', 'regression', 'train', 'fitting', 'model', 'supervised learning', 'deep learning', 'loss', 'label', 'logistic regression', 'predict', 'unsupervised learning', 'understanding', 'classification'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.758;0.508;2020-12-13 12:20:07;Toxic Comment Classification Challenge;[];Logistic regression with words and char n-grams;Python script;12976.0;112;0.98055;0.97889
2018-03-20 16:09:59;;Apache 2.0;https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams;1.0;['pattern', 'vocabulary', 'xgboost', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['autoencoder', 'training data', 'test data', 'regression', 'generation', 'train', 'fitting', 'model', 'understanding', 'deep learning', 'loss', 'label', 'logistic regression', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.811;0.608;2020-12-13 12:20:06;Toxic Comment Classification Challenge;[];Logistic regression with words and char n-grams;Python script;66472.0;510;0.97644;0.97565
2018-02-26 20:02:24;;Apache 2.0;https://www.kaggle.com/yekenot/pooled-gru-fasttext;1.0;['vocabulary', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ann'];['gru', 'filter', 'test data', 'train', 'model', 'output layer', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'label', 'predict', 'classification', 'labeled'];https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge;0.794;0.569;2020-12-13 12:20:07;multiple data sources;[];Pooled GRU + FastText;Python script;37296.0;274;0.98169;0.98302
2019-04-20 17:00:49;Preface;Apache 2.0;https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version;1.0;['pytorch', 'tensorflow', 'keras'];['ner', 'ai', 'cv', 'nn', 'rnn'];['predict', 'training data', 'train', 'fitting', 'model', 'epoch', 'lstm', 'loss', 'relu', 'classification'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.796;0.588;2020-12-13 12:22:44;multiple data sources;['gpu, beginner, deep learning'];Simple LSTM - PyTorch version;Python notebook;39830.0;371;;
2019-05-13 20:26:32;Preface;Apache 2.0;https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda;1.0;['nltk', 'sklearn', 'tensorflow', 'vocabulary', 'keras'];['ner', 'ai', 'dl', 'ml', 'nn', 'rnn', 'ann'];['train', 'model', 'neural network', 'epoch', 'lstm', 'sentiment analysis', 'classification'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.754;0.563;2020-12-13 12:22:44;multiple data sources;['gpu'];How To: Preprocessing for GloVe Part1: EDA;Python notebook;11505.0;247;;
2019-05-14 09:11:57;Preface;Apache 2.0;https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage;1.0;['vocabulary', 'tensorflow', 'keras', 'nltk'];['ner', 'ai', 'dl', 'gan', 'nn', 'rnn', 'ml'];['filter', 'train', 'model', 'neural network', 'epoch', 'validation data', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.757;0.573;2020-12-13 12:22:44;multiple data sources;['gpu'];How To: Preprocessing for GloVe Part2: Usage;Python notebook;12586.0;292;0.93628;0.93628
2019-03-31 17:39:49;;Apache 2.0;https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold;1.0;['tensorflow', 'sklearn', 'keras'];['dl', 'ai', 'nn', 'rl'];['gru', 'filter', 'train', 'fitting', 'model', 'epoch', 'layer', 'loss', 'lstm', 'predict', 'classification'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.768;0.554;2020-12-13 12:22:44;multiple data sources;['gpu'];keras baseline lstm + attention 5-fold;Python notebook;16876.0;216;0.92562;0.92562
2019-05-13 20:05:33;Install requirements;Apache 2.0;https://www.kaggle.com/christofhenkel/loading-bert-using-pytorch-with-tokenizer-apex;1.0;['pytorch'];['dl', 'ai', 'nn'];['train', 'layer', 'model', 'predict'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.747;0.499;2020-12-13 12:22:44;multiple data sources;['gpu'];Loading BERT using pytorch (with tokenizer & apex);Python notebook;9615.0;99;;
2019-05-05 11:43:16;;Apache 2.0;https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution;1.0;['tensorflow', 'lightgbm', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cnn', 'gbm', 'rl', 'nlp', 'nn', 'ann'];['gru', 'test data', 'train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'relu', 'loss', 'label', 'lstm', 'predict', 'rank', 'understanding', 'classification', 'labeled'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.756;0.525;2020-12-13 12:22:44;multiple data sources;['gpu'];Jigsaw Fast Compact Solution;Python script;12097.0;142;0.93303;0.93303
2019-04-09 23:05:35;Preface;Apache 2.0;https://www.kaggle.com/kunwar31/simple-lstm-with-identity-parameters-fastai;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cv', 'nn'];['predict', 'train', 'model', 'epoch', 'lstm', 'loss', 'relu', 'classification'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.777;0.557;2020-12-13 12:22:44;multiple data sources;['gpu, deep learning'];Simple LSTM with Identity Parameters - FastAI;Python notebook;22077.0;226;0.93524;0.93524
2019-04-09 10:00:46;;Apache 2.0;https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ann'];['test data', 'neuron', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification', 'labeled', 'propagation'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.749;0.532;2020-12-13 12:22:44;multiple data sources;['gpu'];Simple LSTM using Identity Parameters Solution;Python script;10146.0;158;0.93351;0.93351
2019-06-22 15:27:42;;Apache 2.0;https://www.kaggle.com/thousandvoices/simple-lstm;1.0;['tensorflow', 'keras', 'gensim'];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['filter', 'logistic regression', 'predict', 'relu', 'neuron', 'train', 'epoch', 'lstm', 'classification', 'labeled', 'model', 'layer', 'loss', 'understanding', 'resnet', 'test data', 'regression', 'fitting', 'deep learning', 'label'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.812;0.617;2020-12-13 12:22:44;multiple data sources;['gpu'];Simple LSTM;Python script;69034.0;595;;
2019-05-30 12:34:10;;Apache 2.0;https://www.kaggle.com/yuval6967/toxic-bert-plain-vanila;1.0;['nltk', 'sklearn', 'pytorch', 'tensorflow', 'keras'];['ner', 'ai', 'dl', 'rl', 'nn', 'rnn', 'ml'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification;0.795;0.59;2020-12-13 12:22:44;multiple data sources;['gpu, classification'];Toxic  BERT plain vanila;Python notebook;38863.0;379;;
2020-10-20 17:35:40;;Apache 2.0;https://www.kaggle.com/augustodenevreze/users-jobs-exploration;1.0;['pattern'];['ner', 'ai', 'gan', 'rl', 'nn', 'ml'];['train', 'recommend'];https://www.kaggle.com/c/job-recommendation;0.591;0.152;2020-12-13 12:23:03;Job Recommendation Challenge;[];users_jobs_exploration;Python notebook;385.0;2;;
2020-02-12 19:41:41;STUDENT COMMUNITY IN KAGGLE;Apache 2.0;https://www.kaggle.com/amiiiney/student-community-on-kaggle;1.0;['xgboost', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'recurrent neural network', 'natural language processing', 'logistic regression', 'predict', 'machine learning', 'recommend', 'linear regression', 'generative adversarial network', 'classification', 'propagation', 'model', 'neural network', 'layer', 'decision tree', 'bayesian', 'regression', 'artificial intelligence', 'deep learning', 'label', 'gradient boosting', 'computer vision', 'random forest', 'natural language', 'convolutional neural network'];https://www.kaggle.com/c/kaggle-survey-2019;0.746;0.538;2020-12-13 12:25:09;multiple data sources;['beginner, data visualization, exploratory data analysis'];Student community on Kaggle;Python notebook;9209.0;170;;
2019-12-11 23:35:55;How to Create Award Winning Data Visualizations  Earlier this week I won the third prize (U$ 6.000) on Kaggle's Machine Learning and Data Science Survey Competition with my notebook Is there any job out there? Kaggle vs Glassdoor. While the competition evaluated several aspects of my submission, I think that the way that I displayed information was a central piece in getting awarded. Here I want to share a little bit of the thought process behind building the data visualizations. With step by step instructions.;Apache 2.0;https://www.kaggle.com/andresionek/how-to-create-award-winning-data-visualizations;0.5;[];['ner', 'ai', 'gan', 'rl', 'nn'];['layer', 'filter', 'label', 'machine learning'];https://www.kaggle.com/c/kaggle-survey-2019;0.707;0.479;2020-12-13 12:25:10;2019 Kaggle Machine Learning & Data Science Survey;[];How to Create Award Winning Data Visualizations;Python notebook;3570.0;76;;
2019-12-03 00:12:44;Is there any job out there? Hello? Hello? Hello?  Is there any job out there? Just nod if you can hire me. Is there anyone at the HR? Come on now...  I hear you're feeling down.  Well I can ease your pain.  Get you on your feet again.  Relax. Adapted from Comfortably Numb by Pink Floyd   A comparison between Kaggle Survey and Glassdoor Jobs ListingsOn last year's survey I did an analysis to understand What Makes a Kaggler Valuable, the idea was basically to identify what makes someone earn more than USD 100k per year when working in data related jobs. One year after I have moved from Brazil to United Kingdom, from Data Science to Data Engineering, and from earning around USD 14k per year to more than halfway to the USD 100k/year. For this year survey, I thought it would be nice to compare Kagglers skills to job listings around the world. To achieve that I have scraped Glassdoor listings looking for the search terms from Question 5 (job title) of this year's survey. Then I compared the results from both survey respondents and Glassdoor Listings. Hope you find it informative and useful to plan your career!;Apache 2.0;https://www.kaggle.com/andresionek/is-there-any-job-out-there-kaggle-vs-glassdoor;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'model', 'understanding', 'layer', 'label', 'recommend'];https://www.kaggle.com/c/kaggle-survey-2019;0.735;0.5;2020-12-13 12:25:09;multiple data sources;['data visualization, jobs and career, survey analysis, +1 moreemployment'];Is there any job out there? Kaggle vs Glassdoor;Python notebook;7062.0;100;;
2019-12-03 00:17:14;What tools Kaggler use?. It is considered that Better the tools better the result. In this analysis i will take you through analysis on that, what tools Kagglers like most.;Apache 2.0;https://www.kaggle.com/dataraj/tools-and-tools;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['label'];https://www.kaggle.com/c/kaggle-survey-2019;0.616;0.458;2020-12-13 12:25:10;multiple data sources;[];Tools and Tools;Python notebook;594.0;58;;
2020-01-10 10:27:32;;Apache 2.0;https://www.kaggle.com/etsc9287/python-vs-r-the-data-science-rivalry;1.0;['xgboost', 'lightgbm', 'pytorch', 'tensorflow', 'pattern', 'keras'];['ner', 'ai', 'dl', 'gbm', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'test data', 'neural network', 'label', 'gradient boosting', 'recommend'];https://www.kaggle.com/c/kaggle-survey-2019;0.68;0.452;2020-12-13 12:25:10;2019 Kaggle Machine Learning & Data Science Survey;[];Python vs R - The Data Science Rivalry;Rmarkdown script;1982.0;54;;
2020-04-01 08:42:10;2019 Kaggle ML & DS SurveyIn this kernel, I will analyze 2019 Kaggle Survey. The survey received 19,717 responses from various professions and students as well as data scientists around the world because of Kaggle community consists of learners from all levels. I focused on data scientists within the respondents, but I also looked at state of other professions. This was the third. There was 16,715 respondents in 2017 and 23,859 respondents in 2018. I'll take a look these years also. Let's begin.;Apache 2.0;https://www.kaggle.com/fatihbilgin/2019-data-science-trends;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'generation', 'artificial intelligence', 'label'];https://www.kaggle.com/c/kaggle-survey-2019;0.744;0.535;2020-12-13 12:25:09;multiple data sources;['data visualization, survey analysis'];ðŸ“ˆ 2019 Data Science Trends ;Python notebook;8814.0;163;;
2019-12-02 20:34:31;;Apache 2.0;https://www.kaggle.com/kailex/education-languages-and-salary;1.0;['xgboost', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'gan', 'rl', 'nn', 'ml'];['filter', 'regression', 'model', 'neural network', 'label', 'logistic regression', 'gradient boosting', 'predict', 'decision tree', 'random forest', 'convolutional neural network'];https://www.kaggle.com/c/kaggle-survey-2019;0.73;0.497;2020-12-13 12:25:10;2019 Kaggle Machine Learning & Data Science Survey;['data visualization, exploratory data analysis, data cleaning'];Education, Languages and Salary;Rmarkdown script;6112.0;97;;
2019-11-14 13:46:07;"Since I'm surrounded by so many gifted and helpful people I couldn't present any conventional, generic Kaggle Notebook because everybody knows that I can't code. Besides, all works that I've already seen are great.Then I decided to answer the questions in a way that I couldn't do in the Survey.  Age, gender, country, level education, title, company's size doesn't matter, only coding. How much money spent on ML: I'm retired, just enjoying life while I can. I spent time but I gained Knowledge. Programming language: Python since I can't pretend to code with R. TPU: I barely used GPU in DL micro-course. Years using ML: Are you kidding? In fact, I do Machine Unlearning. NLP: The 1st time I read this I thought that it was about Neuro-linguist Programming and Bert was a character from Sesame street.  Cloud computing: only in my user name (wolke=cloud). Big data, I'd rather stay with just data. In fact, that are a lot of questions that I could answer:""does not apply to me"". I'm not a bot, but my inseparable team mate is. I'd like to ask: Who is the father of the Bot? He rules my world.";Apache 2.0;https://www.kaggle.com/mpwolke/we-re-all-kaggle-users;0.2;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];[];https://www.kaggle.com/c/kaggle-survey-2019;0.615;0.487;2020-12-13 12:25:10;2019 Kaggle Machine Learning & Data Science Survey;[];WeÂ´re all Kaggle users!;Python notebook;576.0;85;;
2020-03-05 06:27:59;INTRODUCTION: KAGGLE SURVEY ANALYSIS BY âš¡LIGHTNING BOY;Apache 2.0;https://www.kaggle.com/sahib12/from-2018-to-2019-how-trends-have-changed;0.5;[];['ner', 'ai', 'gan', 'rl', 'nn', 'ml'];['machine learning', 'regression', 'automated machine learnin', 'model', 'label', 'logistic regression', 'predict', 'computer vision', 'recommend', 'natural language'];https://www.kaggle.com/c/kaggle-survey-2019;0.7;0.496;2020-12-13 12:25:10;multiple data sources;['beginner, data visualization, utility script'];From 2018 to 2019 How Trends have changed .....;Python notebook;3046.0;95;;
2019-12-02 23:40:02;;Apache 2.0;https://www.kaggle.com/tkubacka/a-story-told-through-a-heatmap;1.0;['pattern'];['ner', 'ai', 'dl', 'cnn', 'gan', 'rl', 'ml', 'nn', 'rnn', 'ann'];['autoencoder', 'machine learning', 'regression', 'understanding', 'neural network', 'lstm', 'label', 'logistic regression', 'gradient boosting', 'clustering', 'recommend', 'bayesian'];https://www.kaggle.com/c/kaggle-survey-2019;0.729;0.51;2020-12-13 12:25:09;2019 Kaggle Machine Learning & Data Science Survey;[];A story told through a heatmap;Python notebook;6031.0;115;;
2019-10-12 07:30:35;Let's load the data.;Apache 2.0;https://www.kaggle.com/bustam/cnn-in-keras-for-kannada-digits;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/Kannada-MNIST;0.731;0.478;2020-12-13 12:26:21;Kannada MNIST;['gpu'];CNN in Keras for Kannada Digits;Python notebook;6257.0;75;0.98840;0.98640
2019-10-07 11:07:56;;Apache 2.0;https://www.kaggle.com/deepakdeepu8978/kannada-mnist-using-cnn-with-acc-98;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/Kannada-MNIST;0.605;0.413;2020-12-13 12:26:21;Kannada MNIST;['gpu, deep learning'];Kannada MNIST using CNN with acc 98%;Python notebook;487.0;34;0.98600;0.98540
2019-11-23 12:08:46;Preprocessing;Apache 2.0;https://www.kaggle.com/faizu07/kannada-mnist-with-fastai;1.0;['sklearn'];['ner', 'ai', 'dl', 'cnn', 'nn', 'ann'];['train', 'fitting', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/Kannada-MNIST;0.636;0.371;2020-12-13 12:26:21;multiple data sources;['gpu'];Kannada-MNIST with FASTAI;Python notebook;837.0;21;0.98060;0.97980
2020-11-26 01:37:40;FASHION CLASS CLASSIFICATION;Apache 2.0;https://www.kaggle.com/faressayah/fashion-mnist-classification-using-cnns;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'cnn', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'neuron', 'train', 'fitting', 'model', 'test data', 'neural network', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/Kannada-MNIST;0.707;0.416;2020-12-13 12:26:21;multiple data sources;['deep learning, computer vision'];Fashion MNIST Classification using CNNs;Python notebook;3581.0;35;;
2019-10-22 02:57:14;Pseudo-Labelling;Apache 2.0;https://www.kaggle.com/jinbao/kannada-mnist-baseline;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/Kannada-MNIST;0.676;0.397;2020-12-13 12:26:21;Kannada MNIST;['gpu'];Kannada-MNIST-baseline;Python notebook;1822.0;28;0.98440;0.98460
2019-12-20 05:37:30;Construct my model;Apache 2.0;https://www.kaggle.com/nnquangw/kannanda-mnist-how-i-get-to-9th-position;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'rl', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/Kannada-MNIST;0.678;0.416;2020-12-13 12:26:21;multiple data sources;['gpu'];Kannanda-MNIST - How I get to 9th position;Python notebook;1907.0;35;;
2019-12-03 16:56:59;Process the training, testing and 'other' datasets, and then check to ensure the arrays look reasonable.;Apache 2.0;https://www.kaggle.com/zinovadr/mnist-fastai-pytorch-0-98720-ps-15-mins-to-run;0.5;[];['ner', 'ai', 'nn', 'ann'];['train', 'epoch', 'layer', 'label', 'loss', 'relu'];https://www.kaggle.com/c/Kannada-MNIST;0.668;0.393;2020-12-13 12:26:21;Kannada MNIST;['gpu'];MNIST FastAI/PyTorch 0.98720 PS -  15 Mins to run;Python notebook;1568.0;27;;
2020-01-07 11:18:30;1. Business Problem;Apache 2.0;https://www.kaggle.com/shivashi11/ad-click-prediction;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['filter', 'machine learning', 'regression', 'training data', 'train', 'model', 'label', 'predict', 'rank', 'natural language'];https://www.kaggle.com/c/kddcup2012-track2;0.567;0.099;2020-12-13 12:26:28;KDD Cup 2012, Track 2;[];ad_click_prediction;Python notebook;264.0;1;;
2017-12-16 03:44:40;to reduce the size of transactions dataframe;Apache 2.0;https://www.kaggle.com/aroragaurav/1-merging-the-data-sets-and-memory-reduction;0.5;[];['ner', 'ai', 'dl', 'cv', 'nn', 'ml'];['train', 'test data', 'predict'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.676;0.268;2020-12-13 12:29:18;multiple data sources;[];1. Merging the data sets and memory reduction;Python notebook;1824.0;7;;
2017-09-19 21:26:36;This kernel explores the training set and the membership table.;Apache 2.0;https://www.kaggle.com/carrie1/exploring-membership-data-and-customer-churn;0.5;[];['ner', 'ai', 'nn'];['train'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.716;0.357;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;[];Exploring membership data and customer churn;Python notebook;4399.0;18;;
2017-11-06 19:18:33;"IntroductionThis kernel will not present new ML techniques, at least not now, but it will focus on different methods to reduce dataframe memory. Since this is my first kernel and moreover, and since english is not my mother tongue, please be indulgent. But any remarks/comments are welcome and appreciated. This notebook will described step-by-step the methods i have use to reduce dataframe memory and in particular focusing on this ""famous to be so big"" user_logs.csv. All methods presented here can be coupled with a ""transformation"" of the dataframe in an SQL DB or HDF-Storage.";Apache 2.0;https://www.kaggle.com/guiyom/user-logs-csv-reduce-memory-with-new-tips;0.5;[];['ner', 'ai', 'nn', 'ml'];['train', 'loss'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.637;0.268;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;[];User_logs.csv, reduce memory with new tips;Python notebook;863.0;7;;
2018-11-04 15:10:12;;Apache 2.0;https://www.kaggle.com/headsortails/should-i-stay-or-should-i-go-kkbox-eda;1.0;['pattern'];['ner', 'ai', 'rl', 'nn', 'ann'];['filter', 'test data', 'train', 'model', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.79;0.563;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;['beginner, data visualization, exploratory data analysis'];Should I stay or should I go? - KKBox EDA;Rmarkdown script;32775.0;247;;
2017-10-30 09:18:39;;Apache 2.0;https://www.kaggle.com/hireme/kaggle-please-do-something-lb-0-0000;1.0;['h2o'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.667;0.319;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;[]; Kaggle please do something  [LB 0.0000];Python script;1535.0;12;0.00000;0.00000
2017-10-20 06:15:31;"user logs have 392 million records!!The kaggle kernal limitations are as follows:  It has a self timeout after processing a ""cell"" for ~10 minutes Memory limit is 8 GB  Link:https://www.kaggle.com/wiki/Scripts  Hence by splitting the file into chunks less than 8 GB (I've used a chunk size of 40 mil rows), we can squeeze in the feature extraction with the timeout of 10 minutes.";Apache 2.0;https://www.kaggle.com/jagangupta/processing-huge-datasets-user-log;0.5;[];['ai'];['train'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.722;0.371;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;[];Processing huge datasets - User log ;Python notebook;5051.0;21;;
2017-11-16 19:54:18;"""Feature Engineering is an art"". Someone once said to spend more time on deriving more and meaningful features. Why did I bold meaningful? Because infomation fed to the model must make sense in whatever form it is given. Creating loads of features having no sense is of no use at all. To add more features, an added advantage would be understanding the real world situation at hand or holding sme (subject ,atter experience). On this second kernel of mine let's see how we can garner more features from the existing ones. This kernel did not run properly upon execution. So I have decided to incorporate memory reduction techniques adopted from my previous kernel mentioned below. To see my first kernel on how to reduce memory effectively SEE THIS KERNEL Loading libraries and data";Apache 2.0;https://www.kaggle.com/jeru666/did-you-think-of-these-features;0.5;[];['ner', 'ai', 'nn', 'rl'];['train', 'understanding', 'model', 'predict'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.748;0.459;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;['feature engineering'];Did you think of these features?;Python notebook;9761.0;59;;
2017-11-16 19:54:11;Here is my first kernel on KKBox. I will be playing with only three of the csv files (train.csv, members.csv and transactions.csv). the file user_logs_1.csv is too big for these kernels. What are we going to see in this kernel?  How to reduce memory consumption of dataframes? Some insights into the data at hand.  Memory ReductionThe memory consumed by each of the above mentioned files are too high. I will first reduce it.;Apache 2.0;https://www.kaggle.com/jeru666/memory-reduction-and-data-insights;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['test data', 'filter', 'train', 'model', 'predict'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.723;0.346;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;['data visualization'];Memory Reduction and Data Insights;Python notebook;5265.0;16;;
2017-09-22 00:26:19;;Apache 2.0;https://www.kaggle.com/kevinbonnes/r-churn-prediction-baseline;1.0;['xgboost', 'h2o'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.738;0.346;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;[];[R] Churn prediction baseline ;R script;7620.0;16;0.00000;0.00000
2017-09-25 01:10:56;;Apache 2.0;https://www.kaggle.com/lebroschar/regressing-during-insomnia-0-21353;1.0;['xgboost', 'sklearn'];['ai', 'rl', 'cv'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.667;0.302;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;[];Regressing During Insomnia [0.21353];Python notebook;1521.0;10;;
2017-10-15 04:19:36;;Apache 2.0;https://www.kaggle.com/loveall/make-user-log-and-transactions-features-lb-0-20;1.0;['xgboost', 'sklearn', 'h2o'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.65;0.302;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;[];make user_log and transactions features,  lb 0.20;Python script;1095.0;10;;
2017-10-01 03:18:31;;Apache 2.0;https://www.kaggle.com/randylaosat/churn-to-the-left-churn-to-the-right-kkbox;0.5;[];['ner', 'ai', 'ml'];['training data', 'filter', 'train', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.675;0.292;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;[];Churn to the Left, Churn to the Right! KKBOX!;Python notebook;1807.0;9;;
2017-09-30 07:22:43;1. Load Libraies and check input files;Apache 2.0;https://www.kaggle.com/rastaman/churn-or-no-churn-exploration-data-analysis;0.5;[];['dl', 'ai', 'nn', 'rl'];['filter', 'training data', 'train', 'label', 'predict', 'classification'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.762;0.45;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;['beginner, data visualization, exploratory data analysis'];Churn or No Churn - Exploration Data Analysis;Python notebook;14457.0;53;;
2017-11-17 17:55:09;;Apache 2.0;https://www.kaggle.com/stravinsky/exploring-the-time-series-data-user-logs-split;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nn'];['train', 'label', 'filter', 'predict'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.682;0.281;2020-12-13 12:29:18;multiple data sources;[];Exploring the Time Series Data  + user_logs Split;Python notebook;2096.0;8;;
2017-12-03 06:18:04;Import library;Apache 2.0;https://www.kaggle.com/sudhirnl7/simple-logistic-regression-wisdom;1.0;['sklearn'];['ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['regression', 'train', 'model', 'label', 'logistic regression', 'predict'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.695;0.302;2020-12-13 12:29:18;WSDM - KKBox's Churn Prediction Challenge;['exploratory data analysis, logistic regression'];Simple logistic regression - Wisdom;Python notebook;2758.0;10;0.61049;0.61062
2018-04-29 13:12:31;;Apache 2.0;https://www.kaggle.com/tackytachie/ml-project-kkbox-churn-or-not-challenge;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['machine learning', 'training data', 'regression', 'test data', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/kkbox-churn-prediction-challenge;0.685;0.268;2020-12-13 12:29:18;multiple data sources;[];ML PROJECT - KKBOX CHURN OR NOT CHALLENGE;Python notebook;2226.0;7;;
2017-10-28 21:52:12;;Apache 2.0;https://www.kaggle.com/adiamaan/eda-and-feature-engineering;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.718;0.383;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];EDA and Feature Engineering;Rmarkdown script;4571.0;24;;
2017-10-08 17:27:43;This kernel provides an overview of the KKBox  competition dataset  with some useful data transformations.;Apache 2.0;https://www.kaggle.com/asindico/kkbox-music-the-essential-kickstart;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['train', 'predict'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.711;0.357;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];KKBox Music - The Essential Kickstart;Python notebook;3926.0;18;;
2017-11-12 19:10:11;;Apache 2.0;https://www.kaggle.com/asmitavikas/feature-engineered-0-68310;1.0;['lightgbm'];['ai', 'nn', 'gbm'];['filter', 'predict', 'train', 'model', 'loss'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.757;0.473;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];Feature Engineered : 0.68310;Python notebook;12362.0;71;;
2017-12-16 09:18:07;Song and Member Merge;Apache 2.0;https://www.kaggle.com/freshwater/basic-of-lgbm;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nn', 'ml'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.675;0.302;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];Basic Of LGBM;Python notebook;1779.0;10;0.65878;0.65663
2017-11-15 18:10:41;What I intend to do?In this kernel, my sole intention is to create a highly feature engineered dataframe after merging existing files, which can be fed directly as input for modeling. By this I mean dataframes for both the train set and test set which would contain same number of columns with encoded features. At a glance I will be looking into the following issues:  Merging Dataframes          -> (completed) Handling Missing Values   -> (completed) Came across something interesting where number of unique values in column 'source_screen_name' were different for both test and train set. Columns like 'genre_ids' have a combination of more than one genre which must be handled appropriately.   Feature Engineering          -> (in progress) number of genres per song number of lyricists per song number of composers per song whether song has features artists number of artists per song whether artist and composer are the same whether artist, composer and lyricist are all the same    (thinking of more features);Apache 2.0;https://www.kaggle.com/jeru666/interesting-kkbox-insights-new-features;0.5;[];['ner', 'ai', 'nlu', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['test data', 'generation', 'train', 'model', 'layer', 'loss', 'label', 'predict', 'rank'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.701;0.34;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;['feature engineering'];Interesting KKBox Insights && New Features;Python notebook;3096.0;15;;
2017-10-29 17:11:33;IRSC Data;Apache 2.0;https://www.kaggle.com/joshwilkins2013/examining-irsc-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['train', 'label', 'test data'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.623;0.319;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];Examining IRSC Data;Python notebook;667.0;12;;
2017-10-23 01:24:05;1. IntroductionThis will be the longest EDA you've ever seen... Let's load some libraries and the data.;Apache 2.0;https://www.kaggle.com/kamilkk/i-have-to-say-this;0.5;[];['ai', 'ml', 'gan'];['train', 'test data', 'training data'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.757;0.499;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];I have to say this...;Python notebook;12395.0;99;;
2017-10-23 01:18:51;;Apache 2.0;https://www.kaggle.com/kamilkk/simple-fast-lgbm-0-6685;1.0;['lightgbm'];['ner', 'ai', 'dl', 'gbm', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.757;0.477;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];Simple & fast LGBM [0.6685];Python script;12362.0;74;;
2017-10-15 10:05:34;This notebook contains mostly exploration of the songs themselves, using the train data only to get the total play count and target variables. I'll do some data exploration of the users later, in a separate notebook. (I apologize in advance for some inefficient code, repeated groupby's, etc., - I didn't really plan out the exploration beforehand, so there are some repeated pieces.);Apache 2.0;https://www.kaggle.com/kunstmord/exploring-the-songs;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'training data', 'label', 'filter'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.738;0.456;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];Exploring the songs;Python notebook;7635.0;57;;
2017-10-03 08:59:37;;Apache 2.0;https://www.kaggle.com/lystdo/beat-kkbox-benchmark-without-using-metadata-0-62;1.0;['xgboost', 'sklearn', 'tensorflow', 'pattern', 'keras'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['filter', 'regression', 'train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'recommend', 'classification'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.718;0.413;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];Beat KKBox Benchmark without using metadata [0.62];Python script;4673.0;34;;
2017-10-24 21:42:44;;Apache 2.0;https://www.kaggle.com/mrooijer/how-to-ensemble-in-r;1.0;['tensorflow', 'xgboost', 'keras'];['ai', 'ml'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'random forest'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.705;0.311;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];How to ensemble (in R);Rmarkdown script;3400.0;11;0.64756;0.64362
2017-09-29 02:29:39;WSDM - KKBox's Music Recommendation Challenge;Apache 2.0;https://www.kaggle.com/priyaananthram/eda-of-music-recommendation-system;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ml'];['train', 'recommend', 'label'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.719;0.311;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];EDA of music recommendation system;Python notebook;4779.0;11;;
2017-11-13 04:16:02;Common user ids in both training and test sets;Apache 2.0;https://www.kaggle.com/sidshady/basic-data-analysis-and-exploration;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['train', 'label', 'training data'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.685;0.327;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];Basic Data analysis and exploration;Python notebook;2203.0;13;;
2017-11-09 01:16:06;;Apache 2.0;https://www.kaggle.com/warpri81/kkbox-cf-svd;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'deep learning', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/kkbox-music-recommendation-challenge;0.682;0.327;2020-12-13 12:31:41;WSDM - KKBox's Music Recommendation Challenge;[];KKBox - CF (SVD);Python script;2069.0;13;;
2016-04-20 18:17:22;;Apache 2.0;https://www.kaggle.com/anokas/implementing-xgboost;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'loss', 'classification'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.683;0.292;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];Implementing XGBoost;Python script;2125.0;9;;
2016-04-29 01:27:24;;Apache 2.0;https://www.kaggle.com/apapiu/exploring-kobe-s-shots;0.5;[];['ai', 'rl'];['train', 'filter'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.769;0.477;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];Exploring Kobe's Shots;Rmarkdown script;17605.0;74;;
2016-04-18 02:05:10;;Apache 2.0;https://www.kaggle.com/bbx396/kobechart;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['train', 'label'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.694;0.357;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];KOBECHART;Python notebook;2655.0;18;;
2016-05-25 04:12:26;;Apache 2.0;https://www.kaggle.com/brandao/winner-script;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['model', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.712;0.281;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];test #8;Python script;3978.0;8;0.60168;0.60168
2016-05-30 22:39:58;;Apache 2.0;https://www.kaggle.com/brandao/xgboost-in-r-kobe-bryant-benchmark;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.726;0.281;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];XGBoost in R - Kobe Bryant - benchmark;R script;5588.0;8;0.60736;0.60736
2016-10-15 15:35:52;The inevitable aging of a starKobe Bryant has been one of the most dominant players in the NBA through out the last two decades, arguably the best since the (2nd) retirement of Michael Jordan in 1998.  However, in this script I will claim that as a human being and despite his physical superiority, Bryant is not immune to the effect of aging. I will show that as the years go by, Kobe has increasingly preferred the long distance shot rather than the lay-up, as his amazing athleticism and speed slowly degrade. While this might simply imply an improvement of his style, and maybe in the overall NBA playing style, the fact that his overall field goals went down shows that this was a restriction that was imposed upon him by the inevitable advancement of his biological clock rather than a positive evolution of him as a player.;Apache 2.0;https://www.kaggle.com/drgilermo/the-inevitable-aging-of-a-star;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['layer', 'label'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.672;0.302;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];The inevitable aging of a star  ;Python notebook;1685.0;10;;
2020-07-28 23:32:58;Purpose;Apache 2.0;https://www.kaggle.com/matt4byu/kobe-bryant-shot-selection-analysis-with-xgboost;1.0;['pattern', 'xgboost'];['ai', 'rl', 'dl', 'cv'];['filter', 'training data', 'train', 'model', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.585;0.327;2020-12-13 12:34:34;Kobe Bryant Shot Selection;['classification, feature engineering, xgboost, +1 morebasketball'];Kobe Bryant Shot Selection Analysis with XGBoost;R notebook;352.0;13;0.60124;0.60124
2017-08-31 18:24:13;;Apache 2.0;https://www.kaggle.com/mayu0116/my-second-exploratory-data-analysis;0.5;[];['ner', 'ai', 'nn'];['train', 'filter'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.64;0.268;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];My Second Exploratory Data Analysis;Rmarkdown script;899.0;7;;
2020-01-28 16:26:04;Image jumperbrasil.lance.com.br  - We'll gonna miss you Kobe;Apache 2.0;https://www.kaggle.com/mpwolke/kobe-bryant-dear-basketball;0.5;[];['ner', 'ai', 'dl', 'cnn', 'nlg', 'cv', 'rl', 'ml', 'nn', 'rnn', 'ann'];['rank', 'layer'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.637;0.439;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];Kobe Bryant - Dear Basketball ;Python notebook;857.0;46;;
2016-05-15 10:48:45;IntroI'm just a beginner who started using pandas one week ago and now trying to do some basic data analysis for the first time. I would very much like to hear your feedback, comments and improvements to my code.;Apache 2.0;https://www.kaggle.com/narimiran/absolute-beginner;0.5;[];['ner', 'ai', 'nn'];['model'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.72;0.311;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];Absolute beginner;Python notebook;4801.0;11;;
2016-04-16 22:55:59;;Apache 2.0;https://www.kaggle.com/ryanburge/shooting-percentage-by-team-and-season;0.2;[];['ai', 'rl'];[];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.68;0.311;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];Shooting Percentage By Team and Season;Rmarkdown script;1974.0;11;;
2016-05-02 02:56:10;;Apache 2.0;https://www.kaggle.com/unbingo/bubble-chart-animated-gif;0.5;[];['ner', 'ai', 'nn', 'rl'];['layer', 'label'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.689;0.302;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];bubble chart animated GIF;Rmarkdown script;2383.0;10;;
2016-04-15 16:03:47;;Apache 2.0;https://www.kaggle.com/wcukierski/shot-location-misses-vs-makes;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['classification', 'filter', 'deep learning'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.71;0.327;2020-12-13 12:34:34;Kobe Bryant Shot Selection;[];Shot Location - Misses vs Makes;R script;3864.0;13;;
2019-09-16 08:19:20;;Apache 2.0;https://www.kaggle.com/xvivancos/kobe-bryant-shot-selection;0.5;[];['dl', 'ner', 'ai', 'nn'];['rank', 'generation', 'filter', 'layer'];https://www.kaggle.com/c/kobe-bryant-shot-selection;0.761;0.518;2020-12-13 12:34:34;Kobe Bryant Shot Selection;['beginner, data visualization, sports, +1 morebasketball'];Kobe Bryant Shot Selection;Rmarkdown script;13850.0;129;;
2019-07-19 02:11:04;;Apache 2.0;https://www.kaggle.com/anokas/kuzushiji-modified-f-1-score;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'recognition', 'model', 'deep learning', 'label', 'predict', 'classification', 'ground truth'];https://www.kaggle.com/c/kuzushiji-recognition;0.678;0.4;2020-12-13 12:36:30;Kuzushiji Recognition;[];Kuzushiji F-1 Score ;Python script;1906.0;29;;
2019-07-18 18:42:09;First, in order to visualise the dataset, we need a font that can display the full range of Japanese characters. We're using Noto Sans, an open source font by Google which can display very almost all the characters used within this competition.;Apache 2.0;https://www.kaggle.com/anokas/kuzushiji-visualisation;0.5;[];['ai', 'nn', 'ann'];['training data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/kuzushiji-recognition;0.765;0.56;2020-12-13 12:36:29;Kuzushiji Recognition;['beginner, data visualization, exploratory data analysis'];Kuzushiji Visualisation;Python notebook;15689.0;238;;
2019-07-23 18:36:34;Applying Ben's PreprocessingSo in this kernel I will try to apply the Ben's preprocessing that is popular in APTOS competition. This preprocessing method dated back to previous diabetic retinopathy competitions (Ben Graham is the competition winner). According to the previous diabetic retinopathy winner the reasoning behind doing this preprocessing (points on the bracket is why I believe this might work for this competition) :  Enhance finer details (Enhance thin strokes) Tackle different illumination problem. (Background color differences especially between B/W and Color scans);Apache 2.0;https://www.kaggle.com/banzaibanzer/applying-ben-s-preprocessing;0.5;[];['ner', 'ai', 'nn', 'cv'];['train', 'model'];https://www.kaggle.com/c/kuzushiji-recognition;0.652;0.371;2020-12-13 12:36:30;Kuzushiji Recognition;[];Applying Ben's Preprocessing;Python notebook;1143.0;21;;
2019-09-27 11:34:28;To explore and analyze all the characters in the Kuzushiji Comeptition, you would have to cut out each character. I wanted to save you some time, so I created this notebook and found a pretty fast way to cut out each char. To save you even some more time I saved the computed results of this notebooks in this dataset. If you want to analyze the Kuzushiji Characters, simple use the dataset.;Apache 2.0;https://www.kaggle.com/christianwallenwein/fastest-way-to-crop-all-images;0.5;[];['ai', 'cv'];['train', 'label', 'filter'];https://www.kaggle.com/c/kuzushiji-recognition;0.66;0.334;2020-12-13 12:36:30;Kuzushiji Recognition;['gpu'];Fastest way to crop all images;Python notebook;1321.0;14;;
2019-09-25 22:50:53;Kuzushiji Page GeneratorThis notebook introduces a new way to automatically generate pages filled with Kuzushiji symbols using the great KMNIST dataset. The aim is to help pretraining models learning both character detection and classification at the same time before moving to the pages from the original competition dataset. Of course, the resulting pages would not make any sense!I have generated 20,000 pages using this script and made them available in the following dataset.If you find this notebook useful, please feel free to give it an upvote!;Apache 2.0;https://www.kaggle.com/frlemarchand/kuzushiji-page-generator;0.5;[];['ner', 'ai', 'ml', 'cv'];['generation', 'train', 'model', 'label', 'classification'];https://www.kaggle.com/c/kuzushiji-recognition;0.597;0.292;2020-12-13 12:36:30;multiple data sources;[];Kuzushiji Page Generator;Python notebook;427.0;9;;
2019-08-01 23:25:14;Inspired by Applying Ben's PreprocessingMy Own Insight: Denoising after Ben's Preprocssing Leads To Better Visual ClarityApplying Ben's Preprocessing introduces noises(I assume gaussian?) to images at a certain level and thus denoising can help reduce such negative effect. (Even some characters inked from back of the page becomes more recognizable). However, the cons:  The denoising takes WAY TOO LONG. On average, each image requires about 21-23 seconds to be denoised.  The denoising algorithm in nature cause slight loss of information. However, in my opinion, the loss of information it cause is acceptable compared to its benefits.;Apache 2.0;https://www.kaggle.com/hanmingliu/denoising-ben-s-preprocessing-better-clarity;0.5;[];['ai', 'cv'];['train', 'recommend', 'model', 'loss'];https://www.kaggle.com/c/kuzushiji-recognition;0.645;0.34;2020-12-13 12:36:30;Kuzushiji Recognition;['gpu, data cleaning, computer vision'];Denoising + Ben's Preprocessing = Better Clarity;Python notebook;996.0;15;;
2019-08-06 00:50:35;Introduction;Apache 2.0;https://www.kaggle.com/kmat2019/centernet-keypoint-detector;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ann'];['train', 'model', 'output layer', 'epoch', 'validation data', 'layer', 'loss', 'label', 'predict', 'relu', 'recommend', 'resnet', 'classification', 'u-net'];https://www.kaggle.com/c/kuzushiji-recognition;0.77;0.547;2020-12-13 12:36:29;Kuzushiji Recognition;['gpu, beginner'];CenterNet -Keypoint Detector-;Python notebook;17922.0;196;0.564;0.561
2019-07-29 22:37:05;;Apache 2.0;https://www.kaggle.com/marvin42/clean-images;1.0;['skimage'];['ai', 'nn', 'cv'];['train', 'label'];https://www.kaggle.com/c/kuzushiji-recognition;0.626;0.346;2020-12-13 12:36:30;Kuzushiji Recognition;['data cleaning'];Clean images;Python notebook;699.0;16;;
2019-08-24 14:40:21;order charactors by clustering columns;Apache 2.0;https://www.kaggle.com/rio114/order-charactors-by-clustering-columns;1.0;['sklearn'];['ner', 'ai', 'ml', 'cv'];['object detection', 'predict', 'train', 'label', 'clustering'];https://www.kaggle.com/c/kuzushiji-recognition;0.63;0.375;2020-12-13 12:36:30;Kuzushiji Recognition;[]; order charactors by clustering columns ;Python notebook;754.0;22;;
2019-07-27 04:08:38;Check for NAN;Apache 2.0;https://www.kaggle.com/rupeshs/eda-starter;0.5;[];['ai', 'cv'];['train', 'label'];https://www.kaggle.com/c/kuzushiji-recognition;0.646;0.302;2020-12-13 12:36:30;Kuzushiji Recognition;['beginner, data visualization, exploratory data analysis'];EDA starter;Python notebook;1024.0;10;;
2019-07-19 22:40:50;The string should be read as space separated series of values where Unicode character, X, Y, Width, and Height are repeated as many times as necessary.;Apache 2.0;https://www.kaggle.com/wakamezake/kuzushiji-pytorch-data-preprocessing;0.5;[];['ai', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/kuzushiji-recognition;0.704;0.429;2020-12-13 12:36:30;Kuzushiji Recognition;['beginner'];Kuzushiji_PyTorch_Data_preprocessing;Python notebook;3352.0;41;;
2019-05-11 19:09:47;Landmark's qty distribution;Apache 2.0;https://www.kaggle.com/akashintsev/train-data-landmarks-qty-distribution;0.5;[];['ai'];['train'];https://www.kaggle.com/c/landmark-recognition-2019;0.565;0.152;2020-12-13 12:41:50;Google Landmark Recognition 2019;[];Train data landmarks qty distribution;Python notebook;255.0;2;;
2019-05-12 06:10:31;introduce;Apache 2.0;https://www.kaggle.com/anhnamxtanh/download-resize-dataset-by-google-colab;0.7;['pillow'];['dl', 'nn', 'cv'];[];https://www.kaggle.com/c/landmark-recognition-2019;0.659;0.152;2020-12-13 12:41:50;Google Landmark Recognition 2019;[];Download, resize dataset by Google Colab;Python notebook;1295.0;2;;
2019-04-17 16:33:38;;Apache 2.0;https://www.kaggle.com/anisayari/download-images-dataset-py3-log-progressbar;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-recognition-2019;0.665;0.352;2020-12-13 12:41:49;Google Landmark Recognition 2019;[]; Download images dataset py3+log+progressbar;Python script;1474.0;17;;
2019-04-09 07:24:14;;Apache 2.0;https://www.kaggle.com/appian/landmark-async-downloader-with-detailed-progress;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['train', 'recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-recognition-2019;0.65;0.188;2020-12-13 12:41:50;Google Landmark Recognition 2019;[];Landmark Async downloader with detailed progress;Python script;1101.0;3;;
2019-04-12 18:18:22;;Apache 2.0;https://www.kaggle.com/archaeocharlie/dl-and-untar-training-data-with-progress-bars;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['training data', 'train', 'recognition', 'deep learning', 'classification'];https://www.kaggle.com/c/landmark-recognition-2019;0.671;0.268;2020-12-13 12:41:50;Google Landmark Recognition 2019;[];DL and Untar Training Data with Progress Bars;Python script;1670.0;7;;
2019-04-09 19:27:38;;Apache 2.0;https://www.kaggle.com/arteam/extract-a-tar-file;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-recognition-2019;0.703;0.281;2020-12-13 12:41:50;Google Landmark Recognition 2019;[];Extract a .tar file;Python script;3298.0;8;;
2019-05-15 01:45:53;;Apache 2.0;https://www.kaggle.com/artyomp/resnet50-baseline;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ann'];['filter', 'recognition', 'predict', 'ground truth', 'training data', 'train', 'epoch', 'recommend', 'classification', 'labeled', 'model', 'neural network', 'loss', 'resnet', 'test data', 'generation', 'fitting', 'deep learning', 'label', 'computer vision'];https://www.kaggle.com/c/landmark-recognition-2019;0.758;0.498;2020-12-13 12:41:49;multiple data sources;['gpu'];ResNet50 baseline;Python script;12718.0;98;0.00000;0.00000
2019-04-09 19:05:59;;Apache 2.0;https://www.kaggle.com/deepak525/dataset-downloader;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-recognition-2019;0.664;0.236;2020-12-13 12:41:50;Google Landmark Recognition 2019;[];Dataset Downloader;Python script;1442.0;5;;
2019-08-16 04:09:48;;Apache 2.0;https://www.kaggle.com/hli2020/resnet50-baseline-julyonline;1.0;['pytorch', 'tensorflow', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'generation', 'train', 'recognition', 'model', 'epoch', 'deep learning', 'loss', 'label', 'predict', 'computer vision', 'resnet', 'classification', 'labeled', 'ground truth'];https://www.kaggle.com/c/landmark-recognition-2019;0.602;0.099;2020-12-13 12:41:50;multiple data sources;['gpu'];ResNet50 baseline;Python script;465.0;1;;
2019-06-14 08:43:03;Google Landmarks Dataset v2 Exploratory Data Analysis(EDA);Apache 2.0;https://www.kaggle.com/huangxiaoquan/google-landmarks-v2-exploratory-data-analysis-eda;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['training data', 'test data', 'train', 'recognition', 'label'];https://www.kaggle.com/c/landmark-recognition-2019;0.672;0.292;2020-12-13 12:41:50;multiple data sources;['beginner, data visualization, exploratory data analysis, +1 moreimage data'];Google Landmarks v2 Exploratory Data Analysis(EDA);Python notebook;1671.0;9;;
2019-04-25 23:27:30;;Apache 2.0;https://www.kaggle.com/paulorzp/train-set-landmark-exploratory-analysis;0.5;[];['ai', 'dl', 'ml', 'rl'];['train', 'model', 'label'];https://www.kaggle.com/c/landmark-recognition-2019;0.69;0.34;2020-12-13 12:41:49;Google Landmark Recognition 2019;[];Train Set Landmark Exploratory Analysis;Python notebook;2462.0;15;;
2019-05-22 13:36:44;Download the tar dataset on kaggle disk;Apache 2.0;https://www.kaggle.com/rparashar/download-the-tar-dataset-directly-on-kaggle-disk;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml'];['train'];https://www.kaggle.com/c/landmark-recognition-2019;0.586;0.214;2020-12-13 12:41:50;Google Landmark Recognition 2019;['gpu'];Download the tar dataset directly on kaggle disk;Python notebook;354.0;4;;
2019-05-14 11:16:35;The notebook executes: download tar and text md5 files check md5 match unarchive into folder move all images to root folder resize images into train folder remove archive, folder with unarchived images, text md5 file resulting size on disk ~44GB;Apache 2.0;https://www.kaggle.com/sermakarevich/download-resize-clean-12-hours-44gb;0.5;[];['ai', 'dl', 'rl'];['train'];https://www.kaggle.com/c/landmark-recognition-2019;0.682;0.346;2020-12-13 12:41:49;Google Landmark Recognition 2019;[];Download/resize/clean >> 12 hours, 44GB;Python notebook;2070.0;16;;
2019-04-09 17:07:25;;Apache 2.0;https://www.kaggle.com/surajdidwania/dataset-download;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['test data', 'train', 'recognition', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/landmark-recognition-2019;0.747;0.253;2020-12-13 12:41:50;Google Landmark Recognition 2019;[];Dataset Download ;Python script;9586.0;6;;
2019-04-08 17:49:40;Note: requires the tqdm package (pip install tqdm) Note to Kagglers: This script will not run directly in Kaggle kernels. You need to download it and run it on your local machine. Downloads images from the Google Landmarks dataset using multiple threads. Images that already exist will not be downloaded again, so the script can resume a partially completed download. All images will be saved in the JPG format with 90% compression quality.;Apache 2.0;https://www.kaggle.com/xiuchengwang/python-dataset-downloader;0.2;[];['ai', 'rl'];[];https://www.kaggle.com/c/landmark-recognition-2019;0.655;0.188;2020-12-13 12:41:50;Google Landmark Recognition 2019;[];Python Dataset Downloader;Python notebook;1196.0;3;;
2019-04-10 07:54:00;;Apache 2.0;https://www.kaggle.com/yaxi1031/merge-500-downloaded-train-folders-into-one-folder;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-recognition-2019;0.642;0.188;2020-12-13 12:41:50;Google Landmark Recognition 2019;[];Merge 500 downloaded train folders into one folder;Python script;937.0;3;;
2020-09-01 07:10:16;Libraries Required;Apache 2.0;https://www.kaggle.com/anshuls235/google-landmark-recognition-eda;0.5;[];['ai', 'nn', 'ann', 'cv'];['filter', 'train', 'recognition', 'layer', 'label'];https://www.kaggle.com/c/landmark-recognition-2020;0.661;0.411;2020-12-13 12:43:16;Google Landmark Recognition 2020;['beginner, data visualization, exploratory data analysis, +2 moredeep learning, classification'];Google Landmark Recognition EDA;Python notebook;1354.0;33;;
2020-08-06 22:51:08;;Apache 2.0;https://www.kaggle.com/camaskew/host-baseline-example;1.0;['tensorflow'];['ner', 'ai', 'nn'];['train', 'recognition', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/landmark-recognition-2020;0.765;0.502;2020-12-13 12:43:16;multiple data sources;[];Host Baseline Example;Python script;15511.0;103;0.0000;0.0000
2020-08-27 05:31:15;;Apache 2.0;https://www.kaggle.com/chandanverma/baseline-landmark-recognition-0-4832;1.0;['tensorflow'];['ner', 'ai', 'nn'];['train', 'recognition', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/landmark-recognition-2020;0.7;0.4;2020-12-13 12:43:16;multiple data sources;[];Baseline Landmark Recognition 0.4832;Python script;3082.0;29;0.0000;0.0000
2020-09-11 05:54:18;;Apache 2.0;https://www.kaggle.com/chirag9073/landmark-recognition-exploratory-data-analysis;0.5;[];['ai', 'cv'];['training data', 'filter', 'train', 'recognition', 'label'];https://www.kaggle.com/c/landmark-recognition-2020;0.695;0.458;2020-12-13 12:43:16;Google Landmark Recognition 2020;[];Landmark Recognition Exploratory Data Analysis;Python notebook;2759.0;58;;
2020-09-20 09:52:09;Thank you for upvoding and comment.I got a comment that the flow is easy to understand, so I updated not only the Japanese ver but also the English ver.;Apache 2.0;https://www.kaggle.com/chumajin/eda-for-biginner-updated-to-english-ver;0.5;[];['ai', 'nn', 'ann', 'cv'];['train', 'recognition', 'label', 'test data'];https://www.kaggle.com/c/landmark-recognition-2020;0.638;0.397;2020-12-13 12:43:16;Google Landmark Recognition 2020;[];æ—¥æœ¬èªž EDA for biginner (updated to English ver);Python notebook;881.0;28;;
2020-09-22 07:49:02;Google Landmark Recognition 2020;Apache 2.0;https://www.kaggle.com/jagdmir/google-landmark-prediction-2020;1.0;['tensorflow'];['ner', 'ai', 'gan', 'cv', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'recognition', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/landmark-recognition-2020;0.682;0.453;2020-12-13 12:43:16;multiple data sources;['gpu'];Google Landmark Prediction - 2020;Python notebook;2088.0;55;;
2020-08-31 21:59:58;;Apache 2.0;https://www.kaggle.com/mohammedessam97/organizer-s-code-submission;1.0;['tensorflow'];['ner', 'ai', 'nn'];['train', 'recognition', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/landmark-recognition-2020;0.717;0.429;2020-12-13 12:43:16;multiple data sources;[];Organizer's Code Submission;Python notebook;4469.0;41;0.4599;0.4842
2020-08-22 15:50:18;Import libraries;Apache 2.0;https://www.kaggle.com/namanj27/eda-google-landmark-recognition-2020;0.5;[];['ai', 'rl', 'cv'];['training data', 'filter', 'train', 'recognition', 'label'];https://www.kaggle.com/c/landmark-recognition-2020;0.668;0.411;2020-12-13 12:43:16;Google Landmark Recognition 2020;[];[EDA] : Google Landmark Recognition 2020;Python notebook;1570.0;33;;
2020-09-02 21:32:13;;Apache 2.0;https://www.kaggle.com/paulorzp/baseline-landmark-recognition-lb-0-48;1.0;['tensorflow'];['ner', 'ai', 'nn'];['train', 'recognition', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/landmark-recognition-2020;0.75;0.498;2020-12-13 12:43:16;multiple data sources;[];Baseline Landmark Recognition LB>0.48;Python script;10317.0;98;0.4596;0.4830
2020-08-06 21:47:53;1. IntroductionGoogle Landmark Recognition is an image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.;Apache 2.0;https://www.kaggle.com/rohitsingh9990/glr-eda-all-you-need-to-know;0.5;[];['ner', 'ai', 'dl', 'cv'];['image classification', 'filter', 'train', 'recognition', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/landmark-recognition-2020;0.686;0.472;2020-12-13 12:43:16;Google Landmark Recognition 2020;[];GLR - EDA(all you need to know);Python notebook;2256.0;70;;
2020-07-30 17:04:09;;Apache 2.0;https://www.kaggle.com/socathie/pre-trained-mobilenetv2-1000-classes-1-epoch;1.0;['keras'];['ner', 'ai', 'nn'];['train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/landmark-recognition-2020;0.675;0.427;2020-12-13 12:43:16;multiple data sources;['gpu, beginner, classification, +1 moreimage data'];Pre-trained MobileNetV2 (1000 classes, 1 epoch);Python notebook;1777.0;40;0.0000;0.0000
2020-09-22 18:07:35;;Apache 2.0;https://www.kaggle.com/abhishektyagi001/landmark-recognition-challenge;0.5;[];['ai', 'dl', 'rl', 'nn', 'ml'];['training data', 'test data', 'train', 'recognition', 'label'];https://www.kaggle.com/c/landmark-recognition-challenge;0.54;0.302;2020-12-13 12:46:43;Google Landmark Recognition Challenge;['regression, government, real estate, +2 morelanguages, research'];Landmark-Recognition-Challenge;Python notebook;175.0;10;;
2018-04-17 12:15:39;;Apache 2.0;https://www.kaggle.com/abinesh100/easy-download-images-in-25-lines-py3;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ann'];['train', 'recognition', 'deep learning', 'computer vision', 'classification'];https://www.kaggle.com/c/landmark-recognition-challenge;0.718;0.319;2020-12-13 12:46:43;Google Landmark Recognition Challenge;['beginner, classification, image data, +1 morecomputer vision'];Easy download images in 25 lines Py3;Python script;4669.0;12;;
2018-04-16 21:23:10;;Apache 2.0;https://www.kaggle.com/anokas/python3-dataset-downloader-with-progress-bar;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['train', 'recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-recognition-challenge;0.755;0.47;2020-12-13 12:46:43;Google Landmark Recognition Challenge;[];Python3 Dataset Downloader with progress bar;Python script;11692.0;68;;
2018-05-05 06:33:25;;Apache 2.0;https://www.kaggle.com/codename007/a-very-extensive-landmark-exploratory-analysis;0.5;[];['ai', 'dl', 'ml', 'rl'];['training data', 'test data', 'train', 'recognition', 'label'];https://www.kaggle.com/c/landmark-recognition-challenge;0.777;0.542;2020-12-13 12:46:43;Google Landmark Recognition Challenge;['beginner, data visualization, exploratory data analysis, +1 moreimage data'];A Very Extensive Landmark  Exploratory Analysis;Python notebook;22075.0;181;;
2018-03-18 05:15:34;GAP metricThe metric for this competition, global average precision, is not one of the standard evaluation metrics in, for example, scikit-learn. Here is the code that I am using to compute it. Feel free to use it if you like. And if you see any errors, please comment, and we'll get them fixed up;Apache 2.0;https://www.kaggle.com/davidthaler/gap-metric;0.5;[];['ner'];['recognition', 'label', 'predict', 'rank', 'ground truth'];https://www.kaggle.com/c/landmark-recognition-challenge;0.709;0.435;2020-12-13 12:46:43;Google Landmark Recognition Challenge;[];GAP metric;Python notebook;3768.0;44;;
2018-05-10 00:21:48;Landmark dataset URL analysis;Apache 2.0;https://www.kaggle.com/emilmelnikov/landmark-dataset-url-analysis;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nn'];['rank', 'recognition', 'filter', 'train'];https://www.kaggle.com/c/landmark-recognition-challenge;0.653;0.214;2020-12-13 12:46:43;Google Landmark Recognition Challenge;[];Landmark dataset URL analysis;Python notebook;1148.0;4;;
2018-09-07 22:56:50;Google Landmark Recogn. Challenge Data Exploration Content  Introduction Load packages Read the data Inspect the data Image paths Image thumbnails Extracting Exif data and GPS data Baseline submission References;Apache 2.0;https://www.kaggle.com/gpreda/google-landmark-recogn-challenge-data-exploration;0.5;[];['ai', 'ml', 'rl'];['training data', 'test data', 'train', 'recognition', 'model', 'label'];https://www.kaggle.com/c/landmark-recognition-challenge;0.745;0.449;2020-12-13 12:46:43;Google Landmark Recognition Challenge;['beginner, exploratory data analysis, image data, +1 morecomputer vision'];Google Landmark Recogn. Challenge Data Exploration;Python notebook;9111.0;52;;
2018-02-03 03:28:11;;Apache 2.0;https://www.kaggle.com/maxwell110/python3-version-image-downloader;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['train', 'recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-recognition-challenge;0.738;0.352;2020-12-13 12:46:43;Google Landmark Recognition Challenge;[];Python3 version --- Image Downloader ---;Python script;7550.0;17;;
2018-02-12 03:33:37;Simple exploration of the Google Landmark Recognition Dataset;Apache 2.0;https://www.kaggle.com/mxdbld/simple-exploration-of-google-recognition;0.5;[];['ai', 'rl', 'ml', 'gan'];['train', 'recognition', 'loss'];https://www.kaggle.com/c/landmark-recognition-challenge;0.696;0.319;2020-12-13 12:46:43;Google Landmark Recognition Challenge;[];Simple exploration of Google Recognition;Python notebook;2781.0;12;;
2018-04-03 09:17:05;;Apache 2.0;https://www.kaggle.com/plarmuseau/open-dataset-theano-tensor-first-image;1.0;['theano'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'activation function', 'filter', 'layer'];https://www.kaggle.com/c/landmark-recognition-challenge;0.613;0.214;2020-12-13 12:46:43;Google Landmark Recognition Challenge;[];Open dataset - theano tensor first image;Python notebook;563.0;4;;
2018-02-03 01:26:29;;Apache 2.0;https://www.kaggle.com/tobwey/landmark-recognition-challenge-image-downloader;0.5;[];['ner', 'ai', 'nlu', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['train', 'recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-recognition-challenge;0.76;0.459;2020-12-13 12:46:43;Google Landmark Recognition Challenge;[];Landmark Recognition Challenge Image Downloader;Python script;13651.0;59;;
2018-05-29 22:57:15;Missing Exif DataThe competition organizers gave us permission to use image Exif data and other metadata to identify landmarks. If you used one of the published image downloader kernels to download your images, then you are probably missing out on this data.  The potentially useful meta data is dropped in the image download and conversion process.  You may want to redownload the images (~ 460GB) with the script shown at the end of this kernel if you want to use these nuggets.;Apache 2.0;https://www.kaggle.com/wesamelshamy/extract-geographical-info-in-image-description;0.5;[];['ner', 'ai', 'rl', 'gan'];['recognition', 'filter', 'test data'];https://www.kaggle.com/c/landmark-recognition-challenge;0.646;0.281;2020-12-13 12:46:43;multiple data sources;[];Extract Geographical Info in Image Description;Python notebook;1012.0;8;;
2018-06-11 23:57:46;www.codeproject.com/KB/recipes/619039/SIFT.JPG  Cathedral of St. John the Baptist in Trnava, Slovakia. Feature ExtractionFor this competition, we will be mostly matching images based on their local features, a.k.a. interest points. A local image feature is a tiny patch in the image that's invariant to image scaling, rotation and change in illumination.  It's like the tip of a tower, or the corner of a window in the image above.  Unlike a random point on the background (sky) in the image above, the tip of the tower can be precise detected in most images of the same scene.  It is geometricly (translation, rotation, ...) and photometricly (brightness, exposure, ...) invariant. A good local feature is like the piece you start with when solving a jigsaw puzzle, except on a much smaller scale.  It's the eye of the cat or the corner of the table, not a piece on a blank wall. The extracted local features must be:  Repeatable and precise so they can be extracted from different images showing the same object. Distinctive to the image, so images with different structure will not have them.  There could be hundreds or thousands of such features in an image.  An image matcher algorithm could still work if some of the features are blocked by an object or badly deformed due to change in brightness or exposure.  Many local feature algorithms are highly efficient and can be used in real-time applications. Due to these requirements, most local feature detectors extract corners and blobs. Local Feature Detection and DescriptionThere is a wealth of algorithms satisfying the above requirements for feature detection (finding interest points on an image) and description (generating a vector representation for them).  They include Harris Corner Detection, Scale Invariant Feature Transform (SIFT), Speeded-Up Robust Features (SURF), Features from Accelerated Segment Test (FAST), and Binary Robust Independent Elementary Features (BRIEF). In this tutorial, we will use Oriented FAST and Rotated BRIEF (ORB) for feature detection and description.  This algorithm was developed and implemented by OpenCV Labs, and it's part of their OpenCV library for computer vision. Let's start by extracting the local features of the image shown in the banner above.  It's the Cathedral of St. John the Baptist in Trnava, Slovakia.;Apache 2.0;https://www.kaggle.com/wesamelshamy/image-feature-extraction-and-matching-for-newbies;0.5;[];['ner', 'dl', 'gan', 'cv', 'nn', 'ann'];['computer vision', 'recognition', 'filter'];https://www.kaggle.com/c/landmark-recognition-challenge;0.77;0.464;2020-12-13 12:46:43;multiple data sources;['beginner, feature engineering, image data, +1 morecomputer vision'];Image Feature Extraction and Matching for Newbies;Python notebook;17752.0;63;;
2018-05-18 06:17:27;;Apache 2.0;https://www.kaggle.com/wolfgangb33r/landmark-recognition-train-a-first-keras-model;1.0;['tensorflow', 'keras'];['ner', 'ai', 'nlp', 'nn', 'ann'];['image classification', 'train', 'recognition', 'model', 'epoch', 'deep learning', 'layer', 'validation data', 'relu', 'loss', 'computer vision', 'classification'];https://www.kaggle.com/c/landmark-recognition-challenge;0.691;0.253;2020-12-13 12:46:43;Google Landmark Recognition Challenge;['beginner, classification, image data, +1 morecomputer vision'];Landmark Recognition train a first Keras model;Python script;2495.0;6;;
2019-04-17 16:30:44;;Apache 2.0;https://www.kaggle.com/anisayari/download-images-dataset-python3-log-progressbar;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-retrieval-2019;0.699;0.371;2020-12-13 12:48:00;Google Landmark Retrieval 2019;[]; Download images dataset  python3+log+progressbar;Python script;3022.0;21;;
2019-04-11 16:54:55;;Apache 2.0;https://www.kaggle.com/automatichourglass/create-a-subset-of-training-dataset;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'training data', 'deep learning'];https://www.kaggle.com/c/landmark-retrieval-2019;0.703;0.188;2020-12-13 12:48:00;multiple data sources;[];Create a subset of Training Dataset;Python script;3295.0;3;;
2020-02-12 03:40:10;I have made an exploration of the dataset. I have shown how to access and display images.;Apache 2.0;https://www.kaggle.com/grapestone5321/exploration-of-the-dataset;0.5;[];['ai', 'rl', 'cv', 'nn', 'ml'];['train', 'test data', 'training data'];https://www.kaggle.com/c/landmark-retrieval-2019;0.684;0.281;2020-12-13 12:48:00;multiple data sources;['gpu'];Exploration of the Dataset;Python notebook;2179.0;8;;
2019-06-14 08:43:03;Google Landmarks Dataset v2 Exploratory Data Analysis(EDA);Apache 2.0;https://www.kaggle.com/huangxiaoquan/google-landmarks-v2-exploratory-data-analysis-eda;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['training data', 'test data', 'train', 'recognition', 'label'];https://www.kaggle.com/c/landmark-retrieval-2019;0.672;0.292;2020-12-13 12:48:00;multiple data sources;['beginner, data visualization, exploratory data analysis, +1 moreimage data'];Google Landmarks v2 Exploratory Data Analysis(EDA);Python notebook;1672.0;9;;
2019-04-09 06:45:53;Note: requires the tqdm package (pip install tqdm) Note to Kagglers: This script will not run directly in Kaggle kernels. You need to download it and run it on your local machine. Downloads images from the Google Landmarks dataset using multiple threads. Images that already exist will not be downloaded again, so the script can resume a partially completed download. All images will be saved in the JPG format with 90% compression quality.;Apache 2.0;https://www.kaggle.com/xiuchengwang/python-dataset-download;0.2;[];['ai', 'rl'];[];https://www.kaggle.com/c/landmark-retrieval-2019;0.767;0.352;2020-12-13 12:48:00;Google Landmark Retrieval 2019;[];Python Dataset Download;Python notebook;16674.0;17;;
2020-07-01 06:15:18;;Apache 2.0;https://www.kaggle.com/camaskew/baseline-submission;0.3;[];[];['model'];https://www.kaggle.com/c/landmark-retrieval-2020;0.748;0.476;2020-12-13 12:53:48;multiple data sources;[];Baseline Model;Python script;9847.0;73;0.24382;0.27154
2020-07-02 23:32:24;Google Landmark Retrieval 2020 - Exploratory Data Analysis</br> Previous Related Competitions Google Landmark Retrieval 2019 Google Landmark Retrieval Challenge  References:  https://www.kaggle.com/seriousran/google-landmark-retrieval-2020-eda https://www.kaggle.com/codename007/google-landmark-retrieval-exploratory-analysis https://www.kaggle.com/huangxiaoquan/google-landmarks-v2-exploratory-data-analysis-eda;Apache 2.0;https://www.kaggle.com/maschasap/exploratory-data-analysis-inference;0.5;[];['ai', 'nn', 'ann', 'cv'];['filter', 'training data', 'train', 'model', 'label', 'labeled'];https://www.kaggle.com/c/landmark-retrieval-2020;0.666;0.393;2020-12-13 12:53:48;Google Landmark Retrieval 2020;['beginner, data visualization, exploratory data analysis'];Exploratory Data Analysis + Inference!;Python notebook;1508.0;27;;
2020-08-10 10:18:40;;Apache 2.0;https://www.kaggle.com/nvnnghia/main-0806;0.3;[];[];['model'];https://www.kaggle.com/c/landmark-retrieval-2020;0.677;0.437;2020-12-13 12:53:48;multiple data sources;[];main_0806;Python notebook;1881.0;45;0.24407;0.27709
2020-08-18 07:25:54;;Apache 2.0;https://www.kaggle.com/prateekagnihotri/change-image-size-to-get-0-281-lb;1.0;['tensorflow', 'keras'];['ai', 'cv'];['predict', 'train', 'model', 'loss', 'rank'];https://www.kaggle.com/c/landmark-retrieval-2020;0.65;0.375;2020-12-13 12:53:48;multiple data sources;[];Change image size to get 0.281 LB;Python notebook;1099.0;22;;
2020-08-14 08:39:36;;Apache 2.0;https://www.kaggle.com/qiubit/comparison-0-271-vs-0-277;0.8;['tensorflow'];[];['model'];https://www.kaggle.com/c/landmark-retrieval-2020;0.621;0.383;2020-12-13 12:53:48;multiple data sources;[];Comparison 0.271 vs 0.277;Python notebook;646.0;24;;
2020-08-05 15:54:49;Google Landmark Retrieval - EDA & Outlier Analysis    This is an image retrieval competition. In simple terms, you have to design a model that takes a given image and finds similar images in a database that contains a large number of varied images. The data consists of images of landmarks - both natural and man-made. Every image has a unique id which is provided in train.csv along with the corresponding landmark_id.;Apache 2.0;https://www.kaggle.com/rai555/google-landmark-retrieval-eda-outlier-analysis;0.5;[];['ai', 'nn', 'ml', 'cv'];['train', 'model', 'label'];https://www.kaggle.com/c/landmark-retrieval-2020;0.663;0.427;2020-12-13 12:53:48;Google Landmark Retrieval 2020;['beginner, data visualization, exploratory data analysis'];Google Landmark Retrieval - EDA & Outlier Analysis;Python notebook;1415.0;40;;
2020-07-03 05:36:35;Google Landmark Retrieval 2020 Convolutional Filter Visualization;Apache 2.0;https://www.kaggle.com/reighns/visualizing-convolution-filters-in-landmarks;0.5;[];['ai', 'cv'];['train', 'model', 'label', 'filter'];https://www.kaggle.com/c/landmark-retrieval-2020;0.646;0.408;2020-12-13 12:53:48;Google Landmark Retrieval 2020;[];Visualizing Convolution Filters in Landmarks;Python notebook;1024.0;32;;
2020-07-01 06:29:31;"Google Landmark Retrieval 2020 - EDA ""This year, we have worked to set this up as a code competition and we have completely refreshed the test and index image sets."" Let's start to dig it! :) Past Related Competitions Google Landmark Retrieval 2019 Google Landmark Retrieval Challenge  The winner in last competition Reference: https://www.kaggle.com/huangxiaoquan/google-landmarks-v2-exploratory-data-analysis-eda";Apache 2.0;https://www.kaggle.com/seriousran/google-landmark-retrieval-2020-eda;0.5;[];['ner', 'ai', 'nn', 'cv'];['train', 'model', 'label', 'training data'];https://www.kaggle.com/c/landmark-retrieval-2020;0.737;0.496;2020-12-13 12:53:48;Google Landmark Retrieval 2020;[];Google Landmark Retrieval 2020 - EDA;Python notebook;7416.0;95;;
2020-07-03 08:41:17;;Apache 2.0;https://www.kaggle.com/subbhashit/google-landmark-retrival-vgg19;1.0;['keras', 'sklearn'];['ai', 'cv'];['train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/landmark-retrieval-2020;0.621;0.375;2020-12-13 12:53:48;Google Landmark Retrieval 2020;['data visualization, deep learning, classification'];Google landmark retrival:VGG19;Python notebook;640.0;22;;
2020-07-01 17:28:58;;Apache 2.0;https://www.kaggle.com/vishalsiram50/images-display-eda;1.0;['caffe'];['ai', 'nn'];['train'];https://www.kaggle.com/c/landmark-retrieval-2020;0.541;0.408;2020-12-13 12:53:48;Google Landmark Retrieval 2020;[];Images display - EDA;Python notebook;179.0;32;;
2018-02-05 22:34:48;;Apache 2.0;https://www.kaggle.com/anokas/py3-image-downloader-w-progress-bar;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['classification', 'deep learning'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.744;0.447;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];Py3 Image downloader w/ progress bar;Python script;8868.0;51;;
2018-03-22 19:14:38;;Apache 2.0;https://www.kaggle.com/bellar/py3-small-image-downloader-w-progress-bar;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.642;0.253;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];Py3 small Image downloader w/ progress bar ;Python script;938.0;6;;
2018-02-19 07:58:53;;Apache 2.0;https://www.kaggle.com/bestaar/deep-metric-learning-with-pretrained-keras-models;1.0;['pattern', 'tensorflow', 'keras'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'recognition', 'model', 'deep learning', 'layer', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.75;0.371;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];Deep Metric Learning with pretrained keras models;Python script;10455.0;21;;
2018-06-06 20:58:02;;Apache 2.0;https://www.kaggle.com/codename007/google-landmark-retrieval-exploratory-analysis;0.5;[];['ai', 'rl', 'ml', 'cv'];['train', 'test data', 'label', 'training data'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.758;0.5;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;['beginner, data visualization, exploratory data analysis, +1 moreimage data'];Google Landmark Retrieval Exploratory Analysis;Python notebook;12776.0;100;;
2018-05-06 02:15:05;;Apache 2.0;https://www.kaggle.com/festa78/distance-between-images-using-botttleneck;0.7;['tensorflow'];['dl'];[];https://www.kaggle.com/c/landmark-retrieval-challenge;0.606;0.214;2020-12-13 12:58:59;multiple data sources;[];Distance between images using botttleneck.;Python notebook;494.0;4;;
2018-05-06 19:10:54;;Apache 2.0;https://www.kaggle.com/festa78/extract-bottleneck-features-using-tensorflow-hub;1.0;['tensorflow'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'recognition', 'layer'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.672;0.188;2020-12-13 12:58:59;multiple data sources;[];Extract bottleneck features using Tensorflow-hub.;Python notebook;1703.0;3;;
2018-05-13 18:35:41;;Apache 2.0;https://www.kaggle.com/insaff/inefficient-way-to-get-similar-photos;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.609;0.152;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];Inefficient way to get similar photos;Python script;522.0;2;;
2018-05-08 19:46:19;;Apache 2.0;https://www.kaggle.com/jihyeseo/landmark-retrieval-image;0.5;['sklearn'];[];[];https://www.kaggle.com/c/landmark-retrieval-challenge;0.609;0.0;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];landmark retrieval image;Python notebook;524.0;0;;
2018-04-04 00:27:16;In this version of the notebook, I have made an exploration of the dataset.  I have shown how to access and display images. I have also added a feature source website for these images. Look forward to adding more features in my next version of the notebook.;Apache 2.0;https://www.kaggle.com/juhi2811/exploratory-analysis-of-the-landmark-images;0.5;[];['ai', 'nn', 'ml', 'rl'];['train', 'test data', 'label', 'training data'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.627;0.214;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];Exploratory analysis of the landmark images;Python notebook;722.0;4;;
2018-02-23 09:33:43;;Apache 2.0;https://www.kaggle.com/madorin/download-images-from-url-save-into-dicstionary;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.673;0.0;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];download images from URL & save into dicstionary;Python notebook;1725.0;0;;
2018-02-07 19:54:55;Helper functions;Apache 2.0;https://www.kaggle.com/mpekalski/download-data-straight-to-compressed-tfrecord;1.0;['tensorflow'];['ai', 'nn', 'ann', 'rl'];['train'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.701;0.302;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];Download data straight to compressed TFRecord;Python notebook;3136.0;10;;
2018-02-23 01:56:57;;Apache 2.0;https://www.kaggle.com/nandadeepd/simpleimagedownloader;0.5;[];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.583;0.0;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];SimpleImageDownloader;Python script;340.0;0;;
2018-08-23 13:09:47;;Apache 2.0;https://www.kaggle.com/offbye/move-downloaded-images-to-id-dirs;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.551;0.0;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];move downloaded  images to id dirs;Python script;206.0;0;;
2018-02-03 01:26:54;;Apache 2.0;https://www.kaggle.com/tobwey/landmark-retrieval-challenge-image-downloader;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['recognition', 'classification', 'deep learning'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.748;0.425;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];Landmark Retrieval Challenge Image Downloader;Python script;9701.0;39;;
2018-05-29 22:52:32;Missing Exif DataThe competition organizers gave us permission to use image Exif data and other metadata to identify landmarks. If you used one of the published image downloader kernels to download your images, then you are probably missing out on this data.  The potentially useful meta data is dropped in the image download and conversion process.  You may want to redownload the images (~ 460GB) with the script shown at the end of this kernel if you want to use these nuggets.;Apache 2.0;https://www.kaggle.com/wesamelshamy/extract-geographical-data-from-image-description;0.5;[];['ner', 'ai', 'rl', 'gan'];['recognition', 'filter', 'test data'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.654;0.281;2020-12-13 12:58:59;multiple data sources;['data visualization, image data, computer vision'];Extract Geographical Data From Image Description;Python notebook;1175.0;8;;
2018-05-29 23:06:32;www.codeproject.com/KB/recipes/619039/SIFT.JPG  Cathedral of St. John the Baptist in Trnava, Slovakia. Feature ExtractionFor this competition, we will be mostly matching images based on their local features, a.k.a. interest points. A local image feature is a tiny patch in the image that's invariant to image scaling, rotation and change in illumination.  It's like the tip of a tower, or the corner of a window in the image above.  Unlike a random point on the background (sky) in the image above, the tip of the tower can be precise detected in most images of the same scene.  It is geometricly (translation, rotation, ...) and photometricly (brightness, exposure, ...) invariant. A good local feature is like the piece you start with when solving a jigsaw puzzle, except on a much smaller scale.  It's the eye of the cat or the corner of the table, not a piece on a blank wall. The extracted local features must be:  Repeatable and precise so they can be extracted from different images showing the same object. Distinctive to the image, so images with different structure will not have them.  There could be hundreds or thousands of such features in an image.  An image matcher algorithm could still work if some of the features are blocked by an object or badly deformed due to change in brightness or exposure.  Many local feature algorithms are highly efficient and can be used in real-time applications. Due to these requirements, most local feature detectors extract corners and blobs. Local Feature Detection and DescriptionThere is a wealth of algorithms satisfying the above requirements for feature detection (finding interest points on an image) and description (generating a vector representation for them).  They include Harris Corner Detection, Scale Invariant Feature Transform (SIFT), Speeded-Up Robust Features (SURF), Features from Accelerated Segment Test (FAST), and Binary Robust Independent Elementary Features (BRIEF). In this tutorial, we will use Oriented FAST and Rotated BRIEF (ORB) for feature detection and description.  This algorithm was developed and implemented by OpenCV Labs, and it's part of their OpenCV library for computer vision. Let's start by extracting the local features of the image shown in the banner above.  It's the Cathedral of St. John the Baptist in Trnava, Slovakia.;Apache 2.0;https://www.kaggle.com/wesamelshamy/tutorial-image-feature-extraction-and-matching;0.5;[];['ner', 'dl', 'gan', 'cv', 'nn', 'ann'];['computer vision', 'recognition', 'filter'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.804;0.453;2020-12-13 12:58:59;multiple data sources;['data visualization, feature engineering, computer vision'];[Tutorial] Image Feature Extraction and Matching;Python notebook;51689.0;55;;
2018-03-20 00:11:27;;Apache 2.0;https://www.kaggle.com/xenosender/html-submission-examiner;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['classification', 'deep learning', 'predict'];https://www.kaggle.com/c/landmark-retrieval-challenge;0.634;0.152;2020-12-13 12:58:59;Google Landmark Retrieval Challenge;[];HTML Submission examiner;Python script;819.0;2;;
2019-04-21 18:23:52;;Apache 2.0;https://www.kaggle.com/abhishek/quite-a-few-features-1-51;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'nn', 'rl'];['train', 'model', 'predict'];https://www.kaggle.com/c/LANL-Earthquake-Prediction;0.727;0.507;2020-12-13 13:01:16;LANL Earthquake Prediction;['feature engineering'];quite a few features [1.51];Python notebook;5715.0;111;2.65671;1.54048
2019-06-04 10:30:07;;Apache 2.0;https://www.kaggle.com/ilu000/1-private-lb-kernel-lanl-lgbm;1.0;['lightgbm', 'sklearn'];['ai', 'rl', 'gbm'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/LANL-Earthquake-Prediction;0.73;0.501;2020-12-13 13:01:16;LANL Earthquake Prediction;['gradient boosting'];#1 private LB kernel LANL lgbm;Python notebook;6106.0;102;2.27922;1.77606
2019-01-11 05:14:58;;Apache 2.0;https://www.kaggle.com/inversion/basic-feature-benchmark;1.0;['sklearn'];['ai', 'nn'];['train', 'label', 'predict'];https://www.kaggle.com/c/LANL-Earthquake-Prediction;0.788;0.574;2020-12-13 13:01:16;LANL Earthquake Prediction;[];Basic Feature Benchmark;Python notebook;31056.0;294;;
2019-04-03 22:25:46;1. OverviewAs stated in the description, the data for this competition comes from a experimental set-up used to study earthquake physics. Our goal is to predict the time remaining before the next laboratory earthquake. The only feature we have is the seismic signal (acoustic data), which is recorded using a piezoceramic sensor and corresponds to the voltage upon deformation (in integers).  Training data: single, continuous segment of experimental data.  Test data: consists of a folder containing many small segments. The data within each test file is continuous, but the test files do not represent a continuous segment of the experiment.  Both the training and the testing set come from the same experiment.  There is no overlap between the training and testing sets.   There are a lot of files in this competition, so let's start with the folders structure:;Apache 2.0;https://www.kaggle.com/jsaguiar/seismic-data-exploration;0.5;[];['ner', 'ai', 'nn', 'rl'];['training data', 'filter', 'test data', 'train', 'label', 'predict'];https://www.kaggle.com/c/LANL-Earthquake-Prediction;0.752;0.535;2020-12-13 13:01:16;LANL Earthquake Prediction;['data visualization, exploratory data analysis'];Seismic Data Exploration;Python notebook;10784.0;164;;
2019-01-31 18:51:39;;Apache 2.0;https://www.kaggle.com/mayer79/rnn-starter-for-huge-time-series;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cnn', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['gru', 'filter', 'test data', 'regression', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/LANL-Earthquake-Prediction;0.784;0.573;2020-12-13 13:01:16;LANL Earthquake Prediction;['gpu, deep learning, rnn'];RNN starter for huge time series;Python script;27772.0;289;2.80102;1.51665
2019-05-04 08:35:06;"Kaggle LANL Earthquake Prediction Exploratory Data AnalysisKevin MaherRegis University MSDS696 Data Science Practicum IIAssociate Professor Dr. Robert MasonMay 2, 2019Spring, 2019; In partial fullfillment of the Master of Science in Data Science degree, Regis University, Denver, CO";Apache 2.0;https://www.kaggle.com/vettejeep/masters-final-project-eda;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'generation', 'train', 'test data', 'model', 'label', 'predict'];https://www.kaggle.com/c/LANL-Earthquake-Prediction;0.717;0.511;2020-12-13 13:01:16;LANL Earthquake Prediction;[];Masters Final Project: EDA;Python notebook;4542.0;117;;
2019-02-13 04:42:17;;Apache 2.0;https://www.kaggle.com/zikazika/memory-problems;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ai', 'dl', 'gbm', 'cv', 'rl', 'nn'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/LANL-Earthquake-Prediction;0.688;0.497;2020-12-13 13:01:16;multiple data sources;[];Memory problems;Python notebook;2331.0;97;;
2019-01-19 09:59:05;;Apache 2.0;https://www.kaggle.com/zikazika/useful-new-features-and-a-optimised-model;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nn', 'ann'];['filter', 'test data', 'regression', 'train', 'model', 'label', 'predict', 'bayesian'];https://www.kaggle.com/c/LANL-Earthquake-Prediction;0.724;0.516;2020-12-13 13:01:16;LANL Earthquake Prediction;[];Useful (new) Features and a optimised model;Python notebook;5308.0;125;;
2016-08-30 15:47:56;Load datasets;Apache 2.0;https://www.kaggle.com/anilnarassiguin/ml-classic-pipeline-python-xgboost;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ann'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/leaf-classification;0.698;0.34;2020-12-13 13:03:36;Leaf Classification;[];ML classic pipeline - Python - XGBoost;Python notebook;2955.0;15;;
2016-08-30 17:26:03;;Apache 2.0;https://www.kaggle.com/bmetka/logistic-regression;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ann'];['training data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/leaf-classification;0.739;0.39;2020-12-13 13:03:36;Leaf Classification;[];Logistic Regression;Python script;7725.0;26;0.03805;0.03805
2016-09-07 06:15:59;;Apache 2.0;https://www.kaggle.com/felixsoul/basic-neural-network-using-tensorflow;1.0;['tensorflow', 'sklearn'];['ner', 'ai', 'nlp', 'nn', 'ann'];['test data', 'train', 'model', 'neural network', 'deep learning', 'layer', 'label', 'predict', 'classification'];https://www.kaggle.com/c/leaf-classification;0.711;0.346;2020-12-13 13:03:36;Leaf Classification;[];Basic Neural Network using Tensorflow;Python script;3923.0;16;;
2017-09-18 04:35:22;Unpack zip files;Apache 2.0;https://www.kaggle.com/hooseygoose/directory-structure-and-moving-files;0.5;[];['ai'];['test data', 'filter', 'train', 'label', 'classification'];https://www.kaggle.com/c/leaf-classification;0.737;0.327;2020-12-13 13:03:36;Leaf Classification;[];Directory Structure and Moving Files;Python notebook;7337.0;13;;
2016-09-19 23:19:21;;Apache 2.0;https://www.kaggle.com/jiashenliu/updatedtry-5-different-classifiers-and-questions;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'random forest', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'logistic regression', 'predict', 'recommend', 'classification', 'naive bayes', 'bayesian'];https://www.kaggle.com/c/leaf-classification;0.749;0.346;2020-12-13 13:03:36;Leaf Classification;[];UpdatedTry 5 different classifiers and *QUESTIONS;Rmarkdown script;9979.0;16;;
2016-11-13 07:46:15;This R code will compare multiple R models (10s..100s)  using the caret wrapper library. The example given here is for fast and accurate performers. Model list https://topepo.github.io/caret/available-models.html Some of the better performers are then averaged to improve the LB score from (meager) 0.49574 to the somewhat better ensemble model LB score: 0.13357.;Apache 2.0;https://www.kaggle.com/tobikaggle/compare-multiple-classification-models-with-caret;0.5;[];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['training data', 'train', 'model', 'validation data', 'loss', 'k-nearest neighbor', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/leaf-classification;0.759;0.34;2020-12-13 13:03:36;Leaf Classification;[];Compare multiple classification models with caret;R notebook;13032.0;15;;
2018-06-26 02:05:09;Using Neural Networks through Keras;Apache 2.0;https://www.kaggle.com/tobikaggle/nn-through-keras-copied-mod-shuffle;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'rl', 'nn', 'cv'];['training data', 'neuron', 'train', 'fitting', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/leaf-classification;0.733;0.371;2020-12-13 13:03:36;Leaf Classification;['gpu'];NN through Keras *Copied* mod shuffle;Python notebook;6668.0;21;;
2020-07-17 00:35:41;;Apache 2.0;https://www.kaggle.com/vchauhanusf/97-48-test-accuracy-leaf-image-classification;1.0;['keras', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ann'];['train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/leaf-classification;0.594;0.352;2020-12-13 13:03:36;Leaf Classification;[];97.48 Test_Accuracy_Leaf_image_classification;Python notebook;404.0;17;;
2019-03-27 22:22:13;Using Neural Networks through Keras;Apache 2.0;https://www.kaggle.com/zenstat/nn-through-keras-learn-through-trial-and-error;1.0;['tensorflow', 'sklearn', 'keras'];['dl', 'ai', 'nn', 'rl'];['training data', 'neuron', 'train', 'fitting', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/leaf-classification;0.706;0.319;2020-12-13 13:03:36;Leaf Classification;[];NN through Keras - Learn through Trial and Error;Python notebook;3469.0;12;;
2015-07-07 06:59:47;;Apache 2.0;https://www.kaggle.com/benhamner/random-forest-benchmark-3;0.5;[];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn', 'ml'];['filter', 'random forest', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.771;0.446;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Random Forest Benchmark;R script;18401.0;50;0.34825;0.35307
2015-08-16 13:56:15;;Apache 2.0;https://www.kaggle.com/candan/colored-histogram;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.688;0.327;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];colored_histogram;R script;2361.0;13;;
2015-07-07 02:29:34;;Apache 2.0;https://www.kaggle.com/devinanzelmo/xgboost-benchmark-0-38019;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.74;0.393;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Xgboost benchmark 0.38019;Python script;7996.0;27;0.38366;0.38051
2015-08-30 04:24:08;;Apache 2.0;https://www.kaggle.com/inversion/leaderboard-progression;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.725;0.413;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Leaderboard Progression;Python script;5444.0;34;;
2015-08-19 20:05:26;;Apache 2.0;https://www.kaggle.com/jpopham91/gini-scoring-simple-and-efficient;0.5;[];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['predict', 'train', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.776;0.467;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Gini Scoring, Simple and Efficient;Python script;21657.0;65;;
2015-08-18 08:17:31;;Apache 2.0;https://www.kaggle.com/justfor/bench-stacked-generalization;1.0;['sklearn'];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn', 'ann'];['training data', 'random forest', 'regression', 'train', 'model', 'validation data', 'deep learning', 'loss', 'label', 'gradient boosting', 'predict', 'linear regression', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.745;0.413;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Bench Stacked Generalization;Python script;9077.0;34;0.36065;0.36141
2015-07-25 18:38:41;;Apache 2.0;https://www.kaggle.com/justfor/explore-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.757;0.435;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Explore Data;Rmarkdown script;12481.0;44;;
2015-07-29 08:17:28;;Apache 2.0;https://www.kaggle.com/madcap/r-xgboost-starter-script;1.0;['xgboost'];['ner', 'ai', 'gbm', 'cv', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.715;0.302;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];R XGBOOST starter script;R script;4260.0;10;0.38393;0.37654
2015-08-14 12:51:51;;Apache 2.0;https://www.kaggle.com/mmueller/xgb-feature-importance-python;1.0;['xgboost', 'sklearn'];['nlp', 'ai', 'nn', 'ner'];['regression', 'train', 'deep learning', 'label', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.805;0.453;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];XGB Feature Importance (Python);Python script;54620.0;55;;
2015-07-14 11:17:15;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/r-xgboost-gini-v2;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['training data', 'train', 'model', 'validation data', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.727;0.327;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];R xgboost Gini  v2;R script;5807.0;13;;
2015-07-13 02:25:51;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/r-xgboost-with-gini-eval-and-stopping;1.0;['xgboost'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['training data', 'train', 'fitting', 'model', 'validation data', 'deep learning', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.772;0.371;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];R xgBoost with Gini eval and stopping;R script;18911.0;21;;
2015-07-23 22:53:47;;Apache 2.0;https://www.kaggle.com/odiseo1982/compare-variables-between-train-and-test;0.5;[];['nlp', 'ai', 'nn', 'ner'];['test data', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.678;0.327;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Compare variables between train and test;R script;1897.0;13;;
2015-07-09 16:48:00;;Apache 2.0;https://www.kaggle.com/oxofff/gini-scorer-cv-gridsearch;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.75;0.327;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Normalized Gini Scorer + XGBoost (0.371);Python script;10451.0;13;;
2015-07-17 05:02:31;;Apache 2.0;https://www.kaggle.com/rocelot/factortonumeric;0.5;[];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn'];['train', 'fitting', 'model', 'deep learning', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.743;0.362;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];factorToNumeric;R script;8675.0;19;;
2015-07-28 02:13:41;;Apache 2.0;https://www.kaggle.com/rshah4/correlation-matrix-visualization;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.743;0.39;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Correlation Matrix Visualization;R script;8642.0;26;;
2015-07-28 14:56:24;;Apache 2.0;https://www.kaggle.com/rshah4/histogram-of-all-fields-with-labels;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.685;0.302;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Histogram of all Fields with Labels;Python script;2224.0;10;;
2015-08-08 15:36:43;;Apache 2.0;https://www.kaggle.com/soutik/blah-xgb;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ann'];['machine learning', 'training data', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.772;0.387;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Blah - XGB;Python script;18971.0;25;0.39101;0.38991
2015-07-08 01:39:34;;Apache 2.0;https://www.kaggle.com/tdevries/calculating-normalized-gini-coefficient;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.759;0.327;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Normalized Gini Coefficient [BROKEN];Python script;13345.0;13;;
2015-07-11 04:35:44;;Apache 2.0;https://www.kaggle.com/titericz/done-done-3;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.715;0.302;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];done done 3;Python script;4354.0;10;0.38799;0.38579
2015-07-18 21:05:34;;Apache 2.0;https://www.kaggle.com/vikasrtr/histogram-of-all-fields;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction;0.748;0.379;2020-12-13 13:14:28;Liberty Mutual Group: Property Inspection Prediction;[];Histogram of all Fields;Python script;9698.0;23;;
2020-11-08 18:01:18;Model Blending Weights OptimisationThis demo shows how to use scipy.optimize to optimise your model blending weights using your models' OOFs. UPDATE: Getting rid of the penalty term by using 'SLSQP' solver with a relatively small tolerance and Jacobian matrix. UPDATE: Calculate the gradients with paper and pencil to accelerate the optimisation... UPDATE: Add numba gradient function;Apache 2.0;https://www.kaggle.com/gogo827jz/optimise-blending-weights-with-bonus-0;0.5;[];['ai', 'dl', 'ml', 'cv'];['test data', 'predict', 'train', 'model', 'loss'];https://www.kaggle.com/c/lish-moa;0.727;0.537;2020-12-13 13:16:12;multiple data sources;[];Optimise Blending Weights with Bonus :0;Python notebook;5721.0;168;;
2020-11-29 01:06:14;;Apache 2.0;https://www.kaggle.com/headsortails/explorations-of-action-moa-eda;1.0;['pattern', 'catboost', 'tensorflow', 'keras'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['recommend', 'filter', 'training data', 'test data', 'train', 'model', 'neural network', 'epoch', 'validation data', 'layer', 'gradient descent', 'loss', 'label', 'clustering', 'predict', 'relu', 'decision tree', 'classification'];https://www.kaggle.com/c/lish-moa;0.791;0.633;2020-12-13 13:16:12;Mechanisms of Action (MoA) Prediction;['beginner, data visualization, exploratory data analysis, +2 moredrugs and medications, tidyverse'];Explorations of Action - MoA EDA;Rmarkdown script;34033.0;801;;
2020-11-30 23:04:51;Mechanisms of Action (MoA) Prediction. Data analysis and visualization;Apache 2.0;https://www.kaggle.com/isaienkov/mechanisms-of-action-moa-prediction-eda;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'label', 'test data', 'predict'];https://www.kaggle.com/c/lish-moa;0.791;0.61;2020-12-13 13:16:12;Mechanisms of Action (MoA) Prediction;['beginner, exploratory data analysis, classification, +2 moretabular data, medicine'];Mechanisms of Action (MoA) Prediction. EDA;Python notebook;33561.0;534;;
2020-10-12 12:24:47;;Apache 2.0;https://www.kaggle.com/kailex/moa-transfer-recipe-with-smoothing;1.0;['tensorflow', 'keras'];['ai', 'rl', 'nn', 'cv'];['filter', 'training data', 'test data', 'train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/lish-moa;0.743;0.531;2020-12-13 13:16:12;Mechanisms of Action (MoA) Prediction;['gpu, beginner, neural networks, +2 morebiology, multilabel classification'];MOA: Transfer Recipe with Smoothing;Rmarkdown script;8503.0;155;0.01646;0.01858
2020-11-15 17:30:35;RankGauss;Apache 2.0;https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids;1.0;['sklearn'];['ai', 'dl', 'gan', 'cv', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'rank', 'relu'];https://www.kaggle.com/c/lish-moa;0.737;0.532;2020-12-13 13:16:12;multiple data sources;['gpu'];Pytorch Transfer Learning with K-Folds by Drug IDs;Python notebook;7314.0;156;0.01625;0.01835
2020-11-21 05:50:20;Mechanisms of Action (MoA) Prediction;Apache 2.0;https://www.kaggle.com/vbmokin/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'epoch', 'loss', 'label', 'predict', 'rank', 'recommend', 'relu'];https://www.kaggle.com/c/lish-moa;0.77;0.56;2020-12-13 13:16:12;multiple data sources;['gpu, feature engineering, pca'];MoA: Pytorch-RankGauss-PCA-NN upgrade & 3D visual;Python notebook;17771.0;236;0.01627;0.01839
2020-03-23 19:41:22;;Apache 2.0;https://www.kaggle.com/brandenkmurray/seq2seq-rnn-with-gru;1.0;['pytorch', 'sklearn'];['ai', 'rl', 'nn', 'rnn', 'ann'];['gru', 'filter', 'train', 'model', 'input layer', 'output layer', 'epoch', 'layer', 'loss', 'label', 'predict', 'rank', 'relu', 'hidden layer'];https://www.kaggle.com/c/liverpool-ion-switching;0.739;0.515;2020-12-13 13:17:30;multiple data sources;['gpu'];Seq2Seq RNN with GRU;Python script;7740.0;124;0.93890;0.94028
2020-03-21 09:38:19;"IntroductionIn this notebook, I'll share my approach to finding synthetic drift function. It is no secret that the drift has been artificially added. In this competition's paper here, you can find the description of the data like below: ""In some datasets additional drift was applied to the final data with MATLAB""  There's an excellent explanation for the drift. Please check Chris' explanation: What is Drift?";Apache 2.0;https://www.kaggle.com/eunholee/remove-drift-using-a-sine-function;0.5;[];['ai', 'ml'];['train', 'model', 'label'];https://www.kaggle.com/c/liverpool-ion-switching;0.708;0.506;2020-12-13 13:17:30;University of Liverpool - Ion Switching;['data cleaning'];remove drift using a sine function;Python notebook;3669.0;109;;
2020-03-13 17:17:20;"This notebook shows how to remove the drift from the training and test data as cleanly as possible. A clean signal is extremely important, since predictions from any ML models depend strongly on the precise value of each data point. The drift is removed by computing the histograms of small signal batches and matching them to an ideal (non-shifted) histogram. The resulting shifts are already much better than those from e.g. a rolling mean. The shifts are then further smoothed by approximating them with 4th degree polynomials.  The resulting clean signal retains the original offset. For the pupose of this competition, the ""signal groups"" (see below) are determined by hand. This could also be done in an automated way (e.g. through analysis of the histograms) in the case of real-world data.";Apache 2.0;https://www.kaggle.com/friedchips/clean-removal-of-data-drift;0.5;[];['ai', 'rl', 'ml', 'nn', 'ann'];['training data', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/liverpool-ion-switching;0.741;0.561;2020-12-13 13:17:30;University of Liverpool - Ion Switching;['data visualization, data cleaning'];Clean Removal of Data Drift;Python notebook;8209.0;241;;
2020-04-02 19:42:41;IMPORTANT EDIT: A basic assumption in this notebook is wrong: the signal data is not the result of a first-order Markov process. In fact, the open_channel data itself is the result of a first-order Markov process. Therefore, the situation is more complicated than described here. Still, most of the arguments remain relevant. I've explained more here. This notebook analyzes and visualizes the training data and demonstrates that the signal data is indeed the result of a simple Markov process (however, a hidden one) combined with Gaussian (white) noise. This puts a strong upper limit on what a model can achieve in this competition.;Apache 2.0;https://www.kaggle.com/friedchips/on-markov-chains-and-the-competition-data;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['training data', 'test data', 'train', 'model', 'label', 'ground truth'];https://www.kaggle.com/c/liverpool-ion-switching;0.716;0.525;2020-12-13 13:17:30;University of Liverpool - Ion Switching;['data visualization, data cleaning'];On Markov Chains and the Competition Data;Python notebook;4388.0;143;;
2020-04-17 04:52:13;reffrencehttps://www.kaggle.com/martxelo/fe-and-ensemble-mlp-and-lgbm;Apache 2.0;https://www.kaggle.com/hirayukis/lightgbm-keras-and-4-kfold;1.0;['xgboost', 'lightgbm', 'sklearn', 'tensorflow', 'keras'];['ai', 'gbm', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/liverpool-ion-switching;0.746;0.492;2020-12-13 13:17:30;multiple data sources;[];Ion Switching reading part CrossVal;Python notebook;9350.0;90;0.93864;0.94100
2020-03-26 01:41:12;Load dataThanks to https://www.kaggle.com/cdeotte/data-without-drift.;Apache 2.0;https://www.kaggle.com/martxelo/fe-and-ensemble-mlp-and-lgbm;1.0;['tensorflow', 'lightgbm', 'sklearn', 'keras'];['ai', 'gbm', 'ml', 'nlp', 'nn', 'ann'];['filter', 'test data', 'regression', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/liverpool-ion-switching;0.737;0.502;2020-12-13 13:17:30;multiple data sources;[];FE and ensemble MLP and LGBM ;Python notebook;7365.0;103;0.93356;0.93897
2020-04-25 13:17:00;Part 1 : Understanding Ion-switching;Apache 2.0;https://www.kaggle.com/mobassir/understanding-ion-switching-with-modeling;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'rl', 'ml', 'nn', 'rnn', 'ann'];['filter', 'predict', 'relu', 'gru', 'training data', 'neuron', 'train', 'epoch', 'lstm', 'propagation', 'model', 'neural network', 'layer', 'loss', 'understanding', 'test data', 'generation', 'deep learning', 'gradient descent', 'label'];https://www.kaggle.com/c/liverpool-ion-switching;0.76;0.531;2020-12-13 13:17:30;multiple data sources;[];Understanding Ion-Switching with modeling;Python notebook;13464.0;154;0.94003;0.94142
2020-05-14 11:42:34;;Apache 2.0;https://www.kaggle.com/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'cv', 'rl', 'nn', 'rnn', 'ann'];['filter', 'training data', 'train', 'model', 'epoch', 'validation data', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/liverpool-ion-switching;0.786;0.569;2020-12-13 13:17:30;multiple data sources;['gpu'];Wavenet with SHIFTED-RFC Proba and CBR;Python script;29451.0;271;0.94162;0.94255
2020-03-04 19:12:49;;Apache 2.0;https://www.kaggle.com/robikscube/ion-switching-5kfold-lgbm-tracking;1.0;['lightgbm', 'sklearn'];['ai', 'gbm', 'cv', 'rl', 'nn', 'ann'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/liverpool-ion-switching;0.73;0.498;2020-12-13 13:17:30;University of Liverpool - Ion Switching;[];Ion Switching - 5kfold LGBM & Tracking ;Python script;6190.0;98;0.91487;0.93723
2020-04-11 15:12:15;;Apache 2.0;https://www.kaggle.com/sggpls/shifted-rfc-pipeline;1.0;['sklearn'];['ai', 'nn', 'ann'];['train', 'predict'];https://www.kaggle.com/c/liverpool-ion-switching;0.733;0.521;2020-12-13 13:17:30;multiple data sources;['feature engineering, random forest'];SHIFTED-RFC Pipeline;Python script;6669.0;134;0.93859;0.94001
2020-03-02 03:33:52;Introduction;Apache 2.0;https://www.kaggle.com/tarunpaparaju/ion-switching-competition-signal-eda;1.0;['statsmodels', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'regression', 'neuron', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/liverpool-ion-switching;0.752;0.585;2020-12-13 13:17:30;University of Liverpool - Ion Switching;['data visualization, exploratory data analysis, biology, +1 moresignal processing'];Ion Switching Competition : Signal EDA ðŸ§ª;Python notebook;10939.0;349;;
2020-08-31 12:53:19;Loan Default Prediction;Apache 2.0;https://www.kaggle.com/abeersaxena/submission-31-08-2020;1.0;['statsmodels', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ann'];['filter', 'random forest', 'regression', 'train', 'model', 'loss', 'label', 'logistic regression', 'predict', 'classification', 'naive bayes'];https://www.kaggle.com/c/loan-default-prediction;0.556;0.099;2020-12-13 13:17:50;multiple data sources;[];notebook23f7a4d565;Python notebook;223.0;1;;
2020-08-04 10:38:06;1;Apache 2.0;https://www.kaggle.com/darisdzakwanhoesien2/loan-default-prediction-imperial-college-london;1.0;['xgboost'];['ai', 'nn', 'rl'];['train', 'loss', 'model', 'predict'];https://www.kaggle.com/c/loan-default-prediction;0.641;0.188;2020-12-13 13:17:50;Loan Default Prediction - Imperial College London;['gpu'];Loan Default Prediction - Imperial College London;Python notebook;919.0;3;1.29936;1.31846
2020-02-02 12:05:48;;Apache 2.0;https://www.kaggle.com/niraligala/loan-default-prediction-n;1.0;['sklearn'];['ner', 'ai', 'nn', 'cv'];['test data', 'random forest', 'regression', 'train', 'fitting', 'model', 'loss', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/loan-default-prediction;0.64;0.0;2020-12-13 13:17:50;Loan Default Prediction - Imperial College London;[];Loan Default Prediction_N;Python notebook;900.0;0;;
2019-01-18 00:27:58;;Apache 2.0;https://www.kaggle.com/panamby/loan-default-prediction;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ann'];['machine learning', 'random forest', 'regression', 'train', 'model', 'loss', 'label', 'logistic regression', 'k-nearest neighbor', 'predict', 'classification', 'naive bayes'];https://www.kaggle.com/c/loan-default-prediction;0.775;0.447;2020-12-13 13:17:50;Loan Default Prediction - Imperial College London;['data visualization, classification, feature engineering, +2 moredata cleaning, lending']; Loan Default Prediction;Python notebook;20729.0;51;;
2020-01-15 20:45:58;;Apache 2.0;https://www.kaggle.com/prathyushaprathyu/loan-default-prediction-prathyusha;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'ann', 'cv'];['machine learning', 'random forest', 'regression', 'train', 'model', 'loss', 'label', 'logistic regression', 'k-nearest neighbor', 'predict', 'classification', 'naive bayes'];https://www.kaggle.com/c/loan-default-prediction;0.595;0.0;2020-12-13 13:17:50;multiple data sources;['gpu']; Loan Default Prediction_Prathyusha;Python notebook;410.0;0;;
2020-05-14 20:31:54;Reading the input files;Apache 2.0;https://www.kaggle.com/rajvidoshi/loan-default-prediction-with-pca;1.0;['sklearn'];['ai', 'rl'];['machine learning', 'test data', 'regression', 'train', 'fitting', 'model', 'validation data', 'loss', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/loan-default-prediction;0.637;0.188;2020-12-13 13:17:50;Loan Default Prediction - Imperial College London;[];Loan Default Prediction with PCA;Python notebook;855.0;3;;
2020-01-06 02:35:28;Import data from csv file;Apache 2.0;https://www.kaggle.com/vinaykumars/loan-default-prediction;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'ml', 'cv'];['filter', 'training data', 'regression', 'test data', 'train', 'random forest', 'model', 'loss', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/loan-default-prediction;0.639;0.0;2020-12-13 13:17:50;Loan Default Prediction - Imperial College London;[];Loan_Default_Prediction;Python notebook;897.0;0;;
2020-08-25 00:22:53;Scene Visualisation;Apache 2.0;https://www.kaggle.com/jpbremer/lyft-scene-visualisations;1.0;['pillow'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml'];['filter', 'model', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;0.713;0.468;2020-12-13 13:21:26;multiple data sources;[];Lyft Scene Visualisations;Python notebook;4107.0;66;;
2020-09-11 02:04:53;Combining Lyft Multimode Models;Apache 2.0;https://www.kaggle.com/kneroma/combining-lyft-multimode-models;0.5;[];['ai', 'nn', 'ann', 'rl'];['model', 'test data', 'predict'];https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;0.675;0.442;2020-12-13 13:21:28;multiple data sources;[];Combining Lyft Multimode Models;Python notebook;1779.0;48;49.862;46.189
2020-09-18 12:38:05;Motion Prediction with PointNet;Apache 2.0;https://www.kaggle.com/kneroma/inference-motion-prediction-with-pointnet;1.0;['pytorch', 'tensorflow'];['ai', 'nn', 'ml'];['image segmentation', 'filter', 'train', 'model', 'epoch', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;0.679;0.421;2020-12-13 13:21:28;multiple data sources;['gpu'];[Inference] Motion Prediction with PointNet;Python notebook;1948.0;37;69.127;66.804
2020-09-17 08:08:38;Motion Prediction with PointNet;Apache 2.0;https://www.kaggle.com/kneroma/training-motion-prediction-with-pointnet;1.0;['pytorch', 'tensorflow'];['ner', 'ai', 'dl', 'ml', 'nn', 'ann'];['image segmentation', 'filter', 'test data', 'train', 'model', 'epoch', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;0.671;0.427;2020-12-13 13:21:28;Lyft Motion Prediction for Autonomous Vehicles;['gpu'];[Training] Motion Prediction with PointNet;Python notebook;1664.0;40;;
2020-08-30 22:17:05;Zarr files and L5kit data for dummies;Apache 2.0;https://www.kaggle.com/kneroma/zarr-files-and-l5kit-data-for-dummies;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl'];['train', 'model', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;0.679;0.446;2020-12-13 13:21:28;Lyft Motion Prediction for Autonomous Vehicles;[];Zarr files and L5kit data for dummies;Python notebook;1962.0;50;;
2020-10-26 10:50:33;;Apache 2.0;https://www.kaggle.com/kool777/ultimate-google-colab-training-batch-size-64;1.0;['pytorch'];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;0.672;0.423;2020-12-13 13:21:28;multiple data sources;['gpu, beginner, exploratory data analysis, +2 moreimage data, python'];Ultimate Google Colab Training [batch size 64];Python notebook;1690.0;38;;
2020-10-03 19:53:33;;Apache 2.0;https://www.kaggle.com/pestipeti/lyft-l5kit-unofficial-fix;0.5;[];['ai', 'rl'];['train', 'predict'];https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;0.697;0.439;2020-12-13 13:21:28;Lyft Motion Prediction for Autonomous Vehicles;['utility script'];Lyft - L5Kit (unofficial fix);Python script;2881.0;46;;
2020-08-28 00:03:26;In this kernel I wanted to set up a very simple benchmark to understand the inference pipeline and the data available to us as well as lay down a very low bar for prediction. A simple but reasonable baseline for this task is to just assume the car will continue with the same heading and speed it is currently traveling at. In this kernel I take the difference between the last two known points and then extrapolate using the last known velocity. This will by no means win the competition but can at least function as a sanity check to see how good the models actually are in comparison to this naive method.;Apache 2.0;https://www.kaggle.com/ryches/lyft-constant-velocity-extrapolation-baseline;0.5;[];['ai', 'nn', 'rl'];['filter', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;0.681;0.449;2020-12-13 13:21:28;multiple data sources;[];Lyft Constant Velocity Extrapolation Baseline;Python notebook;2015.0;52;;
2020-09-09 16:41:20;Raster Size EnsemblingThis notebook serves as 1) a simple demo showing how to submit multi-mode submissions and 2) to talk about ensembling raster sizes. As you can read about here raster_size parameter changes how much of the scene the rasterized image shows: a larger raster size increases the region that the model sees during training. As such, it is an important hyperparameter. Does our model need to see agents from very far away? Or should we force it to focus on agents near it? Why don't we do both? We can train models on different raster sizes and then ensemble their predictions via the multi-mode submission ability of this competition NotesAll the pre-trained models are saved here, so you can just add this dataset to a Kaggle kernel and run inference with it, like I do here to get the different raster size predictions that I am blending. You could also just download the output of this kernel if you want the prediction .csv. Current LB scores:  (300 raster_size, pixel_size .5, 10 history_num_frames, 75000 steps ) - 129.99 (450 raster_size, pixel_size .5, 10 history_num_frames, 25000 steps ) - 135.899 (600 raster_size, pixel_size .2, 10 history_num_frames, 75000 steps ) - 121.555  Defer to Peter's notebook here for further training specific details;Apache 2.0;https://www.kaggle.com/tuckerarrants/lyft-ensembling-raster-sizes;0.5;[];['ai', 'nn', 'ann'];['train', 'model', 'layer', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles;0.689;0.427;2020-12-13 13:21:28;multiple data sources;[];Lyft - Ensembling Raster Sizes;Python notebook;2419.0;40;57.232;55.412
2020-10-04 21:23:05;;Apache 2.0;https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'train', 'model', 'understanding', 'validation data', 'loss', 'label', 'predict', 'rank', 'recommend', 'ground truth'];https://www.kaggle.com/c/m5-forecasting-accuracy;0.826;0.671;2020-12-13 13:22:57;multiple data sources;['beginner, data visualization, exploratory data analysis, +1 moretidyverse'];Back to (predict) the future - Interactive M5 EDA;Rmarkdown script;116027.0;1661;;
2020-04-17 05:43:51;;Apache 2.0;https://www.kaggle.com/kyakovlev/m5-lags-features;0.5;[];['ner', 'ai', 'gbm', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/m5-forecasting-accuracy;0.76;0.56;2020-12-13 13:22:57;multiple data sources;[];M5 - Lags features;Python notebook;13466.0;237;;
2020-03-26 17:40:10;;Apache 2.0;https://www.kaggle.com/kyakovlev/m5-simple-fe;1.0;['sklearn'];['ner', 'ai', 'cv'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/m5-forecasting-accuracy;0.77;0.577;2020-12-13 13:22:57;M5 Forecasting - Accuracy;[];M5 - Simple FE;Python notebook;18052.0;307;;
2020-04-17 12:35:21;;Apache 2.0;https://www.kaggle.com/kyakovlev/m5-three-shades-of-dark-darker-magic;1.0;['pattern', 'xgboost', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'model', 'loss', 'label', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/m5-forecasting-accuracy;0.82;0.618;2020-12-13 13:22:57;multiple data sources;[];M5 - Three shades of Dark: Darker magic;Python notebook;91174.0;609;5.39065;0.47506
2020-05-12 15:20:04;;Apache 2.0;https://www.kaggle.com/kyakovlev/m5-witch-time;0.5;[];['ner', 'ai', 'rl'];['predict'];https://www.kaggle.com/c/m5-forecasting-accuracy;0.758;0.532;2020-12-13 13:22:57;multiple data sources;[];M5 - Witch Time;Python notebook;12988.0;157;5.39065;0.46193
2020-06-17 17:48:52;M5 SetupThis competition is a little different from others. The true labels of the public leaderboard are now revealed. The following is an excerpt from the M5 Competition Guide: After the end of the validation phase, i.e., from June 1, 2020 to 30 June of the same year, the participants will be provided with the actual values of the 28 days of data used for scoring their performance during the validation phase. They will be asked then to re-estimate or adjust (if needed) their forecasting models in order to submit their final forecasts and prediction intervals for the following 28 days, i.e., the data used for the final evaluation of the participants. During this time, there will be no leaderboard, meaning that no feedback will be given to the participants about their score after submitting their forecasts. Thus, although the participants will be free to (re)submit their forecasts any time they wish (a maximum of 5 entries per day), they will not be aware of their absolute, as well as their relative performance. The final ranks of the participants will be made available only at the end of competition, when the test data will be made available. This is done in order for the competition to simulate reality as closely as possible, given that in real life forecasters do not know the future.  So while the public LB on Kaggle will either get infested by scores that use the true labels or Kaggle will freeze the public LB, we now have access to the actual labels and hence can calculate the validation scores (and rank as of 31st May, 2020) at various levels of aggregations ourselves. The weights used in this notebook are the weights for the public LB (validation data). Note that the private LB (evaluation data) uses a different set of weights. A summary of the weights comparison is shared here: https://www.kaggle.com/rohanrao/m5-the-weighing-scale Note that the final private LB ranking will be based on the test data at the end of the competition.;Apache 2.0;https://www.kaggle.com/rohanrao/m5-how-to-get-your-public-lb-score-rank;0.5;[];['ai', 'rl'];['test data', 'train', 'model', 'validation data', 'label', 'predict', 'rank', 'recommend'];https://www.kaggle.com/c/m5-forecasting-accuracy;0.73;0.554;2020-12-13 13:22:57;multiple data sources;[];M5: How to get your public LB score & rank;Python notebook;6124.0;216;;
2020-03-19 04:21:52;æ—¥æœ¬äººã®æ–¹ãŒå§‹ã‚ã‚„ã™ã„ã‚ˆã†ã«ã€æ—¥æœ¬èªžè¨³ã‚„è§£èª¬ã‚’ä»˜ã‘åŠ ãˆãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚ (ã“ã¡ã‚‰ã®notebook)[https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration]ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚ è‡ªåˆ†ãªã‚Šã®åˆ†æžã‚„modelãªã©ã‚’è¿½åŠ ã—ã¦ã„ãäºˆå®šã§ã™ã€‚;Apache 2.0;https://www.kaggle.com/takahiro1127/starter-data-exploration-lstm;1.0;['xgboost', 'lightgbm', 'sklearn', 'tensorflow', 'pattern', 'keras'];['ner', 'ai', 'dl', 'cnn', 'gbm', 'gan', 'rl', 'ml', 'nn', 'ann'];['training data', 'train', 'model', 'epoch', 'validation data', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu'];https://www.kaggle.com/c/m5-forecasting-accuracy;0.752;0.554;2020-12-13 13:22:57;M5 Forecasting - Accuracy;[];ã€æ—¥æœ¬èªžã€‘Starter Data Exploration ã¨ LSTM ;Python notebook;10784.0;217;;
2020-03-09 02:36:07;Introduction;Apache 2.0;https://www.kaggle.com/tarunpaparaju/m5-competition-eda-models;1.0;['statsmodels', 'sklearn', 'pattern'];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['filter', 'machine learning', 'training data', 'train', 'model', 'validation data', 'loss', 'predict'];https://www.kaggle.com/c/m5-forecasting-accuracy;0.789;0.621;2020-12-13 13:22:57;M5 Forecasting - Accuracy;['data visualization, exploratory data analysis, economics, +1 moresignal processing'];M5 Competition : EDA + Models ðŸ“ˆ ;Python notebook;32306.0;643;;
2020-06-09 00:18:28;Versions unpdates:Version 14: changed the object so that scores_df is an attribute that shows scores for all series and all quantiles. I also showed a few examples of how to visualize these scores, but I didn't even scratch the surface into other things you can do with this dataframe of scores, namely compare to other models and find which performs best on which series/quantile combinations. version 11: added an easy to use WSPLEvaluator object to my utility script. You can find this at the end of the notebook.;Apache 2.0;https://www.kaggle.com/chrisrichardmiles/m5u-wsplevaluator-weighted-scaled-pinball-loss;0.5;[];['ner', 'ai', 'ml', 'gan'];['train', 'model', 'epoch', 'loss', 'label', 'predict', 'ground truth'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.615;0.352;2020-12-13 13:24:02;multiple data sources;[];M5U - WSPLEvaluator - weighted scaled pinball loss;Python notebook;579.0;17;;
2020-10-04 21:23:05;;Apache 2.0;https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'train', 'model', 'understanding', 'validation data', 'loss', 'label', 'predict', 'rank', 'recommend', 'ground truth'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.826;0.671;2020-12-13 13:24:02;multiple data sources;['beginner, data visualization, exploratory data analysis, +1 moretidyverse'];Back to (predict) the future - Interactive M5 EDA;Rmarkdown script;116028.0;1661;;
2020-06-30 01:48:39;Variables to help with aggregation;Apache 2.0;https://www.kaggle.com/kamalnaithani/m5uncertainity-score;1.0;['tensorflow', 'lightgbm', 'sklearn', 'keras'];['ner', 'ai', 'gbm', 'rl', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.699;0.39;2020-12-13 13:24:02;multiple data sources;[];M5Uncertainity Score;Python notebook;3022.0;26;0.18120;0.11802
2020-04-09 07:14:18;This notebook will help you switching easily from M5 accuracy prediction to uncertainty . We just use a multiplier scheme under the hood.;Apache 2.0;https://www.kaggle.com/kneroma/from-point-to-uncertainty-prediction;0.5;[];['ai'];['train', 'predict'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.742;0.502;2020-12-13 13:24:02;multiple data sources;[];From point to uncertainty prediction;Python notebook;8338.0;104;0.26121;0.17921
2020-04-04 19:44:34;;Apache 2.0;https://www.kaggle.com/konradb/beat-the-benchmark-snaive;0.5;[];['ai', 'nn'];['train', 'model', 'predict'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.673;0.379;2020-12-13 13:24:02;M5 Forecasting - Uncertainty;[];Beat the benchmark: snaive;R script;1739.0;23;0.26434;0.25345
2020-04-05 06:14:26;Credit to Konrad Banachewicz.BTW, the magic is risky for the private lb.;Apache 2.0;https://www.kaggle.com/nxrprime/coefficient-multiplier;0.2;[];['ai'];[];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.725;0.393;2020-12-13 13:24:02;multiple data sources;[];Coefficient Multiplier;Python notebook;5523.0;27;0.26112;0.24832
2020-03-23 22:17:23;;Apache 2.0;https://www.kaggle.com/robertburbidge/lightgbm-poisson-w-scaled-pinball-loss;1.0;['tensorflow', 'lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nn'];['filter', 'test data', 'regression', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.712;0.413;2020-12-13 13:24:02;M5 Forecasting - Uncertainty;[];LightGBM Poisson w/ Scaled Pinball Loss;Python notebook;4063.0;34;0.84206;0.86069
2020-10-26 10:40:22;M5 Weights The evaluation metric of this competition is quite unique in the way that the weights of observations is dependent on historic values of the time series and hence changes for every validation / test dataset. This means that the public LB and the private LB have different weights. Now that the validation data is released, we can calculate the weights used for the test data and also compare the weights between the public LB and private LB. This notebook demonstrates some of the changes / differences between the weights and hopefully it can be used to ensure models don't overfit to the public LB. The final weights are also saved in the output of the notebook. I've also shared a notebook of how you can deep dive into analyzing your submission on the public LB: https://www.kaggle.com/rohanrao/m5-anatomy-of-the-public-lb;Apache 2.0;https://www.kaggle.com/rohanrao/m5-the-weighing-scale;0.5;[];['ai'];['test data', 'train', 'model', 'validation data', 'label'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.705;0.475;2020-12-13 13:24:02;multiple data sources;[];M5: The Weighing Scale;Python notebook;3425.0;72;;
2020-04-21 16:31:16;If you like it, please upvote:);Apache 2.0;https://www.kaggle.com/szmnkrisz97/point-to-uncertainty-different-ranges-per-level;0.5;[];['ner', 'ai'];['train', 'model', 'predict'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.735;0.467;2020-12-13 13:24:02;multiple data sources;[];Point to uncertainty - different ranges per level;Python notebook;7038.0;65;0.22893;0.15905
2020-05-26 22:21:13;Simple Historical Quantiles;Apache 2.0;https://www.kaggle.com/szmnkrisz97/simple-quantiles-of-training-set;0.5;[];['ai', 'rl', 'gan'];['train', 'model', 'predict'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.702;0.4;2020-12-13 13:24:02;M5 Forecasting - Uncertainty;[];Simple Quantiles of Training Set;Python notebook;3208.0;29;0.27048;0.25705
2020-07-01 04:04:57;Modeling;Apache 2.0;https://www.kaggle.com/ulrich07/parallel-linear-regression-silver-medal-v1;1.0;['sklearn'];['ner', 'ai', 'ml'];['regression', 'model', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.687;0.416;2020-12-13 13:24:02;multiple data sources;['linear regression, statistical analysis'];parallel-linear-regression (silver medal V1);Python notebook;2283.0;35;0.17092;0.15071
2020-06-27 16:12:38;This notebook is based on this. This is for Uncertainty submission from Accuracy submission.;Apache 2.0;https://www.kaggle.com/yujiariyasu/point-to-uncertainty-for-private;0.5;[];['ai'];['train', 'model', 'predict'];https://www.kaggle.com/c/m5-forecasting-uncertainty;0.664;0.411;2020-12-13 13:24:02;multiple data sources;[];Point to uncertainty for private;Python notebook;1423.0;33;0.23132;0.14399
2015-07-01 10:28:09;;Apache 2.0;https://www.kaggle.com/abhishek/beating-the-benchmark-v1-0;1.0;['pattern', 'xgboost', 'sklearn'];['nlp', 'ai', 'nn', 'ner'];['test data', 'random forest', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.734;0.346;2020-12-13 13:34:47;Machinery Tube Pricing;[];Beating the Benchmark v1.0;Python script;6767.0;16;0.37389;0.38863
2015-07-02 18:09:19;;Apache 2.0;https://www.kaggle.com/ademyttenaere/0-2748-with-rf-and-log-transformation;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/machinery-tube-pricing;0.778;0.54;2020-12-13 13:34:47;Machinery Tube Pricing;[];0.2748 with RF and log transformation;R script;23102.0;177;0.26842;0.26447
2015-07-04 00:19:01;;Apache 2.0;https://www.kaggle.com/ademyttenaere/build-complete-train-and-test-db;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'test data', 'deep learning'];https://www.kaggle.com/c/machinery-tube-pricing;0.726;0.421;2020-12-13 13:34:47;Machinery Tube Pricing;[];Build complete train and test db;R script;5638.0;37;;
2015-07-07 18:56:04;;Apache 2.0;https://www.kaggle.com/andrewmatteson/beating-the-benchmark-v1-0;1.0;['xgboost', 'sklearn'];['nlp', 'ai', 'nn', 'ner'];['test data', 'random forest', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.684;0.236;2020-12-13 13:34:47;Machinery Tube Pricing;[];Beating the Benchmark v1.1 ~.28;Python script;2167.0;5;0.28876;0.29321
2015-07-01 00:22:37;;Apache 2.0;https://www.kaggle.com/dchudz/head-all-the-files;1.0;['pattern'];['ner', 'ai', 'dl', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/machinery-tube-pricing;0.665;0.253;2020-12-13 13:34:47;Machinery Tube Pricing;[];head all the files;Python script;1467.0;6;;
2015-07-01 20:03:45;;Apache 2.0;https://www.kaggle.com/euclides/files-info-python;1.0;['pattern'];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/machinery-tube-pricing;0.658;0.236;2020-12-13 13:34:47;Machinery Tube Pricing;[];Files Info - Python;Python script;1271.0;5;;
2016-01-02 22:15:43;;Apache 2.0;https://www.kaggle.com/fchollet/keras-starter-code;1.0;['keras', 'sklearn', 'theano'];['nlp', 'ai', 'nn', 'ner'];['test data', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.755;0.413;2020-12-13 13:34:47;Machinery Tube Pricing;[];Keras starter code;Python script;11807.0;34;0.47443;0.47513
2015-07-22 07:39:35;;Apache 2.0;https://www.kaggle.com/jkapila/0-24-with-xgboost-in-r;1.0;['xgboost'];['ner', 'ai', 'nlp', 'nn', 'ann'];['generation', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.757;0.334;2020-12-13 13:34:47;Machinery Tube Pricing;[];0.24 with xgboost in R;R script;12458.0;14;0.00000;0.00000
2015-07-05 04:32:18;;Apache 2.0;https://www.kaggle.com/jpopham91/rmlse-vectorized;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['classification', 'deep learning'];https://www.kaggle.com/c/machinery-tube-pricing;0.735;0.375;2020-12-13 13:34:47;Machinery Tube Pricing;[];RMLSE (Vectorized);Python script;7072.0;22;;
2015-07-15 21:21:41;;Apache 2.0;https://www.kaggle.com/kumareshd/xgbooost;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'nlp', 'nn', 'ann'];['test data', 'random forest', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.729;0.292;2020-12-13 13:34:47;Machinery Tube Pricing;[];xgbooost;Python script;5961.0;9;;
2015-08-16 02:58:06;;Apache 2.0;https://www.kaggle.com/kumareshd/xgbooost-222;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'nlp', 'nn', 'ann'];['test data', 'random forest', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.728;0.281;2020-12-13 13:34:47;Machinery Tube Pricing;[];xgbooost 222;Python script;5942.0;8;0.22407;0.22857
2015-07-02 00:20:47;;Apache 2.0;https://www.kaggle.com/marknagelberg/rmsle-function;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/machinery-tube-pricing;0.782;0.456;2020-12-13 13:34:47;Machinery Tube Pricing;[];RMSLE function;Python script;25652.0;57;;
2015-07-14 22:38:27;;Apache 2.0;https://www.kaggle.com/nickel/bracket-pricing-variable-and-fixed-cost;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ann'];['regression', 'train', 'model', 'deep learning', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.673;0.268;2020-12-13 13:34:47;Machinery Tube Pricing;[];bracket pricing variable and fixed cost;Python script;1739.0;7;;
2015-07-08 21:48:35;;Apache 2.0;https://www.kaggle.com/nickel/tubes-processing;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/machinery-tube-pricing;0.685;0.281;2020-12-13 13:34:47;Machinery Tube Pricing;[];tubes processing;Python script;2216.0;8;0.00000;0.00000
2015-08-07 17:25:51;;Apache 2.0;https://www.kaggle.com/sebda1/rf-full-fe-fselection;1.0;['pattern'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ann'];['test data', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.673;0.236;2020-12-13 13:34:47;Machinery Tube Pricing;[];RF Full FE FSelection;R script;1725.0;5;0.25437;0.25504
2015-07-09 13:29:57;;Apache 2.0;https://www.kaggle.com/tanayz/python-implementation-of-r-benchmark;1.0;['pattern', 'sklearn'];['ner', 'ai', 'dl', 'ml', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/machinery-tube-pricing;0.678;0.302;2020-12-13 13:34:47;Machinery Tube Pricing;[];Python implementation of R benchmark;Python script;1909.0;10;0.50161;0.50438
2015-07-03 22:44:26;;Apache 2.0;https://www.kaggle.com/tenaciousterrier/parallel-rf-adapted-from-arnaud;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['predict', 'train', 'deep learning', 'random forest', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.692;0.292;2020-12-13 13:34:47;Machinery Tube Pricing;[];Parallel RF (adapted from Arnaud);R script;2593.0;9;0.29123;0.28784
2015-07-11 01:25:20;;Apache 2.0;https://www.kaggle.com/titericz/test-beating-the-benchmark-fork;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'nlp', 'nn', 'ann'];['test data', 'random forest', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.625;0.214;2020-12-13 13:34:47;Machinery Tube Pricing;[];"test (""Beating the benchmark"" fork)";Python script;689.0;4;0.23936;0.24385
2015-07-12 01:11:56;;Apache 2.0;https://www.kaggle.com/titericz/xgbooost-new-new-new-new-new;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'nlp', 'nn', 'ann'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.712;0.311;2020-12-13 13:34:47;Machinery Tube Pricing;[];xgbooost new new new new new;Python script;4043.0;11;0.28243;0.28762
2015-07-15 22:09:42;;Apache 2.0;https://www.kaggle.com/yifanxie/training-set-split-for-cross-validation;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'nlp', 'nn', 'ann'];['training data', 'random forest', 'regression', 'train', 'model', 'deep learning', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/machinery-tube-pricing;0.742;0.334;2020-12-13 13:34:47;Machinery Tube Pricing;[];Training set split for cross validation;Python script;8383.0;14;0.00000;0.00000
2020-03-01 16:31:27;"Heatmap All March Machine Learning Mania 2015 SubmissionsThe plot format is introduced here, along with generated heatmaps for all the 2017 entries. To recap: it is easiest to read the row for each team, where white means 50:50, red indicates probably winning, blue means probably losing, the deeper the color, the higher the probability. In this Notebook the play-in matches are removed, leaving a 64x64 grid showing the four 16x16 regions, and all possible tournament matches. The outline is:  list all csv submissions for each: load; score; save heatmap display selection zip all 613 heatmaps for download";Apache 2.0;https://www.kaggle.com/jtrotman/beautiful-mania-2015;0.5;[];['ner', 'ai', 'dl', 'gan', 'ml', 'nn', 'ann'];['machine learning', 'loss', 'label', 'predict', 'rank', 'ground truth'];https://www.kaggle.com/c/march-machine-learning-mania-2015;0.512;0.099;2020-12-13 13:34:55;multiple data sources;['data visualization, basketball'];Beautiful Mania 2015;Python notebook;119.0;1;;
2016-03-12 13:14:59;;Apache 2.0;https://www.kaggle.com/anokas/extree;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'train', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.582;0.188;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];EXTREE ;Python script;337.0;3;0.00000;0.00000
2016-03-12 01:05:36;;Apache 2.0;https://www.kaggle.com/anokas/extree-optimised;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'train', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.638;0.214;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];"EXTREE optimised ;)";Python script;867.0;4;0.00000;0.00000
2016-03-11 21:12:41;;Apache 2.0;https://www.kaggle.com/anokas/extree-without-cheating;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'train', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.567;0.188;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];EXTREE without cheating;Python script;266.0;3;;
2016-02-12 01:21:40;;Apache 2.0;https://www.kaggle.com/anokas/seed-benchmark-optimised;1.0;['pattern'];['nlp', 'ai', 'nn', 'ner'];['deep learning', 'classification', 'machine learning', 'predict'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.682;0.302;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Seed Benchmark optimised (0.575882);R script;2087.0;10;0.00000;0.00000
2016-02-21 20:52:00;;Apache 2.0;https://www.kaggle.com/broome/summarize-by-season;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['filter', 'machine learning', 'train', 'model', 'deep learning', 'loss', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.636;0.214;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Summarize by season;R notebook;841.0;4;;
2016-04-05 18:45:37;;Apache 2.0;https://www.kaggle.com/dmytrolystopad/elo-rating-try;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'train', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.721;0.292;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Elo rating_try;Python script;4957.0;9;;
2016-03-04 19:17:55;;Apache 2.0;https://www.kaggle.com/evgenyeltyshev/genetic-programming-1;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'train', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.615;0.188;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Genetic Programming 1;Python script;581.0;3;0.00000;0.00000
2016-03-05 02:12:06;;Apache 2.0;https://www.kaggle.com/jaredcross/beating-seed-benchmark-without-leakage;0.5;[];['ai', 'dl'];['filter', 'predict', 'train', 'model', 'loss'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.643;0.253;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Beating Seed Benchmark without Leakage;Rmarkdown script;960.0;6;;
2016-02-15 04:45:11;;Apache 2.0;https://www.kaggle.com/jaredcross/getting-started;0.5;[];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.76;0.418;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];getting started;Rmarkdown script;13666.0;36;0.00000;0.00000
2016-02-26 04:27:42;;Apache 2.0;https://www.kaggle.com/jaredcross/log5-logistic-regression-and-bradley-te;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'model', 'loss', 'logistic regression', 'predict'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.72;0.292;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Log5, the Logit Link and Bradley-Terry;Rmarkdown script;4843.0;9;0.00000;0.00000
2016-02-12 17:53:59;;Apache 2.0;https://www.kaggle.com/maccam912/winner-by-seed;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'machine learning', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.685;0.281;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Winner by Seed;Python notebook;2220.0;8;0.00000;0.00000
2016-02-12 14:37:24;;Apache 2.0;https://www.kaggle.com/raddar/seed-benchmark;1.0;['pattern'];['ner', 'ai', 'nlp', 'nn', 'ann'];['machine learning', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.692;0.292;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Seed Benchmark;R script;2541.0;9;0.00000;0.00000
2016-02-12 15:15:19;;Apache 2.0;https://www.kaggle.com/raddar/xgboost-regular-season-averages;1.0;['pattern', 'xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['filter', 'machine learning', 'training data', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.695;0.236;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];xgboost regular season averages;R script;2767.0;5;0.00000;0.00000
2016-02-20 14:36:51;;Apache 2.0;https://www.kaggle.com/ralston3/ncaam-exploratory-analysis;0.5;[];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['machine learning', 'deep learning', 'label', 'loss', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.71;0.346;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];ncaam_exploratory_analysis;Python notebook;3835.0;16;;
2016-04-01 17:52:06;;Apache 2.0;https://www.kaggle.com/raym26/my-bracket;0.5;[];['ner', 'ai', 'dl', 'gan', 'gbm', 'nlp', 'nn'];['filter', 'machine learning', 'random forest', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.62;0.188;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];my bracket;R script;631.0;3;;
2016-03-01 15:49:49;;Apache 2.0;https://www.kaggle.com/scirpus/genetic-programming-1;1.0;['sklearn'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['machine learning', 'generation', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.734;0.379;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Genetic Programming;Python script;6748.0;23;0.00000;0.00000
2016-02-16 01:04:54;;Apache 2.0;https://www.kaggle.com/signochastic/elo-rating-try;0.5;[];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'predict', 'train', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.692;0.188;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Elo rating_try;Python script;2564.0;3;0.00000;0.00000
2016-02-19 03:09:57;;Apache 2.0;https://www.kaggle.com/wacaxx/elo-benchmark-playerratings-in-r;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn'];['machine learning', 'regression', 'deep learning', 'layer', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.745;0.302;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Elo Benchmark - PlayerRatings in R;R script;9162.0;10;0.00000;0.00000
2016-02-11 21:11:45;;Apache 2.0;https://www.kaggle.com/wacaxx/seed-benchmark-data-table-in-r;1.0;['pattern'];['nlp', 'ai', 'nn', 'ner'];['machine learning', 'understanding', 'deep learning', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.661;0.214;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Seed Benchmark - data.table in R;R script;1354.0;4;0.00000;0.00000
2016-02-15 21:04:20;;Apache 2.0;https://www.kaggle.com/walterhan/scrape-kenpom-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['machine learning', 'deep learning', 'loss', 'rank', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2016;0.692;0.281;2020-12-13 13:44:45;March Machine Learning Mania 2016;[];Scrape Kenpom Data;Python script;2592.0;8;;
2017-02-18 08:50:28;;Apache 2.0;https://www.kaggle.com/adamwlev/counts-of-each-seed-that-have-reached-each-round;0.0;[];[];[];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.573;0.188;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];counts of each seed that have reached each round;Python notebook;290.0;3;;
2017-02-22 06:41:45;;Apache 2.0;https://www.kaggle.com/ajniggles/logistic-regression-and-game-round-calculator;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn'];['training data', 'test data', 'regression', 'train', 'fitting', 'model', 'loss', 'logistic regression', 'predict', 'rank'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.732;0.379;2020-12-13 13:48:32;March Machine Learning Mania 2017;[];Logistic Regression and Game Round Calculator;Rmarkdown script;6463.0;23;0.00000;0.00000
2017-02-07 20:04:21;Study of regular and tourney compact datasets;Apache 2.0;https://www.kaggle.com/arti32lehtonen/study-of-regular-and-tourney-compact-datasets;0.5;[];['ai', 'nn'];['label', 'loss'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.635;0.214;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];Study of regular and tourney compact datasets;Python notebook;829.0;4;;
2017-02-08 19:39:46;;Apache 2.0;https://www.kaggle.com/caioaao/load-data-to-sqlite;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'machine learning', 'deep learning'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.663;0.188;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];Load data to sqlite;Python script;1399.0;3;;
2017-02-15 02:34:22;As a NCAA outsider, I am super interested in a question that are there safe bets between two teams looking at their historical confrontations. For example, team A has been beating team B since day one. Beyond that, the percentage of team A beating team B historically would also be a fairly good feature for modelling. It should be useful as a prior for Bayes theorem models.;Apache 2.0;https://www.kaggle.com/enfeizhan/history-proved-easy-bets;0.5;[];['dl', 'ai', 'nn', 'rl'];['model'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.617;0.188;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];History Proved Easy Bets;Python notebook;601.0;3;;
2017-02-10 20:39:47;Simple Linear Model Using Seed Difference in RThis is another example of using the difference between tournament seeds to predict tournament wins.  This was inspired by Kasper P. Lauritzen's kernel for this year's tournament and Jared Cross's kernel for last year's tournament.  I will be using R to produce a simple linear model using the lm function.;Apache 2.0;https://www.kaggle.com/freeradical/simple-linear-model-using-seed-difference-in-r;0.5;[];['ai', 'dl', 'ml'];['filter', 'predict', 'train', 'model', 'loss'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.707;0.311;2020-12-13 13:48:32;March Machine Learning Mania 2017;[];Simple Linear Model Using Seed Difference in R;R notebook;3603.0;11;;
2017-02-11 03:03:00;This notebook can help you aggregate regular season per-game averages, which may help you when generating training data.;Apache 2.0;https://www.kaggle.com/hstove/calculating-regular-season-averages;0.5;[];['ner', 'ai', 'nn'];['train', 'training data'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.591;0.188;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];Calculating Regular Season Averages;Python notebook;385.0;3;;
2017-02-03 10:40:47;;Apache 2.0;https://www.kaggle.com/kplauritzen/notebookde27b18258;1.0;['sklearn'];['ai', 'nn', 'ann', 'cv'];['training data', 'test data', 'regression', 'train', 'model', 'loss', 'label', 'logistic regression', 'predict'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.729;0.375;2020-12-13 13:48:32;March Machine Learning Mania 2017;[];Logistic Regression on Tournament seeds;Python notebook;6061.0;22;;
2017-02-02 12:07:40;;Apache 2.0;https://www.kaggle.com/mlennox562/regular-season-compact-results-visualisation;0.5;[];['ner', 'ai', 'nn', 'rl'];['filter'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.676;0.268;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];Regular Season Compact Results Visualisation;Python notebook;1822.0;7;;
2017-02-04 23:49:43;;Apache 2.0;https://www.kaggle.com/opanichev/lb-0-6179-simple-btb;1.0;['sklearn'];['ner', 'ai', 'nlp', 'nn', 'ml'];['machine learning', 'predict', 'train', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.653;0.236;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];[LB 0.6179] Simple BTB;Python script;1152.0;5;;
2017-04-05 18:52:29;;Apache 2.0;https://www.kaggle.com/raddar/19-place-ml-solution;1.0;['pattern', 'xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'machine learning', 'regression', 'train', 'model', 'deep learning', 'loss', 'label', 'logistic regression', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.699;0.334;2020-12-13 13:48:32;March Machine Learning Mania 2017;[];19 place ML solution;R script;3001.0;14;;
2017-03-16 18:18:07;;Apache 2.0;https://www.kaggle.com/raddar/shiny-app-to-lookup-your-predictions-for-a-game;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.676;0.319;2020-12-13 13:48:32;March Machine Learning Mania 2017;[];shiny app to lookup your predictions for a game;R script;1839.0;12;;
2017-02-03 13:02:53;;Apache 2.0;https://www.kaggle.com/scirpus/genetic-programming-5;1.0;['sklearn'];['ner', 'ai', 'dl', 'nlp', 'nn'];['machine learning', 'generation', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.696;0.281;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];Genetic Programming;Python script;2776.0;8;0.00000;0.00000
2017-02-22 18:08:57;;Apache 2.0;https://www.kaggle.com/vijayakapoor/r-code-with-logistic-regression-glm;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['machine learning', 'training data', 'regression', 'train', 'model', 'deep learning', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.733;0.236;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];R code with logistic regression, glm;R script;6692.0;5;0.00000;0.00000
2017-02-06 02:31:56;;Apache 2.0;https://www.kaggle.com/wacaxx/understanding-elo;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['machine learning', 'model', 'layer', 'loss', 'rank', 'understanding'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.709;0.311;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];Understanding Elo;Rmarkdown script;3710.0;11;;
2017-03-04 22:53:56;There are a lot of complex rules regarding how teams are selected and seeded for the NCAA basketball tournament.  I thought it would be interesting to visualize the progress of seeds through the tourney.;Apache 2.0;https://www.kaggle.com/willieliao/survival-of-seeds-as-of-2016;0.5;[];['dl', 'ai', 'nn', 'gan'];['label'];https://www.kaggle.com/c/march-machine-learning-mania-2017;0.659;0.281;2020-12-13 13:48:33;March Machine Learning Mania 2017;[];Survival of Seeds as of 2016;R notebook;1312.0;8;;
2020-10-02 11:11:41;;Apache 2.0;https://www.kaggle.com/anshumoudgil/basketball-2020-vectors-feature-engg-strategy;1.0;['pattern'];['ner', 'ai', 'gan', 'rl', 'nn', 'ann'];['rank', 'classification', 'layer'];https://www.kaggle.com/c/march-madness-analytics-2020;0.625;0.387;2020-12-13 13:49:58;multiple data sources;['data visualization, feature engineering, sports, +1 morecategorical data'];Basketball 2020: vectors-feature engg.-Strategy;Rmarkdown script;685.0;25;;
2020-12-06 06:18:58;;Apache 2.0;https://www.kaggle.com/anshumoudgil/games-strategy-ideas-from-eda;0.5;[];['ner', 'ai', 'nn'];['layer'];https://www.kaggle.com/c/march-madness-analytics-2020;0.697;0.427;2020-12-13 13:49:58;multiple data sources;['data visualization, exploratory data analysis, feature engineering, +2 moresports, categorical data'];Games - Strategy ideas from EDA;Rmarkdown script;2837.0;40;;
2020-05-29 22:20:39;"Web of the ""Madness"": Network Approach to Basketball Analytics";Apache 2.0;https://www.kaggle.com/artvolgin/network-approach-to-basketball-analytics;0.5;[];['ner', 'ai', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['linear regression', 'filter', 'regression', 'model', 'layer', 'clustering', 'loss', 'label', 'predict', 'rank', 'understanding', 'labeled', 'bayesian'];https://www.kaggle.com/c/march-madness-analytics-2020;0.6;0.352;2020-12-13 13:49:58;multiple data sources;['basketball'];Network Approach to Basketball Analytics;R notebook;451.0;17;;
2020-04-30 15:19:02;Does The Best Team Win?Google Cloud & NCAA March Madness Analytics Report by Bethany Connolly;Apache 2.0;https://www.kaggle.com/bethanyconnolly/does-the-best-team-win;0.5;[];['ai', 'nlu', 'dl', 'rl', 'nn', 'ann'];['rank', 'loss', 'label', 'predict'];https://www.kaggle.com/c/march-madness-analytics-2020;0.557;0.311;2020-12-13 13:49:58;Google Cloud & NCAAÂ® March Madness Analytics;[];Does The Best Team Win?;Python notebook;227.0;11;;
2020-03-08 21:09:25;;Apache 2.0;https://www.kaggle.com/headsortails/jump-shot-to-conclusions-march-madness-eda;1.0;['pattern', 'xgboost'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'fitting', 'model', 'layer', 'loss', 'label', 'predict', 'rank', 'understanding', 'classification', 'ground truth'];https://www.kaggle.com/c/march-madness-analytics-2020;0.764;0.556;2020-12-13 13:49:58;multiple data sources;['beginner, data visualization, exploratory data analysis'];Jump Shot to Conclusions - March Madness EDA;Rmarkdown script;15185.0;223;;
2020-04-13 11:04:28;;Apache 2.0;https://www.kaggle.com/jaseziv83/applying-pythagorean-expectation-to-major-sports;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'regression', 'fitting', 'model', 'layer', 'loss', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/march-madness-analytics-2020;0.721;0.435;2020-12-13 13:49:58;multiple data sources;['data visualization, exploratory data analysis, regression'];Applying Pythagorean Expectation to Major Sports;Rmarkdown script;5024.0;44;;
2020-03-14 22:49:02;;Apache 2.0;https://www.kaggle.com/jaseziv83/moreyball-in-the-college-game-a-full-ncaa-eda;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['filter', 'model', 'layer', 'loss', 'label', 'predict', 'rank'];https://www.kaggle.com/c/march-madness-analytics-2020;0.707;0.477;2020-12-13 13:49:58;multiple data sources;['beginner, data visualization, exploratory data analysis'];Moreyball in the College Game... A Full NCAA EDA;Rmarkdown script;3589.0;74;;
2020-02-25 16:45:28;;Apache 2.0;https://www.kaggle.com/nxrprime/right-left-shoot-march-madness-eda-and-analysis;1.0;['xgboost', 'lightgbm'];['ner', 'ai', 'gan', 'gbm', 'cv', 'rl', 'nn', 'ml'];['filter', 'regression', 'layer', 'loss', 'label', 'predict', 'rank'];https://www.kaggle.com/c/march-madness-analytics-2020;0.652;0.334;2020-12-13 13:49:58;multiple data sources;[];Right, Left, Shoot: March Madness EDA and Analysis;Rmarkdown script;1139.0;14;;
2020-03-28 15:42:18;2020 March MadnessIn this notebook I explore the 2020 Men's and Women's NCAA basketball data. Hopefully you find the analysis and code helpful. Feel free to use any of the helper functions in your code but please reference this as the original source.;Apache 2.0;https://www.kaggle.com/robikscube/2020-march-madness-data-first-look-eda;0.5;[];['ner', 'ai', 'gan', 'rl', 'nn', 'ml'];['layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/march-madness-analytics-2020;0.765;0.565;2020-12-13 13:49:58;multiple data sources;[];ðŸ€ 2020 March Madness Data - First Look EDA;Python notebook;15522.0;255;;
2020-02-27 06:18:36;"Plotting The CourtKaggle and the NCAA have provided event day for seasons 2015-2020. In this notebook I provide some helper functions for plotting these events on a ""court"" using python. Please feel free to use these plots in your code, just reference this kernel when you do so.";Apache 2.0;https://www.kaggle.com/robikscube/ncaa-basketball-court-plot-helper-functions;0.5;[];['ner', 'ai', 'nn'];['layer', 'label'];https://www.kaggle.com/c/march-madness-analytics-2020;0.681;0.379;2020-12-13 13:49:58;multiple data sources;[];ðŸŽŸï¸ NCAA Basketball Court Plot Helper Functions;Python notebook;2028.0;23;;
2020-02-28 11:48:47;;Apache 2.0;https://www.kaggle.com/shettysaanvi/jump-shot-to-conclusions-sanvi-march-madness-eda;1.0;['pattern', 'xgboost'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'fitting', 'model', 'layer', 'loss', 'label', 'predict', 'rank', 'understanding', 'classification', 'ground truth'];https://www.kaggle.com/c/march-madness-analytics-2020;0.613;0.327;2020-12-13 13:49:58;multiple data sources;[];Jump Shot to Conclusions - sanvi March Madness EDA;Rmarkdown script;562.0;13;;
2020-02-15 10:34:20;Introductionhe below analysis will be conducted on both the regular season and tournament statistics. It will begin by taking a high level view at the NCAA championships - who has won them and how. It will analyse pre-tournament seedings and will also look at some of the data contained in the Massey Ordinal data, specifically Pomeroy and Sagarin rankings, and will try to analyse what a different ranking means to a teamâ€™s performance. Then it will move on to analyse the averages per regular season of the teams and then take those season averages of each team and plot their correlation with wins. The analysis will then included a detailed look at Advanced Metrics and analyse their relationship with team performance, both at a per game level, and by taking a look at the overall season performance. It will highlight the performance of the champion team in each season with regards to these metrics. This section will show the importance of some of these advanced metrics, and how they can help explain basketball games better than the traditional box score statistics. It will continue highlighting the champion teamâ€™s performance with regards to tradition box score stats for the season. The analysis will then move on to comparing the differences in team stats between regular season play and tournament play to see whether the tournament games yield a different style of play. Finally, per game statistics will be analysed for 2014 - 2018 and compared between winning and losing teams, and also looks at if there is a difference between winning and losing in both tournament and regular season games. I will build upon this kernel throughout the competition. Feel free to add in any suggestions. If you like the analysis, an upvote would also be greatly appreciated! Update: My model was very strong on Wofford - it even has them progressing to the final four! I couldnâ€™t understand why, so I have added in some analysis on 2019 data and visualised Woffordâ€™s advanced stats for 2019 and compared theirs to those of past winners. Hope you enjoy! Update 2: I have included an analysis of the upcoming Final Four Games. I hope itâ€™s helpful to you!;Apache 2.0;https://www.kaggle.com/tayyabali55/data-exploration-for-men-s-ncaa-in-detail;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['rank', 'model', 'label', 'layer'];https://www.kaggle.com/c/march-madness-analytics-2020;0.608;0.34;2020-12-13 13:49:58;Google Cloud & NCAAÂ® March Madness Analytics;['data visualization, exploratory data analysis'];ðŸ€Data Exploration for Men's NCAA in Detail;Python notebook;512.0;15;;
2020-02-23 15:09:57;NCAAÂ® March Madness: Exploratory Analysis;Apache 2.0;https://www.kaggle.com/vikassingh1996/ncaa-march-madness-exploratory-analysis-fe;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['predict', 'model', 'label', 'loss', 'rank', 'understanding'];https://www.kaggle.com/c/march-madness-analytics-2020;0.617;0.302;2020-12-13 13:49:58;multiple data sources;['beginner, exploratory data analysis, feature engineering, +1 moresports'];ðŸ€NCAAÂ® March Madness: Exploratory Analysis + FE;Python notebook;599.0;10;;
2016-10-04 21:39:17;;Apache 2.0;https://www.kaggle.com/alijs1/experiments-with-spectrograms;0.5;[];['ner', 'ai', 'cnn', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.653;0.214;2020-12-13 13:55:26;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Experiments with spectrograms;Python script;1151.0;4;;
2016-09-13 22:02:03;This is my implementation of the CV procedure as I understand it should be based on the forum discussion. The idea is to make sure that all 6 clips from a particular hour are either all in train or all in test set. The script expects input data a pandas dataframe with the following columns:  patient (patient number i.e. 1, 2 or 3) segment (based on the filename) outcome (the class i.e. 0 or 1 for interictal and preictal) sequence (the sequence number found in the source file) all other features generated for each file;Apache 2.0;https://www.kaggle.com/asterios/proper-cross-validation;0.5;[];['ner', 'ai', 'cv'];['train', 'regression', 'predict'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.715;0.319;2020-12-13 13:55:25;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Proper Cross-Validation;Python notebook;4272.0;12;;
2016-09-09 15:40:07;;Apache 2.0;https://www.kaggle.com/avilesmarcel/open-mat-in-python-pandas-dataframe;0.5;[];['dl', 'ai', 'nn', 'ann'];['train'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.763;0.253;2020-12-13 13:55:26;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];open mat in python pandas dataframe;Python notebook;14769.0;6;;
2016-10-21 00:12:01;;Apache 2.0;https://www.kaggle.com/bzamecnik/brain-sounds;1.0;['pattern', 'sklearn'];['ner', 'ai', 'nlp', 'nn', 'ann'];['predict', 'train', 'deep learning', 'layer', 'loss', 'classification'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.688;0.292;2020-12-13 13:55:26;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Brain sounds;Python script;2366.0;9;;
2016-10-24 05:39:05;https://www.kaggle.com/c/melbourne-university-seizure-prediction/forums/t/24683/another-data-corruption;Apache 2.0;https://www.kaggle.com/changgyu/another-data-corruption;0.5;[];['ai'];['train', 'predict'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.671;0.236;2020-12-13 13:55:26;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Another data corruption?;Python notebook;1658.0;5;;
2016-10-02 00:26:43;This is the direct translation of the Matlab getting started code shared by the organizers.  This feature extractor was used as part of a winning solution in a past competition. EDIT: I have fixed the mistakes in the original Matlab code in this new version. NOTE: I just tried to translate it from the Matlab version as close as possible.              Use at your own risk. Correctness is not guaranteed and there might be bugs in              this translation and/or the original Matalb version. So, debug very well before              incorporating it to your pipeline. DISCLAIMER: I still haven't tried it with my training pipeline so I am not sure for correctness yet. GIVE BACK: Please, consider to fork and contribute back your feature additions and/or corrections.;Apache 2.0;https://www.kaggle.com/deepcnn/feature-extractor-matlab2python-translated;0.5;[];['ai', 'nn', 'ann', 'gan'];['train', 'epoch'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.751;0.44;2020-12-13 13:55:25;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Feature Extractor Matlab2python translated;Python notebook;10739.0;47;;
2016-11-16 18:09:03;Spectrogram pairs for visual comparison of 1s and 0 pairs side by side for all the 16 channels;Apache 2.0;https://www.kaggle.com/deepcnn/spectrogram-pairs;0.5;[];['ai', 'nn', 'ann', 'rl'];['train'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.689;0.302;2020-12-13 13:55:25;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Spectrogram Pairs;Python notebook;2416.0;10;;
2016-09-11 03:09:55;;Apache 2.0;https://www.kaggle.com/jeffhebert/seizure-spectrograms;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['filter', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.722;0.367;2020-12-13 13:55:25;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Seizure Spectrograms;R script;5047.0;20;;
2016-09-12 15:51:20;;Apache 2.0;https://www.kaggle.com/openneuron/begin-with-r-generate-features-2;1.0;['pattern'];['ner', 'ai', 'nlp', 'nn', 'ann'];['filter', 'neuron', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.671;0.253;2020-12-13 13:55:26;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Begin with R: generate features (2);R script;1669.0;6;;
2016-09-15 10:42:59;;Apache 2.0;https://www.kaggle.com/pakozm/dropoutcounts;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.686;0.253;2020-12-13 13:55:26;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];DropoutCounts;Python script;2271.0;6;;
2016-09-15 12:50:53;;Apache 2.0;https://www.kaggle.com/pakozm/zerorunlengthdistribution;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.665;0.302;2020-12-13 13:55:25;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];ZeroRunLengthDistribution;Python script;1463.0;10;;
2016-09-11 16:46:32;EDA Single EEG file;Apache 2.0;https://www.kaggle.com/solomonk/single-eeg-fft-entropy;1.0;['sklearn'];['ai', 'cv', 'ml', 'nn', 'ann'];['training data', 'regression', 'train', 'model', 'label', 'labeled'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.741;0.357;2020-12-13 13:55:25;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Single EEG + FFT + Entropy;Python notebook;8058.0;18;;
2016-11-19 07:53:58;;Apache 2.0;https://www.kaggle.com/valadi/2016092090;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.641;0.214;2020-12-13 13:55:26;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];2016092090;Python script;928.0;4;;
2016-10-06 00:15:37;;Apache 2.0;https://www.kaggle.com/vincento/0-6-lb;1.0;['pattern', 'sklearn'];['ner', 'ai', 'nlp', 'nn', 'rnn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'lstm', 'label', 'predict', 'classification'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.628;0.214;2020-12-13 13:55:26;Melbourne University AES/MathWorks/NIH Seizure Prediction;[]; (~0.6 LB) ;Python script;730.0;4;;
2016-11-05 09:58:32;;Apache 2.0;https://www.kaggle.com/zfturbo/seizure-boost-0-6-lb;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/melbourne-university-seizure-prediction;0.763;0.463;2020-12-13 13:55:25;Melbourne University AES/MathWorks/NIH Seizure Prediction;[];Seizure Boost (~0.6 LB);Python script;14785.0;62;0.56777;0.56389
2019-03-15 05:02:15;Feature Engineering for March Madness;Apache 2.0;https://www.kaggle.com/aashita/feature-engineering-for-march-madness;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['filter', 'train', 'model', 'predict', 'rank'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.659;0.292;2020-12-13 13:57:27;multiple data sources;['beginner, feature engineering'];Feature Engineering for March Madness ;Python notebook;1294.0;9;;
2018-02-21 19:49:21;Intro;Apache 2.0;https://www.kaggle.com/aphaniteja/exploratory-madness;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'predict', 'layer', 'label', 'loss'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.666;0.292;2020-12-13 13:57:27;Google Cloud & NCAAÂ® ML Competition 2018-Men's;[];Exploratory madness;Python notebook;1497.0;9;;
2018-03-16 02:07:05;;Apache 2.0;https://www.kaggle.com/captcalculator/a-very-extensive-ncaa-exploratory-analysis;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['loss', 'model', 'label', 'predict'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.771;0.519;2020-12-13 13:57:27;Google Cloud & NCAAÂ® ML Competition 2018-Men's;[];A Very Extensive NCAA Exploratory Analysis;Rmarkdown script;18419.0;131;;
2018-03-15 08:16:07;Add feature;Apache 2.0;https://www.kaggle.com/circle811/simple-logistic-regression;1.0;['sklearn'];['ai', 'cv'];['predict', 'regression', 'train', 'model', 'logistic regression', 'loss'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.621;0.281;2020-12-13 13:57:27;Google Cloud & NCAAÂ® ML Competition 2018-Men's;[];Simple Logistic Regression;Python notebook;646.0;8;0.58668;0.58668
2018-03-07 09:53:25;Enriching NCAATourneyDetailedResults.csv with Advanced StatsIf the opponents of a team score only 75.2 points on average, it could be more about the pace at which the team played instead of their skill on the defensive end.The given Box score numbers are an incomplete standard of a team's performance.Advanced Metrics  in basketball provide a deeper understanding of a team's performance. Possession is used to normalize basketball statistics - offensive/defensive efficiency and other metrics are all based on how the possession is calculated. Team performance should be measured on a per-possession basis. Possession =0.96[(Field Goal Attempts)+(Turnovers)+0.44(Free Throw Attempts)-(Off.Rebounds)]  (Notice: Possession values are not calculated by using Play-By-Play data, as it seems like they do not necessarily add up to the final stats of the game and an estimation will do just fine)For more information click here Now let's add some new features that can acutally be used for predictive modelling and ranking a team.;Apache 2.0;https://www.kaggle.com/lnatml/feature-engineering-with-advanced-stats;0.5;[];['ai', 'nn', 'rl'];['filter', 'model', 'label', 'predict', 'rank', 'understanding'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.735;0.446;2020-12-13 13:57:27;Google Cloud & NCAAÂ® ML Competition 2018-Men's;['feature engineering'];Feature Engineering with Advanced Stats;Python notebook;6976.0;50;;
2018-04-02 10:13:35;;Apache 2.0;https://www.kaggle.com/maniceet/histograms-of-championship-match;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nlp', 'nn', 'ml'];['filter', 'machine learning', 'test data', 'model', 'deep learning', 'layer', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.668;0.319;2020-12-13 13:57:27;multiple data sources;[];Histograms of Championship Match;R script;1571.0;12;;
2018-02-21 11:13:47;;Apache 2.0;https://www.kaggle.com/maxphilipp/creating-a-tidy-dataset-for-starters-using-dplyr;1.0;['pattern'];['ner', 'nn'];['filter', 'predict'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.656;0.334;2020-12-13 13:57:27;Google Cloud & NCAAÂ® ML Competition 2018-Men's;[];Creating a Tidy dataset for starters using dplyr;Rmarkdown script;1229.0;14;;
2019-01-25 00:32:01;Analyzing a Decade of NCAA Championship RankingsAs last year's winner of the Men's March Madness Competition I am eager to defend my title.  To kick things off, I wanted to start with a lightweight and fun animation with Plotly.  I chose to look at the Pomeroy Rankings for the last 10 National Championship winnning teams.  The below animation displays the change in the Pomeroy Rankings (team-by-team) for the last 15 weeks of the given regular season. Although its a simple animation, there are a few interesting insights that can infered from it.  The 2 Connecticut teams (2011 & 2014) were the only teams to have a ranking over 10 at any point in their championship seasons. 4 teams never had a rank over 5 at any point in their championship seasons. With the exception of the 2 Connecticut teams, all teams ranked in the top 5 at week #1 in the animation. Only one of those teams, Duke - 2015, finished outside the top 5 (7) at week #15.  So when I'm filling out my bracket this year and working on the algorithm for the 2019 Competition, I'm going to strongly weight teams that have had consistently strong Pomeroy rankings throughout the season. Feel free to check out my code and let me know what you think.  Thanks!;Apache 2.0;https://www.kaggle.com/mtodisco10/ncaa-men-s-bball-champions-ranking-animation;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['rank', 'label', 'filter'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.603;0.253;2020-12-13 13:57:27;Google Cloud & NCAAÂ® ML Competition 2018-Men's;['data visualization, exploratory data analysis'];NCAA Men's Bball Champions - Ranking Animation;Python notebook;475.0;6;;
2018-03-03 10:07:25;IntroductionIn this kernel I would like to explore the NCAA data with a quick look at some classical and advanced stats in order to understand which statistics may be more useful for predicting the Ws & Ls in the tournament. ICYMI: https://stats.nba.com/help/glossary/ Having a detailed information set about every single games of the different seasons, the first step is to create a dataset which contains aggregated stats for each single team.;Apache 2.0;https://www.kaggle.com/pozz13/introduction-to-basic-moreyball-eda;0.5;[];['ner', 'ai', 'nlu', 'rl'];['loss', 'filter', 'predict'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.664;0.292;2020-12-13 13:57:27;Google Cloud & NCAAÂ® ML Competition 2018-Men's;['beginner, data visualization, exploratory data analysis'];Introduction to basic Moreyball - EDA;R notebook;1443.0;9;;
2018-03-03 15:59:33;IntroductionThis script uses just the FiveThirtyEight ELO ratings created by Liam Kirwin (https://www.kaggle.com/lpkirwin/fivethirtyeight-elo-ratings), the tournament seeds, and a simple logistic regression to produce a pretty strong overall log loss score of 0.53. If you take this scipt as a base and add in team's win/loss, points scored, etc in the record in the regular season and secondary tournmanets you can almost certainly make gain on this score. 1. Set upFirst we load key packages and the data we will need to train our make features and train our models.;Apache 2.0;https://www.kaggle.com/robertsturrock/prediction-using-fivethirtyeight-elo-ratings;0.5;[];['ner', 'ai', 'nn'];['filter', 'regression', 'train', 'model', 'loss', 'logistic regression', 'predict', 'rank'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.711;0.34;2020-12-13 13:57:27;multiple data sources;['feature engineering, logistic regression, basketball'];Prediction using FiveThirtyEight ELO ratings;R notebook;3917.0;15;;
2018-02-20 22:28:41;;Apache 2.0;https://www.kaggle.com/taffeylewis/for-fun-my-original-synthetic-spreads;0.5;[];['ner', 'ai', 'dl', 'cv', 'ml'];['loss', 'predict'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.708;0.416;2020-12-13 13:57:27;Google Cloud & NCAAÂ® ML Competition 2018-Men's;[];"For Fun: My original ""Synthetic Spreads""";Rmarkdown script;3627.0;35;;
2018-02-21 21:34:37;;Apache 2.0;https://www.kaggle.com/zeemeen/ncaa-trueskill-script;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['layer', 'bayesian', 'classification', 'deep learning'];https://www.kaggle.com/c/mens-machine-learning-competition-2018;0.682;0.357;2020-12-13 13:57:27;Google Cloud & NCAAÂ® ML Competition 2018-Men's;[];NCAA_TrueSkill_Script;Python script;2077.0;18;0.00000;0.00000
2019-03-15 05:02:15;Feature Engineering for March Madness;Apache 2.0;https://www.kaggle.com/aashita/feature-engineering-for-march-madness;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['filter', 'train', 'model', 'predict', 'rank'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.659;0.292;2020-12-13 13:59:30;multiple data sources;['beginner, feature engineering'];Feature Engineering for March Madness ;Python notebook;1295.0;9;;
2019-03-20 21:43:18;Akash Gupta............ March madness men's;Apache 2.0;https://www.kaggle.com/akash14/google-march-madness-men-s;1.0;['statsmodels', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ml'];['regression', 'train', 'model', 'loss', 'logistic regression', 'predict', 'rank'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.635;0.371;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;[];google march madness men's;Python notebook;824.0;21;12.06131;12.06131
2019-04-10 19:59:35;March Madness 2019 Predictions Data CleaningHere I make the training data used for the model. Part of the beauty of the model is in its simplicity, taking only scoring margin and home and away as features. As such, there is only some filtering (to 2019), name changing, and light feature creation to be done on the training set.;Apache 2.0;https://www.kaggle.com/alexkaechele/march-madness-2019-predictions;0.5;[];['ner', 'ai', 'dl', 'gan', 'nn', 'ml'];['filter', 'training data', 'train', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.7;0.292;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;['basketball, bayesian statistics'];March Madness 2019 Predictions;Python notebook;3067.0;9;;
2019-02-16 19:13:31;;Apache 2.0;https://www.kaggle.com/ateplyuk/lgbm-str;1.0;['lightgbm', 'sklearn'];['ai', 'rl', 'gbm'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.641;0.397;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;[];Lgbm_str;Python notebook;928.0;28;0.00000;0.00000
2019-03-19 10:38:35;Code to simulate realizations of the tournament based on your Finalsubmission.csvWill give you distribution of expected winners/final four teams etc... and relative probabilities (like fivethirtyeight)Will also give you your expectation value for the score your braket will get.All you have to do is replace with your Finalpredictions.csv below;Apache 2.0;https://www.kaggle.com/dbrout/simulate-tourney-to-optimize-espn-com-submission;0.5;[];['ner', 'ai', 'dl', 'gan', 'nn', 'ml'];['label', 'test data', 'predict'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.662;0.319;2020-12-13 13:59:30;multiple data sources;[];Simulate Tourney to Optimize ESPN.com Submission;Python notebook;1382.0;12;;
2019-04-03 11:21:26;;Apache 2.0;https://www.kaggle.com/jaseziv83/a-recent-deep-look-at-the-men-s-ncaab;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['filter', 'model', 'loss', 'label', 'predict', 'rank'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.756;0.536;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;['data visualization, exploratory data analysis, data cleaning, +1 morebasketball'];A RECENT deep look at the Men's NCAAB;Rmarkdown script;12159.0;167;;
2019-02-15 22:08:37;;Apache 2.0;https://www.kaggle.com/jaseziv83/functions-to-tidy-detailed-stats;0.2;[];['ner', 'ai', 'nn'];[];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.646;0.34;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;['feature engineering, data cleaning, basketball'];Functions to tidy detailed stats;Rmarkdown script;1012.0;15;;
2019-02-15 08:09:11;;Apache 2.0;https://www.kaggle.com/jaseziv83/starter-random-forrest;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['filter', 'random forest', 'regression', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.649;0.346;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;[];Starter Random Forrest;R script;1082.0;16;0.00000;0.00000
2019-03-20 23:22:06;;Apache 2.0;https://www.kaggle.com/jaylew/ncaa2019-mbb;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'nn', 'ml'];['machine learning', 'regression', 'train', 'model', 'loss', 'logistic regression', 'predict', 'rank', 'random forest'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.658;0.253;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;[];NCAA2019_MBB;Python notebook;1272.0;6;;
2019-03-21 02:48:17;;Apache 2.0;https://www.kaggle.com/jazivxt/courtside-seat-2019m-competitiveness;1.0;['sklearn'];['ai'];['predict', 'regression', 'fitting', 'model', 'loss', 'rank'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.701;0.408;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;['logistic regression, basketball'];Courtside Seat 2019M #Competitiveness;Python notebook;3155.0;32;2.89084;2.89084
2019-03-01 16:46:12;IntroductionMassey Ordinals is a compilation of ranking systems. In this notebook we compare those systems in terms of their performance. In other words, we get a ranking of rankings.;Apache 2.0;https://www.kaggle.com/joseleiva/massey-s-ordinal-s-ordinals;0.5;[];['dl', 'ner', 'ai', 'nn'];['rank', 'loss', 'label', 'predict'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.646;0.281;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;[];Massey's Ordinal's Ordinals;Python notebook;1010.0;8;0.00000;0.00000
2019-02-16 14:16:57;Analyzing a Decade of NCAA Championship RankingsAs last year's winner of the Men's March Madness Competition I am eager to defend my title.  To kick things off, I wanted to start with a lightweight and fun animation with Plotly.  I chose to look at the Pomeroy Rankings for the last 10 National Championship winnning teams.  The below animation displays the change in the Pomeroy Rankings (team-by-team) for the last 15 weeks of the given regular season. Although its a simple animation, there are a few interesting insights that can infered from it.  The 2 Connecticut teams (2011 & 2014) were the only teams to have a ranking over 10 at any point in their championship seasons. 4 teams never had a rank over 5 at any point in their championship seasons. With the exception of the 2 Connecticut teams, all teams ranked in the top 5 at week #1 in the animation. Only one of those teams, Duke - 2015, finished outside the top 5 (7) at week #15.  So when I'm filling out my bracket this year and working on the algorithm for the 2019 Competition, I'm going to strongly weight teams that have had consistently strong Pomeroy rankings throughout the season. Feel free to check out my code and let me know what you think.  Thanks!;Apache 2.0;https://www.kaggle.com/mtodisco10/a-decade-of-championship-rankings-animated;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['rank', 'label', 'filter'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.645;0.34;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;['data visualization, exploratory data analysis, basketball'];A Decade of  Championship Rankings Animated;Python notebook;1000.0;15;;
2019-03-18 21:15:35;;Apache 2.0;https://www.kaggle.com/omniactio/basic-logistic-regression-with-cross-validation;1.0;['statsmodels'];['ner', 'ai', 'cv', 'nn', 'ml'];['regression', 'train', 'model', 'loss', 'logistic regression', 'predict', 'rank'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.712;0.383;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;[];Basic Logistic Regression with Cross Validation ;Python notebook;4029.0;24;0.17825;0.17825
2019-03-16 16:16:49;;Apache 2.0;https://www.kaggle.com/redfoongus/logistic-regression-using-mean-massey;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['filter', 'regression', 'train', 'deep learning', 'logistic regression', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/mens-machine-learning-competition-2019;0.634;0.268;2020-12-13 13:59:30;Google Cloud & NCAAÂ® ML Competition 2019-Men's;[];Logistic Regression using Mean Massey;R script;808.0;7;0.00000;0.00000
2018-01-01 20:40:06;;Apache 2.0;https://www.kaggle.com/agrigorev/tensorflow-starter-conv1d-embeddings-0-442-lb;1.0;['vocabulary', 'tensorflow', 'keras', 'nltk'];['ner', 'ai', 'dl', 'cnn', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['gru', 'filter', 'test data', 'train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'relu', 'loss', 'predict', 'computer vision', 'understanding', 'classification', 'hidden layer'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.738;0.483;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];Tensorflow starter: conv1d + embeddings (0.442 LB);Python script;7459.0;80;99.00000;0.44258
2018-01-17 09:35:18;;Apache 2.0;https://www.kaggle.com/anttip/wordbatch-ftrl-fm-lgb-lbl-0-42555;1.0;['pattern', 'lightgbm', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn', 'ml'];['training data', 'test data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.747;0.502;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];Wordbatch FTRL+FM+LGB (LBL 0.42555);Python script;9433.0;104;99.00000;0.42561
2017-11-30 21:39:25;;Apache 2.0;https://www.kaggle.com/apapiu/ridge-script;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.745;0.523;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];Ridge Script;Python script;9032.0;139;99.00000;0.47088
2017-12-05 18:26:28;;Apache 2.0;https://www.kaggle.com/captcalculator/a-very-extensive-mercari-exploratory-analysis;0.5;[];['ai', 'rl', 'nn', 'cv'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.773;0.555;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];A Very Extensive Mercari Exploratory Analysis;Rmarkdown script;19491.0;221;;
2018-02-06 11:14:16;;Apache 2.0;https://www.kaggle.com/isaienkov/rnn-with-keras-ridge-sgdr-0-43553;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'rnn'];['gru', 'regression', 'train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.757;0.493;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];RNN_with_Keras_Ridge_SGDR_0.43553;Python script;12369.0;92;99.00000;0.43455
2018-02-21 08:59:37;;Apache 2.0;https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s;1.0;['sklearn', 'mxnet', 'tensorflow', 'pattern', 'keras'];['ner', 'ai', 'gan', 'cv', 'nlp', 'nn', 'rnn', 'ml'];['activation function', 'neuron', 'train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification', 'hidden layer'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.794;0.577;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];Mercari Golf: 0.3875 CV in 75 LOC, 1900 s;Python script;37430.0;311;;
2018-02-13 16:01:49;;Apache 2.0;https://www.kaggle.com/rumbok/ridge-lb-0-41944;1.0;['pattern', 'sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.753;0.537;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];Ridge (LB 0.41943);Python script;11292.0;169;99.00000;0.41944
2018-02-08 20:59:35;;Apache 2.0;https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.761;0.512;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];More Effective Ridge LGBM Script (LB 0.44823);Python script;14096.0;119;;
2018-01-25 18:41:12;;Apache 2.0;https://www.kaggle.com/tunguz/wordbatch-ftrl-fm-lgb-lbl-0-42506;1.0;['lightgbm', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn', 'ann'];['test data', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.729;0.45;2020-12-13 14:05:42;multiple data sources;[];Wordbatch FTRL+FM+LGB (LBL 0.42506);Python script;6026.0;53;99.00000;0.44769
2017-12-02 09:19:07;Introduction;Apache 2.0;https://www.kaggle.com/vrtjso/mercari-eda-more-info-than-you-can-imagine;0.5;[];['ai', 'rl', 'nlp', 'nn', 'ml'];['filter', 'machine learning', 'training data', 'test data', 'train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.755;0.525;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;['beginner, data visualization, exploratory data analysis'];Mercari EDA - More Info Than You Can Imagine;R notebook;11762.0;142;;
2018-02-22 10:14:02;;Apache 2.0;https://www.kaggle.com/whitebird/mercari-price-3rd-0-3905-cv-at-pb-in-3300-s;1.0;['pattern', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['gru', 'filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.724;0.458;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];Mercari Price:3rd  0.3905 CV at Pb in 3300 s;Python script;5398.0;58;0.00000;0.00000
2018-01-21 09:13:27;;Apache 2.0;https://www.kaggle.com/yyll008/gru-25-12-12-with-keras-512-64-relu-sgdr-lb0-432;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'rnn'];['gru', 'regression', 'train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/mercari-price-suggestion-challenge;0.728;0.447;2020-12-13 14:05:42;Mercari Price Suggestion Challenge;[];GRU(25-12-12)_with_Keras(512-64,relu)_SGDR_LB0.432;Python script;5861.0;51;99.00000;0.43524
2017-05-31 00:01:25;;Apache 2.0;https://www.kaggle.com/arsenyinfo/easy-feature-selection-pipeline-0-55-at-lb;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['regression', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.728;0.444;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];Easy Feature Selection pipeline: 0.55+ at LB;Python script;5883.0;49;0.54421;0.55440
2017-06-03 09:03:14;;Apache 2.0;https://www.kaggle.com/gpapadop79/mercedez-baseline-2;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.693;0.425;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];Mercedez_Baseline 2;Python script;2629.0;39;0.54335;0.56193
2017-06-16 03:32:48;;Apache 2.0;https://www.kaggle.com/hakeem/stacked-then-averaged-models-0-5697;1.0;['xgboost', 'sklearn', 'tpot'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['test data', 'generation', 'train', 'fitting', 'model', 'understanding', 'deep learning', 'loss', 'label', 'gradient boosting', 'predict', 'rank', 'decision tree', 'classification', 'labeled'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.782;0.522;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];stacked then averaged models [~ 0.5697];Python script;25601.0;137;0.55215;0.56782
2019-01-03 16:34:17;;Apache 2.0;https://www.kaggle.com/headsortails/mercedas-2-feature-interactions;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'model', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.746;0.495;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;['data visualization, exploratory data analysis'];MercEDAs 2 - feature interactions;Rmarkdown script;9380.0;94;;
2019-01-04 21:42:09;;Apache 2.0;https://www.kaggle.com/headsortails/mercedas-update2-intrinsic-noise;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'fitting', 'model', 'clustering', 'predict', 'rank'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.765;0.543;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;['beginner, data visualization, exploratory data analysis, +1 morefeature engineering'];MercEDAs - update2 - intrinsic noise;Rmarkdown script;15768.0;184;;
2017-06-18 13:06:54;;Apache 2.0;https://www.kaggle.com/msp48731/biased-predictions-how-to-deal-with-them;1.0;['pattern', 'xgboost'];['ner', 'ai', 'rl', 'cv'];['regression', 'train', 'model', 'clustering', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.741;0.446;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];Biased predictions - How to deal with them?;Rmarkdown script;8129.0;50;;
2017-06-26 23:23:15;;Apache 2.0;https://www.kaggle.com/msp48731/feature-engineering-and-visualization;1.0;['pattern'];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['machine learning', 'train', 'model', 'clustering', 'label', 'predict', 'rank'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.774;0.515;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;['data visualization, pca'];Feature Engineering and Visualization;Rmarkdown script;19961.0;124;;
2017-06-15 23:10:23;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/simple-xgboost-starter;1.0;['xgboost'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml'];['training data', 'train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.71;0.453;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];Simple xgboost starter;Rmarkdown script;3821.0;55;;
2017-07-07 01:42:16;;Apache 2.0;https://www.kaggle.com/paulorzp/one-line-solution-in-python-lb-0-55453;0.5;[];['nlp', 'ai', 'nn', 'ner'];['rank', 'classification', 'train', 'deep learning'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.694;0.425;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];One Line Solution in Python (LB:0.55453);Python script;2662.0;39;0.54062;0.55452
2017-07-11 11:57:56;;Apache 2.0;https://www.kaggle.com/raddar/raddar-extratrees;0.5;[];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.726;0.418;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];raddar extraTrees ;R script;5653.0;36;0.55144;0.55444
2017-06-05 13:45:21;;Apache 2.0;https://www.kaggle.com/robertoruiz/a-magic-feature;0.5;[];['nlp', 'ai', 'nn', 'ner'];['regression', 'train', 'deep learning', 'linear regression', 'classification'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.727;0.429;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];A magic feature ;R script;5759.0;41;;
2017-07-04 14:23:47;;Apache 2.0;https://www.kaggle.com/robertoruiz/my-frustrated-approach;0.5;[];['dl', 'ai', 'nn'];['train'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.697;0.429;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];My frustrated approach;Rmarkdown script;2863.0;41;;
2017-07-04 09:08:34;;Apache 2.0;https://www.kaggle.com/tilii7/you-want-outliers-we-got-them-outliers;1.0;['sklearn'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'test data', 'random forest', 'train', 'fitting', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.749;0.439;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;['outlier analysis'];You want outliers? We got them outliers!;Python script;10031.0;46;;
2017-07-08 10:01:08;;Apache 2.0;https://www.kaggle.com/umbertogriffo/deep-learning;1.0;['tensorflow', 'sklearn', 'keras', 'theano'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['activation function', 'regression', 'neuron', 'train', 'model', 'input layer', 'output layer', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'rectifier', 'predict', 'relu', 'classification', 'hidden layer'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.772;0.488;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;['deep learning, regression'];Deep Learning;Python script;18853.0;86;;
2017-07-11 11:43:20;;Apache 2.0;https://www.kaggle.com/wti200/xgboost-with-one-hot-encoding-r;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/mercedes-benz-greener-manufacturing;0.762;0.444;2020-12-13 14:11:17;Mercedes-Benz Greener Manufacturing;[];XGboost with one-hot-encoding - (R);R script;14349.0;49;;
2019-01-01 12:17:38;"Following is an aggregation of the details of most of the columns in the training data. The details were obtained by googling.  MachineIdentifier - Individual machine ID. Training dataset contains 89,21,483 (8 mn) unique machines.  ProductName - Defender state information e.g. win8defender win8defender     8826520 mse                94873 mseprerelease         53 scep                  22 windowsintune          8 fep                    7  EngineVersion - Defender state information e.g. 1.1.12603.0. There are 70 unique Engine versions in the training data. The 5 most common ones are : 1.1.15200.1    3845067 1.1.15100.1    3675915 1.1.15000.2     265218 1.1.14901.4     212408 1.1.14600.4     160585  AppVersion - Defender state information e.g. 4.9.10586.0. This has a long tailed frequency distribution, with a 110 unique App versions. Most common 5 versions are : 4.18.1807.18075     5139224 4.18.1806.18062      850929 4.12.16299.15        359871 4.10.209.0           272455 4.13.17134.1         257270  AvSigVersion - Defender state information e.g. 1.217.1014.0. Has 8,531 unique values.  IsBeta - Defender state information e.g. false IsBeta  HasDetections 0       0                4462557         1                4458859 1       0                     34         1                     33  RtpStateBitfield - (Most likely - RTP state:  (Enabled or Disabled)    source. Expected binary values, don't understand why values are multiple integers though. Frequency table -  7.0     8651487  0.0      190701 NaN        32318  8.0       21974  5.0       20328  3.0        3029  1.0        1625  35.0         21  IsSxsPassiveMode - Google searches suggest that this a active/passive mode of operation for Windows Defender. If another third party primary antivirus exists on the system, the Defender enters Passive mode. Passive mode obviously offers reduced functionality.Source 0    8766840 1     154643 Data shows that a machine which is in passive mode, has a higher prevalence of malwares. IsSxsPassiveMode  HasDetections 0                 1                4402017                   0                4364823 1                 0                  97768                   1                  56875  DefaultBrowsersIdentifier - ID for the machine's default browser. [This column has 2017 unique values, so these couldn't be direct browser name mappings. So probably, a browser-version combination ? Anyhow, this is what the top ten in the frequency chart look like - 239.0     46056 3195.0    42692 1632.0    28751 3176.0    24220 146.0     20756 1910.0    19416 1727.0    17393 2064.0    13990 2725.0    13338 1160.0    12594   AVProductStatesIdentifier - ID for the specific configuration of a user's antivirus software. 28,970 unique float values.  AVProductsInstalled - Assuming this to be the number of anti-virus products installed. 90% of the machines have 1-2 products installed. We should probably drop the one row with 0 AVProductsInstalled. Again, ""probably"".  1.0    6208893  2.0    2459008  3.0     208103 NaN       36221  4.0       8757  5.0        471  6.0         28  7.0          1  0.0          1  AVProductsEnabled - NA  1.0    8654101  2.0     198652 NaN       36221  0.0      25958  3.0       6075  4.0        453  5.0         23  HasTpm - True if machine has tpm. A Trusted Platform Module (TPM) is a specialized chip on an endpoint device that stores RSA encryption keys specific to the host system for hardware authentication. Each TPM chip contains an RSA key pair called the Endorsement Key (EK). The pair is maintained inside the chip and cannot be accessed by software.[source - google search] 1    8814167 0     107316  CountryIdentifier - ID for the country the machine is located in. This has 222 unique int64 IDs. Wikipedia cites 255+ countries and independent territories. source If these are exact country codes, then Austria (43) has the highest number of rows in this data set, while USA(001) has just 2 %.  CityIdentifier - ID for the city the machine is located in. 1,07,366 unique cities and huge number(~5%) of NaNs.  OrganizationIdentifier - ID for the organization the machine belongs in, organization ID is mapped to both specific companies and broad industries. There are 49 unique organisations, 50% of the computers being under one org, another 25% not-classified. Here's a breakup of the top 5 values - 27.0    4196457 NaN      2751518  18.0    1764175  48.0      63845  50.0      45502  GeoNameIdentifier - ID column for the 292 geographic regions, a machine is located in.  LocaleEnglishNameIdentifier - English name of Locale ID of the current user. The column contains 276 locale int64 IDs. ""A locale is neither a language nor a country, the same language may be spoken in multiple countries (often with subtle differences) and a single country may speak multiple languages. A locale is therefore an area where a particular language is spoken which may (or may not) align with geographical and/or political boundaries."" source  Platform - Calculates platform name (of OS related properties and processor property). Frequency table - windows10      8618715 windows8        194508 windows7         93889 windows2016      14371  Processor - This is the process architecture of the installed operating system. Frequency - x64      8105435 x86       815702 arm64        346  OsVer - Version of the current operating system. 10.0.0.0        8632545 6.3.0.0          194447 6.1.1.0           93268 6.1.0.0             582 10.0.3.0            225 10.0.1.0            141  OsBuild - Build of the current operating system. Has 76 unique build numbers, of which ~5 form the majority. Distribution of the top 10 values - 17134    3915521 16299    2503681 15063     780270 14393     730819 10586     411606 10240     270192 9600      194508 7601       93306 17692       3184  OsSuite - Product suite mask for the current operating system. This has a very skewed distribution.  OsPlatformSubRelease - Returns the OS Platform sub-release (Windows Vista, Windows 7, Windows 8, TH1, TH2). Less skewed distribution than OsSuite. rs4           3915526 rs3           2503681 rs2            780270 rs1            730819 th2            411606 th1            270192 windows8.1     194508 windows7        93889 prers5          20992  OsBuildLab - Build lab that generated the current OS. Example: 9600.17630.amd64fre.winblue_r7.150109-2022. Top 5 values by counts- 17134.1.amd64fre.rs4_release.180410-1804                     3658199 16299.431.amd64fre.rs3_release_svc_escrow.180502-1908        1252674 16299.15.amd64fre.rs3_release.170928-1534                     961060 15063.0.amd64fre.rs2_release.170317-1834                      718033 17134.1.x86fre.rs4_release.180410-1804                        257074  SkuEdition - The goal of this feature is to use the Product Type defined in the MSDN to map to a 'SKU-Edition' name that is useful in population reporting. The valid Product Type are defined in %sdxroot%\data\windowseditions.xml. This API has been used since Vista and Server 2008, so there are many Product Types that do not apply to Windows 10. The 'SKU-Edition' is a string value that is in one of three classes of results. The design must hand each class. Home               5514341 Pro                3224164 Invalid              78054 Education            40694 Enterprise           34357 Enterprise LTSB      20702 Cloud                 5589 Server                3582  IsProtected - This is a calculated field derived from the Spynet Report's AV Products field. Returns: a. TRUE if there is at least one active and up-to-date antivirus product running on this machine. b. FALSE if there is no active AV product on this machine, or if the AV is active, but is not receiving the latest updates. c. null if there are no Anti Virus Products in the report. Returns: Whether a machine is protected. [jk: A machine that is not protected by an anti-virus has a lower chance of infection than one that is protected.] IsProtected  HasDetections 0.0          0                 298904              1                 184253 1.0          1                4261098              0                4141184  AutoSampleOptIn - This is the SubmitSamplesConsent value passed in from the service, available on CAMP 9+. [No clue what this is.] 0    8921225 1        258  PuaMode - Pua Enabled mode from the service. ""The Potentially Unwanted Applications (PUA) protection feature in Windows Defender Antivirus can identify and block PUAs from downloading and installing on endpoints in your network. These applications are not considered viruses, malware, or other types of threats, but might perform actions on endpoints that adversely affect their performance or use. PUA can also refer to applications that are considered to have a poor reputation."" source  SMode - This field is set to true when the device is known to be in 'S Mode', as in, Windows 10 S mode, where only Microsoft Store apps can be installed  0.0    8379843 NaN      537759  1.0       3881  IeVerIdentifier -  Retrieves which version of Internet Explorer is running on this device.source This has 303 unique values. Here are the most frequent values, uptil a NaN. 137.0    3885842  117.0    1767931  108.0     474390  111.0     467828  98.0      354411  135.0     217458  53.0      204952  74.0      202542  94.0      173593  105.0     173448  333.0     156391  107.0     128633  103.0     114952  96.0       83559 NaN         58894  SmartScreen - This is the SmartScreen enabled string value from registry. This is obtained by checking in order, HKLM\SOFTWARE\Policies\Microsoft\Windows\System\SmartScreenEnabled and HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\SmartScreenEnabled. If the value exists but is blank, the value ""ExistsNotSet"" is sent in telemetry. Windows Defender SmartScreen helps to protect your employees if they try to visit sites previously reported as phishing or malware websites, or if an employee tries to download potentially malicious files. This only applies to Win 10 and Win 10 mobile.  Firewall - This attribute is true (1) for Windows 8.1 and above if windows firewall is enabled, as reported by the service. 1.0     8641014 0.0      189119 NaN       91350  UacLuaenable - This attribute reports whether or not the ""administrator in Admin Approval Mode"" user type is disabled or enabled in UAC. The value reported is obtained by reading the regkey HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System\EnableLUA. UAC stands for User Access Control and here is an interesting discussion on its usage & its annoyances. discussion  Census_MDC2FormFactor - A grouping based on a combination of Device Census level hardware characteristics. The logic used to define Form Factor is rooted in business and industry standards and aligns with how people think about their device. (Examples: Smartphone, Small Tablet, All in One, Convertible...)  Census_DeviceFamily - AKA DeviceClass. Indicates the type of device that an edition of the OS is intended for.  This has a very high proportion of Windows Desktop and very few of Windows Server and Windows.  Census_OEMNameIdentifier - NA  Census_OEMModelIdentifier - NA  Census_ProcessorCoreCount - Number of logical cores in the processor. Four core processors are the most common. There are 45 unique values in total:  4.0      5430193  2.0      2311969  8.0       865004  12.0       92702  1.0        70390  6.0        69910 NaN         41306  16.0       18551  3.0        13580  32.0        2136  Census_ProcessorManufacturerIdentifier - NA. Frequency distribution -  5.0     7839318  1.0     1040292 NaN        41313  10.0        339  3.0         218  9.0           1  7.0           1  4.0           1  Census_ProcessorModelIdentifier - NA. 3,428 unique values  Census_ProcessorClass - A classification of processors into high/medium/low. Initially used for Pricing Level SKU. No longer maintained and updated This column is mostly empty.  Census_PrimaryDiskTotalCapacity - Amount of disk space on primary disk of the machine in MB. Most popular disk sizes are 500gb and 1Tb, in that order. Unique value count: 5,735.  Census_PrimaryDiskTypeName - Friendly name of Primary Disk Type - HDD or SSD. HDD is twice as popular as SSD, in this dataset. HDD            5806804 SSD            2466808 UNKNOWN         358251 Unspecified     276776 NaN              12844  Census_SystemVolumeTotalCapacity - The size of the partition that the System volume is installed on in MB. 5,36,848 unique system volume sizes exist. The following frequency count was created after converting the data into Gigabytes and then binning them. a. <= 50          689171 b. 50-100         960770 c. 100 - 250     2845915 d. 250 - 500     2695227 e. 500 - 1000    1601322 f. > 1Tb           76014 Source code for creating the above cuts: _ = (round(df[col]/1032, 0)) pd.cut(_,  â€‹      labels=['a. <= 50', 'b. 50-100', 'c. 100 - 250', 'd. 250 - 500', 'e. 500 - 1000', 'f. > 1Tb'], â€‹      bins=[0, 50, 100, 250, 500, 1000, 10**4]).value_counts().sort_index()  Census_HasOpticalDiskDrive - True indicates that the machine has an optical disk drive (CD/DVD). Most systems don't have a disk drive. This column could provide an idea of how old the chassis is. And, older chassis might correlate with higher malware rates. Interestingly, none of the machines have a disk drive. 0.0    8921483 Name: Census_HasOpticalDiskDrive, dtype: int64  Census_TotalPhysicalRAM - Retrieves the physical RAM in MB. Since these are in MB, these are converted to GB by dividing them with 1032 and then the frequency count is created:  4.0       4102156  8.0       2200604  2.0       1108274  16.0       532480  6.0        400223  12.0       160345  3.0        156306 NaN          80533 Most popular - 4 Gb, 8 Gb, 2 Gb and 16Gb. Source code: (round(df['Census_TotalPhysicalRAM']/1032, 0)).value_counts(dropna=False).sort_values(ascending=False)  Census_ChassisTypeName - Retrieves a numeric representation of what type of chassis the machine has. A value of 0 means xx. Not all values are numerical in this column and the most popular ones don't seem to be mutually exclusive. For example, in the top5 common values, both Notebook  and Portable could be used interchangeably. Notebook               5248812 Desktop                1872125 Laptop                  685581 Portable                360903 AllinOne                204295  Census_InternalPrimaryDiagonalDisplaySizeInInches - Retrieves the physical diagonal length in inches of the primary display. Contains 785 unique display sizes. These can be used together withCensus_ChassisTypeName to differentiate the devices further.  Census_InternalPrimaryDisplayResolutionHorizontal - Retrieves the number of pixels in the horizontal direction of the internal display. 2,180 unique values are present. Most popular ones are 1,366 and 1,920 resolution. These values might indicate the age of the device. Higher resolutions most likely mean that the devices are relatively new, compared to the rest. This hypothesis would also be affected by Census_InternalPrimaryDiagonalDisplaySizeInInches. Most likely, larger displays will have higher resolution.  Census_InternalPrimaryDisplayResolutionVertical - Retrieves the number of pixels in the vertical direction of the internal display. 1,560 unique values.  Census_PowerPlatformRoleName - Indicates the OEM preferred power management profile. This value helps identify the basic form factor of the device  Census_InternalBatteryType - NA. Has a lot of inconsistent naming schemes.  For example - '#', 'lion', '4cel', 'l&#TAB#'. Majority of the machines are still represented in less than 10 labels. Some of these seem similar, for example - lion, li-i and liio could possibly be placeholders for lithium-ion batteries.  Census_InternalBatteryNumberOfCharges - Assuming this to be the number of battery cycles. If battery cycles are set to zero, could it be that these devices were in the first cycle of battery charge / are VMs or desktops ? What makes it more interesting is that 56% of the machines are in their first cycle of battery charge OR are non-battery operated.  Census_OSVersion - Numeric OS version Example - 10.0.10130.0. Contains 469 unique values.  Census_OSArchitecture - Architecture on which the OS is based. Derived from OSVersionFull. amd64    8105885 x86       815252 arm64        346  Census_OSBranch - Branch of the OS extracted from the OsVersionFull. 32 unique values. Five most common values: rs4_release                  4009158 rs3_release                  1237321 rs3_release_svc_escrow       1199767 rs2_release                   797066 rs1_release                   785534  Census_OSBuildNumber - OS Build number extracted from the OsVersionFull. Example - OsBuildNumber = 10512 or 10240. 165 unique values.  Census_OSBuildRevision - OS Build revision extracted from the OsVersionFull. Example - OsBuildRevision = 1000 or 16458. 285 unique values.  Census_OSEdition - Edition of the current OS. Sourced from HKLM\Software\Microsoft\Windows NT\CurrentVersion@EditionID in registry. 33 unique values. This column may also be linked to license type(Census_ActivationChannel). Top 5: Core                           3469991 Professional                   3130566 CoreSingleLanguage             1945461 CoreCountrySpecific             166100 ProfessionalEducation            56698  Census_OSSkuName - OS edition friendly name (currently Windows only). 30 unique values. Highly skewed by high numbers of:  'CORE', 'PROFESSIONAL', 'CORE_SINGLELANGUAGE' & 'CORE_COUNTRYSPECIFIC'  Census_OSInstallTypeName - Friendly description of what install was used on the machine i.e. clean UUPUpgrade        2608037 IBSClean          1650733 Update            1593308 Upgrade           1251559 Other              840121 Reset              649201 Refresh            205842 Clean               69073 CleanPCRefresh      53609  Census_OSInstallLanguageIdentifier - NA. 39 unique values.  Census_OSUILocaleIdentifier - NA. 147 unique values. 31, 34 and 30 are the top 3 Locale indentifiers.  Census_OSWUAutoUpdateOptionsName - Friendly name of the WindowsUpdate auto-update settings on the machine. FullAuto                                 3954497 UNKNOWN                                  2519925 Notify                                   2034254 AutoInstallAndRebootAtMaintenanceTime     371475 Off                                        26961 DownloadNotify                             14371  Census_IsPortableOperatingSystem - Indicates whether OS is booted up and running via Windows-To-Go on a USB stick. Assuming, 0 to be a ""False"" for Portability 0    8916619 1       4864  Census_GenuineStateName - Friendly name of OSGenuineStateID. 0 = Genuine. Expected integer values thanks to the description, but looks like we won't need to encode it. IS_GENUINE         7877597 INVALID_LICENSE     801692 OFFLINE             228366 UNKNOWN              13826 TAMPERED                 2  Census_ActivationChannel - Retail license key or Volume license key for a machine. Retail            4727589 OEM:DM            3413350 Volume:GVLK        450954 OEM:NONSLP         317980 Volume:MAK           8028 Retail:TB:Eval       3582  GVLK (group volume license key) is just an acronym for a volume license version of Windows, It's basically a bulk/volume license for Windows. Mostly available to large enterprises and govt institutions and comes with premium support. GLVK also needs a KMS (key management service) for deployment.source  KMS Client and Volume MAK product keys, are volume license keys that are not-for-resale.  They are issued by organizations for use on client computers associated in some way with the organization.  Volume license keys may not be transferred with the computer if the computer changes ownership.  OEM SLP and COA SLP product keys, are issued by large computer manufacturers and use SLP (System Locked Pre-installation) technology to bind the license to the original motherboard via the BIOS and software. The OEM SLP keys self-activate if the corresponding data in the BIOS is correct.  OEM SLP keys, which the user can read in the MGADiag report or software like KeyFinder, cannot be used by the end user to manually activate Windows.  The COA SLP key is printed on a sticker affixed to the side of the computer case (desktops), or on the bottom of the case (laptops), or in the battery compartment (newer laptops).  This is the key for the user to enter manually should he need to activate Windows himself.  OEM System Builder**, product keys are for use by smaller system builders, computer shops, consultants, and others who provide computers and services to their customers.  A system builder is defined by the System Builder license as ""an original equipment manufacturer, an assembler, a refurbisher, or a software pre-installer that sells the Customer System(s) to a third party.""  Retail, product keys are what the customer gets when he buys a Full Packaged Product (FPP), commonly known as a ""boxed copy"", of Windows from a retail merchant or purchases Windows online from the Microsoft Store.  OEM NON-SLP then means license keys from an OEM but not bound to a System Locked Pre-installation. OEM keys are not-for-resale and may not be transferred to another computer.  They may, however, be transferred with the computer if the computer is transferred to new ownership.   â€‹         Source  Census_IsFlightingInternal - 'Flighting' in Windows Defender context means making new development features available as soon as possible, during the development cycle. This does not refer to a public release. The 'internal' most likely means the Window Insider community. source NaN     7408759 0.0     1512703 1.0          21  Census_IsFlightsDisabled - Indicates if the machine is participating in flighting. 0.0     8760872 NaN      160523 1.0          88 Assuming, 0 would mean that majority of the devices have 'flighting' enabled. Which then means, most of these machines are from the Windows Insider program.  Census_FlightRing - The ring that the device user would like to receive flights for. This might be different from the ring of the OS which is currently installed if the user changes the ring after getting a flight from a different ring. Retail      8355679 NOT_SET      287803 Unknown      243438 WIS           10648 WIF           10322 RP             9860 Disabled       3722 OSG               7 Canary            3 Invalid           1  Census_ThresholdOptIn - NA NaN     5667325  0.0    3253342  1.0        816  Census_FirmwareManufacturerIdentifier - NA. 712 unique values, with majority concentrated in the top 5 firmware manufacturer identifiers.  Census_FirmwareVersionIdentifier - NA. Around 50K unique values.  Census_IsSecureBootEnabled - Indicates if Secure Boot mode is enabled. (from google)Microsoft Secure Boot is a component of Microsoft's Windows 8 operating system that relies on the UEFI specification's secure boot functionality to help prevent malicious software applications and ""unauthorized"" operating systems from loading during the system start-up process. There is a 50-50 breakup of samples based on this criteria: 0    4585438 1    4336045 Each of these categories, has a 50-50 % breakup of malware classifications.  Census_IsWIMBootEnabled - NA. WimBoot is an alternative way for OEMs to deploy Windows. A WimBoot deployment boots and runs Windows directly out of a compressed Windows Image File (WIM). This WIM file is immutable, and access to it is managed by a new file system filter driver (WoF.sys). The result is a significant reduction in the disk space required to install Windows. source NaN     5659703  0.0    3261779  1.0          1  Census_IsVirtualDevice - Identifies a Virtual Machine (machine learning model). It is self explaratory except for the phrase:""(machine learning model)""  0.0    8842840  1.0      62690 NaN       15953  Census_IsTouchEnabled - Is this a touch device ? Most are not. 0    7801452 1    1120031  Census_IsPenCapable - Is the device capable of pen input ? Most are not. 0    8581834 1     339649  Census_IsAlwaysOnAlwaysConnectedCapable - Retreives information about whether the battery enables the device to be AlwaysOnAlwaysConnected .To specify whether Wi-Fi should remain on when the screen times out, set AlwaysOnAlwaysConnected to one of the following values source.aspx): States:  0 or 'Disabled': Disables Wi-Fi from always being on when the screen times out. The Keep Wi-Fi on when the screen times out in the Settings > Wi-Fi > manage screen is turned off. 1 or 'Enabled': Enables Wi-Fi to always be on by default when the screen times out. The Keep Wi-Fi on when the screen times out in the Settings > Wi-Fi > manage screen is turned on. 0.0    8341972 1.0     508168 NaN       71343  Wdft_IsGamer - Indicates whether the device is a gamer device or not based on its hardware combination.    0.0    6174143  1.0    2443889 NaN      303451  Wdft_RegionIdentifier - NA. 15 unique values. Microsoft documentation mentions regional identifier";Apache 2.0;https://www.kaggle.com/airbourne/data-dictionary;0.5;[];['ner', 'ai', 'dl', 'gan', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'train', 'model', 'label', 'classification'];https://www.kaggle.com/c/microsoft-malware-prediction;0.711;0.477;2020-12-13 14:13:18;Microsoft Malware Prediction;['exploratory data analysis, data cleaning'];Data dictionary;Python notebook;3915.0;74;;
2019-02-17 12:47:07;;Apache 2.0;https://www.kaggle.com/bogorodvo/lightgbm-baseline-model-using-sparse-matrix;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['test data', 'random forest', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'k-means', 'predict', 'classification'];https://www.kaggle.com/c/microsoft-malware-prediction;0.8;0.594;2020-12-13 14:13:18;Microsoft Malware Prediction;[];LightGBM. Baseline Model Using Sparse Matrix;Python script;45287.0;409;0.63325;0.69255
2019-03-19 21:08:33;Manual Model Manipulation TrickIn this kernel, we load a public Kaggle model's submission file, view it's malware infection rate over time, and modify it manually to match what train.csv's malware rate looks like over time. The original submission file scores Public LB 0.689 and Private LB 0.635. After correction, the updated file scores Public LB 0.693 and Private LB 0.750. I used this trick during the competition to increase Public LB score. But it wasn't until after the competition's end that I learned how to increase Private LB score. In the private test dataset, all computers on and after November 20, 2018 have HasDetections=0 approximately. (Why is this?);Apache 2.0;https://www.kaggle.com/cdeotte/private-leaderboard-0-750;0.5;[];['ai', 'nn', 'ml', 'cv'];['training data', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/microsoft-malware-prediction;0.719;0.492;2020-12-13 14:13:18;multiple data sources;[];Private Leaderboard - [0.750];Python notebook;4745.0;90;0.75060;0.69325
2019-06-19 08:29:57;Microsoft Malware Prediction - EDA   Word of introductionA vulnerability assessment against malware (malicious software) is one of the most important topics in the cyber security domain. Everyone wants to feel safe, right? The problem of malware infection may touch everyone from individual private people, through companies, up to governmental agencies. The malware itself is infectious software (mostly Trojan horses and viruses, but not only) that tries to misuse or cause damage to a computer, server as well as to extract/steal sensitive or private information. These data can be for example your bank account details, details of your credit cards, various passwords or any other confidential data (e.g. companyâ€™s internal documents). In order to counteract to these threats, software providers (like our host IBM) are introducing active and passive countermeasures. These are for example anti-viruses and anti-malware software (active). In the case of IBM, these products are for example Microsoft Security Essentials (MSE) or well-known Windows Defender. Despite these active countermeasures, malware can get into the system by using security defects in the software itself, e.g. in various versions/builds of the operating system, internet browsers or their plugins. Therefore, operating systems and other software need to be updated from time to time in order to improve their performance and security. In this competition, we are asked to assess the vulnerability of the machine (computer, server, etc.) based on its configuration. If you want to learn more about the IBM's approach in fighting malware you may like to read this article or navigate to IBM Research page to learn more about general research topics in IBM. Exploratory Data Analysis.The first step is to read libraries we will use in our analysis. The basic libraries set for data analysis:  numpy - the fundamental package for scientific computing pandas - datastructures and data analysis tools library matplotlib - data visualisation seaborn - data visualisation;Apache 2.0;https://www.kaggle.com/datark1/malware-prediction-eda;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/microsoft-malware-prediction;0.743;0.481;2020-12-13 14:13:18;Microsoft Malware Prediction;[];Malware Prediction - EDA;Python notebook;8639.0;78;;
2019-02-10 15:54:10;Download repo from https://github.com/guoday/ctrNet-tool;Apache 2.0;https://www.kaggle.com/guoday/nffm-baseline-0-690-on-lb;1.0;['tensorflow'];['ai', 'dl', 'gan', 'nn', 'ann'];['test data', 'train', 'model', 'epoch', 'loss'];https://www.kaggle.com/c/microsoft-malware-prediction;0.754;0.527;2020-12-13 14:13:18;Microsoft Malware Prediction;['gpu'];NFFM baseline (0.690 on LB);Python notebook;11627.0;147;0.63283;0.68851
2019-02-17 19:50:32;Download repo from https://github.com/guoday/ctrNet-tool;Apache 2.0;https://www.kaggle.com/guoday/xdeepfm-baseline;1.0;['tensorflow', 'sklearn'];['ai', 'dl', 'gan', 'nn', 'ann'];['test data', 'train', 'model', 'epoch', 'layer', 'loss', 'relu'];https://www.kaggle.com/c/microsoft-malware-prediction;0.729;0.482;2020-12-13 14:13:18;Microsoft Malware Prediction;['gpu'];xdeepfm baseline;Python notebook;5993.0;79;0.63263;0.69089
2019-02-05 18:22:01;;Apache 2.0;https://www.kaggle.com/kailex/ms-malware-starter;1.0;['lightgbm'];['ner', 'ai', 'gbm', 'nlp', 'nn'];['train', 'deep learning', 'label', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/microsoft-malware-prediction;0.739;0.495;2020-12-13 14:13:18;Microsoft Malware Prediction;['classification, feature engineering, data cleaning, +1 moregradient boosting'];ms_malware_starteR;R script;7679.0;94;0.63788;0.68913
2019-02-20 00:25:05;Based on Kernels & Insights from Aditya Soni, Chris Deotte External (Scrapped Datasets) Useful for EDA :) Time Series EDA - Malware  Best single model  Updated 19-02-2019 Also use other two available dates to add few more rows to private test split, stil did not reach full 33% of private test set  Added private_lb_submission.csv output (all rows I think belong to public lb have prediction forced to zero), as expected score .5 auc;Apache 2.0;https://www.kaggle.com/rquintino/2-months-train-1-month-public-1-day-private;0.5;[];['dl', 'ai', 'nn', 'rl'];['train', 'model', 'filter', 'predict'];https://www.kaggle.com/c/microsoft-malware-prediction;0.73;0.488;2020-12-13 14:13:18;multiple data sources;[];2 months train, 1 month public, 1 day private?;Python notebook;6160.0;86;;
2018-12-16 11:46:25;The data given for the MS Malware Prediction challenge is huge!Training set - 4.08 GBTest Set - 3.54 GBWhen the normal read_csv() function is applied to read the data the processing time is way too long and the kernel eventually dies given the huge size.;Apache 2.0;https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data;0.5;[];['ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['train', 'model', 'predict'];https://www.kaggle.com/c/microsoft-malware-prediction;0.732;0.516;2020-12-13 14:13:18;Microsoft Malware Prediction;[];Large Data Loading Trick with MS-Malware data;Python notebook;6422.0;125;;
2019-02-18 09:06:06;;Apache 2.0;https://www.kaggle.com/sjb1988/lgb-python-basic-features-only;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ai', 'dl', 'gbm', 'gan', 'cv', 'rl', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/microsoft-malware-prediction;0.73;0.473;2020-12-13 14:13:18;Microsoft Malware Prediction;[];LGB - Python basic features only;Python notebook;6181.0;71;;
2019-03-02 15:26:31;Load the Totality of the DataThe data is quite big here, and all of it cannot be loaded at once with a simple read_csv call.A solution is to specify types, to gain memory (for example switching from float64 to float32);Apache 2.0;https://www.kaggle.com/theoviel/load-the-totality-of-the-data;0.5;[];['ai', 'dl', 'gan', 'nn', 'ann'];['train', 'model', 'predict'];https://www.kaggle.com/c/microsoft-malware-prediction;0.769;0.581;2020-12-13 14:13:18;Microsoft Malware Prediction;[];Load the Totality of the Data;Python notebook;17314.0;328;;
2019-03-23 15:49:03;;Apache 2.0;https://www.kaggle.com/tunguz/malware-feature-engineering-full-train-and-test;1.0;['lightgbm', 'sklearn'];['ai', 'dl', 'gbm', 'gan', 'nn', 'ann'];['train', 'model', 'label'];https://www.kaggle.com/c/microsoft-malware-prediction;0.732;0.485;2020-12-13 14:13:18;Microsoft Malware Prediction;[];Malware Feature Engineering  - Full Train and Test;Python notebook;6549.0;82;;
2019-01-08 16:58:05;1. Read and check dataset;Apache 2.0;https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'gan', 'nn', 'ann'];['filter', 'machine learning', 'train', 'model', 'label'];https://www.kaggle.com/c/microsoft-malware-prediction;0.762;0.554;2020-12-13 14:13:18;Microsoft Malware Prediction;[];My EDA - I want to see all!;Python notebook;14133.0;218;;
2018-06-23 18:09:02;;Apache 2.0;https://www.kaggle.com/ambarish/rotten-reviews;1.0;['vocabulary'];['cv', 'ai', 'nn', 'gan'];['filter', 'test data', 'regression', 'train', 'model', 'label', 'logistic regression', 'predict'];https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;0.676;0.352;2020-12-13 14:14:35;Movie Review Sentiment Analysis (Kernels Only);['data visualization, text mining'];Rotten Reviews;Rmarkdown script;1820.0;17;0.51849;0.51849
2018-09-24 20:55:56;Analyzing Movie ReviewsArindam Dutta 24-09-2018 (Version-14);Apache 2.0;https://www.kaggle.com/arindamgot/romanticreviews-moodring-directedgraph-ngram-lda;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'train', 'fitting', 'model', 'natural language processing', 'clustering', 'label', 'labeled', 'predict', 'recommend', 'sentiment analysis', 'classification', 'natural language'];https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;0.707;0.393;2020-12-13 14:14:35;Movie Review Sentiment Analysis (Kernels Only);['data visualization, exploratory data analysis, nlp, +2 moretext data, text mining'];RomanticReviews:MoodRing+DirectedGraph+Ngram+LDA;R notebook;3576.0;27;;
2018-06-24 21:15:43;;Apache 2.0;https://www.kaggle.com/codename007/glove-lstm;1.0;['tensorflow', 'sklearn', 'keras'];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'sentiment analysis', 'classification'];https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;0.701;0.379;2020-12-13 14:14:35;multiple data sources;[]; Glove + LSTM;Python script;3118.0;23;0.65422;0.65422
2018-07-10 11:21:18;Importing Library;Apache 2.0;https://www.kaggle.com/nafisur/keras-lstm;1.0;['tensorflow', 'sklearn', 'keras', 'nltk'];['ai', 'rl', 'gan'];['filter', 'test data', 'train', 'model', 'epoch', 'layer', 'loss', 'lstm', 'predict', 'classification'];https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;0.705;0.413;2020-12-13 14:14:35;Movie Review Sentiment Analysis (Kernels Only);[];Keras-LSTM;Python notebook;3430.0;34;0.64593;0.64593
2018-07-13 15:00:43;Importing Library;Apache 2.0;https://www.kaggle.com/nafisur/keras-models-lstm-cnn-gru-bidirectional-glove;1.0;['tensorflow', 'sklearn', 'keras', 'nltk'];['ai', 'cnn', 'gan', 'rl', 'nn'];['gru', 'filter', 'test data', 'train', 'model', 'epoch', 'layer', 'loss', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;0.75;0.478;2020-12-13 14:14:35;multiple data sources;['gpu'];Keras-models:LSTM,CNN,GRU,Bidirectional,glove;Python notebook;10398.0;75;;
2018-07-09 15:10:18;Loading dataset and basic visualization;Apache 2.0;https://www.kaggle.com/nafisur/sentiment-analysis-for-beginner;1.0;['sklearn', 'nltk'];['ai', 'nn', 'ml', 'cv'];['test data', 'regression', 'train', 'model', 'label', 'logistic regression', 'predict'];https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;0.714;0.39;2020-12-13 14:14:35;Movie Review Sentiment Analysis (Kernels Only);[];Sentiment Analysis: For Beginner;Python notebook;4169.0;26;;
2018-07-15 05:24:54;Combined LSTM-CNN Models;Apache 2.0;https://www.kaggle.com/parth05rohilla/bi-lstm-and-cnn-model-top-10;1.0;['lightgbm', 'nltk', 'sklearn', 'tensorflow', 'keras'];['ner', 'ai', 'dl', 'cnn', 'gbm', 'gan', 'rl', 'nn'];['gru', 'filter', 'regression', 'train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'sentiment analysis', 'convolutional neural network'];https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;0.752;0.413;2020-12-13 14:14:35;multiple data sources;['gpu, beginner, cnn, +1 morelstm'];Bi-LSTM and CNN model-TOP 10%;Python notebook;10790.0;34;;
2018-07-20 14:03:31;;Apache 2.0;https://www.kaggle.com/taindow/deep-learning-with-r-sentiment-analysis;1.0;['keras'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'recommend', 'sentiment analysis', 'classification'];https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only;0.738;0.371;2020-12-13 14:14:35;multiple data sources;['gpu, deep learning, classification, +1 moretext data'];Deep Learning with R: Sentiment Analysis;Rmarkdown script;7584.0;21;0.64276;0.64276
2017-06-27 18:11:25;;Apache 2.0;https://www.kaggle.com/clustifier/basic-two;1.0;['pattern', 'sklearn'];['ner', 'ai', 'dl', 'nlp', 'nn'];['filter', 'regression', 'train', 'model', 'deep learning', 'loss', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.704;0.383;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;['nlp, logistic regression']; Basic two;Python script;3368.0;24;0.00000;0.00000
2019-01-07 18:06:03;;Apache 2.0;https://www.kaggle.com/headsortails/personalised-medicine-eda-with-tidy-r;0.5;[];['ner', 'ai', 'gan', 'gbm', 'rl', 'nn'];['filter', 'training data', 'test data', 'train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.79;0.573;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;['beginner, data visualization, exploratory data analysis, +2 morefeature engineering, nlp'];Personalised Medicine - EDA with tidy R;Rmarkdown script;33432.0;289;;
2017-06-30 09:26:41;;Apache 2.0;https://www.kaggle.com/kevinbonnes/basic-tf-idf-model-0-647-lb;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'sentiment analysis', 'classification'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.726;0.397;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;['nlp, xgboost'];Basic TF-IDF model [0.647 LB];R script;5646.0;28;;
2017-08-08 16:38:59;Donated to Cancer Treatment Too;Apache 2.0;https://www.kaggle.com/megemini/modified-the1owl-redefining-treatment-0-57018;1.0;['xgboost', 'sklearn'];['ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['test data', 'regression', 'train', 'fitting', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.678;0.352;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;[];"Modified:""the1owl: Redefining Treatment""[0.57018]";Python notebook;1914.0;17;;
2017-08-16 15:10:24;;Apache 2.0;https://www.kaggle.com/merckel/nci-thesaurus-naive-bayes-vs-rf-gbm-glm-dl;1.0;['xgboost', 'h2o'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'ml', 'nn', 'ann'];['machine learning', 'random forest', 'train', 'model', 'epoch', 'deep learning', 'loss', 'label', 'gradient boosting', 'text classification', 'predict', 'rectifier', 'classification', 'naive bayes'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.713;0.357;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;[];NCI Thesaurus & Naive Bayes (vs RF, GBM, GLM & DL);Rmarkdown script;4137.0;18;;
2017-08-22 09:16:54;Load training_text and training_variants;Apache 2.0;https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm;1.0;['xgboost', 'nltk', 'gensim', 'sklearn', 'tensorflow', 'pattern', 'keras'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['training data', 'test data', 'regression', 'random forest', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'logistic regression', 'lstm', 'predict', 'classification'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.807;0.518;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;['exploratory data analysis, nlp, lstm, +1 moreadvanced'];Basic NLP: Bag of Words, TF-IDF, Word2Vec, LSTM;Python notebook;58178.0;128;;
2017-09-28 01:46:43;;Apache 2.0;https://www.kaggle.com/ryanzhang/use-stage-1-solution-as-stage-2-submission;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.687;0.346;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;[];Use stage 1 solution as stage 2 submission;Python script;2326.0;16;2.19722;0.14012
2017-06-27 06:39:01;In this notebook, let us explore the given data and understand it so as to build our models. Objective: Develop algorithms to classify genetic mutations based on clinical evidence (text) First let us import the necessary modules.;Apache 2.0;https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-personalized-medicine;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['generation', 'train', 'model', 'layer', 'loss', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.748;0.46;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;['beginner, linguistics'];Simple Exploration Notebook-Personalized Medicine;Python notebook;9708.0;60;;
2017-07-16 18:58:33;;Apache 2.0;https://www.kaggle.com/swanny/splitting-variations-into-features-0-595-lb;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'test data', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.701;0.357;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;[];Splitting Variations Into Features  [0.595 LB];R script;3108.0;18;;
2017-09-29 09:13:27;As per this answer from discussion forum, we can use the test labels from stage 1 for training. I've concatenated all former csv's into a single DataFrame for my own use, thought it would be helpfull for others as well.  This notebook will read all data into memory, join relevant files on relevant id's, then output a stage2 train DataFrame which has [3689, 4] (3321 training values + 368 stage 1 test values) shape with ['Class', 'Gene', 'Text', 'Variation'] columns. Since the relased test ID's and former training ID's conflict with each other, I've just reset the index on final training DataFrame to make them unique and cause less confusion. If you want to keep it as is you can skip cell 4.;Apache 2.0;https://www.kaggle.com/umutto/using-stage-1-test-results-for-stage-2-training;0.5;[];['ai'];['train', 'training data', 'label', 'filter'];https://www.kaggle.com/c/msk-redefining-cancer-treatment;0.652;0.327;2020-12-13 14:17:31;Personalized Medicine: Redefining Cancer Treatment;[];Using stage 1 test results for stage 2 training;Python notebook;1132.0;13;;
2020-06-05 17:15:32;;Apache 2.0;https://www.kaggle.com/adelyagarieva/kernel45f13df162;0.5;[];['dl', 'ai', 'nn'];['train', 'label', 'classification'];https://www.kaggle.com/c/multilabel-bird-species-classification-nips2013;0.504;0.0;2020-12-13 14:17:49;Multi-label Bird Species Classification - NIPS 2013;[];kernel45f13df162;Python notebook;106.0;0;;
2019-03-27 10:09:21;;Apache 2.0;https://www.kaggle.com/leirahua/bird-songs-generate-spectrograms;0.5;[];['dl', 'ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/multilabel-bird-species-classification-nips2013;0.689;0.0;2020-12-13 14:17:49;Multi-label Bird Species Classification - NIPS 2013;['gpu'];Bird songs - generate spectrograms;Python notebook;2429.0;0;;
2019-03-28 01:20:49;;Apache 2.0;https://www.kaggle.com/leirahua/bird-songs-pad-and-resize-spectrogram;0.5;[];['dl', 'ner', 'ai', 'nn'];['train', 'label', 'classification'];https://www.kaggle.com/c/multilabel-bird-species-classification-nips2013;0.643;0.236;2020-12-13 14:17:49;multiple data sources;[];Bird songs - pad and resize spectrogram;Python notebook;953.0;5;;
2019-03-28 01:40:01;Clean up labels;Apache 2.0;https://www.kaggle.com/leirahua/classify-bird-songs-using-spectrogram;1.0;['pytorch'];['ner', 'ai', 'nn', 'cnn'];['train', 'model', 'epoch', 'loss', 'label', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/multilabel-bird-species-classification-nips2013;0.645;0.152;2020-12-13 14:17:49;multiple data sources;['gpu'];Classify bird songs using spectrogram;Python notebook;988.0;2;0.86224;0.87101
2018-07-31 22:33:45;Bayesian Optimization with XGBoost;Apache 2.0;https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost;1.0;['xgboost', 'sklearn'];['ai', 'dl', 'rl', 'cv', 'nn'];['train', 'model', 'label', 'predict', 'bayesian'];https://www.kaggle.com/c/new-york-city-taxi-fare-prediction;0.773;0.467;2020-12-13 14:20:01;New York City Taxi Fare Prediction;['gpu, feature engineering, xgboost'];Bayesian Optimization with XGBoost;Python notebook;19470.0;65;3.03767;3.03767
2018-09-15 02:30:42;;Apache 2.0;https://www.kaggle.com/jsylas/python-version-of-top-ten-rank-r-22-m-2-88;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/new-york-city-taxi-fare-prediction;0.718;0.427;2020-12-13 14:20:01;New York City Taxi Fare Prediction;[];Python Version  of Top Ten Rank R 22 M (2.88) ;Python script;4584.0;40;;
2018-09-08 07:21:03;;Apache 2.0;https://www.kaggle.com/jsylas/top-ten-rank-r-22m-rows-2-90-lightgbm;1.0;['xgboost', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn'];['filter', 'test data', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/new-york-city-taxi-fare-prediction;0.726;0.379;2020-12-13 14:20:01;New York City Taxi Fare Prediction;[];Top Ten Rank - R 22M Rows(2.90) LightGBM;R script;5666.0;23;2.90634;2.90634
2020-09-06 14:59:45;Data Cleaning;Apache 2.0;https://www.kaggle.com/justjun0321/exploratory-geoclustering-to-modeling;1.0;['sklearn'];['ai', 'rl'];['predict', 'train', 'model', 'label', 'clustering'];https://www.kaggle.com/c/new-york-city-taxi-fare-prediction;0.693;0.367;2020-12-13 14:20:01;New York City Taxi Fare Prediction;['feature engineering, clustering'];Exploratory + GeoClustering to modeling;Python notebook;2627.0;20;6.79414;6.79414
2018-09-10 08:45:20;;Apache 2.0;https://www.kaggle.com/obrienmitch94/nyc-taxi-fare-prediction;1.0;['xgboost'];['ner', 'ai', 'rl', 'nn', 'ml'];['filter', 'training data', 'train', 'fitting', 'model', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/new-york-city-taxi-fare-prediction;0.727;0.397;2020-12-13 14:20:01;New York City Taxi Fare Prediction;['data visualization, xgboost, pca'];NYC Taxi Fare Prediction;Rmarkdown script;5770.0;28;;
2018-08-03 12:01:01;Bayesian Optimization with XGBoost;Apache 2.0;https://www.kaggle.com/sanket30/bayesian-optimization-with-xgb-hyperparameter;1.0;['xgboost', 'sklearn', 'lightgbm'];['ai', 'dl', 'gbm', 'cv', 'rl', 'nn'];['train', 'model', 'label', 'predict', 'bayesian'];https://www.kaggle.com/c/new-york-city-taxi-fare-prediction;0.679;0.352;2020-12-13 14:20:01;New York City Taxi Fare Prediction;['gpu'];Bayesian Optimization with Xgb+hyperparameter;Python notebook;1931.0;17;3.23598;3.23598
2018-07-29 14:58:13;New York City Taxi Fare Prediction   In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. This notebook focuses on the Exploratory Data Analysis of the following dataset. Currently the plots are implemented with plotly and custom map styles are possible with Mapbox. Maps and the EDA will be updated twice a week.;Apache 2.0;https://www.kaggle.com/shaz13/simple-exploration-notebook-map-plots-v2;0.5;[];['ner', 'ai', 'nn', 'rl'];['filter', 'training data', 'train', 'model', 'predict'];https://www.kaggle.com/c/new-york-city-taxi-fare-prediction;0.738;0.491;2020-12-13 14:20:01;New York City Taxi Fare Prediction;['data visualization, exploratory data analysis, feature engineering, +2 moredailychallenge, learn'];Simple Exploration Notebook + Map Plots v2;Python notebook;7516.0;89;;
2019-10-29 19:38:23;Overall analysis;Apache 2.0;https://www.kaggle.com/bgmello/neural-networks-feature-engineering-for-the-win;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['predict', 'train', 'model', 'epoch', 'layer', 'loss', 'relu', 'random forest'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.847;0.607;2020-12-13 14:21:44;NFL Big Data Bowl;['feature engineering, data cleaning, neural networks'];neural networks + feature engineering for the win;Python notebook;273275.0;507;0.014249;0.014249
2019-11-05 19:02:45;;Apache 2.0;https://www.kaggle.com/coolcoder22/nn-19-features;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'nlg', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.725;0.505;2020-12-13 14:21:44;NFL Big Data Bowl;[];NN-19 features;Python script;5407.0;108;;
2019-11-14 20:49:40;V 12.  Fixed orientation V 13.  Added 2019 (first stage test) V 14. Added Voronoi areas V 16.  Handling of unbounded Voronoi regions;Apache 2.0;https://www.kaggle.com/cpmpml/initial-wrangling-voronoi-areas-in-python;0.5;[];['ai', 'dl', 'rl', 'nn', 'rnn', 'ml'];['train', 'recommend', 'test data', 'layer'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.767;0.57;2020-12-13 14:21:44;multiple data sources;[];initial wrangling & Voronoi areas in Python;Python notebook;16635.0;278;;
2019-10-09 23:26:20;"NFL Big Data Bowl 2020 Official Starter NotebookIntroductionIn this competition you will predict how many yards a team will gain on a rushing play in an NFL regular season game.  You will loop through a series of rushing plays; for each play, you'll receive the position, velocity, orientation, and more for all 22 players on the field at the moment of handing the ball off to the rusher, along with many other features such as teams, stadium, weather conditions, etc.  You'll use this information to predict how many yards the team will gain on the play as a cumulative probability distribution.  Once you make that prediction, you can move on to the next rushing play. This competition is different from most Kaggle Competitions in that:  You can only submit from Kaggle Notebooks, and you may not use other data sources, GPU, or internet access. This is a two-stage competition.  In Stage One you can edit your Notebooks and improve your model, where Public Leaderboard scores are based on your predictions on rushing plays from the first few weeks of the 2019 regular season.  At the beginning of Stage Two, your Notebooks are locked, and we will re-run your Notebooks over the following several weeks, scoring them based on their predictions relative to live data as the 2019 regular season unfolds. You must use our custom kaggle.competitions.nflrush Python module.  The purpose of this module is to control the flow of information to ensure that you are not using future data to make predictions for the current rushing play.  If you do not use this module properly, your code may fail when it is re-run in Stage Two.  In this Starter Notebook, we'll show how to use the nflrush module to get the training data, get test features and make predictions, and write the submission file.TL;DR: End-to-End Usage Example from kaggle.competitions import nflrush env = nflrush.make_env()  # Training data is in the competition dataset as usual train_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False) train_my_model(train_df)  for (test_df, sample_prediction_df) in env.iter_test():   predictions_df = make_my_predictions(test_df, sample_prediction_df)   env.predict(predictions_df)  env.write_submission_file() Note that train_my_model and make_my_predictions are functions you need to write for the above example to work.";Apache 2.0;https://www.kaggle.com/dster/nfl-big-data-bowl-official-starter-notebook;0.5;[];['ner', 'ai', 'nn', 'rl'];['training data', 'test data', 'train', 'model', 'layer', 'predict'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.791;0.579;2020-12-13 14:21:44;NFL Big Data Bowl;['beginner, sports'];NFL Big Data Bowl Official Starter Notebook;Python notebook;34104.0;318;;
2019-11-20 18:16:54;NFL Big Data Bowl;Apache 2.0;https://www.kaggle.com/fatihbilgin/nfl-big-data-visualization;0.5;[];['ner', 'ai', 'dl', 'gan', 'ml', 'nn', 'ann'];['filter', 'train', 'layer', 'label', 'understanding'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.748;0.537;2020-12-13 14:21:44;NFL Big Data Bowl;['beginner, data visualization, exploratory data analysis'];NFL Big Data Visualization;Python notebook;9862.0;168;;
2019-10-15 22:56:34;;Apache 2.0;https://www.kaggle.com/jaseziv83/comprehensive-cleaning-and-eda-of-all-variables;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.734;0.542;2020-12-13 14:21:44;NFL Big Data Bowl;['exploratory data analysis, feature engineering, data cleaning'];Comprehensive Cleaning and EDA of ALL Variables;Rmarkdown script;6837.0;182;;
2019-10-26 06:28:05;Official Github and source of the code: Big-Data-BowlImportant The code uses gganimate and requires the package gifski but I can't install the package here in kaggle, it requires a rust compiler. You can solve this downloading the code, and running one of the following options: apt-get install cargo 2nd option (conda) conda install rust   As soon as I solve the problem hre, I'll generate gifs for all the plays :);Apache 2.0;https://www.kaggle.com/jesucristo/animated-visualization;0.5;[];['ner', 'ai', 'dl', 'gan', 'ml', 'nn', 'ann'];['layer', 'label', 'filter'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.71;0.477;2020-12-13 14:21:44;NFL Big Data Bowl;['beginner, data visualization'];ðŸˆ Animated Visualization;R notebook;3857.0;74;;
2019-11-12 16:14:58;Presentation;Apache 2.0;https://www.kaggle.com/pednt9/vip-hint-coded;0.5;[];['dl', 'ai', 'nn', 'rnn'];['train', 'model', 'label', 'layer'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.709;0.497;2020-12-13 14:21:44;NFL Big Data Bowl;['feature engineering, research'];VIP Hint coded;Python notebook;3771.0;96;;
2019-10-22 19:57:07;2020 NFL Big Data Bowl In this notebook I will attempt to provide a basic overview of the data given in the NFL Big Data Bowl kaggle challenge. We will attempt to better understand each variable provided to us in the train.csv data file. From the competition overview: In this competition, you will develop a model to predict how many yards a team will gain on given rushing plays as they happen. You'll be provided game, play, and player-level data, including the position and speed of players as provided in the NFLâ€™s Next Gen Stats data. And the best part - you can see how your model performs from your living room, as the leaderboard will be updated week after week on the current seasonâ€™s game data as it plays out.;Apache 2.0;https://www.kaggle.com/robikscube/big-data-bowl-comprehensive-eda-with-pandas;0.5;[];['ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'layer', 'label', 'predict'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.723;0.514;2020-12-13 14:21:44;NFL Big Data Bowl;[];ðŸŸï¸ Big Data Bowl - Comprehensive EDA with Pandas;Python notebook;5153.0;121;;
2020-10-14 22:42:48;NFL Big Data BowlPlotting the fieldUsing some code that I developed for last year's NFL challenege, this notebook shows how to plot player positions during a play on the football field. Using matplotlib we can call the create_football_field function to create a figure with the football field drawn out. You can then overlay any information from the training data to help visualize how players positions look on the field. The design is loosely based off of the 1991 video game Techo Super Bowl. A game which I spent many hours playing in my next door neighbor's basement growing up (I wasn't allowed to own a video game console so we had to play at his house).;Apache 2.0;https://www.kaggle.com/robikscube/nfl-big-data-bowl-plotting-player-position;0.5;[];['dl', 'ai', 'nn', 'rl'];['train', 'training data', 'layer'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.767;0.574;2020-12-13 14:21:44;multiple data sources;[];NFL Big Data Bowl - Plotting Player Position;Python notebook;16554.0;296;;
2019-10-15 08:48:20;Checking out what the yardage cumulative distribution function looks like from all of our data;Apache 2.0;https://www.kaggle.com/ryches/model-free-benchmark;0.5;[];['dl', 'ai', 'nn'];['train', 'predict'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.715;0.51;2020-12-13 14:21:44;NFL Big Data Bowl;[];Model-Free Benchmark;Python notebook;4323.0;115;;
2019-11-02 06:19:04;Cox proportional hazard model;Apache 2.0;https://www.kaggle.com/sryo188558/cox-proportional-hazard-model;1.0;['statsmodels'];['ner', 'ai', 'dl', 'nn', 'ann'];['training data', 'train', 'model', 'layer', 'predict'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.734;0.532;2020-12-13 14:21:44;NFL Big Data Bowl;[];Cox proportional hazard model;Python notebook;6793.0;157;0.013946;0.013946
2019-10-15 17:07:11;Dealing with player tracking dataOne initial but tricky issue when working with NFL player tracking data is that sometimes players are moving left to right down the field, and other times those same players are moving right to left. In this walk-through, I do the following:  Standardize player tracking coordinates so that offensive units are moving in the same direction throughout the entirety of the game, while also sharing tweaks that should be made regarding team names.  Create a sample Voronoi diagram using tracking coordinates.  Standardize the direction variable for the ball carrier, and provide video to represent of that standardization manifests itself within a play.  Create angle charts for player movement at a snapshot of time within a play.  Create sonar maps for player direction.   1. Standardizing coordinatesOne recommended technique for working with this data is to standardize play directions, so that the offensive team is always moving in the same direction. This idea is particularly important for this year's Big Data Bowl event on Kaggle, where many participants will look to engineer football-specific features. To start off, let's read in the data, creating a dummy variable for plays moving left to right (ToLeft), as well as an indicator for whether or not the player is the ball carrier (IsBallCarrier). Next, I add in a few tweaks to account for different factors between the PossessionTeam/FieldPosition variables and the HomeTeamAbbr/VisitorTeamAbbr.;Apache 2.0;https://www.kaggle.com/statsbymichaellopez/nfl-tracking-wrangling-voronoi-and-sonars;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'rnn'];['filter', 'train', 'model', 'layer', 'loss', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/nfl-big-data-bowl-2020;0.761;0.554;2020-12-13 14:21:44;NFL Big Data Bowl;[];NFL tracking: wrangling, Voronoi, and sonars;R notebook;13866.0;217;;
2020-01-02 00:33:19;;Apache 2.0;https://www.kaggle.com/aleksandradeis/nfl-injury-analysis;1.0;['tensorflow', 'xgboost', 'sklearn'];['ner', 'ai', 'dl', 'nn', 'ann'];['machine learning', 'train', 'model', 'layer', 'label', 'predict'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.739;0.513;2020-12-13 14:23:02;multiple data sources;['data visualization, exploratory data analysis'];NFL Injury Analysis;Python notebook;7778.0;120;;
2020-01-03 00:38:45;NFL 1st and Future - Analytics By Ben Jenkins and Steve Jenkins Key Findings  We have verified that synthetic surfaces have a greater probability of injury.â€¯ From the data provided, synthetic surfaces have approximately 1.8 times greater injury rate compared to natural surfaces  Injuries tend to be more severe on synthetic surfaces in terms of days missed, and injuries on synthetic turf tend to more dispersed across the field. This suggests that non-contact injuries mayâ€¯play a larger role in synthetic compared to natural turf. Temperature plays a role in injuries. Low temperatures (10 degrees to 40 degrees) is a predictor of low injury rates. There are differences in speed and acceleration between the injured and non-injured population. The injured population has a higher average max speed and acceleration. For a given weight of player, greater acceleration would create more force (Force=Mass  times  Acceleration) and likelihood of injury.  Players tend to be injured early in the game. This tendency is more pronounced for natural playing surfaces compared to synthetic.â€¯Early in the game players tend to run faster and may generate more force leading to increased injury rate.â€¯There is a weak negative correlation between the number of plays a player participates in during a game, and their speed. The player location plays a role in injury incidence. A model was built to predict injuries based on the variables provided in the competition. The model's performance and findings are discussed in the presentation provided. The injury model suggests that there is a higher chance for injuries at low yardage (x axis is the long axis of field).  At high yardage, the model suggests that there is less chance for injury Recommendations for NFL  Substitute natural turf for synthetic turf. This analysis shows that this could significantly reduce the lower-limb injury incidence. Play in indoor stadiums. This allows the ability to control weather and temperature, which are shown to contribute to injuries. A variety of other factors considered in this analysis cannot be realistically addressed by the NFL, without drastic changes to the rules of the game.  We note that player speed, acceleration, number of plays, orientation and location on the field impact the probability of injury. Other factors not considered in this analysis such as type of footwear, padding, etc. may be useful to reduce injury Recommendations for Future Analysis  More data from lower-limb injuries is needed to allow the model to more accurately predict injury occurrence.  For example, collecting data on additional seasons would provide more information on injury trends. Higher quality data, such as helmet mounted accelerometers would likely improve injury prediction. Another key improvement to the model would be a detection of the exact time at which the injury occurred. This would allow analysis of the pre and post injury movements by the player. In some cases, the time of the injury may not be available.;Apache 2.0;https://www.kaggle.com/benjenkins96/nfl-1st-and-future-analysis;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['filter', 'model', 'layer', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.655;0.292;2020-12-13 14:23:02;multiple data sources;[];NFL 1st and Future - Analysis;Python notebook;1203.0;9;;
2019-12-01 15:35:12;The purpose of this notebook is to provide a method for visualizing player movement, focusing on injury plays. The thought is that this may help generate ideas of how to analyze the data. An animated plot function using Plotly is provided that shows player movement on the field, with accompanying speed and acceleration subplots.;Apache 2.0;https://www.kaggle.com/bgpablo/visualization-of-player-movement-speed-accel;0.5;[];['ner', 'ai', 'dl', 'nn', 'ann'];['layer', 'label', 'filter'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.646;0.393;2020-12-13 14:23:02;NFL 1st and Future - Analytics;[];Visualization of Player Movement, Speed, Accel;Python notebook;1005.0;27;;
2020-01-08 03:52:58;IntroductionPlease upvote, motivate me to improve this kernel.We're tasked to investigate the relationship between the playing surface and the injury and performance of National Football League (NFL) athletes and to examine factors that may contribute to lower extremity injuries.;Apache 2.0;https://www.kaggle.com/chandraroy/nfl-analytics;0.5;[];['dl', 'ner', 'ai', 'nn'];['layer', 'understanding', 'filter'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.627;0.311;2020-12-13 14:23:02;NFL 1st and Future - Analytics;[];NFL Analytics;Python notebook;717.0;11;;
2019-11-28 15:12:11;Lets start with some basic visualizations of our Dataset with the most important features:The PlayList dataset! First we need to look at some basic numbers, for example how many Injuries happened on which Surface;Apache 2.0;https://www.kaggle.com/habbeda/contradictory-discovery-solved;0.5;[];['ner', 'ai', 'rl'];['layer'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.6;0.292;2020-12-13 14:23:02;NFL 1st and Future - Analytics;[];Contradictory Discovery - Solved?;R notebook;451.0;9;;
2019-12-03 04:53:40;;Apache 2.0;https://www.kaggle.com/jaseziv83/an-analysis-of-nfl-injuries;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['layer', 'label', 'filter'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.738;0.491;2020-12-13 14:23:02;NFL 1st and Future - Analytics;['data visualization, exploratory data analysis, feature engineering, +1 moredata cleaning'];An Analysis of NFL Injuries;Rmarkdown script;7489.0;89;;
2019-12-25 06:03:18;The issueThe purpose of this notebook is to adjust player orientation between the two seasons for which we have data. The orientation issue is described in this post from Peter Hurford as part of the NFL DataBowl challenge. ASSUMPTION: We are given data for 2017 and 2018, or at least for two seasons with the same apparent difference in measuring orientation. CAUTION: Use this data adjustment at your own risk. It may or may not be valid. But it probably is directionally correct.;Apache 2.0;https://www.kaggle.com/jpmiller/how-to-adjust-orientation;0.5;[];['ner', 'rl'];['layer'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.623;0.357;2020-12-13 14:23:02;NFL 1st and Future - Analytics;[];How to Adjust Orientation;Python notebook;661.0;18;;
2020-04-14 20:38:58;Reducing Lower Body Injuries;Apache 2.0;https://www.kaggle.com/jpmiller/nfl-1standfuture-report;1.0;['pattern', 'xgboost', 'sklearn', 'h2o'];['ner', 'ai', 'cv', 'rl', 'nn', 'ml'];['machine learning', 'generation', 'train', 'model', 'layer', 'label', 'gradient boosting', 'predict', 'rank', 'labeled'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.712;0.471;2020-12-13 14:23:02;multiple data sources;[];NFL 1standFuture Report ðŸˆ;Python notebook;3980.0;69;;
2019-12-16 06:33:18;There's been some discussion around dealing with PlayerTrackData.csv and it's strain on system memory. As some have pointed out, one solution is to use Kaggle kernels. Personally I don't like to develop with a Jupyter notebook, especially a remote one. I'd much rather work on my local machine and use something like VS Code with the python extension and all the other goodies. Today I finally got into the player track data. My desktop with 24MBGB RAM could barely handle the file straight up. So I used Pandas chunk feature along with my MemReducer utility script. There are many versions of memory reducers out there, but this one is more effective than most. It's actually pretty simple - you can find it here. Also, for this data I prefer to have the Players, Games, and Plays separated vs. the hyphenated thing (makes it easier for grouping and filtering plus gives us memory-efficient integers instead of silly strings). Here's what I'm doing:;Apache 2.0;https://www.kaggle.com/jpmiller/using-track-data-with-small-memory;0.5;[];['ai', 'dl'];['train', 'filter', 'layer'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.623;0.375;2020-12-13 14:23:02;multiple data sources;[];Using Track Data on a Memory Budget;Python notebook;670.0;22;;
2019-12-11 19:00:44;NFL 1st and Future 2019Can you investigate the relationship between the playing surface and the injury and performance of NFL athletes?;Apache 2.0;https://www.kaggle.com/pednt9/nfl-1st-and-future-express-comprehensive-eda;0.5;[];['ai'];['layer', 'filter'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.623;0.281;2020-12-13 14:23:02;NFL 1st and Future - Analytics;[];NFL 1st and Future â€“Â Express Comprehensive EDA;Python notebook;672.0;8;;
2019-12-22 17:51:33;"NFL 1st and Future 2019Can you investigate the relationship between the playing surface and the injury and performance of NFL athletes? This kernel is made in hopes of helping those interested in joining the competition get a jump start on the data. Much of the text was taken directly from the competition description. However be sure to read the official rules and data description on the kaggle website here. tl;dr: In this challenge, you're tasked to investigate the relationship between the playing surface and the injury and performance of National Football League (NFL) athletes and to examine factors that may contribute to lower extremity injuries. Submissions will be judged by the NFL based on how well they address:  Representation of player movement, including, but not limited to, the development of novel metrics that characterize player movement on the field: Identification of specific variables that present an elevated risk of injury: Evaluation of differences in player movement between playing surfaces:  Submissions will be scored using the following rubric:  Creativity and Presentation (5 points) Methodology (5 points) Application (5 points)";Apache 2.0;https://www.kaggle.com/robikscube/nfl-1st-and-future-analytics-intro;0.5;[];['ai', 'dl', 'rl', 'nn', 'ann'];['layer'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.747;0.518;2020-12-13 14:23:02;NFL 1st and Future - Analytics;['data visualization, sports'];NFL 1st and Future - Analytics Intro;Python notebook;9616.0;129;;
2020-01-03 00:42:36;Interactive Analysis By DB, LB and WR (PART II);Apache 2.0;https://www.kaggle.com/zinovadr/nfl-tableau-analysis;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['train', 'layer'];https://www.kaggle.com/c/nfl-playing-surface-analytics;0.618;0.346;2020-12-13 14:23:02;multiple data sources;[];NFL Tableau Analysis ;Python notebook;610.0;16;;
2018-12-05 09:48:42;All files apart from the NGS files are quite small. The NGS files however, each can contain up to 9 million rows. In that case we need to reduce the amount of memory they take up. In this kernel I will show you how to do this in an easy way. The NGS datasets contains player position, speed and direction data for each player during the entire course of the play. The NGS dataset is the only dataset that contains Timeas a variable.;Apache 2.0;https://www.kaggle.com/akosciansky/how-to-import-large-csv-files-and-save-efficiently;0.5;[];['ai', 'dl'];['layer'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.663;0.352;2020-12-13 14:25:10;NFL Punt Analytics Competition;[];How to Import Large CSV Files and Save Efficiently;Python notebook;1401.0;17;;
2018-12-10 00:03:53;Watch the Videos Showing InjuryFor manual viewing (with eyes) it's easiest to see collisions causing injury by looking for the jersey numbers in the videos. This notebook's output generates a table catered to simple display with minimal distraction data for manual viewing.;Apache 2.0;https://www.kaggle.com/coakeson/watch-the-videos-showing-injury;0.5;[];['ner', 'dl', 'ml', 'nn', 'ann'];['layer'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.672;0.39;2020-12-13 14:25:10;NFL Punt Analytics Competition;[];Watch the Videos Showing Injury;Python notebook;1700.0;26;;
2018-12-04 20:12:00;Brief EDAThis is a starter kernel for the NFL Analytics Competition! I'm not eligible to compete and it doesn't go into much depth but I do get to revel in the fact that I was the first! Maybe it will spur some inspiration. If you find it helpful, that's great! If you find it completely unhelpful, awesome! I challenge you to make something better and win $20,000 :) I start with the Video Review data because it seems fairly high-level. After a quick look, I find a particular game that was of interest because it has two punt plays where players recieved concussons.;Apache 2.0;https://www.kaggle.com/crawford/nfl-punt-analytics-starter-kernel;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['layer', 'label'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.709;0.455;2020-12-13 14:25:09;NFL Punt Analytics Competition;[];NFL Punt Analytics Starter Kernel;Python notebook;3751.0;56;;
2019-01-12 18:54:32;;Apache 2.0;https://www.kaggle.com/gaborfodor/exploratory-data-analysis-external-data;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'layer', 'label', 'loss', 'rank'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.685;0.352;2020-12-13 14:25:10;multiple data sources;['data visualization, exploratory data analysis, feature engineering, +2 moredata cleaning, sports'];Exploratory Data Analysis & External Data;Python notebook;2209.0;17;;
2018-12-18 11:45:46;NFL Rules Wordcloud;Apache 2.0;https://www.kaggle.com/gaborfodor/nfl-rules-wordcloud;0.2;[];['ner', 'nlp'];[];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.677;0.431;2020-12-13 14:25:09;multiple data sources;['data visualization, sports'];NFL Rules Wordcloud;Python notebook;1882.0;42;;
2019-01-12 18:17:35;;Apache 2.0;https://www.kaggle.com/gaborfodor/the-speed-the-acceleration-and-the-collision;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['layer', 'label', 'filter', 'predict'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.631;0.346;2020-12-13 14:25:10;multiple data sources;['data visualization, exploratory data analysis, feature engineering, +1 moresports'];The Speed, the Acceleration and the Collision;Python notebook;769.0;16;;
2019-02-10 03:23:32;NFL Punt Analytics ProposalThirty seven known punt-related concussions occurred during the 2016 and 2017 NFL seasons. By reviewing the evidence from all 6,681 punt plays that are recorded from these two seasons, I seek to show that the following three rule changes will improve player safety while being implementable and preserving the excitement and unpredictability of the great game of football.  Proposal #1: Award a five yard bonus to the receiving team on a fair catch.  Proposal #2: Require single coverage of gunners by the receiving team. Proposal #3: Install helmet sensors to monitor deceleration.  In support of proposal #1, I attempt to quantify the risk-reward tradeoff of reducing the number of punt returns and increasing the number of fair catches. I find that 32% of punt returns conditional on a punt received yield fewer than five yards on the return. In the current framework, punt returners have an incentive to attempt a return whenever they think their expected value of returning is greater than 0 yards. In the new framework, punt returns will have an incentive to make a return only when they believe the expected value of returning is greater than 5 yards. The concussion rate is 10 injuries per 1000 plays for a fielded return versus 1.8 injuries per 1000 plays for a fair catch, so we expect a 80% decrease in concussions for each incremental fair catch that is called. On the other hand, punt returners will continue to be able to try to go for a return and make a play if they see an opportunity to go for it. In support of proposal #2, I analyze the injury rate conditioning on both the choice of coverage and on the yards between the line of scrimmage and the end zone, as these factors are not independent of one another. Teams are more likely to choose double coverage when there is more open field, i.e. greater yards to go between the kicking team and the receiving team's end zone. On the other hand, plays where the field is longer also have higher injury rates, perhaps because the possibility of a touchback or coffin corner is reduced and the coverage team has to run farther to reach the spot of punt reception, resulting in a more open field and higher speed play. I show that even when one conditions on yards-to-go, double coverage generates somewhat higher injury rates than single coverage in these long field situations. On the other hand, double coverage does not appear to help the receiving team make longer punt returns, i.e. the punt returner is able to generate just as much excitement whether or not the gunners were single covered or not. Enforcing single coverage would then appear to reduce injuries without limiting excitement, and so seems like a costless design proposal, although the statistical significance of these findings (recall there are only 37 concussion events in this entire sample) is limited by the small sample size. In support of proposal #3, I show that deceleration appears more important than velocity in determining injury, but that the NGS data is inherently limited in its ability to measure deceleration.. The code below attempts to measure each player's velocity and deceleration at the time of a tackle event. I find that players who are not injured on a play have velocity as great as or greater than players involved in an injury generating tackle. On the other hand, the injured player experiences much greater deceleration than uninjured players. This suggests that deceleration is a more important factor in injury than velocity. The NGS data provides (x, y) coordinates at 100 millisecond resolution, so a player who decelerates from 9.8 m/s to 0.0 m/s would be recorded as having experienced at most 10 g's in deceleration, since 10g = (9.8m/s - 0.0m/s) / 0.1s. In fact the player could have experienced much greater deceleration, e.g. he could have decelerated to 0 in 20 milliseconds, but the resolution of the NGS system is not granular enough to show this. For the purpose of measuring head injury, the deceleration experienced at the head would seem to be a critical indicator of whether injury is likely to have occurred. This data would be collected not to take players out of the game or to limit their playing time, but rather to better study the conditions and plays that result in greater head deceleration and likely injury. The notebook below provides the analysis and figures that support each of these three proposals. There is an extensive amount of formatting code and data preparation code, which I have attempted to hide, so that the focus can be on the interesting parts - the output and figures. Additional observations are found at the end. Final thoughts concludes.;Apache 2.0;https://www.kaggle.com/hallayang/nfl-punt-analytics-proposal;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nn'];['filter', 'regression', 'reward', 'layer', 'label', 'logistic regression', 'predict'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.715;0.367;2020-12-13 14:25:10;NFL Punt Analytics Competition;[];nfl-punt-analytics-proposal;Python notebook;4265.0;20;;
2018-12-29 08:13:42;This analysis is for getting an idea of what's happening in the punts that result in a concussion. We'll look primarily at who is concussed and the primary partner of the conussed player.;Apache 2.0;https://www.kaggle.com/jdemeo/nfl-concussion-video-analysis;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['layer', 'label'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.666;0.352;2020-12-13 14:25:10;multiple data sources;[];NFL Video Analysis;Python notebook;1482.0;17;;
2019-01-21 08:17:30;Reducing Concussions for Punt Plays;Apache 2.0;https://www.kaggle.com/jpmiller/nfl-punt-analytics;1.0;['pattern', 'sklearn'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['machine learning', 'filter', 'recurrent neural network', 'train', 'artificial intelligence', 'model', 'understanding', 'neural network', 'layer', 'label', 'predict', 'rank', 'recommend', 'classification'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.768;0.517;2020-12-13 14:25:08;multiple data sources;['data visualization, sports'];ðŸˆ  NFL Punt Analytics;Python notebook;17081.0;127;;
2018-12-05 09:41:06;All files apart from the NGS files are quite small. The NGS files however, each can contain up to 9 million rows. In that case we need to reduce the amount of memory they take up. In this kernel I will show you how to do this in an easy way. The NGS datasets contains player position, speed and direction data for each player during the entire course of the play. The NGS dataset is the only dataset that contains Timeas a variable.;Apache 2.0;https://www.kaggle.com/kmader/convert-to-feather-for-use-in-other-kernels;0.5;[];['ai', 'dl'];['layer'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.702;0.357;2020-12-13 14:25:10;NFL Punt Analytics Competition;[];Convert to Feather (for use in other kernels);Python notebook;3201.0;18;;
2018-12-05 17:21:01;OverviewHere we use some of the code from the starter kernel from Chris Crawford and the great clean-up and feather code from Alessandro Kosciansky to start annd process the data;Apache 2.0;https://www.kaggle.com/kmader/previewing-the-games;0.5;[];['ner', 'ai', 'dl', 'ml', 'nn', 'ann'];['layer', 'label'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.657;0.397;2020-12-13 14:25:10;multiple data sources;[];Previewing the Games;Python notebook;1264.0;28;;
2018-12-07 21:42:50;Get the Speed of the Players Involved in InjuryGiven a punt play with an injury, the functions below will return a data frame with the max speed (in miles/hour) and avg speed for both the injured player and the primary partner.;Apache 2.0;https://www.kaggle.com/mtodisco10/speedy-speed-mph-functions;0.5;[];['ner', 'ai'];['layer', 'filter'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.632;0.379;2020-12-13 14:25:10;NFL Punt Analytics Competition;[];Speedy Speed (MPH) Functions;Python notebook;786.0;23;;
2018-12-16 18:40:25;This notebook looks at variables correlated with concussions.;Apache 2.0;https://www.kaggle.com/returnofsputnik/variables-that-are-correlated-with-concussions;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn'];['layer'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.691;0.421;2020-12-13 14:25:09;NFL Punt Analytics Competition;[];Variables that are correlated with concussions;R notebook;2499.0;37;;
2018-12-07 17:14:22;NFL Punt Analytics Competition - Starter EDA In this competition we are tasked with analzying punt plays for player safety and proposing rule changes.;Apache 2.0;https://www.kaggle.com/robikscube/analyzing-nfl-punt-data-starter-notebook-and-eda;0.5;[];['dl', 'ner', 'ai', 'nn'];['layer', 'label'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.66;0.362;2020-12-13 14:25:10;NFL Punt Analytics Competition;['sports'];Analyzing NFL Punt Data - Starter Notebook and EDA;Python notebook;1322.0;19;;
2018-12-27 05:15:27;Interactive Plot of Plays with CodeNOTE: INTERACTIVITY REQUIRES FORKING THE NOTEBOOK AND RUNNING IN EDIT MODE - IT WILL NOT WORK IN KAGGLE'S RENDERED NOTEBOOK In this notebook I'm sharing some code I put together to allow you to interactively scroll through an individual play and see the position of the players on the field as scroll. It's still a work in progress but has been helpful in my analysis. Please let me if you have any suggestions. There are a lot of things I still plan to add (players' speed, any event notes, etc). Example of how it works (it's a little laggy):  Please upvote if you find this helpful. Please reference this notebook if you use the code elseware.;Apache 2.0;https://www.kaggle.com/robikscube/nfl-punt-data-interactive-plots-using-bokeh;0.5;[];['dl', 'ner', 'nn', 'ann'];['layer', 'model', 'label', 'filter'];https://www.kaggle.com/c/NFL-Punt-Analytics-Competition;0.708;0.439;2020-12-13 14:25:09;NFL Punt Analytics Competition;['data visualization, exploratory data analysis, sports'];NFL Punt Data - Interactive Plots using Bokeh;Python notebook;3661.0;46;;
2017-07-02 01:30:12;;Apache 2.0;https://www.kaggle.com/benhamner/running-inception-v3;1.0;['tensorflow'];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack;0.736;0.281;2020-12-13 14:26:28;multiple data sources;[];Running Inception V3;Python script;7150.0;8;;
2017-07-05 01:31:17;;Apache 2.0;https://www.kaggle.com/cjansen/gan-exemple-in-tensorflow;0.0;[];[];[];https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack;0.651;0.0;2020-12-13 14:26:28;NIPS 2017: Defense Against Adversarial Attack;[];GAN exemple in Tensorflow;Python notebook;1105.0;0;;
2017-07-05 01:47:39;;Apache 2.0;https://www.kaggle.com/cjansen/generative-adversarial-network-in-tensorflow;1.0;['tensorflow'];['ner', 'ai', 'rl', 'nlp', 'nn'];['generative adversarial network', 'generation', 'classification', 'deep learning'];https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack;0.686;0.152;2020-12-13 14:26:28;NIPS 2017: Defense Against Adversarial Attack;[];Generative Adversarial Network in TensorFlow;Python script;2234.0;2;;
2018-05-08 20:09:50;;Apache 2.0;https://www.kaggle.com/jihyeseo/adv-attack-image-classify-robust;0.5;['sklearn'];[];[];https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack;0.6;0.0;2020-12-13 14:26:28;NIPS 2017: Defense Against Adversarial Attack;[];adv attack image classify robust;Python notebook;445.0;0;;
2017-07-03 23:16:51;;Apache 2.0;https://www.kaggle.com/benhamner/fgsm-attack-example;1.0;['tensorflow'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack;0.753;0.346;2020-12-13 14:28:32;multiple data sources;[];FGSM Attack Example;Python script;11169.0;16;;
2017-07-07 13:22:41;I was curious about how much an adversarial image can be, with the limits of 4-16 infinity norm. So I generated this NB to share my results.;Apache 2.0;https://www.kaggle.com/bguberfain/visualizing-max-perturbation;0.2;[];['ner', 'cv'];[];https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack;0.688;0.327;2020-12-13 14:28:32;multiple data sources;[];Visualizing Max Perturbation ;Python notebook;2358.0;13;;
2017-07-05 01:34:41;;Apache 2.0;https://www.kaggle.com/cjansen/gans-with-tensorflow;1.0;['tensorflow'];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn'];['generation', 'classification', 'deep learning'];https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack;0.669;0.099;2020-12-13 14:28:32;NIPS 2017: Non-targeted Adversarial Attack;[];GANs with TensorFlow;Python script;1595.0;1;;
2018-05-08 20:14:53;;Apache 2.0;https://www.kaggle.com/jihyeseo/fool-classification-image;0.5;['sklearn'];[];[];https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack;0.54;0.0;2020-12-13 14:28:32;NIPS 2017: Non-targeted Adversarial Attack;[];fool classification image;Python notebook;176.0;0;;
2017-07-27 19:20:07;;Apache 2.0;https://www.kaggle.com/linmx0130/pytorch-fgsm-demo;1.0;['pytorch'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'vgg', 'loss', 'label', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack;0.693;0.0;2020-12-13 14:28:32;NIPS 2017: Non-targeted Adversarial Attack;[];PyTorch FGSM Demo;Python script;2626.0;0;;
2017-10-03 07:11:43;;Apache 2.0;https://www.kaggle.com/avsanjay/version-1;1.0;['tensorflow'];['ner', 'ai', 'nn'];['filter', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack;0.565;0.0;2020-12-13 14:30:03;NIPS 2017: Targeted Adversarial Attack;[];version 1;Python notebook;256.0;0;;
2017-10-03 07:06:25;;Apache 2.0;https://www.kaggle.com/avsanjay/version-2;1.0;['tensorflow'];['ner', 'ai', 'nn', 'rl'];['train', 'model', 'filter', 'predict'];https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack;0.572;0.0;2020-12-13 14:30:03;NIPS 2017: Targeted Adversarial Attack;[];version 2;Python notebook;286.0;0;;
2017-07-02 02:16:05;;Apache 2.0;https://www.kaggle.com/benhamner/targeted-attack-example;1.0;['tensorflow'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack;0.684;0.152;2020-12-13 14:30:03;multiple data sources;[];Targeted Attack Example;Python script;2170.0;2;;
2017-07-05 01:35:49;;Apache 2.0;https://www.kaggle.com/cjansen/gans-with-tensorflow-nips;1.0;['tensorflow'];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn'];['generation', 'classification', 'deep learning'];https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack;0.648;0.099;2020-12-13 14:30:03;NIPS 2017: Targeted Adversarial Attack;[];GANs with TensorFlow NIPS;Python script;1062.0;1;;
2018-05-08 20:15:24;;Apache 2.0;https://www.kaggle.com/jihyeseo/attack-image-into-specific-target-class;0.5;['sklearn'];[];[];https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack;0.545;0.0;2020-12-13 14:30:03;NIPS 2017: Targeted Adversarial Attack;[];attack image into specific target class;Python notebook;189.0;0;;
2017-04-01 13:58:51;Goal of this kernel (Simply detect Sea Lions in test images) Initial Exploration Extract individual images of Sea Lions from Training Set Train a classifier on these images Test on Test Set (By sliding window);Apache 2.0;https://www.kaggle.com/asymptote/count-extract-sea-lions;0.5;[];['ai', 'cv'];['train', 'ground truth'];https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;0.727;0.429;2020-12-13 14:32:38;NOAA Fisheries Steller Sea Lion Population Count;[];Count & Extract Sea Lions;Python notebook;5756.0;41;;
2017-04-03 05:23:33;;Apache 2.0;https://www.kaggle.com/davidmercury/visualize-the-sunbath-network-of-sea-lion;0.0;[];[];[];https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;0.605;0.188;2020-12-13 14:32:38;NOAA Fisheries Steller Sea Lion Population Count;[];Visualize the sunbath network of sea lion;Python notebook;489.0;3;;
2017-04-02 14:42:19;;Apache 2.0;https://www.kaggle.com/jannis96/short-segmentation-attempt;0.5;[];['ai', 'nn', 'cv'];['train'];https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;0.616;0.214;2020-12-13 14:32:38;NOAA Fisheries Steller Sea Lion Population Count;[];Short segmentation attempt;Python notebook;593.0;4;;
2017-03-30 04:40:15;By now we have realized that the images and dotted images have a fair number of discrepancies.. Here's a simple script for approximating counts of dots of each color in dotted images.;Apache 2.0;https://www.kaggle.com/michaelzxu/counting-dots-and-not-sea-lions;0.5;[];['ai', 'cv'];['train'];https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;0.668;0.311;2020-12-13 14:32:38;NOAA Fisheries Steller Sea Lion Population Count;[];Counting dots and not sea lions;Python notebook;1551.0;11;;
2017-06-03 00:47:25;;Apache 2.0;https://www.kaggle.com/nikitabu/brrrr;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;0.597;0.099;2020-12-13 14:32:38;NOAA Fisheries Steller Sea Lion Population Count;[];brrrr;Python script;424.0;1;25.59571;26.52946
2017-03-31 12:32:45;If you just wonder what the images looks like.;Apache 2.0;https://www.kaggle.com/oysteijo/just-displaying-the-images;0.5;[];['ai'];['train'];https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;0.549;0.152;2020-12-13 14:32:38;NOAA Fisheries Steller Sea Lion Population Count;[];Just displaying the images;Python notebook;201.0;2;;
2017-04-05 00:39:02;;Apache 2.0;https://www.kaggle.com/threeplusone/sea-lion-coordinates;1.0;['skimage', 'pillow'];['ner', 'ai', 'dl', 'gan', 'cv', 'ml', 'nlp', 'nn', 'ann'];['train', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;0.752;0.48;2020-12-13 14:32:38;NOAA Fisheries Steller Sea Lion Population Count;[];Sea Lion Coordinates;Python script;10870.0;77;;
2017-06-12 15:21:54;;Apache 2.0;https://www.kaggle.com/toshik/relations-between-counts-and-image-size;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;0.59;0.152;2020-12-13 14:32:38;NOAA Fisheries Steller Sea Lion Population Count;[];Relations between counts and image-size;R script;380.0;2;;
2017-03-31 14:14:56;Updated Data exploration notebookWe need to know the data we work with. Train data contains:  csv file with train images and labels  images without markers image with markers MismatchedTrainImages.txt;Apache 2.0;https://www.kaggle.com/vfdev5/updated-data-exploration;0.5;[];['ai', 'nn', 'ann', 'cv'];['test data', 'training data', 'filter', 'train', 'label'];https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count;0.607;0.214;2020-12-13 14:32:38;NOAA Fisheries Steller Sea Lion Population Count;[];Updated Data exploration;Python notebook;508.0;4;;
2020-04-07 14:48:08;;Apache 2.0;https://www.kaggle.com/solyoh21/fish-1-test;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/noaa-right-whale-recognition;0.613;0.0;2020-12-13 14:32:45;Right Whale Recognition;[];Fish 1 test;Python notebook;558.0;0;;
2017-12-23 22:41:28;;Apache 2.0;https://www.kaggle.com/cbartel/random-forest-using-elemental-properties;1.0;['sklearn'];['ner', 'ai', 'dl'];['training data', 'test data', 'train', 'model', 'label', 'predict', 'rank', 'decision tree', 'random forest'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.719;0.472;2020-12-13 14:37:12;multiple data sources;['data visualization, feature engineering'];Random forest using elemental properties;Python notebook;4723.0;70;;
2018-01-09 00:30:06;Data Preparation;Apache 2.0;https://www.kaggle.com/giginim/tensorflow-neural-network;1.0;['tensorflow', 'sklearn'];['ner', 'ai', 'nn', 'rl'];['train', 'model', 'neural network', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.706;0.362;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;[];TensorFlow Neural Network;Python notebook;3525.0;19;;
2018-02-05 16:27:31;;Apache 2.0;https://www.kaggle.com/haimfeld87/simple-catboost;1.0;['catboost', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['predict', 'train', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.738;0.418;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;[];Simple CatBoost;Python script;7587.0;36;0.06848;0.05403
2018-10-18 17:11:41;;Apache 2.0;https://www.kaggle.com/headsortails/resistance-is-futile-transparent-conductors-eda;1.0;['pattern'];['ner', 'ai', 'rl', 'nn', 'ann'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'clustering', 'label', 'predict', 'rank', 'understanding'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.752;0.535;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;['beginner, data visualization, exploratory data analysis'];Resistance is futile - Transparent Conductors EDA;Rmarkdown script;10897.0;163;;
2019-02-14 15:43:12;;Apache 2.0;https://www.kaggle.com/hireme/two-outputs-regressor-with-lightgbm;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.725;0.268;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;['regression, gradient boosting'];Two Outputs Regressor with LightGBM;Python script;5524.0;7;;
2018-01-16 17:32:06;;Apache 2.0;https://www.kaggle.com/holar9/hands-on-cubist-brnn;0.5;[];['ner', 'ai', 'nlp', 'nn', 'rnn'];['filter', 'neuron', 'train', 'fitting', 'model', 'neural network', 'deep learning', 'label', 'predict', 'classification', 'bayesian'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.674;0.371;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;[];Hands on Cubist + BRNN;R script;1744.0;21;0.06922;0.05174
2018-02-09 07:38:25;;Apache 2.0;https://www.kaggle.com/janpreets/using-the-atomic-coordinates-for-prediction;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn'];['filter', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.715;0.418;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;[];Using the atomic coordinates for prediction;Python script;4279.0;36;0.06988;0.05046
2017-12-20 05:39:05;;Apache 2.0;https://www.kaggle.com/johnfarrell/nomad2018-simple-lgbm-starter;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.673;0.334;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;[];Nomad2018 Simple LGBM Starter;Python notebook;1728.0;14;0.07016;0.05696
2017-12-21 10:42:16;;Apache 2.0;https://www.kaggle.com/pecooper/xgboost-benchmark-simple-usage-of-geometry-files;1.0;['xgboost'];['ner', 'ai', 'gbm', 'nlp', 'nn'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.66;0.281;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;[];Xgboost Benchmark - Simple usage of geometry files;R script;1338.0;8;0.07049;0.05751
2017-12-19 02:14:53;;Apache 2.0;https://www.kaggle.com/tejasrinivas/xgb-starter-0-0584-on-lb;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.648;0.327;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;[];XGB Starter 0.0584 on LB;Python script;1045.0;13;;
2018-01-12 06:20:57;Please see the discussion: https://www.kaggle.com/c/nomad2018-predict-transparent-conductors/discussion/47154 if you don't know it.;Apache 2.0;https://www.kaggle.com/tonyyy/find-the-same-geometry;0.5;[];['ner', 'ai', 'rl'];['train', 'test data', 'predict'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.629;0.319;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;[];Find the same geometry;Python notebook;739.0;12;;
2018-01-23 12:05:31;This notebook shows a way to obtain atomic coordinates from xyz files. We can calculate bond lengths, bond angles, coordination numbers, and so on from atomic coordinates. Some of them might be able to be used as good features.   Reading geometry files Reduced coordinates Geometrical properties;Apache 2.0;https://www.kaggle.com/tonyyy/how-to-get-atomic-coordinates;0.5;[];['ner', 'ai', 'dl', 'nn', 'ann'];['filter', 'test data', 'train', 'label', 'recommend'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.745;0.5;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;['data visualization, feature engineering'];How to get atomic coordinates;Python notebook;9096.0;101;;
2017-12-30 20:16:24;;Apache 2.0;https://www.kaggle.com/tunguz/simple-catboost;1.0;['catboost', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['predict', 'train', 'fitting', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/nomad2018-predict-transparent-conductors;0.685;0.311;2020-12-13 14:37:12;Nomad2018 Predicting Transparent Conductors;[];Simple CatBoost;Python script;2210.0;11;0.06821;0.05362
2017-08-18 12:14:56;;Apache 2.0;https://www.kaggle.com/abhishek/no-crap-only-models;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['training data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/nyc-taxi-trip-duration;0.673;0.393;2020-12-13 14:39:46;New York City Taxi Trip Duration;[];"No Crap, Only Models! ;)";Python script;1726.0;27;0.42100;0.42368
2018-04-13 16:02:11;;Apache 2.0;https://www.kaggle.com/crailtap/basic-network-analysis-tutorial;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'machine learning', 'training data', 'train', 'model', 'deep learning', 'clustering', 'label', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/nyc-taxi-trip-duration;0.778;0.505;2020-12-13 14:39:46;multiple data sources;[];Basic Network Analysis Tutorial;Python notebook;22732.0;108;;
2017-08-14 08:29:19;;Apache 2.0;https://www.kaggle.com/damianpanek/interactive-eda-nytaxi-highchar-leaflet;0.5;[];['ai'];['train'];https://www.kaggle.com/c/nyc-taxi-trip-duration;0.699;0.429;2020-12-13 14:39:46;multiple data sources;['data visualization, exploratory data analysis, geospatial analysis'];Interactive EDA NYtaxi  - highchar/leaflet;Rmarkdown script;3018.0;41;;
2017-07-22 02:59:04;Read Data;Apache 2.0;https://www.kaggle.com/donniedarko/darktaxi-tripdurationprediction-lb-0-385;1.0;['xgboost', 'sklearn'];['ai', 'rl', 'cv', 'nn', 'ann'];['train', 'model', 'test data', 'predict'];https://www.kaggle.com/c/nyc-taxi-trip-duration;0.704;0.408;2020-12-13 14:39:46;New York City Taxi Trip Duration;['xgboost, clustering, geospatial analysis, +1 moreadvanced'];--DarkTaxi-- TripDurationPrediction (LB: 0.385);Python notebook;3330.0;32;;
2020-02-22 20:09:30;;Apache 2.0;https://www.kaggle.com/headsortails/nyc-taxi-eda-update-the-fast-the-curious;1.0;['pattern', 'xgboost'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'predict', 'machine learning', 'training data', 'train', 'recommend', 'classification', 'model', 'layer', 'loss', 'understanding', 'decision tree', 'test data', 'regression', 'fitting', 'validation data', 'label', 'gradient boosting', 'random forest'];https://www.kaggle.com/c/nyc-taxi-trip-duration;0.822;0.628;2020-12-13 14:39:46;multiple data sources;['data visualization, exploratory data analysis, feature engineering, +2 moredata cleaning, geospatial analysis'];NYC Taxi EDA - Update: The fast & the curious;Rmarkdown script;100407.0;724;;
2017-07-28 16:10:44;;Apache 2.0;https://www.kaggle.com/merckel/autoencoder-and-deep-features;1.0;['h2o'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nn', 'ann'];['autoencoder', 'filter', 'regression', 'train', 'model', 'neural network', 'epoch', 'layer', 'label', 'gradient boosting', 'predict', 'supervised learning', 'hidden layer'];https://www.kaggle.com/c/nyc-taxi-trip-duration;0.731;0.427;2020-12-13 14:39:46;New York City Taxi Trip Duration;[];Autoencoder and Deep Features;Rmarkdown script;6286.0;40;;
2017-07-20 06:04:47;Welcome to a very special playground competition! In this kernel I'll read in the data, make a quick data visualization, and finally end with a very basic submission to the competition.;Apache 2.0;https://www.kaggle.com/mrisdal/last-place-laura-benchmark;0.5;[];['ai'];['training data', 'filter', 'train', 'predict', 'random forest'];https://www.kaggle.com/c/nyc-taxi-trip-duration;0.676;0.397;2020-12-13 14:39:46;New York City Taxi Trip Duration;['data visualization'];Last-Place Laura Benchmark;R notebook;1819.0;28;;
2017-07-21 16:37:18;;Apache 2.0;https://www.kaggle.com/opanichev/lightgbm-regressor;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/nyc-taxi-trip-duration;0.779;0.411;2020-12-13 14:39:46;New York City Taxi Trip Duration;[];LightGBM Regressor;Python script;23267.0;33;0.43855;0.44084
2017-09-20 09:37:10;This kernel explores the dataset of 2016 New York parties. The data was extracted from party related noise complaints in the city. The whole dataset is available at New York Open Data portal. IntroductionPrevious EDA's showed that the number of rides increases during the weekends, especially at night. This is expected because that is a prime time for going out and having fun. So I figured that finding the rides that take a passenger home from the party would help us in producing a better model.  Unfortunately, there is no database of all the best parties in the Big Apple, but during parties, people tend to get carried away and turn the music too loud which makes some neighbors unhappy (also probably because they weren't invited).  So they submit a complaint to a city hotline so the local police would check in on the party and ask the host to dial it down. Luckily the city of New York is very modern one and they embrace the Open Data movement and try to make all data freely available.;Apache 2.0;https://www.kaggle.com/somesnm/new-york-parties-eda;0.5;[];['ai', 'nn'];['test data', 'train', 'model', 'epoch', 'label'];https://www.kaggle.com/c/nyc-taxi-trip-duration;0.717;0.403;2020-12-13 14:39:46;multiple data sources;['data visualization, exploratory data analysis, cities and urban areas'];New York Parties EDA;Python notebook;4550.0;30;;
2019-08-22 19:41:05;;Apache 2.0;https://www.kaggle.com/drobchak1988/competitions-open-images2019-instance-segmentation;1.0;['tensorflow'];['ner', 'dl', 'cnn', 'rl', 'nn'];['model', 'label', 'predict', 'resnet', 'labeled'];https://www.kaggle.com/c/open-images-2019-instance-segmentation;0.663;0.214;2020-12-13 14:40:23;Open Images 2019 - Instance Segmentation;['gpu'];Competitions_Open_Images2019_Instance_Segmentation;Python notebook;1419.0;4;;
2020-09-07 20:01:17;Intersection over Union;Apache 2.0;https://www.kaggle.com/elvinagammed/segmentation-metrics-object-detection;0.5;[];['ai', 'nn'];['loss', 'model', 'predict'];https://www.kaggle.com/c/open-images-2019-instance-segmentation;0.568;0.236;2020-12-13 14:40:23;Open Images 2019 - Instance Segmentation;['image data'];Segmentation Metrics (Object Detection);Python notebook;267.0;5;;
2019-07-13 04:34:10;;Apache 2.0;https://www.kaggle.com/gipark2001/kernel47a54e8363;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/open-images-2019-instance-segmentation;0.621;0.0;2020-12-13 14:40:23;Open Images 2019 - Instance Segmentation;[];kernel47a54e8363;Python notebook;641.0;0;;
2019-07-14 17:37:25;ObjectiveObjective of this kernel is  To explore the image data set  Understand the assets given How to quickly(optimally) load the data Basic exploratory analysis and statistics on the images;Apache 2.0;https://www.kaggle.com/gowrishankarin/basic-eda-with-plotly-and-images;0.5;[];['ai', 'dl', 'cv', 'ml', 'nn', 'ann'];['layer', 'image classification', 'classification', 'predict'];https://www.kaggle.com/c/open-images-2019-instance-segmentation;0.684;0.302;2020-12-13 14:40:23;Open Images 2019 - Instance Segmentation;[];Basic EDA with Plotly and Images;Python notebook;2151.0;10;;
2019-09-22 15:29:34;;Apache 2.0;https://www.kaggle.com/sagar744/segmentaion;1.0;['tensorflow', 'skimage'];['ai', 'cnn', 'rl', 'nn', 'ann'];['r-cnn', 'train', 'model', 'epoch', 'layer', 'label', 'loss', 'resnet'];https://www.kaggle.com/c/open-images-2019-instance-segmentation;0.601;0.0;2020-12-13 14:40:23;Open Images 2019 - Instance Segmentation;['gpu'];segmentaion;Python notebook;456.0;0;;
2019-07-12 09:26:37;View one image from data;Apache 2.0;https://www.kaggle.com/shivamanhar/oi-image-segmentation;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/open-images-2019-instance-segmentation;0.655;0.0;2020-12-13 14:40:23;Open Images 2019 - Instance Segmentation;[];kernelf790c29746;Python notebook;1202.0;0;;
2019-06-26 09:21:48;I have left certain parts as exercise to the reader. This model is quite fast and should give good results if used properly. In case of any questions, feel free to ask. Upvotes are appreciated if this helps you.;Apache 2.0;https://www.kaggle.com/abhishek/training-fast-rcnn-using-torchvision;0.5;[];['ai', 'nn', 'cnn', 'rl'];['train', 'model', 'epoch', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/open-images-2019-object-detection;0.756;0.492;2020-12-13 14:41:47;multiple data sources;[];training fast rcnn using torchvision;Python notebook;12076.0;90;;
2019-08-19 07:10:46;;Apache 2.0;https://www.kaggle.com/akashdeepjassal/tensorflow-hub-inception-resnet-v2-submission;1.0;['tensorflow'];['dl', 'ai', 'nn', 'rl'];['predict'];https://www.kaggle.com/c/open-images-2019-object-detection;0.695;0.327;2020-12-13 14:41:47;multiple data sources;[];Tensorflow Hub Inception Resnet V2 Submission;Python notebook;2729.0;13;;
2019-06-21 14:10:01;;Apache 2.0;https://www.kaggle.com/cttsai/inference-script-with-pretrained-models-from-tfhub;1.0;['tensorflow'];['ner', 'ai', 'dl', 'nlp', 'nn'];['object detection', 'train', 'model', 'deep learning', 'predict', 'resnet', 'classification'];https://www.kaggle.com/c/open-images-2019-object-detection;0.664;0.311;2020-12-13 14:41:47;Open Images 2019 - Object Detection;['gpu'];Inference script with pretrained models from TFHub;Python script;1424.0;11;0.05603;0.06179
2019-06-19 07:42:50;;Apache 2.0;https://www.kaggle.com/manishrai/data-pipelined;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['train', 'label', 'layer'];https://www.kaggle.com/c/open-images-2019-object-detection;0.689;0.311;2020-12-13 14:41:47;multiple data sources;[];Data_pipelined;Python notebook;2393.0;11;;
2019-10-10 16:03:26;https://github.com/fizyr/keras-retinanet/blob/master/examples/ResNet50RetinaNet.ipynb;Apache 2.0;https://www.kaggle.com/rblcoder/object-detection-fizyr-keras-retinanet;1.0;['tensorflow', 'opencv-python', 'keras', 'pillow'];['ai', 'rl', 'cv', 'nn', 'ml'];['train', 'model', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/open-images-2019-object-detection;0.609;0.214;2020-12-13 14:41:47;Open Images 2019 - Object Detection;['gpu'];Object detection: fizyr/keras-retinanet;Python notebook;519.0;4;;
2019-07-27 16:56:14;https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&type=detection&c=%2Fm%2F04yx4;Apache 2.0;https://www.kaggle.com/rblcoder/open-images-2019-detection-scores-0-5;1.0;['tensorflow'];['ai', 'cnn', 'rl', 'nn', 'ml'];['train', 'model', 'label', 'predict', 'resnet', 'labeled'];https://www.kaggle.com/c/open-images-2019-object-detection;0.68;0.319;2020-12-13 14:41:47;Open Images 2019 - Object Detection;['gpu'];Open Images 2019: detection_scores > 0.5  ;Python notebook;1971.0;12;;
2019-06-04 12:44:07;;Apache 2.0;https://www.kaggle.com/siddhrath/data-fetching-with-aws;0.5;[];['ner', 'ai', 'nn', 'rl'];['train'];https://www.kaggle.com/c/open-images-2019-object-detection;0.703;0.362;2020-12-13 14:41:47;Open Images 2019 - Object Detection;['gpu'];kernel009951e73d;Python notebook;3268.0;19;;
2019-08-05 08:48:15;Exploring classes and Heirarchy;Apache 2.0;https://www.kaggle.com/thanatoz/understanding-open-image-v5-classes-hierarchy;1.0;['pillow'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['train', 'label', 'training data'];https://www.kaggle.com/c/open-images-2019-object-detection;0.687;0.357;2020-12-13 14:41:47;Open Images 2019 - Object Detection;[];Understanding Open Image v5 classes hierarchy ;Python notebook;2300.0;18;;
2019-06-12 19:18:37;Baseline predictions using TFHub;Apache 2.0;https://www.kaggle.com/vikramtiwari/baseline-predictions-using-inception-resnet-v2;1.0;['tensorflow'];['ner', 'ai', 'dl', 'cnn', 'rl', 'nn'];['object detection', 'model', 'label', 'predict', 'understanding', 'resnet', 'labeled'];https://www.kaggle.com/c/open-images-2019-object-detection;0.747;0.467;2020-12-13 14:41:47;Open Images 2019 - Object Detection;['gpu, deep learning, neural networks'];Baseline predictions using inception resnet v2;Python notebook;9582.0;65;;
2019-07-11 00:25:01;OID Mapping Dictionary (Object ID);Apache 2.0;https://www.kaggle.com/yw6916/generate-prediction-string;1.0;['tensorflow', 'keras', 'pillow'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nn', 'ann'];['train', 'label', 'filter', 'predict'];https://www.kaggle.com/c/open-images-2019-object-detection;0.587;0.334;2020-12-13 14:41:47;Open Images 2019 - Object Detection;[];kernel86843af483;Python notebook;361.0;14;;
2019-07-08 20:36:07;Loading all needed modules;Apache 2.0;https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading;1.0;['skimage'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model'];https://www.kaggle.com/c/open-images-2019-object-detection;0.751;0.496;2020-12-13 14:41:47;multiple data sources;[];Benchmark 2019: Speed of image reading;Python notebook;10691.0;95;;
2019-08-13 12:30:24;Add missing datasets to achiv high score.;Apache 2.0;https://www.kaggle.com/anshuljdhingra/visual-relationship;0.5;[];['ai'];['train', 'label', 'predict'];https://www.kaggle.com/c/open-images-2019-visual-relationship;0.592;0.099;2020-12-13 14:42:02;multiple data sources;[];Visual Relationship;R notebook;395.0;1;;
2019-07-31 15:19:42;Add missing datasets to achiv high score.;Apache 2.0;https://www.kaggle.com/danish788/beat-the-lb;0.5;[];['ai', 'dl', 'cv'];['train', 'label', 'predict'];https://www.kaggle.com/c/open-images-2019-visual-relationship;0.699;0.367;2020-12-13 14:42:02;multiple data sources;[]; Beat the LB;R notebook;3014.0;20;0.00328;0.00364
2019-08-16 10:29:36;Googld AI Open Images - Visual Relationship TrackThis kernel just explore the dataset for starting the competition in kaggle :) train dataset is here: https://www.kaggle.com/mahmoudmohsen213/vrd01 Contents Description of the Competition Explore the Train Dataset Explore the Test Dataset;Apache 2.0;https://www.kaggle.com/hli2020/googld-ai-visual-relationship-data-exploration;0.5;[];['ai', 'nn', 'cv'];['test data', 'filter', 'object detection', 'train', 'label', 'predict'];https://www.kaggle.com/c/open-images-2019-visual-relationship;0.599;0.0;2020-12-13 14:42:02;multiple data sources;[];Googld AI Visual Relationship - Data Exploration;Python notebook;444.0;0;;
2019-08-17 07:21:53;Special thanks to the author of this kernel: https://www.kaggle.com/titericz/beat-the-benchmark-lb-0-0032. Kindly upvote the original kernel.;Apache 2.0;https://www.kaggle.com/jian1201/revised-beat-the-benchmark;0.5;[];['ai', 'dl'];['train', 'label', 'predict'];https://www.kaggle.com/c/open-images-2019-visual-relationship;0.583;0.253;2020-12-13 14:42:02;multiple data sources;[];Revised Beat the Benchmark;R notebook;341.0;6;;
2019-06-06 15:35:17;Googld AI Open Images - Visual Relationship TrackThis kernel just explore the dataset for starting the competition in kaggle :) train dataset is here: https://www.kaggle.com/mahmoudmohsen213/vrd01 Contents Description of the Competition Explore the Train Dataset Explore the Test Dataset;Apache 2.0;https://www.kaggle.com/seriousran/googld-ai-visual-relationship-data-exploration;0.5;[];['ai', 'nn', 'cv'];['object detection', 'test data', 'train', 'label', 'predict'];https://www.kaggle.com/c/open-images-2019-visual-relationship;0.727;0.444;2020-12-13 14:42:02;multiple data sources;[];Googld AI Visual Relationship - Data Exploration;Python notebook;5732.0;49;;
2020-07-19 05:52:14;;Apache 2.0;https://www.kaggle.com/korosensie/open-images-instance-segmentation-rvc-2020;0.5;[];['nn', 'cnn', 'cv'];['image segmentation', 'model', 'object detection'];https://www.kaggle.com/c/open-images-instance-segmentation-rvc-2020;0.591;0.281;2020-12-13 14:42:26;Open Images Instance Segmentation RVC 2020 edition;[];Open Images Instance Segmentation RVC 2020 ;Python notebook;386.0;8;;
2020-06-04 21:55:00;At first install pixellib library;Apache 2.0;https://www.kaggle.com/shawon10/object-detection-and-image-segmentation-pixellib;0.5;[];['ai', 'nn', 'cnn', 'cv'];['image segmentation', 'model', 'object detection'];https://www.kaggle.com/c/open-images-instance-segmentation-rvc-2020;0.741;0.34;2020-12-13 14:42:26;Open Images Instance Segmentation RVC 2020 edition;['beginner, computer vision'];Object Detection and Image Segmentation PixelLib;Python notebook;8217.0;15;;
2020-08-31 14:46:43;Copied from OPEN Image EDA;Apache 2.0;https://www.kaggle.com/vsevicky/open-images-object-detection-rvc-2020-edition;1.0;['skimage'];['ai', 'rl', 'cv', 'nn', 'ann'];['train', 'label', 'layer'];https://www.kaggle.com/c/open-images-instance-segmentation-rvc-2020;0.597;0.214;2020-12-13 14:42:27;multiple data sources;[];Open Images Object Detection RVC 2020 edition;Python notebook;429.0;4;;
2020-05-25 10:49:18;;Apache 2.0;https://www.kaggle.com/anmolarora01/google-open-images-object-detection-rvc;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.589;0.0;2020-12-13 14:43:30;Open Images Object Detection RVC 2020 edition;[];Google Open Images Object Detection RVC;Python notebook;374.0;0;;
2020-05-19 20:50:29;;Apache 2.0;https://www.kaggle.com/ateplyuk/open-images-2020-tfhub;1.0;['tensorflow'];['dl'];['model', 'predict'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.666;0.302;2020-12-13 14:43:30;Open Images Object Detection RVC 2020 edition;['gpu'];open-images-2020-tfhub;Python notebook;1491.0;10;0.056;0.061
2020-07-07 15:54:18;References: https://www.kaggle.com/rstogi896/tf-hub-nms;Apache 2.0;https://www.kaggle.com/c7934597/tensorflow-hub-faster-rcnn;1.0;['tensorflow'];['ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ml'];['r-cnn', 'filter', 'object detection', 'model', 'layer', 'label', 'predict', 'resnet', 'ground truth'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.548;0.214;2020-12-13 14:43:30;Open Images Object Detection RVC 2020 edition;['gpu'];Tensorflow Hub Faster Rcnn;Python notebook;198.0;4;;
2020-08-20 15:52:59;;Apache 2.0;https://www.kaggle.com/hal1001k/object-detection-faster-rcnn;1.0;['caffe', 'tensorflow'];['ai', 'dl', 'cnn', 'cv', 'nn', 'ml'];['resnet', 'label', 'predict'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.595;0.236;2020-12-13 14:43:30;Open Images Object Detection RVC 2020 edition;['gpu'];Object Detection (Faster-RCNN);Python notebook;410.0;5;;
2020-08-15 01:23:59;;Apache 2.0;https://www.kaggle.com/hisudha/google-object-detection-rvc-2020-tfhub;1.0;['tensorflow'];['dl'];['model', 'predict'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.488;0.099;2020-12-13 14:43:30;Open Images Object Detection RVC 2020 edition;[];google-object-detection-rvc-2020-tfhub;Python notebook;86.0;1;;
2020-08-13 20:29:45;At first install pixellib library;Apache 2.0;https://www.kaggle.com/hisudha/google-object-detection-rvc-2020-v1-ipynb;1.0;['opencv-python', 'pillow'];['ai', 'nn', 'cnn', 'cv'];['image segmentation', 'model', 'object detection'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.479;0.0;2020-12-13 14:43:30;Open Images Object Detection RVC 2020 edition;[];google-object-detection-rvc_2020-v1.ipynb;Python notebook;76.0;0;;
2020-08-14 23:09:19;References: https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c https://github.com/justcallmewilliam/iccv19-silco/tree/master/cl_utils/mAP_lib https://www.kaggle.com/vikramtiwari/baseline-predictions-using-inception-resnet-v2;Apache 2.0;https://www.kaggle.com/hisudha/tf-hub-rcnn-v1;1.0;['tensorflow'];['ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ml'];['r-cnn', 'filter', 'object detection', 'model', 'layer', 'label', 'predict', 'resnet', 'ground truth'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.491;0.099;2020-12-13 14:43:30;Open Images Object Detection RVC 2020 edition;[];tf_hub_rcnn_v1;Python notebook;89.0;1;;
2020-07-27 15:44:20;References: https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c https://github.com/justcallmewilliam/iccv19-silco/tree/master/cl_utils/mAP_lib https://www.kaggle.com/vikramtiwari/baseline-predictions-using-inception-resnet-v2;Apache 2.0;https://www.kaggle.com/manavtrivedi/tf-hub-nms;1.0;['tensorflow'];['ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ml'];['r-cnn', 'filter', 'object detection', 'model', 'layer', 'label', 'predict', 'resnet', 'ground truth'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.532;0.188;2020-12-13 14:43:30;Open Images Object Detection RVC 2020 edition;['gpu'];TF HUB + NMS;Python notebook;156.0;3;;
2020-06-12 15:57:30;References: https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c https://github.com/justcallmewilliam/iccv19-silco/tree/master/cl_utils/mAP_lib https://www.kaggle.com/vikramtiwari/baseline-predictions-using-inception-resnet-v2;Apache 2.0;https://www.kaggle.com/rstogi896/tf-hub-nms;1.0;['tensorflow'];['ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ml'];['r-cnn', 'filter', 'object detection', 'model', 'layer', 'label', 'predict', 'resnet', 'ground truth'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.651;0.327;2020-12-13 14:43:30;Open Images Object Detection RVC 2020 edition;['gpu, beginner, deep learning, +1 moretransfer learning'];TF HUB + NMS;Python notebook;1112.0;13;;
2020-05-28 21:07:55;IMAGE PROCESSING / OBJECT DETECTION;Apache 2.0;https://www.kaggle.com/shantanu1118/image-processing;1.0;['tensorflow'];['ner', 'ai', 'automl', 'cv', 'rl', 'ml', 'nn', 'ann'];['object detection', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.61;0.099;2020-12-13 14:43:30;multiple data sources;[];Image Processing ;Python notebook;535.0;1;;
2020-06-04 22:11:01;At first install pixellib library;Apache 2.0;https://www.kaggle.com/shawon10/google-object-detection-by-pixellib-mask-rcnn;0.5;[];['ai', 'nn', 'cnn', 'cv'];['image segmentation', 'model', 'object detection'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.689;0.4;2020-12-13 14:43:29;Open Images Object Detection RVC 2020 edition;['beginner'];Google Object Detection by PixelLib + Mask RCNN;Python notebook;2396.0;29;;
2020-05-20 07:17:35;;Apache 2.0;https://www.kaggle.com/soham1024/kernel3b3f4a4664;0.8;['tensorflow'];[];['resnet', 'predict'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.66;0.302;2020-12-13 14:43:30;multiple data sources;[];kernel3b3f4a4664;Python notebook;1327.0;10;0.395;0.424
2020-08-31 14:46:43;Copied from OPEN Image EDA;Apache 2.0;https://www.kaggle.com/vsevicky/open-images-object-detection-rvc-2020-edition;1.0;['skimage'];['ai', 'rl', 'cv', 'nn', 'ann'];['train', 'label', 'layer'];https://www.kaggle.com/c/open-images-object-detection-rvc-2020;0.597;0.214;2020-12-13 14:43:30;multiple data sources;[];Open Images Object Detection RVC 2020 edition;Python notebook;430.0;4;;
2020-07-18 07:59:22;Contents Load dependancies Understanding Dicom's Pixel distribution and Tissue densitites EDA Looking at patient ID00082637202201836229724 viewing the slices in chronological order   Next Steps;Apache 2.0;https://www.kaggle.com/avirdee/understanding-dicoms;0.5;[];['ner', 'ai', 'gan'];['train', 'understanding', 'model'];https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression;0.725;0.526;2020-12-13 14:44:51;OSIC Pulmonary Fibrosis Progression;['beginner, exploratory data analysis'];Understanding DICOMSâœ”;Python notebook;5441.0;145;;
2020-09-18 07:16:25;1. Introduction;Apache 2.0;https://www.kaggle.com/gunesevitan/osic-pulmonary-fibrosis-progression-eda;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression;0.735;0.517;2020-12-13 14:44:51;OSIC Pulmonary Fibrosis Progression;['data visualization, exploratory data analysis, data cleaning, +2 moreimage data, tabular data'];OSIC Pulmonary Fibrosis Progression - EDA;Python notebook;7007.0;127;;
2020-07-17 06:29:44;Laplace Log Likelihood Image Credits: https://en.wikipedia.org/wiki/Laplace_distribution The evaluation metric of this competition is a modified version of Laplace Log Likelihood. Read more about it on the Evaluation Page.  This notebook explores the functioning of this metric with some examples and visualizations.;Apache 2.0;https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood;0.5;[];['ner', 'ai', 'ml', 'rl'];['train', 'model', 'validation data', 'label', 'predict'];https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression;0.749;0.572;2020-12-13 14:44:51;multiple data sources;['beginner, data visualization'];OSIC: Understanding Laplace Log Likelihood;Python notebook;9981.0;286;;
2020-07-13 09:52:05;;Apache 2.0;https://www.kaggle.com/twinkle0705/your-starter-notebook-for-osic;0.5;[];['ai', 'ml', 'gan'];['filter', 'machine learning', 'training data', 'test data', 'train', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression;0.736;0.55;2020-12-13 14:44:51;OSIC Pulmonary Fibrosis Progression;['beginner, data visualization, exploratory data analysis'];Your Starter Notebook for OSIC!;Python notebook;7104.0;205;;
2020-07-23 10:57:27;BASELINE NN;Apache 2.0;https://www.kaggle.com/ulrich07/osic-multiple-quantile-regression-starter;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'cnn', 'ml'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'ground truth'];https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression;0.771;0.577;2020-12-13 14:44:51;OSIC Pulmonary Fibrosis Progression;[];Osic-Multiple-Quantile-Regression-Starter;Python notebook;18416.0;310;-6.9212;-6.8322
2020-10-07 10:42:40;OSIC Pulmonary Fibrosis Progression;Apache 2.0;https://www.kaggle.com/vbmokin/higher-lb-score-by-tuning-mloss-upgrade-visual;1.0;['tensorflow', 'sklearn', 'keras', 'pillow'];['ner', 'ai', 'cnn', 'cv', 'ml', 'nn', 'ann'];['regression', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet', 'ground truth'];https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression;0.753;0.514;2020-12-13 14:44:51;multiple data sources;['gpu, classification'];Higher LB score by tuning mloss - upgrade & visual;Python notebook;11060.0;122;-6.8553;-6.8283
2020-07-07 23:46:29;Library;Apache 2.0;https://www.kaggle.com/yasufuminakama/osic-lgb-baseline;1.0;['lightgbm', 'sklearn'];['ai', 'dl', 'gbm', 'cv', 'rl', 'nn'];['filter', 'regression', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression;0.738;0.533;2020-12-13 14:44:51;OSIC Pulmonary Fibrosis Progression;[];OSIC / LGB baseline;Python notebook;7457.0;159;-7.0037;-6.9605
2015-04-21 17:57:34;;Apache 2.0;https://www.kaggle.com/abhishek/beating-the-benchmark-v2-0;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['random forest', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.706;0.311;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Beating the Benchmark v2.0;Python script;3463.0;11;0.61334;0.61314
2015-04-22 02:34:58;;Apache 2.0;https://www.kaggle.com/benhamner/random-forest-benchmark-r-1;0.5;[];['nlp', 'ai', 'nn', 'ner'];['random forest', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.756;0.34;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Random Forest Benchmark (R);R script;11992.0;15;0.64478;0.64281
2015-04-21 00:51:37;;Apache 2.0;https://www.kaggle.com/benhamner/t-sne-visualization-1;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.75;0.327;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];t-SNE Visualization;R script;10406.0;13;;
2015-04-30 22:21:29;;Apache 2.0;https://www.kaggle.com/benhamner/top-10-leaderboard-performance-over-time;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'classification', 'propagation'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.711;0.327;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Top 10 Leaderboard Performance Over Time;Rmarkdown script;3952.0;13;;
2015-06-13 23:40:28;;Apache 2.0;https://www.kaggle.com/cbourguignat/why-calibration-works;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['random forest', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.772;0.465;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Why calibration works;Python script;18906.0;64;;
2015-06-16 00:28:27;;Apache 2.0;https://www.kaggle.com/hsperr/finding-ensamble-weights;1.0;['xgboost', 'sklearn'];['nlp', 'ai', 'nn', 'ner'];['regression', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.79;0.51;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Finding Ensemble Weights;Python script;32686.0;116;;
2015-06-18 18:05:53;;Apache 2.0;https://www.kaggle.com/mark42/confusion-matrix-with-probabilities;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.723;0.292;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];confusion matrix with probabilities;Python script;5182.0;9;;
2015-11-18 05:04:38;;Apache 2.0;https://www.kaggle.com/omarelgabry/otto-product-classification-predictions;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'ml', 'nlp', 'nn', 'ann'];['machine learning', 'regression', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.718;0.302;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Otto Product Classification Predictions;Python notebook;4587.0;10;0.58716;0.58555
2015-05-07 12:51:31;;Apache 2.0;https://www.kaggle.com/rudikruger/rf-gbm;0.5;[];['ner', 'ai', 'gbm', 'cv', 'nlp', 'nn'];['predict', 'training data', 'random forest', 'train', 'model', 'deep learning', 'gradient boosting', 'loss', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.734;0.302;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];RF & GBM;R script;6765.0;10;0.51553;0.50995
2020-09-11 10:17:52;;Apache 2.0;https://www.kaggle.com/sachinsharma1123/otto-group-classification-acc-82;1.0;['xgboost', 'sklearn'];['ai', 'nn'];['random forest', 'regression', 'train', 'model', 'support vector machines', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.559;0.292;2020-12-13 14:52:36;Otto Group Product Classification Challenge;['beginner, data visualization, multiclass classification'];otto-group-classification(acc 82%);Python notebook;233.0;9;;
2015-05-08 20:02:19;;Apache 2.0;https://www.kaggle.com/sushanttripathy/wrapper-for-models-ensemble;1.0;['sklearn'];['ner', 'ai', 'nlp', 'nn', 'ann'];['predict', 'training data', 'train', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.699;0.311;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Wrapper for models ensemble ;Python script;2999.0;11;;
2015-05-10 09:30:34;;Apache 2.0;https://www.kaggle.com/thakurrajanand/deep-learning-h2o-0-44;1.0;['h2o'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'epoch', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.711;0.327;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Deep Learning H2O - 0.44;R script;3934.0;13;;
2019-12-09 09:55:39;GBDT Implementation (Cython);Apache 2.0;https://www.kaggle.com/threecourse/gbdt-implementation-cython-kaggle-days-tokyo;1.0;['xgboost', 'sklearn'];['ai', 'rl'];['regression', 'train', 'model', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.66;0.302;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];GBDT Implementation(Cython) - Kaggle Days Tokyo;Python notebook;1339.0;10;;
2019-12-09 14:05:19;GBDT Implementation;Apache 2.0;https://www.kaggle.com/threecourse/gbdt-implementation-kaggle-days-tokyo;1.0;['xgboost', 'sklearn'];['ai', 'rl'];['regression', 'train', 'model', 'loss', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.69;0.39;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];GBDT Implementation - Kaggle Days Tokyo;Python notebook;2454.0;26;;
2015-04-20 23:01:59;;Apache 2.0;https://www.kaggle.com/tks0123456789/class-wise-feature-importance;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'classification', 'deep learning'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.747;0.379;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Class-wise feature importance;R script;9507.0;23;;
2015-05-04 08:34:07;;Apache 2.0;https://www.kaggle.com/tqchen/testx;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'model', 'understanding', 'deep learning', 'clustering', 'loss', 'label', 'gradient boosting', 'predict', 'decision tree', 'classification', 'propagation'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.721;0.292;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];xgboost;Rmarkdown script;5008.0;9;;
2015-05-08 19:10:28;;Apache 2.0;https://www.kaggle.com/tqchen/understanding-xgboost-model-on-otto-data;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'model', 'understanding', 'deep learning', 'clustering', 'loss', 'label', 'k-means', 'gradient boosting', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/otto-group-product-classification-challenge;0.829;0.574;2020-12-13 14:52:36;Otto Group Product Classification Challenge;[];Understanding XGBoost Model on Otto Data;Rmarkdown script;127474.0;294;;
2016-10-06 02:24:38;This notebook is based on sample version with first 10,000,000 rows. It contains two main parts:  Exploration by country Exploration by State for USA data  Any comments and improvements are welcome!;Apache 2.0;https://www.kaggle.com/andreyg/explore-user-base-by-geo;0.5;[];['ner', 'ai', 'gan', 'cv', 'rl', 'nn', 'ml'];['filter'];https://www.kaggle.com/c/outbrain-click-prediction;0.74;0.477;2020-12-13 15:00:19;Outbrain Click Prediction;[];Explore user base by GEO;Python notebook;7860.0;74;;
2016-10-09 21:48:58;Use this one simple trick to get to the top of the leaderboard - Grandmasters hate him!Here's quite a high-level EDA - since the data is so huge, we want to get a better understanding of what we actually have. If this EDA helps you, make sure to leave an upvote to motivate me to make more! :) First off: What files do we have?;Apache 2.0;https://www.kaggle.com/anokas/outbrain-eda;0.5;[];['ai', 'nn', 'rl'];['train', 'understanding', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/outbrain-click-prediction;0.787;0.575;2020-12-13 15:00:19;Outbrain Click Prediction;['exploratory data analysis'];Outbrain EDA;Python notebook;29959.0;300;;
2016-10-09 18:08:44;;Apache 2.0;https://www.kaggle.com/anokas/regularized-btb-0-635;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/outbrain-click-prediction;0.698;0.352;2020-12-13 15:00:19;Outbrain Click Prediction;[];Regularized BTB ~0.635;Python script;2920.0;17;;
2016-12-26 15:53:50;;Apache 2.0;https://www.kaggle.com/ashhafez/evaluate-map-score-without-groupby-in-python;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/outbrain-click-prediction;0.699;0.352;2020-12-13 15:00:19;Outbrain Click Prediction;[];Evaluate map score without groupby in python;Python script;2989.0;17;;
2017-03-22 11:12:47;;Apache 2.0;https://www.kaggle.com/clustifier/btb-0-63523-evaluation;1.0;['xgboost'];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/outbrain-click-prediction;0.749;0.45;2020-12-13 15:00:19;Outbrain Click Prediction;[];BTB 0.63523 + evaluation;Python script;10013.0;53;;
2016-11-06 20:06:29;;Apache 2.0;https://www.kaggle.com/clustifier/pandas-is-cool-lb-0-63714;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/outbrain-click-prediction;0.718;0.393;2020-12-13 15:00:19;Outbrain Click Prediction;[];pandas is cool! LB: 0.63714;Python script;4642.0;27;;
2016-10-27 16:43:11;;Apache 2.0;https://www.kaggle.com/franckjay/easy-random-forests;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['predict', 'train', 'deep learning', 'random forest', 'classification'];https://www.kaggle.com/c/outbrain-click-prediction;0.699;0.319;2020-12-13 15:00:19;Outbrain Click Prediction;[];Easy Random Forests;Python script;3007.0;12;;
2016-10-17 21:22:27;In this notebook, I leverage the power of a Spark cluster to explore the large (~88GB) page_views.csv dataset and analyze its relationshiop with events.csv. After some hours of processing, we've got answer to questions like:  How to join page_views.csv and events.csv? Is events.csv a subset of page_views.csv? Are there additional page views for users in events.csv?;Apache 2.0;https://www.kaggle.com/gspmoreira/unveiling-page-views-csv-with-pyspark;0.2;[];['nn'];[];https://www.kaggle.com/c/outbrain-click-prediction;0.771;0.513;2020-12-13 15:00:19;Outbrain Click Prediction;[];Unveiling page_views.csv with PySpark;Python notebook;18744.0;120;;
2016-10-14 19:57:57;As I checked if access information for landing page of ad clicks is in page_views.csv for clicks_test.csv in this script, I tryed to check for clicks_train.csv to estimate this efects.;Apache 2.0;https://www.kaggle.com/its7171/is-landing-access-for-ad-clicks-in-page-views-csv;0.5;[];['ai', 'dl'];['train', 'test data'];https://www.kaggle.com/c/outbrain-click-prediction;0.685;0.292;2020-12-13 15:00:19;Outbrain Click Prediction;[];Is landing access for ad clicks in page_views.csv;Python notebook;2231.0;9;;
2016-10-13 21:52:56;;Apache 2.0;https://www.kaggle.com/its7171/leakage-solution;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/outbrain-click-prediction;0.741;0.387;2020-12-13 15:00:19;Outbrain Click Prediction;[];leakage solution?;Python script;8160.0;25;;
2016-10-16 03:04:51;;Apache 2.0;https://www.kaggle.com/jiweiliu/extract-leak-in-30-mins-with-small-memory;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/outbrain-click-prediction;0.744;0.393;2020-12-13 15:00:19;Outbrain Click Prediction;[];Extract leak in 30 mins with small memory;Python script;8831.0;27;;
2016-10-08 13:36:47;We are told there is a time-based train/test split. Let's have a look at it, and a small explore of the time-structure of the data in general.;Apache 2.0;https://www.kaggle.com/joconnor/date-exploration-and-train-test-split;0.5;[];['ner', 'ai', 'rl'];['train', 'test data', 'label', 'training data'];https://www.kaggle.com/c/outbrain-click-prediction;0.747;0.427;2020-12-13 15:00:19;Outbrain Click Prediction;[];Date Exploration and Train/Test Split;Python notebook;9445.0;40;;
2016-10-13 10:55:08;;Apache 2.0;https://www.kaggle.com/jpuigde/simple-r-0-63693;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/outbrain-click-prediction;0.735;0.437;2020-12-13 15:00:19;Outbrain Click Prediction;[];Simple_R ~ 0.63693;R script;7009.0;45;;
2016-10-05 22:50:13;;Apache 2.0;https://www.kaggle.com/laurae2/sneak-peak-at-the-data;0.5;[];['ner', 'ai', 'cnn', 'gan', 'cv', 'rl', 'nn', 'ml'];['machine learning', 'train', 'epoch', 'label', 'predict', 'rank', 'recommend'];https://www.kaggle.com/c/outbrain-click-prediction;0.751;0.444;2020-12-13 15:00:19;Outbrain Click Prediction;[];Sneak peak at the data;Rmarkdown script;10582.0;49;;
2017-01-12 16:09:37;;Apache 2.0;https://www.kaggle.com/mpearmain/pandas2libffm;1.0;['sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/outbrain-click-prediction;0.734;0.4;2020-12-13 15:00:19;Outbrain Click Prediction;[];pandas2libFFM;Python script;6792.0;29;;
2016-10-20 13:13:55;;Apache 2.0;https://www.kaggle.com/qqgeogor/keras-based-fm;1.0;['tensorflow', 'sklearn', 'keras'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/outbrain-click-prediction;0.755;0.435;2020-12-13 15:00:19;Outbrain Click Prediction;[];keras based fm;Python script;11659.0;44;;
2017-01-07 03:24:29;;Apache 2.0;https://www.kaggle.com/qqgeogor/pypy-implementation-of-fm-with-adam;0.5;[];['nlp', 'ai', 'nn', 'ner'];['predict', 'train', 'epoch', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/outbrain-click-prediction;0.667;0.292;2020-12-13 15:00:19;Outbrain Click Prediction;[];pypy implementation of fm(with adam);Python script;1539.0;9;;
2016-11-01 04:01:57;;Apache 2.0;https://www.kaggle.com/sudalairajkumar/ftrl-starter-with-leakage-vars;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['training data', 'regression', 'train', 'model', 'epoch', 'deep learning', 'validation data', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/outbrain-click-prediction;0.751;0.452;2020-12-13 15:00:19;Outbrain Click Prediction;[];FTRL Starter (with leakage vars);Python script;10734.0;54;;
2016-11-10 13:51:56;;Apache 2.0;https://www.kaggle.com/titericz/giba1-data-table-is-cool-lb-0-63714;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['predict', 'train', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/outbrain-click-prediction;0.728;0.357;2020-12-13 15:00:19;Outbrain Click Prediction;[];Giba1 - Data.table is cool! LB: +0.63714;R script;5946.0;18;;
2016-11-13 06:03:58;;Apache 2.0;https://www.kaggle.com/tkm2261/bigquery-is-cool-lb-0-63692;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/outbrain-click-prediction;0.688;0.281;2020-12-13 15:00:19;Outbrain Click Prediction;[];BigQuery is cool! LB:0.63692+;Python script;2358.0;8;;
2016-05-25 15:23:21;;Apache 2.0;https://www.kaggle.com/atanahel/file-corruption;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/painter-by-numbers;0.651;0.099;2020-12-13 15:04:10;Painter by Numbers;[];File corruption;Python script;1118.0;1;;
2020-04-20 15:38:20;;Apache 2.0;https://www.kaggle.com/brunomartens/painterbynumbers;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'vgg', 'label', 'loss', 'resnet', 'classification'];https://www.kaggle.com/c/painter-by-numbers;0.512;0.0;2020-12-13 15:04:10;Painter by Numbers;[];PainterByNumbers;Python notebook;118.0;0;;
2017-05-31 07:50:32;;Apache 2.0;https://www.kaggle.com/chenbk/notebook41968f8863;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/painter-by-numbers;0.487;0.0;2020-12-13 15:04:10;Painter by Numbers;[];Notebook41968f8863;Python notebook;85.0;0;;
2017-04-09 18:59:17;;Apache 2.0;https://www.kaggle.com/ecdraayer/painter;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/painter-by-numbers;0.54;0.0;2020-12-13 15:04:10;Painter by Numbers;[];Painter;Python notebook;175.0;0;;
2016-09-18 23:07:33;;Apache 2.0;https://www.kaggle.com/gzgzgz/artwork-classification;1.0;['skimage'];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/painter-by-numbers;0.694;0.099;2020-12-13 15:04:10;Painter by Numbers;[];artwork_classification;Python script;2660.0;1;;
2016-11-19 22:53:00;;Apache 2.0;https://www.kaggle.com/jaywhang/notebook29459c115e;0.0;[];[];[];https://www.kaggle.com/c/painter-by-numbers;0.462;0.0;2020-12-13 15:04:10;Painter by Numbers;[];Notebook29459c115e;Python notebook;61.0;0;;
2018-05-08 20:22:37;;Apache 2.0;https://www.kaggle.com/jihyeseo/images-painter-classify;1.0;['sklearn'];['ai'];['train'];https://www.kaggle.com/c/painter-by-numbers;0.544;0.0;2020-12-13 15:04:10;Painter by Numbers;[];images painter classify;Python notebook;186.0;0;;
2016-10-14 05:17:50;Essential to always understand the nature of data. Goal of this R workbook is to share what I learnt as I  start to look into dataset without doing any tangible data processing. I've released also the ggplot code which I found very useful is understanding, and conveying the message.;Apache 2.0;https://www.kaggle.com/johnaji/notebookea493c1202;0.5;[];['dl', 'ai', 'nn'];['train', 'understanding', 'filter'];https://www.kaggle.com/c/painter-by-numbers;0.682;0.253;2020-12-13 15:04:10;Painter by Numbers;[];Top Genre - Basic Understanding of Dataset;R notebook;2069.0;6;;
2016-11-10 17:49:36;;Apache 2.0;https://www.kaggle.com/kumarp/notebook2958fa2cb0;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/painter-by-numbers;0.471;0.0;2020-12-13 15:04:10;Painter by Numbers;[];Notebook2958fa2cb0;Python notebook;69.0;0;;
2017-02-24 18:24:50;Essential to always understand the nature of data. Goal of this R workbook is to share what I learnt as I  start to look into dataset without doing any tangible data processing. I've released also the ggplot code which I found very useful is understanding, and conveying the message.;Apache 2.0;https://www.kaggle.com/leomacias/top-genre-basic-understanding-of-dataset;0.5;[];['dl', 'ai', 'nn'];['train', 'understanding', 'filter'];https://www.kaggle.com/c/painter-by-numbers;0.521;0.0;2020-12-13 15:04:10;Painter by Numbers;[];Top Genre - Basic Understanding of Dataset;R notebook;134.0;0;;
2019-10-11 01:13:07;;Apache 2.0;https://www.kaggle.com/parkerzmartin/painterbynumbersset;0.5;[];['ner', 'ai', 'cv', 'rl', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/painter-by-numbers;0.597;0.152;2020-12-13 15:04:10;Painter by Numbers;[];painterbynumbersset;Python notebook;426.0;2;;
2017-01-17 12:28:34;;Apache 2.0;https://www.kaggle.com/phirwe/frank-first-test-4948d378;0.5;[];['ai', 'nn', 'ml'];['train'];https://www.kaggle.com/c/painter-by-numbers;0.481;0.0;2020-12-13 15:04:10;Painter by Numbers;[];Frank-first-test 4948d378;Python notebook;78.0;0;;
2019-01-29 07:30:24;;Apache 2.0;https://www.kaggle.com/protiknag08/artistic-style-recognition;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'recognition', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/painter-by-numbers;0.548;0.0;2020-12-13 15:04:10;Painter by Numbers;['gpu'];Artistic Style Recognition;Python script;197.0;0;;
2017-12-13 06:45:56;;Apache 2.0;https://www.kaggle.com/sonali18395/fork-of-artwork-classification;1.0;['skimage'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/painter-by-numbers;0.522;0.0;2020-12-13 15:04:10;Painter by Numbers;[];Fork of artwork_classification;Python script;136.0;0;;
2020-03-23 16:12:45;;Apache 2.0;https://www.kaggle.com/windmen/special-for-danil;0.5;[];['ai', 'nn', 'cv'];['train', 'label'];https://www.kaggle.com/c/painter-by-numbers;0.467;0.0;2020-12-13 15:04:10;Painter by Numbers;[];Special For Danil;Python notebook;65.0;0;;
2020-04-18 14:15:23;;Apache 2.0;https://www.kaggle.com/wouterdpg/painterbynumbers;1.0;['pytorch', 'sklearn'];['ai', 'nn'];['train', 'model', 'epoch', 'vgg', 'label', 'loss', 'resnet', 'classification'];https://www.kaggle.com/c/painter-by-numbers;0.565;0.099;2020-12-13 15:04:10;Painter by Numbers;[];PainterByNumbers;Python notebook;255.0;1;;
2016-05-28 15:48:40;;Apache 2.0;https://www.kaggle.com/ymcdull/frank-first-test-1;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/painter-by-numbers;0.644;0.099;2020-12-13 15:04:10;Painter by Numbers;[];Frank-first-test;Python notebook;970.0;1;0.50000;0.50000
2020-09-01 22:06:24;OverviewThe High Definition-Advanced Imaging Technology (HD-AIT) system files supplied in this contest range in size from 10MB to over 2GB per subject.  With just under 1200 examples, we'll need to figure out how to make or find more and almost any approach will have to substantially reduce the size of the 512x660x16 image data to be fed into a machine learning model.  In the instructions, the organizers suggest that one may even be able to win the contest with one of the smaller image suites. So in this notebook, I take a whack at preprocessing the lowest res images we have and providing some basic building blocks for the preprocessing pipeline. A quick disclaimer, I'm not an expert on these systems or the related scans.  If you see something I've misunderstood or you think I've made an error, let me know and I'll correct it.  This contest is aimed at a critical problem.  I'm in this to help us get better at threat detection.  The community can definitely improve the predictive veracity of these scans.  Anyway, I'm excited to get going so let's jump in. To begin I collect all of the imports used in the notebook at the top.  It makes it easier when you're converting to a preprocessing script.;Apache 2.0;https://www.kaggle.com/ahmedmurad1990/eda-and-generation;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['machine learning', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.482;0.152;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];EDA and generation;Python notebook;79.0;2;;
2017-07-06 05:35:35;Organizing Labels with pandasThis shows how the data labels for the Stage 1 training scans can be organized in a pandas dataframe.  By separating the scan IDs and body zones, we can easily get information such as how many threats are in each zone or how many threats are in the scans. First, let's import some dependencies.;Apache 2.0;https://www.kaggle.com/akruger/organizing-labels-in-pandas;0.5;[];['ai', 'gan'];['train', 'label', 'filter', 'labeled'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.671;0.214;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Organizing Labels in pandas;Python notebook;1659.0;4;;
2017-09-04 01:01:31;These are (unofficial) functions to read and view the competition files. Most of the information in the file header is identical across scans and thus not relevant to the competition.;Apache 2.0;https://www.kaggle.com/congweilin/reading-images;0.2;[];['ner', 'nn', 'ann'];[];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.679;0.214;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Reading Images;Python notebook;1934.0;4;;
2017-12-04 19:46:02;;Apache 2.0;https://www.kaggle.com/equationist/reading-images-in-julia;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['label', 'classification', 'deep learning'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.633;0.214;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Reading Images in Julia;Julia script;792.0;4;;
2017-07-22 09:56:37;Saving to vtr fileThe aim of this notebook is to show how to save .a3d files to vtr for visualizing them with paraview. https://www.paraview.org/ https://pypi.python.org/pypi/PyEVTK;Apache 2.0;https://www.kaggle.com/ironbar/saving-3d-files-to-vtr-for-later-visualization;0.5;[];['ner', 'ai', 'nn', 'ann'];['model'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.696;0.253;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Saving 3d files to .vtr for later visualization;Python notebook;2829.0;6;;
2017-09-30 17:15:48;OverviewThe High Definition-Advanced Imaging Technology (HD-AIT) system files supplied in this contest range in size from 10MB to over 2GB per subject.  With just under 1200 examples, we'll need to figure out how to make or find more and almost any approach will have to substantially reduce the size of the 512x660x16 image data to be fed into a machine learning model.  In the instructions, the organizers suggest that one may even be able to win the contest with one of the smaller image suites. So in this notebook, I take a whack at preprocessing the lowest res images we have and providing some basic building blocks for the preprocessing pipeline. A quick disclaimer, I'm not an expert on these systems or the related scans.  If you see something I've misunderstood or you think I've made an error, let me know and I'll correct it.  This contest is aimed at a critical problem.  I'm in this to help us get better at threat detection.  The community can definitely improve the predictive veracity of these scans.  Anyway, I'm excited to get going so let's jump in. To begin I collect all of the imports used in the notebook at the top.  It makes it easier when you're converting to a preprocessing script.;Apache 2.0;https://www.kaggle.com/jbfarrar/exploratory-data-analysis-and-example-generation;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['machine learning', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.799;0.555;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;['exploratory data analysis, computer vision'];Exploratory Data Analysis and Example Generation;Python notebook;43763.0;219;;
2018-07-01 11:52:03;OverviewThe High Definition-Advanced Imaging Technology (HD-AIT) system files supplied in this contest range in size from 10MB to over 2GB per subject.  With just under 1200 examples, we'll need to figure out how to make or find more and almost any approach will have to substantially reduce the size of the 512x660x16 image data to be fed into a machine learning model.  In the instructions, the organizers suggest that one may even be able to win the contest with one of the smaller image suites. So in this notebook, I take a whack at preprocessing the lowest res images we have and providing some basic building blocks for the preprocessing pipeline. A quick disclaimer, I'm not an expert on these systems or the related scans.  If you see something I've misunderstood or you think I've made an error, let me know and I'll correct it.  This contest is aimed at a critical problem.  I'm in this to help us get better at threat detection.  The community can definitely improve the predictive veracity of these scans.  Anyway, I'm excited to get going so let's jump in. To begin I collect all of the imports used in the notebook at the top.  It makes it easier when you're converting to a preprocessing script.;Apache 2.0;https://www.kaggle.com/khandelwallaksya/eda-and-generation;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['machine learning', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.57;0.152;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;['gpu'];EDA and generation;Python notebook;278.0;2;;
2017-06-28 05:00:00;;Apache 2.0;https://www.kaggle.com/millerintllc/expected-income-per-day-of-competition;0.5;[];['nlp', 'ai', 'nn', 'ner'];['label', 'classification', 'deep learning'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.725;0.292;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Expected income per day of competition;R script;5406.0;9;;
2017-06-23 03:18:58;???;Apache 2.0;https://www.kaggle.com/mtfall/omg-no-data-in-this-competition-s-kernel-d;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.703;0.214;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];OMG, no data in this competition's kernel :D;Python notebook;3286.0;4;;
2018-08-23 19:04:03;Table of Contents The competition Summary of my approach and results Algorithm details Instructions for running my code What I've learned about competing;Apache 2.0;https://www.kaggle.com/nathanrm/full-solution-cylindrical-coordinate-method;0.5;[];['ner', 'ai', 'cnn', 'gan', 'cv', 'rl', 'nn', 'ann'];['filter', 'test data', 'regression', 'train', 'fitting', 'model', 'layer', 'label', 'logistic regression', 'predict'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.658;0.152;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Full solution (cylindrical coordinate method);Python notebook;1286.0;2;;
2017-06-23 07:23:05;;Apache 2.0;https://www.kaggle.com/philippsp/baseline-lb-0-29089;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.734;0.311;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Baseline [LB 0.29089];Rmarkdown script;6814.0;11;;
2017-06-25 21:14:29;;Apache 2.0;https://www.kaggle.com/philippsp/reading-images-in-r;0.2;[];['ann', 'nn', 'ml', 'ner'];[];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.76;0.439;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;['beginner, computer vision'];Reading Images in R;Rmarkdown script;13571.0;46;;
2017-07-31 09:42:04;"These are slightly better versions of the (unofficial) functions to read and view the competition files. Most of the information in the file header is identical across scans and thus not relevant to the competition. Changesread_header is made faster using struct unpack. read_header only reads 512 bytes (vs orig 532) spare_end is changed from float32 to int16 as previous spares were int16 I infered this to be a mistake. read_header no longer returns arrays of scalars; makes life better for pandas. read_data is refactored removing redundant code";Apache 2.0;https://www.kaggle.com/sammiller/reading-images-refactored;0.2;[];['ner', 'nn', 'ann'];[];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.662;0.236;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Reading Images Refactored;Python notebook;1379.0;5;;
2017-12-15 17:39:17;;Apache 2.0;https://www.kaggle.com/tunguz/random-second-stage-submission-lb-0-000000000000;0.5;[];['nlp', 'ai', 'nn', 'ner'];['label', 'classification', 'deep learning'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.649;0.236;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Random Second Stage Submission (LB 0.000000000000);Python script;1074.0;5;0.99908;0.00000
2017-07-31 10:06:58;OverviewThe High Definition-Advanced Imaging Technology (HD-AIT) system files supplied in this contest range in size from 10MB to over 2GB per subject.  With just under 1200 examples, almost any approach will have to substantially reduce the size of the data to be fed into a machine learning model.  In the instructions, the organizers suggest that one may even be able to win the contest with one of the smaller image suites. So in this notebook, I take a whack at preprocessing the lowest res images we have and providing some basic building blocks for the preprocessing pipeline. A quick disclaimer, I'm not an expert on these systems or the related scans.  If you see something I've misunderstood or you think I've made an error, let me know and I'll correct it.  This contest is aimed at a critical problem.  I'm in this to help us get better at threat detection.  The community can definitely improve the predictive veracity of these scans.  Anyway, I'm excited to get going so let's jump in. To begin I collect all of the imports used in the notebook at the top.  It makes it easier when you're converting to a preprocessing script.;Apache 2.0;https://www.kaggle.com/virgodata/exploratory-data-analysis-and-example-generation;0.5;[];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['machine learning', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.693;0.327;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;['exploratory data analysis'];Exploratory Data Analysis and Example Generation;Python notebook;2604.0;13;;
2017-06-22 18:09:37;These are (unofficial) functions to read and view the competition files. Most of the information in the file header is identical across scans and thus not relevant to the competition.;Apache 2.0;https://www.kaggle.com/wcukierski/reading-images;0.2;[];['ann', 'nn', 'ml', 'ner'];[];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.786;0.493;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;['beginner, computer vision'];Reading Images;Python notebook;29246.0;92;;
2017-09-26 08:25:23;Predictions based off mean probabilities by zone;Apache 2.0;https://www.kaggle.com/zusmani/predictions-based-on-zone-means-0-29089;0.3;[];[];['label', 'predict'];https://www.kaggle.com/c/passenger-screening-algorithm-challenge;0.66;0.302;2020-12-13 15:06:52;Passenger Screening Algorithm Challenge;[];Predictions based on zone means (0.29089);Python notebook;1317.0;10;0.00000;0.00000
2018-12-29 03:01:07;;Apache 2.0;https://www.kaggle.com/abhishek/maybe-something-interesting-here;1.0;['pattern', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'rl'];['regression', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/petfinder-adoption-prediction;0.73;0.494;2020-12-13 15:07:40;PetFinder.my Adoption Prediction;[];maybe something interesting here...;Python notebook;6243.0;93;;
2020-01-11 15:43:50;"As you've probably read in Andy's announcement, Team ""Bestpetting"", the prior first team in this competition, has been removed because of cheating. I've helped PetFinder.my convert the top solutions of their competition to a production system in the past few months and I recently discovered that Bestpetting had been cheating. I'd like to thank Andy from PetFinder.my for very professionally handling this scenario and for always being very helpful and dependable when working together.";Apache 2.0;https://www.kaggle.com/bminixhofer/how-bestpetting-cheated;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['label', 'test data', 'predict'];https://www.kaggle.com/c/petfinder-adoption-prediction;0.811;0.596;2020-12-13 15:07:40;multiple data sources;[];How Bestpetting cheated;Python notebook;65548.0;418;;
2019-01-25 22:31:03;;Apache 2.0;https://www.kaggle.com/christofhenkel/weighted-kappa-loss-for-keras-tensorflow;1.0;['tensorflow'];['ai'];['train', 'model', 'loss'];https://www.kaggle.com/c/petfinder-adoption-prediction;0.737;0.473;2020-12-13 15:07:40;PetFinder.my Adoption Prediction;[];Weighted Kappa Loss for Keras/ Tensorflow;Python notebook;7305.0;71;;
2019-03-25 22:48:18;Stratified Group k-Fold Cross-ValidationProvides train/test indices to split data into train/test sets. Data are splitted in a way to fulfil the following criteria:  Folds are made by preserving the percentage of samples for each class. The same group will not appear in two different folds.;Apache 2.0;https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/petfinder-adoption-prediction;0.774;0.531;2020-12-13 15:07:40;PetFinder.my Adoption Prediction;[];Stratified Group k-Fold Cross-Validation;Python notebook;20115.0;155;;
2019-02-19 09:45:50;;Apache 2.0;https://www.kaggle.com/jaseziv83/an-extensive-eda-of-petfinder-my-data;1.0;['xgboost'];['ai', 'gan', 'rl', 'nlp', 'nn', 'ml'];['filter', 'test data', 'train', 'model', 'label', 'predict', 'sentiment analysis'];https://www.kaggle.com/c/petfinder-adoption-prediction;0.729;0.489;2020-12-13 15:07:40;PetFinder.my Adoption Prediction;['data visualization, classification, feature engineering, +2 morexgboost, text mining'];An Extensive EDA of Petfinder.my data;Rmarkdown script;6073.0;87;;
2019-03-07 04:07:29;Forked from Baseline Modeling;Apache 2.0;https://www.kaggle.com/ranjoranjan/single-xgboost-model;1.0;['xgboost', 'sklearn', 'tensorflow', 'pattern', 'keras'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/petfinder-adoption-prediction;0.754;0.523;2020-12-13 15:07:40;multiple data sources;['gpu'];Single XGBoost model;Python notebook;11365.0;139;0.45378;0.45378
2019-04-13 02:43:24;Forked from Baseline Modeling;Apache 2.0;https://www.kaggle.com/reppy4620/xgboost;1.0;['xgboost', 'sklearn', 'tensorflow', 'pattern', 'keras'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/petfinder-adoption-prediction;0.74;0.497;2020-12-13 15:07:40;multiple data sources;[];XGBoost;Python notebook;7896.0;96;;
2019-07-06 16:34:39;imports and functions;Apache 2.0;https://www.kaggle.com/wuyhbb/final-small;1.0;['xgboost', 'lightgbm', 'nltk', 'gensim', 'catboost', 'sklearn', 'pytorch', 'pattern'];['ner', 'ai', 'dl', 'gbm', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['gru', 'filter', 'test data', 'regression', 'train', 'model', 'epoch', 'layer', 'clustering', 'loss', 'label', 'lstm', 'predict', 'rank', 'resnet', 'relu'];https://www.kaggle.com/c/petfinder-adoption-prediction;0.717;0.467;2020-12-13 15:07:40;multiple data sources;['gpu'];final-small;Python notebook;4536.0;65;0.32604;0.00000
2015-06-09 21:27:30;;Apache 2.0;https://www.kaggle.com/arnoudcommandeur/visualisation-of-trips-by-google-maps;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.693;0.152;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Visualisation of trips by Google Maps;Rmarkdown script;2638.0;2;;
2015-05-30 09:08:24;;Apache 2.0;https://www.kaggle.com/arunkt/visualization-of-taxi-trip-end-points;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['training data', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.605;0.099;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Visualization of taxi trip end points;Python script;488.0;1;;
2015-10-26 16:12:45;;Apache 2.0;https://www.kaggle.com/ashamli/all-trips;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.636;0.099;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];All trips;Python script;850.0;1;;
2015-05-22 10:36:37;;Apache 2.0;https://www.kaggle.com/benhamner/last-location-benchmark;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.684;0.214;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Last Location Benchmark;R script;2181.0;4;2.60662;3.31765
2015-05-23 00:22:13;;Apache 2.0;https://www.kaggle.com/benhamner/test-trips-map;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.731;0.334;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Test Trips Map;R script;6324.0;14;;
2015-05-23 01:19:26;;Apache 2.0;https://www.kaggle.com/gopaldutt/test-trips-map;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.617;0.152;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Test Trips Map;R script;600.0;2;;
2015-05-23 16:42:29;;Apache 2.0;https://www.kaggle.com/hochthom/visualization-of-taxi-trip-end-points;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['training data', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.772;0.455;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Visualization of taxi trip end points;Python script;18891.0;56;;
2018-05-08 19:13:45;;Apache 2.0;https://www.kaggle.com/jihyeseo/handle-rds-and-zip-eda;1.0;['sklearn'];['ai'];['train'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.549;0.0;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];handle rds and zip eda;Python notebook;202.0;0;;
2017-06-02 02:08:20;1 èŽ·å–æ•°æ®æ•°æ®å–è‡ªæµ‹è¯•é›†ï¼Œç­›é€‰å‡ºæµ‹è¯•é›†ä¸­æ‰€æœ‰GPSè®°å½•æ­£å¸¸çš„æ•°æ®;Apache 2.0;https://www.kaggle.com/joncle/notebook0f2646ced6;1.0;['sklearn'];['ai', 'cv'];['regression', 'train', 'model', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.573;0.0;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Notebook0f2646ced6;Python notebook;292.0;0;;
2017-06-01 15:02:20;;Apache 2.0;https://www.kaggle.com/joncle/notebookc2931820a9;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.543;0.0;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Notebookc2931820a9;Python notebook;183.0;0;;
2017-05-28 11:31:15;;Apache 2.0;https://www.kaggle.com/joncle/test1;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.588;0.0;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];test1;Python script;366.0;0;;
2015-06-27 03:30:44;;Apache 2.0;https://www.kaggle.com/mcwitt/heatmap;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.734;0.311;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];heatmap;Python script;6855.0;11;;
2015-05-22 11:05:51;;Apache 2.0;https://www.kaggle.com/nabilblk/last-location-benchmark;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.639;0.099;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Last Location Benchmark;R script;893.0;1;2.60662;3.31765
2015-06-20 23:26:01;;Apache 2.0;https://www.kaggle.com/prayas/last-location-benchmark;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.672;0.099;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Last Location Benchmark;R script;1698.0;1;243.19939;242.20163
2016-11-21 13:31:42;Importing Data;Apache 2.0;https://www.kaggle.com/raymonmina/notebook98c3fcc906;1.0;['sklearn'];['ai', 'cv'];['regression', 'train', 'model', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.649;0.099;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Notebook0f2646ced6;Python notebook;1067.0;1;;
2015-05-27 11:02:08;;Apache 2.0;https://www.kaggle.com/sircausticmail/getting-the-most-visited-places;1.0;['sklearn'];['ner', 'ai', 'nlp', 'nn', 'ml'];['training data', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.7;0.253;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Getting the Most Visited Places;Python script;3083.0;6;;
2018-07-27 23:08:13;;Apache 2.0;https://www.kaggle.com/sivaram123/visualization-of-taxi-trip-end-points;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['training data', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.569;0.0;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Visualization of taxi trip end points;Python script;274.0;0;;
2015-06-12 01:15:04;;Apache 2.0;https://www.kaggle.com/thomas92/plot-of-trips;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.721;0.292;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Plot of trips;Python script;4937.0;9;;
2020-04-18 10:16:21;Prediction taxi trajectory;Apache 2.0;https://www.kaggle.com/urayukitaka/prediction-taxi-trajectory;1.0;['sklearn'];['ai', 'nn', 'ml', 'cv'];['filter', 'test data', 'regression', 'train', 'fitting', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.637;0.152;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Prediction taxi trajectory;Python notebook;864.0;2;2.58340;2.78860
2015-05-31 02:14:18;;Apache 2.0;https://www.kaggle.com/willieliao/test-set-sampling-cutoff;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i;0.67;0.236;2020-12-13 15:15:57;ECML/PKDD 15: Taxi Trajectory Prediction (I);[];Test set sampling cutoff;R script;1631.0;5;;
2015-05-23 02:42:38;;Apache 2.0;https://www.kaggle.com/benhamner/max-time-elapsed-mean-time-benchmark;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.697;0.268;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Max(Time Elapsed,Mean Time) Benchmark;R script;2860.0;7;0.64462;0.62349
2015-05-29 09:26:04;;Apache 2.0;https://www.kaggle.com/benhamner/speed-visualization;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['training data', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.686;0.236;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Speed visualization ;Python script;2274.0;5;;
2015-05-23 02:51:05;;Apache 2.0;https://www.kaggle.com/benhamner/test-set-taxi-current-locations-map;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.644;0.188;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Test Set Taxi Current Locations (Map);R script;981.0;3;;
2015-07-01 06:29:10;;Apache 2.0;https://www.kaggle.com/chevli/polylinelength;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.663;0.099;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];PolyLineLength;Python script;1420.0;1;0.74171;0.75966
2015-06-26 12:09:18;;Apache 2.0;https://www.kaggle.com/clrajapaksha/hillclimb-fork;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.572;0.0;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];HillClimb fork;R script;285.0;0;;
2015-09-06 07:29:42;;Apache 2.0;https://www.kaggle.com/darshanbutiya/darshan1;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.629;0.0;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];darshan1;Python script;741.0;0;;
2015-06-19 16:28:24;;Apache 2.0;https://www.kaggle.com/dataista/0-60-on-lb-w-max-mean-time-plus-trick;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.681;0.214;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];0,60 on LB w/ max(mean,time) plus trick;Python script;2053.0;4;;
2015-06-15 11:02:15;;Apache 2.0;https://www.kaggle.com/dataista/time-prob-density-plot;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.673;0.152;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];TIME prob density plot;Python script;1724.0;2;;
2015-06-27 21:34:45;;Apache 2.0;https://www.kaggle.com/drvivi/add-a-constant-value-score-0-5995;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.635;0.0;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Add a constant value - score 0.5995 ;Python script;830.0;0;0.59623;0.59949
2017-03-09 00:37:06;;Apache 2.0;https://www.kaggle.com/elnazmr/notebook77d5e3a79a;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.556;0.0;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Notebook77d5e3a79a;R notebook;223.0;0;;
2015-06-05 19:51:36;;Apache 2.0;https://www.kaggle.com/gshguru/max-time-elapsed-mean-time-benchmark;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.663;0.099;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Max(Time Elapsed,Mean Time) Benchmark;R script;1419.0;1;0.60229;0.58675
2015-07-02 02:01:01;;Apache 2.0;https://www.kaggle.com/hansolo/beat-the-benchmark;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.642;0.0;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Beat The Benchmark;Python script;944.0;0;0.60650;0.59975
2015-06-24 12:46:52;;Apache 2.0;https://www.kaggle.com/isuruherath/find-vincenty-distance-ref-google;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.675;0.188;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Find Vincenty distance (ref:google);R script;1793.0;3;;
2018-05-08 17:46:04;;Apache 2.0;https://www.kaggle.com/jihyeseo/trip-eda-rda-unzip;1.0;['sklearn'];['ai', 'nn'];['train', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.54;0.0;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];trip eda rda unzip;Python notebook;176.0;0;;
2015-11-26 18:21:14;;Apache 2.0;https://www.kaggle.com/kgayanm/mytest;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'layer', 'predict', 'classification'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.613;0.0;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];MyTest;R script;556.0;0;;
2017-05-28 14:42:31;;Apache 2.0;https://www.kaggle.com/sharmanaman/notebook41a040061b;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.547;0.0;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Notebook41a040061b;Python notebook;196.0;0;;
2017-01-07 07:29:57;;Apache 2.0;https://www.kaggle.com/squaresun/notebook81fb279e60;0.0;[];[];[];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.495;0.0;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Notebook81fb279e60;R notebook;94.0;0;;
2015-06-20 03:07:29;;Apache 2.0;https://www.kaggle.com/titericz/hillclimb-1;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.651;0.099;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];HillClimb 1;R script;1105.0;1;0.60434;0.58663
2015-05-30 10:12:20;;Apache 2.0;https://www.kaggle.com/wikunia/speed-visualization;0.5;[];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.734;0.383;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Speed visualization ;Python script;6860.0;24;;
2015-07-07 21:40:28;;Apache 2.0;https://www.kaggle.com/willieliao/beat-the-benchmark;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii;0.747;0.346;2020-12-13 15:24:39;ECML/PKDD 15: Taxi Trip Time Prediction (II);[];Beat The Benchmark;Python script;9476.0;16;0.73404;0.76241
2020-01-03 05:05:32;This paper shows a way to regress pose.;Apache 2.0;https://www.kaggle.com/diegojohnson/representing-rotation-with-rotation-vector;0.5;[];['ai', 'rl', 'cv'];['train', 'propagation', 'regression', 'predict'];https://www.kaggle.com/c/pku-autonomous-driving;0.67;0.387;2020-12-13 15:26:06;Peking University/Baidu - Autonomous Driving;[];Representing Rotation with Rotation Vector;Python notebook;1613.0;25;;
2019-10-28 01:31:07;"Augmented RealityIn this notebook, we'll augment an image by adding a 3D wireframe overlay on cars. ðŸ˜Š References Code for pin-hole camera projection is borrowed from Li Linye (@zstusnoopy)'s ""Visualize the location and 3d bounding box of car"" notebook Code to load 3D car model is from my notebook ""Load a 3D car model""";Apache 2.0;https://www.kaggle.com/ebouteillon/augmented-reality;0.5;[];['ai', 'rl', 'cv'];['train', 'model', 'predict'];https://www.kaggle.com/c/pku-autonomous-driving;0.725;0.512;2020-12-13 15:26:06;Peking University/Baidu - Autonomous Driving;['beginner, data visualization'];Augmented Reality;Python notebook;5412.0;119;;
2019-10-23 23:14:58;Load and display 3D modelsIn this notebook, we'll see how to load a car from a JSON file and display it in 3D.;Apache 2.0;https://www.kaggle.com/ebouteillon/load-a-3d-car-model;0.5;[];['ml'];['model'];https://www.kaggle.com/c/pku-autonomous-driving;0.707;0.452;2020-12-13 15:26:06;Peking University/Baidu - Autonomous Driving;['beginner, data visualization'];Load a 3D car model;Python notebook;3586.0;54;;
2020-01-12 09:01:21;# Center-resnext50 Starter;Apache 2.0;https://www.kaggle.com/mobassir/center-resnext50-baseline;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['regression', 'train', 'recognition', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet', 'u-net', 'ground truth'];https://www.kaggle.com/c/pku-autonomous-driving;0.75;0.496;2020-12-13 15:26:06;multiple data sources;['gpu'];Center-resnext50 baseline;Python notebook;10365.0;95;;
2019-11-23 09:33:57;CenterResnet Starter;Apache 2.0;https://www.kaggle.com/phoenix9032/center-resnet-starter;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['regression', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet', 'u-net', 'ground truth'];https://www.kaggle.com/c/pku-autonomous-driving;0.732;0.485;2020-12-13 15:26:06;multiple data sources;['gpu'];Center-Resnet Starter;Python notebook;6460.0;82;0.038;0.039
2020-02-22 09:08:27;Table of Contents Import modules Configure parameters Get annotations Get all model-types Plot some figures Plot all 3D car models Visualize some images Conclusion;Apache 2.0;https://www.kaggle.com/phunghieu/a-quick-simple-eda;0.5;[];['ai', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['gru', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/pku-autonomous-driving;0.756;0.546;2020-12-13 15:26:06;Peking University/Baidu - Autonomous Driving;['beginner, data visualization, exploratory data analysis, +1 moreimage data'];A quick & simple EDA;Python notebook;12111.0;192;;
2019-10-24 21:08:52;The goal of the competition is to develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environmentFrom the problem description:  The primary data is images of cars and related pose information. The pose information is formatted as strings, as follows: model type, yaw, pitch, roll, x, y, z  A concrete example with two cars in the photo: 5 0.5 0.5 0.5 0.0 0.0 0.0 32 0.25 0.25 0.25 0.5 0.4 0.7  Submissions (per sample_submission.csv) are very similar, with the addition of a confidence score, and the removal of the model type. You are not required to predict the model type of the vehicle in question. ID, PredictionString ID_1d7bc9b31,0.5 0.5 0.5 0.0 0.0 0.0 1.0 indicating that this prediction has a confidence score of 1.0.   ** So, the goal is to predict one or more sets of **: yaw, pitch, roll, x, y, z, confidence **for each picture**:;Apache 2.0;https://www.kaggle.com/seshadrikolluri/vehicle-angle-prediction-understanding-eda;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'training data', 'predict'];https://www.kaggle.com/c/pku-autonomous-driving;0.739;0.5;2020-12-13 15:26:06;Peking University/Baidu - Autonomous Driving;['beginner, data visualization'];Vehicle Angle Prediction: Understanding / EDA;Python notebook;7798.0;101;;
2019-10-26 02:04:26;3D Interactive Car with PlotlyImpressed by these kernels, I used these kernels and the plotly official documentation to create the following visualizations:  Eric Bouteillon(@ebouteillon) : Load a 3D car model Phung Hieu(@phunghieu) A quick & simple EDA  Plotly : Surface Triangulation in Python/v3   I hope you all get good results.;Apache 2.0;https://www.kaggle.com/subinium/3d-interactive-car-with-plotly;0.5;[];['ai', 'ml'];['model', 'label'];https://www.kaggle.com/c/pku-autonomous-driving;0.691;0.425;2020-12-13 15:26:06;Peking University/Baidu - Autonomous Driving;['data visualization, utility script'];3D Interactive CarðŸš— with Plotly;Python notebook;2504.0;39;;
2019-10-26 00:48:08;"Hi everyone! This is my first kaggle notebook, but I hope that you'll find it usefull. Most significant features:  useful train.csv unpacking procedure EDA of full data, not ""first car only"" proper masking and vizualization without alpha hacks x, y, z mapping to image plane baseline submission without ml";Apache 2.0;https://www.kaggle.com/theshockwaverider/eda-visualization-baseline;0.5;[];['ai', 'ml'];['train', 'model', 'predict'];https://www.kaggle.com/c/pku-autonomous-driving;0.673;0.4;2020-12-13 15:26:06;Peking University/Baidu - Autonomous Driving;['data visualization, exploratory data analysis, utility script'];EDA, visualization, baseline;Python notebook;1730.0;29;0.000;0.001
2017-04-21 20:30:17;Classifying the Amazon RainforestWelcome back to another satellite imagery competition - these seem to be in fashion lately :) This time, unlike other recent satellite imagery competitions, we have to add tags to each image (which are segments of a larger image of the Amazon Rainforest). However, since each image can have multiple labels, that makes this a multi-label classification challenge as opposed to standard multi-class problem. And as always, if this helped you, some upvotes would be very much appreciated - that's where I get my motivation! :D Time to get straight into the data:;Apache 2.0;https://www.kaggle.com/anokas/data-exploration-analysis;0.5;[];['ai', 'nn', 'cv'];['train', 'label', 'training data', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.793;0.592;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;['exploratory data analysis'];Data Exploration & Analysis;Python notebook;36329.0;395;;
2017-05-01 19:52:51;;Apache 2.0;https://www.kaggle.com/anokas/fixed-f2-score-in-python;1.0;['sklearn'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'deep learning', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.739;0.46;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;[];Fixed F2 Score in Python;Python script;7721.0;60;;
2017-05-02 16:08:34;;Apache 2.0;https://www.kaggle.com/anokas/simple-keras-starter;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'relu', 'loss', 'label', 'predict', 'rank', 'understanding', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.747;0.491;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;[];Simple Keras Starter;Python script;9614.0;89;;
2017-04-24 00:57:50;;Apache 2.0;https://www.kaggle.com/arsenyinfo/f-beta-score-for-keras;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'deep learning', 'layer', 'label', 'predict', 'rank', 'understanding', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.741;0.425;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;[];F-beta score for Keras;Python script;8141.0;39;;
2017-04-21 23:01:56;;Apache 2.0;https://www.kaggle.com/dengzc/classify-rainforest;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'layer', 'understanding', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.697;0.362;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;[];Classify Rainforest;R script;2879.0;19;;
2017-07-22 00:36:28;Planet: Understanding the Amazon deforestation from Space challenge;Apache 2.0;https://www.kaggle.com/ekami66/0-92837-on-private-lb-solution-with-keras;0.5;[];['ner', 'ai', 'nn', 'ann'];['understanding'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.771;0.53;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;[];[0.92837 on private LB] Solution with Keras;Python notebook;18441.0;152;;
2018-08-01 14:34:34;Multi-label classification;Apache 2.0;https://www.kaggle.com/hortonhearsafoo/fast-ai-lesson-2;1.0;['pytorch', 'sklearn', 'pattern'];['ner', 'ai', 'dl', 'cv'];['filter', 'train', 'model', 'epoch', 'label', 'loss', 'understanding', 'resnet', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.732;0.397;2020-12-13 15:31:34;multiple data sources;['gpu'];fast.ai lesson 2;Python notebook;6498.0;28;;
2017-06-12 12:56:45;;Apache 2.0;https://www.kaggle.com/kelexu/keras-lb-0-913;1.0;['xgboost', 'theano', 'sklearn', 'tensorflow', 'keras'];['ner', 'ai', 'cnn', 'cv', 'rl', 'nlp', 'nn'];['training data', 'test data', 'train', 'fitting', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'understanding', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.729;0.421;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;[];Keras LB 0.913;Python script;6059.0;37;;
2017-06-11 07:26:35;;Apache 2.0;https://www.kaggle.com/opanichev/xgb-starter;1.0;['xgboost', 'sklearn', 'tpot'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'label', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.722;0.463;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;[];XGB Starter;Python script;5146.0;62;0.88004;0.88192
2017-04-30 00:21:59;;Apache 2.0;https://www.kaggle.com/paulorzp/find-best-f2-score-threshold;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'deep learning', 'label', 'understanding', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.727;0.411;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;[];Find Best F2-score threshold;Python script;5713.0;33;;
2017-07-21 08:03:43;;Apache 2.0;https://www.kaggle.com/petrosgk/keras-vgg19-0-93028-private-lb;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'vgg', 'loss', 'label', 'predict', 'understanding', 'resnet', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.708;0.431;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;['neural networks'];Keras VGG19 [0.93028  Private LB];Python script;3633.0;42;;
2017-07-20 11:07:20;;Apache 2.0;https://www.kaggle.com/sashakorekov/end-to-end-resnet50-with-tta-lb-0-93;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ann'];['test data', 'train', 'model', 'output layer', 'understanding', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'recommend', 'resnet', 'classification'];https://www.kaggle.com/c/planet-understanding-the-amazon-from-space;0.713;0.413;2020-12-13 15:31:34;Planet: Understanding the Amazon from Space;[];End-to-End ResNet50 with TTA [LB ~0.93];Python script;4071.0;34;;
2020-04-27 05:27:27;;Apache 2.0;https://www.kaggle.com/a03102030/plant-pathology-2020-resnet50;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nn', 'cv'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/plant-pathology-2020-fgvc7;0.692;0.4;2020-12-13 15:33:04;Plant Pathology 2020 - FGVC7;['beginner, cnn'];Plant Pathology 2020 : ResNet50  ;Python notebook;2574.0;29;0.94513;0.94309
2020-03-21 09:29:36;TPU;Apache 2.0;https://www.kaggle.com/ateplyuk/fork-of-plant-2020-tpu-915e9c;1.0;['tensorflow', 'keras', 'pillow'];['ai', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/plant-pathology-2020-fgvc7;0.715;0.472;2020-12-13 15:33:04;Plant Pathology 2020 - FGVC7;['tpu'];Fork of Plant 2020 TPU 915e9c;Python notebook;4357.0;70;0.97465;0.97780
2020-04-01 11:11:27;;Apache 2.0;https://www.kaggle.com/dataraj/fastai-tutorial-for-image-classification;1.0;['pytorch'];['ner', 'ai', 'dl', 'cnn', 'ml', 'nn', 'ann'];['image segmentation', 'image classification', 'filter', 'training data', 'test data', 'train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'computer vision', 'recommend', 'resnet', 'classification'];https://www.kaggle.com/c/plant-pathology-2020-fgvc7;0.668;0.411;2020-12-13 15:33:04;Plant Pathology 2020 - FGVC7;['gpu'];Fastai tutorial for image classification;Python notebook;1557.0;33;0.92712;0.95364
2020-03-22 13:43:43;TPU or GPU detection;Apache 2.0;https://www.kaggle.com/dimakyn/classification-densenet201-efficientnetb7;1.0;['tensorflow', 'sklearn', 'keras', 'pillow'];['ai', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/plant-pathology-2020-fgvc7;0.7;0.39;2020-12-13 15:33:04;Plant Pathology 2020 - FGVC7;['tpu'];Classification DenseNet201, EfficientNetB7;Python notebook;3075.0;26;0.96877;0.97517
2020-05-22 06:28:57;load datasets and choose image_size;Apache 2.0;https://www.kaggle.com/nightwolfbrooks/data-augmentation-and-keras-cnn;1.0;['sklearn', 'albumentations', 'tensorflow', 'keras', 'skimage'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ml'];['test data', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/plant-pathology-2020-fgvc7;0.676;0.393;2020-12-13 15:33:04;Plant Pathology 2020 - FGVC7;['gpu'];Data Augmentation and Keras CNN;Python notebook;1827.0;27;0.94572;0.96225
2020-03-11 22:15:46;Image metadata;Apache 2.0;https://www.kaggle.com/pestipeti/eda-plant-pathology-2020;0.5;[];['ai', 'cv'];['train', 'label'];https://www.kaggle.com/c/plant-pathology-2020-fgvc7;0.674;0.411;2020-12-13 15:33:04;Plant Pathology 2020 - FGVC7;['data visualization, exploratory data analysis'];EDA - Plant Pathology 2020;Python notebook;1768.0;33;;
2020-04-15 17:36:08;Version - 0.3 for TPU;Apache 2.0;https://www.kaggle.com/piantic/pytorch-tpu;1.0;['pytorch', 'albumentations', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nn', 'rnn'];['filter', 'train', 'model', 'epoch', 'loss', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/plant-pathology-2020-fgvc7;0.697;0.383;2020-12-13 15:33:04;Plant Pathology 2020 - FGVC7;['beginner, deep learning'];Pytorch + TPU! ðŸ’¡;Python notebook;2836.0;24;;
2020-03-23 05:13:03;Introduction;Apache 2.0;https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['activation function', 'filter', 'training data', 'train', 'model', 'neural network', 'epoch', 'validation data', 'layer', 'supervised learning', 'loss', 'label', 'predict', 'relu', 'resnet', 'classification', 'labeled', 'ground truth'];https://www.kaggle.com/c/plant-pathology-2020-fgvc7;0.787;0.615;2020-12-13 15:33:04;Plant Pathology 2020 - FGVC7;['data visualization, exploratory data analysis, deep learning, +1 morecomputer vision'];Plant Pathology 2020 : EDA + Models ðŸŒ¿;Python notebook;30478.0;575;;
2020-03-24 23:31:52;;Apache 2.0;https://www.kaggle.com/victorlouisdg/plant-pathology-opencv-background-removal;0.5;[];['ai', 'cv'];['train'];https://www.kaggle.com/c/plant-pathology-2020-fgvc7;0.675;0.449;2020-12-13 15:33:04;Plant Pathology 2020 - FGVC7;[];Plant Pathology - OpenCV Background Removal;Python notebook;1792.0;52;;
2018-06-15 11:07:58;;Apache 2.0;https://www.kaggle.com/ashishpatel26/plant-seed-classification-using-vgg16;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'gan', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'train', 'model', 'epoch', 'deep learning', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/plant-seedlings-classification;0.681;0.327;2020-12-13 15:35:24;multiple data sources;['gpu'];Plant Seed Classification using VGG16;Python notebook;2049.0;13;;
2018-06-06 12:17:11;;Apache 2.0;https://www.kaggle.com/ashishpatel26/plant-seedling-classification-using-vgg19;1.0;['keras', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'train', 'model', 'epoch', 'deep learning', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/plant-seedlings-classification;0.612;0.292;2020-12-13 15:35:24;multiple data sources;['gpu'];plant-seedling-classification-using-vgg19;Python notebook;554.0;9;;
2018-01-10 06:17:14;;Apache 2.0;https://www.kaggle.com/dingkun/xception-model-training-pipeline-lb-0-9798;1.0;['tensorflow', 'keras'];['ner', 'ai', 'nn', 'rl'];['train', 'model', 'epoch', 'layer', 'loss'];https://www.kaggle.com/c/plant-seedlings-classification;0.72;0.319;2020-12-13 15:35:24;Plant Seedlings Classification;[];Xception model training pipeline (LB ~ 0.9798);Python notebook;4837.0;12;;
2017-12-14 16:01:08;Plant Seedlings Segmentation with pure Computer Vision;Apache 2.0;https://www.kaggle.com/gaborvecsei/plant-seedlings-fun-with-computer-vision;0.5;[];['ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['object detection', 'train', 'label', 'predict', 'computer vision'];https://www.kaggle.com/c/plant-seedlings-classification;0.763;0.5;2020-12-13 15:35:24;Plant Seedlings Classification;['data visualization, data cleaning, computer vision, +1 moreplants'];Plant Seedlings Fun with Computer Vision;Python notebook;14737.0;100;;
2017-12-18 13:50:39;PCA;Apache 2.0;https://www.kaggle.com/gaborvecsei/plants-t-sne;1.0;['sklearn'];['ai', 'cv', 'ml', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/plant-seedlings-classification;0.756;0.397;2020-12-13 15:35:24;Plant Seedlings Classification;['beginner, data visualization, pca, +1 moredimensionality reduction'];Plants PCA & t-SNE ( w/ image scatter plot);Python notebook;11990.0;28;;
2018-03-13 18:17:43;Plant *Seedlings* Classification;Apache 2.0;https://www.kaggle.com/matrixb/cnn-svm-xgboost;1.0;['tensorflow', 'xgboost', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['filter', 'machine learning', 'training data', 'train', 'fitting', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/plant-seedlings-classification;0.764;0.393;2020-12-13 15:35:24;multiple data sources;[];CNN + SVM + XGBoost;Python notebook;15128.0;27;;
2018-06-29 14:37:18;;Apache 2.0;https://www.kaggle.com/meenavyas/plant-seedlings-classification;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['activation function', 'filter', 'test data', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/plant-seedlings-classification;0.713;0.302;2020-12-13 15:35:24;Plant Seedlings Classification;[];plant-seedlings-classification;Python notebook;4096.0;10;0.69269;0.69269
2018-02-27 14:09:31;;Apache 2.0;https://www.kaggle.com/miklgr500/keras-simple-model-0-97103-best-public-score;1.0;['skimage', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cnn', 'gan', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'recommend', 'classification'];https://www.kaggle.com/c/plant-seedlings-classification;0.773;0.463;2020-12-13 15:35:24;multiple data sources;['beginner, classification, image data, +2 morecnn, plants'];Keras simple model (0.97103 Best Public Score);Python script;19386.0;62;;
2017-12-03 02:29:23;;Apache 2.0;https://www.kaggle.com/xingyuyang/cnn-with-keras;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'ann', 'rl'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/plant-seedlings-classification;0.711;0.352;2020-12-13 15:35:24;Plant Seedlings Classification;['beginner'];CNN with Keras;Python notebook;3947.0;17;;
2018-11-20 05:42:37;;Apache 2.0;https://www.kaggle.com/cttsai/forked-lgbm-w-ideas-from-kernels-and-discuss;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/PLAsTiCC-2018;0.727;0.505;2020-12-13 15:38:22;PLAsTiCC Astronomical Classification;[];Forked DART w/ ideas from kernels and discuss;Python script;5716.0;108;1.10493;1.08012
2018-11-21 15:18:54;;Apache 2.0;https://www.kaggle.com/cttsai/xgb-from-ideas-from-kernels-and-discussion;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/PLAsTiCC-2018;0.682;0.423;2020-12-13 15:38:22;PLAsTiCC Astronomical Classification;[];XGB from ideas-from-kernels-and-discussion;Python script;2091.0;38;1.12335;1.09711
2018-09-30 21:36:02;Galactic vs Extragalactic ObjectsThe astronomical transients that appear in this challenge can be separated into two distinct groups: ones that are in our Milky Way galaxy (galactic) and ones that are outside of our galaxy (extragalactic). As described in the data note, all of the galactic objects have been assigned a host galaxy photometric redshift of 0. We can use this information to immediately classify every object as either galactic or extragalactic and remove a lot of potential options from the classification. Doing so results in matching the naive benchmark. We find that all of the classes are either uniquely galactic or extragalactic except for class 99 which represents the unknown objects that aren't in the training set.;Apache 2.0;https://www.kaggle.com/kyleboone/naive-benchmark-galactic-vs-extragalactic;0.5;[];['ai', 'rl'];['training data', 'train', 'label', 'predict', 'classification'];https://www.kaggle.com/c/PLAsTiCC-2018;0.738;0.51;2020-12-13 15:38:22;PLAsTiCC Astronomical Classification;[];Naive Benchmark - Galactic vs Extragalactic;Python notebook;7511.0;116;2.15816;2.15881
2018-11-02 02:45:07;Loading Libraries;Apache 2.0;https://www.kaggle.com/meaninglesslives/simple-neural-net-for-time-series-classification;1.0;['lightgbm', 'catboost', 'sklearn', 'tensorflow', 'keras'];['ner', 'ai', 'ml', 'gbm'];['neuron', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/PLAsTiCC-2018;0.781;0.553;2020-12-13 15:38:22;PLAsTiCC Astronomical Classification;['gpu'];Simple Neural Net for Time Series Classification;Python notebook;24752.0;213;1.44575;1.37531
2018-11-22 09:20:26;Can you distinguish between the different classes with just your eyes? New: Multiband frequencies are now fitted with the gatspy package and the fitted values are plotted. This should give us better period estimation results on periodic classes.;Apache 2.0;https://www.kaggle.com/mithrillion/all-classes-light-curve-characteristics-updated;0.5;[];['ai', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/PLAsTiCC-2018;0.732;0.514;2020-12-13 15:38:22;PLAsTiCC Astronomical Classification;[];All Classes Light Curve Characteristics (updated);Python notebook;6415.0;122;;
2018-11-27 07:56:43;;Apache 2.0;https://www.kaggle.com/ogrellier/plasticc-in-a-kernel-meta-and-data;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn'];['predict', 'test data', 'train', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/PLAsTiCC-2018;0.752;0.526;2020-12-13 15:38:22;PLAsTiCC Astronomical Classification;[];PLAsTiCC_in_a_kernel_meta_and_data;Python script;10993.0;144;1.44812;1.42560
2018-11-02 11:20:54;Light curve can tell you a lot about the type of variable object, especially when the object is periodic. This kernel is about to examine, how to extract additional features from the lightcurves using periodograms and phase curves. We will use scipy.signal.lobscargle which is something like fourier analysis for unevenly distributed data. Also known as Least-squares spectral analysis (LSSA).;Apache 2.0;https://www.kaggle.com/rejpalcz/feature-extraction-using-period-analysis;0.5;[];['ner', 'ai', 'cnn', 'rl', 'nn', 'rnn', 'ann'];['train', 'understanding', 'label', 'classification'];https://www.kaggle.com/c/PLAsTiCC-2018;0.701;0.431;2020-12-13 15:38:22;PLAsTiCC Astronomical Classification;[];Feature extraction using period analysis;Python notebook;3158.0;42;;
2018-11-04 15:53:28;Loading Libraries;Apache 2.0;https://www.kaggle.com/rooshroosh/fork-simple-mlp-for-time-series-classification;1.0;['lightgbm', 'catboost', 'sklearn', 'tensorflow', 'keras'];['ner', 'ai', 'ml', 'gbm'];['neuron', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/PLAsTiCC-2018;0.708;0.403;2020-12-13 15:38:22;PLAsTiCC Astronomical Classification;['gpu'];Fork Simple MLP for Time Series Classification;Python notebook;3698.0;30;1.57118;1.50202
2018-11-01 08:25:37;;Apache 2.0;https://www.kaggle.com/sergeylebedev/light-curve-equalization;0.5;[];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/PLAsTiCC-2018;0.671;0.405;2020-12-13 15:38:22;PLAsTiCC Astronomical Classification;[];light curve equalization;R script;1637.0;31;;
2018-12-22 16:41:45;Time Series CNN;Apache 2.0;https://www.kaggle.com/yuval6967/3rd-place-cnn;1.0;['tensorflow', 'keras'];['ner', 'ai', 'cnn', 'cv', 'ml', 'nn', 'ann'];['train', 'model', 'neural network', 'epoch', 'validation data', 'layer', 'loss', 'predict', 'relu', 'convolutional neural network'];https://www.kaggle.com/c/PLAsTiCC-2018;0.698;0.437;2020-12-13 15:38:22;multiple data sources;['gpu, classification, cnn'];3rd Place CNN;Python notebook;2918.0;45;;
2020-07-08 11:44:46;Load data;Apache 2.0;https://www.kaggle.com/kenkpixdev/poker-comb-without-ml-model-accuracy-1-00;1.0;['pillow'];['ner', 'ai', 'ml'];['train', 'test data', 'training data', 'filter'];https://www.kaggle.com/c/poker-rule-induction;0.587;0.099;2020-12-13 15:40:17;Poker Rule Induction;[];Poker comb. without ml model. Accuracy - 1.00;Python notebook;362.0;1;;
2020-05-25 22:12:07;HARD CODING THE WHOLE THING;Apache 2.0;https://www.kaggle.com/prakharrathi25/iterative-proker-hand-prediction;1.0;['sklearn'];['ai', 'nn'];['machine learning', 'training data', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/poker-rule-induction;0.627;0.214;2020-12-13 15:40:17;Poker Rule Induction;[];Iterative Proker Hand Prediction;Python notebook;720.0;4;;
2017-10-31 03:14:48;Based on olivier's script;Apache 2.0;https://www.kaggle.com/aharless/xgboost-cv-lb-284;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'rl', 'cv', 'ml'];['test data', 'train', 'fitting', 'model', 'validation data', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.794;0.565;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;['xgboost, gradient boosting'];XGBoost CV (LB .284);Python notebook;37231.0;257;;
2017-11-26 18:24:46;;Apache 2.0;https://www.kaggle.com/andrewmvd/lightgbm-in-r;1.0;['lightgbm'];['ai', 'dl', 'gbm', 'cv', 'rl', 'ml'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.805;0.517;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;['classification, gradient boosting'];LightGBM in R;Rmarkdown script;53377.0;127;;
2018-09-08 03:54:58;;Apache 2.0;https://www.kaggle.com/aquatic/entity-embedding-neural-net;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ann'];['train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.792;0.542;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;['gpu, neural networks'];Entity Embedding Neural Net;Python script;34751.0;182;;
2017-10-10 21:07:36;"Gini Coefficient - An Intuitive ExplanationI was struggling a bit with the definition of the Scoring Metric. Googling ""Gini Coefficient"" gives you mostly economic explanations. Here is a descriptive explanation with regard to the challenge. First, let's define our predictions and their actual values:";Apache 2.0;https://www.kaggle.com/batzner/gini-coefficient-an-intuitive-explanation;0.5;[];['ai', 'nn', 'ml'];['loss', 'label', 'predict'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.79;0.565;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;[];Gini Coefficient - An Intuitive Explanation;Python notebook;33436.0;257;;
2017-10-29 15:08:35;Introduction;Apache 2.0;https://www.kaggle.com/bertcarremans/data-preparation-exploration;1.0;['sklearn'];['ner', 'ai', 'dl', 'ml', 'nn', 'ann'];['machine learning', 'training data', 'regression', 'test data', 'train', 'model', 'label', 'random forest'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.797;0.587;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;['data visualization, data cleaning'];Data Preparation & Exploration;Python notebook;41464.0;362;;
2020-02-06 05:34:21;;Apache 2.0;https://www.kaggle.com/headsortails/steering-wheel-of-fortune-porto-seguro-eda;1.0;['pattern', 'xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'fitting', 'model', 'understanding', 'validation data', 'layer', 'loss', 'label', 'gradient boosting', 'predict', 'rank', 'decision tree', 'recommend'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.81;0.627;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;['beginner, data visualization, exploratory data analysis, +1 morefeature engineering'];Steering Wheel of Fortune - Porto Seguro EDA;Rmarkdown script;63733.0;712;;
2017-10-23 20:05:32;"This notebook displays the results of a search for noisy features. This search has been carried out using Light GBM in RandomForest mode (to avoid the hassle of how many rounds fo I need to run ? ) The file noisy_feature_check_results.csv contains the average importances of each feature and their corresponding shadows over 30 runs. Standard deviation of the importances is also available. Shadows are simply shuffled copies of real features. Comparing features to their shadows is an easy way to assess their genuine forecasting power. This is extensively used in Boruta packages (python and R) For information, Python Boruta packages has selected the following features (the rest is considered noise !!!):  ps_ind_01 ps_ind_03 ps_ind_05_cat ps_ind_07_bin ps_ind_15 ps_ind_16_bin ps_reg_01 ps_reg_02 ps_reg_03 ps_car_01_cat ps_car_03_cat ps_car_07_cat ps_car_12 ps_car_13 ps_car_14 ps_car_15  The classifier used for the task is a LGBMClassifier with the following parameters:  boosting_type=""rf"", num_leaves=1024, max_depth=6, n_estimators=500, subsample=.623, colsample_bytree=.5  Now let's review some results";Apache 2.0;https://www.kaggle.com/ogrellier/noise-analysis-of-porto-seguro-s-features;0.5;[];['ai', 'nn', 'gbm'];['predict'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.748;0.506;2020-12-13 15:44:39;multiple data sources;[];Noise analysis of Porto Seguro's features ;Python notebook;9892.0;109;;
2017-10-27 20:06:06;;Apache 2.0;https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ml'];['test data', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'rank', 'recommend', 'classification'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.772;0.527;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;[];XGB classifier, upsampling LB 0.283;Python script;18978.0;147;0.28594;0.28192
2017-11-30 08:40:28;;Apache 2.0;https://www.kaggle.com/vpaslay/lb-0-287-porto-seguro-mix;0.5;[];['nlp', 'ai', 'nn', 'ner'];['training data', 'test data', 'train', 'deep learning', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.763;0.53;2020-12-13 15:44:39;multiple data sources;['classification, ensembling'];LB 0.287 - Porto Seguro Mix;Python script;14768.0;152;0.28986;0.28725
2017-11-30 06:08:46;;Apache 2.0;https://www.kaggle.com/xiaozhouwang/2nd-place-lightgbm-solution;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.773;0.516;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;[];2nd Place Lightgbm Solution;Python script;19706.0;126;;
2017-11-30 06:29:31;;Apache 2.0;https://www.kaggle.com/xiaozhouwang/2nd-place-solution-nn-model;1.0;['tensorflow', 'xgboost', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['unsupervised learning', 'train', 'model', 'epoch', 'deep learning', 'layer', 'supervised learning', 'loss', 'label', 'relu', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.756;0.513;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;[];2nd place solution NN model;Python script;12013.0;120;;
2017-10-27 00:07:15;;Apache 2.0;https://www.kaggle.com/yekenot/simple-stacker-lb-0-284;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.773;0.539;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;['beginner, ensembling'];Simple Stacker LB 0.284;Python script;19475.0;174;0.28960;0.28416
2017-11-27 02:14:59;;Apache 2.0;https://www.kaggle.com/yifanxie/porto-seguro-tutorial-end-to-end-ensemble;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nn', 'ann'];['machine learning', 'training data', 'regression', 'generation', 'train', 'test data', 'model', 'label', 'logistic regression', 'predict', 'rank', 'random forest', 'naive bayes'];https://www.kaggle.com/c/porto-seguro-safe-driver-prediction;0.731;0.507;2020-12-13 15:44:39;Porto Seguroâ€™s Safe Driver Prediction;[];Porto Seguro Tutorial: end-to-end ensemble;Python notebook;6328.0;111;;
2015-05-12 06:16:08;;Apache 2.0;https://www.kaggle.com/abhishek/vote-me-up;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['random forest', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.766;0.507;2020-12-13 15:54:56;West Nile Virus Prediction;[];"Beating the Benchmark ;) (0.71+)";Python script;15999.0;110;0.67110;0.71560
2015-04-23 06:22:33;;Apache 2.0;https://www.kaggle.com/binghsu/xgboost-starter-code-python-0-69;1.0;['xgboost'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['training data', 'test data', 'regression', 'train', 'deep learning', 'loss', 'label', 'logistic regression', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.783;0.367;2020-12-13 15:54:56;West Nile Virus Prediction;[];XGBoost Starter Code (python), 0.69;Python script;26887.0;20;0.67948;0.69874
2015-05-20 15:05:29;;Apache 2.0;https://www.kaggle.com/bitsofbits/simple-lasagne-nn;1.0;['sklearn', 'theano'];['nlp', 'ai', 'nn', 'ner'];['test data', 'regression', 'train', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.753;0.397;2020-12-13 15:54:56;West Nile Virus Prediction;[];Simple Lasagne NN;Python script;11085.0;28;0.70176;0.73110
2015-04-22 18:45:29;;Apache 2.0;https://www.kaggle.com/dchudz/map-of-mosquito-counts-on-one-day;0.5;[];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.756;0.357;2020-12-13 15:54:56;West Nile Virus Prediction;[];Counts Distribution and Map On One Day;R script;12111.0;18;;
2015-04-22 18:11:53;;Apache 2.0;https://www.kaggle.com/dchudz/when-are-there-records-at-each-site;0.5;[];['nlp', 'ai', 'nn', 'ner'];['filter', 'training data', 'test data', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.744;0.319;2020-12-13 15:54:56;West Nile Virus Prediction;[];When are there records at each site?;R script;8744.0;12;;
2015-05-19 21:11:43;;Apache 2.0;https://www.kaggle.com/domcastro/check-your-validation-30-70-split;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'validation data', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.695;0.319;2020-12-13 15:54:56;West Nile Virus Prediction;[];Check your validation 30% 70% split;R script;2758.0;12;;
2015-06-17 09:32:45;;Apache 2.0;https://www.kaggle.com/duttaroy/enhanced;1.0;['sklearn', 'theano'];['ner', 'ai', 'nlu', 'dl', 'nlp', 'nn'];['filter', 'test data', 'regression', 'train', 'fitting', 'model', 'input layer', 'output layer', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'rank', 'classification', 'hidden layer'];https://www.kaggle.com/c/predict-west-nile-virus;0.753;0.379;2020-12-13 15:54:56;West Nile Virus Prediction;[];enhanced;Python script;11257.0;23;0.77256;0.78915
2016-01-02 19:21:06;;Apache 2.0;https://www.kaggle.com/fchollet/keras-deep-net-starter-code;1.0;['keras', 'sklearn', 'theano'];['ner', 'ai', 'cv', 'nlp', 'nn'];['training data', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.764;0.421;2020-12-13 15:54:56;West Nile Virus Prediction;['deep learning'];Keras deep net starter code;Python script;15237.0;37;0.66534;0.68659
2015-05-05 13:19:44;;Apache 2.0;https://www.kaggle.com/hugomc/find-the-closest-weather-station;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/predict-west-nile-virus;0.718;0.375;2020-12-13 15:54:56;West Nile Virus Prediction;[];Find the closest weather station;R script;4611.0;22;;
2015-06-03 22:23:35;;Apache 2.0;https://www.kaggle.com/iezepov/painless-data-wrangling-with-dplyr;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/predict-west-nile-virus;0.723;0.327;2020-12-13 15:54:56;West Nile Virus Prediction;[];Painless data wrangling with dplyr;R script;5218.0;13;;
2020-09-23 19:57:58;;Apache 2.0;https://www.kaggle.com/irfanmansuri/west-nile-virus;1.0;['sklearn'];['ai', 'nn'];['train', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/predict-west-nile-virus;0.497;0.319;2020-12-13 15:54:56;West Nile Virus Prediction;[];West_Nile_Virus;Python notebook;97.0;12;;
2015-06-10 16:43:39;;Apache 2.0;https://www.kaggle.com/kotomord/simple-lasagne-nn;1.0;['sklearn', 'theano'];['ner', 'ai', 'dl', 'nlp', 'nn'];['filter', 'test data', 'regression', 'train', 'fitting', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.74;0.346;2020-12-13 15:54:56;West Nile Virus Prediction;[];Simple Lasagne NN;Python script;7986.0;16;0.76581;0.77944
2015-06-02 20:44:20;;Apache 2.0;https://www.kaggle.com/mennotaanman/motion;0.5;[];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.755;0.46;2020-12-13 15:54:56;West Nile Virus Prediction;[];Motion;R script;11676.0;60;;
2015-04-23 05:41:13;;Apache 2.0;https://www.kaggle.com/mlandry/h2o-starter;1.0;['h2o'];['ner', 'ai', 'cv', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.75;0.423;2020-12-13 15:54:56;West Nile Virus Prediction;[];Starter Logistic Regression in R;R script;10411.0;38;0.65085;0.65253
2015-05-03 03:54:24;;Apache 2.0;https://www.kaggle.com/neilsummers/west-nile-heatmap-by-year;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/predict-west-nile-virus;0.724;0.375;2020-12-13 15:54:56;West Nile Virus Prediction;[];West Nile heatmap by year;Python script;5386.0;22;;
2015-06-24 19:15:06;;Apache 2.0;https://www.kaggle.com/oconnoda/population-model;0.5;[];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.74;0.357;2020-12-13 15:54:56;West Nile Virus Prediction;[];Population Model;Rmarkdown script;7885.0;18;;
2015-06-05 14:49:11;;Apache 2.0;https://www.kaggle.com/oconnoda/trap-locations-with-mosquito-count;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/predict-west-nile-virus;0.737;0.39;2020-12-13 15:54:56;West Nile Virus Prediction;[];Bubble Plot of Trap Activity;R script;7345.0;26;;
2015-05-01 05:20:14;;Apache 2.0;https://www.kaggle.com/thenokondi/facet-map-by-year-and-virus-status;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/predict-west-nile-virus;0.675;0.319;2020-12-13 15:54:56;West Nile Virus Prediction;[];Facet map by year and virus status;R script;1802.0;12;;
2015-06-12 22:26:12;;Apache 2.0;https://www.kaggle.com/triskelion/random-late-benchmark;1.0;['keras', 'sklearn', 'theano'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'layer', 'predict', 'classification'];https://www.kaggle.com/c/predict-west-nile-virus;0.665;0.346;2020-12-13 15:54:56;West Nile Virus Prediction;[];Random late benchmark;Python script;1474.0;16;0.49158;0.50876
2015-04-26 16:34:25;;Apache 2.0;https://www.kaggle.com/vascovv/west-nile-heatmap;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/predict-west-nile-virus;0.752;0.446;2020-12-13 15:54:56;West Nile Virus Prediction;[];West Nile heatmap;Python script;11005.0;50;;
2020-04-27 06:46:10;reading train and test dataset;Apache 2.0;https://www.kaggle.com/mohitsital/random-forest-hyperparameter-tuning;1.0;['sklearn'];['ai', 'nn', 'cv'];['test data', 'train', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/predict-who-is-more-influential-in-a-social-network;0.661;0.099;2020-12-13 15:55:08;Influencers in Social Networks;[];learn4: Random Forest Hyperparameter tuning ;Python notebook;1357.0;1;;
2016-08-27 00:22:15;;Apache 2.0;https://www.kaggle.com/abriosi/raddar-0-98-xgboost-sparse-matrix-python;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ml'];['machine learning', 'training data', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.773;0.498;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];raddar 0.98 xgboost sparse matrix python;Python script;19693.0;98;;
2016-08-01 23:21:41;;Apache 2.0;https://www.kaggle.com/anokas/beat-the-benchmark-0-54940;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.694;0.34;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];Beat the Benchmark! (0.54940);Python script;2655.0;15;;
2016-08-05 18:48:26;Exploration of the date features;Apache 2.0;https://www.kaggle.com/anokas/time-travel-eda;1.0;['pattern', 'sklearn'];['ai'];['train', 'model', 'predict'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.778;0.547;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];Time Travel (EDA);Python notebook;22870.0;194;;
2016-08-03 22:40:14;;Apache 2.0;https://www.kaggle.com/apapiu/redhat-eda;0.5;[];['ner', 'ai', 'dl', 'cv'];['filter', 'train', 'model', 'layer', 'predict', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.764;0.502;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];Redhat EDA;Rmarkdown script;15251.0;104;;
2016-08-07 08:44:30;;Apache 2.0;https://www.kaggle.com/cartographic/featurehashing-essentials;0.5;[];['ai'];['train', 'test data', 'model', 'training data'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.705;0.379;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];FeatureHashing essentials;Rmarkdown script;3428.0;23;;
2016-08-06 17:11:28;;Apache 2.0;https://www.kaggle.com/cartographic/r-starter-around-0-98-auc;1.0;['xgboost'];['ai'];['training data', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.745;0.435;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];R starter (around 0.98 auc);Rmarkdown script;9064.0;44;0.97858;0.97872
2016-08-17 09:16:18;;Apache 2.0;https://www.kaggle.com/dmi3kno/redhat-hack-in-plain-english-eda;1.0;['pattern', 'xgboost'];['ai', 'dl', 'gan', 'rl', 'ml'];['test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.753;0.494;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];RedHat Hack in plain English (EDA);Rmarkdown script;11337.0;93;;
2016-08-17 03:14:19;;Apache 2.0;https://www.kaggle.com/ijkilchenko/python-ver-of-group-1-and-date-trick;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ann'];['filter', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.742;0.449;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];Python ver. of group_1 and date trick;Python script;8338.0;52;0.98708;0.98702
2016-08-22 12:06:34;;Apache 2.0;https://www.kaggle.com/laurae2/raddar-goes-ham-with-leak;1.0;['tensorflow', 'xgboost'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['linear regression', 'filter', 'test data', 'regression', 'neuron', 'train', 'random forest', 'model', 'neural network', 'epoch', 'deep learning', 'gradient descent', 'loss', 'label', 'gradient boosting', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.746;0.449;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];raddar goes ham with leak;R script;9298.0;52;0.99072;0.99064
2016-08-11 12:38:24;;Apache 2.0;https://www.kaggle.com/loisso/lb-0-987-group-1-and-date-trick;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.776;0.526;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];LB 0.987, group_1 and date trick;R script;21243.0;144;;
2016-08-02 22:16:02;;Apache 2.0;https://www.kaggle.com/maternaj/random-forest-by-h2o-0-96-on-lb;1.0;['xgboost', 'h2o'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn', 'ml'];['machine learning', 'test data', 'random forest', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.712;0.327;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];Random Forest by H2O (~0.96 on LB);R script;4060.0;13;;
2016-08-31 17:12:49;;Apache 2.0;https://www.kaggle.com/ozagordi/exploit-leak-with-dplyr;0.5;[];['ner', 'ai', 'dl', 'rl'];['training data', 'filter', 'test data', 'train', 'label', 'predict'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.697;0.362;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];Exploit leak with dplyr;Rmarkdown script;2851.0;19;0.97566;0.97572
2016-09-02 05:25:22;;Apache 2.0;https://www.kaggle.com/qqgeogor/keras-benchmark;1.0;['xgboost', 'sklearn', 'keras', 'theano'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ml'];['gru', 'regression', 'train', 'fitting', 'model', 'input layer', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'understanding', 'classification', 'hidden layer'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.74;0.416;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];keras benchmark;Python script;7933.0;35;0.97928;0.97973
2016-08-04 15:44:40;;Apache 2.0;https://www.kaggle.com/raddar/0-98-xgboost-on-sparse-matrix;1.0;['pattern', 'xgboost', 'sklearn'];['ner', 'ai', 'gan', 'cv', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification', 'bayesian'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.768;0.469;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];0.98 xgboost on sparse matrix;R script;17173.0;67;;
2016-08-06 18:45:35;;Apache 2.0;https://www.kaggle.com/scirpus/lr-0-978744;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gan', 'cv', 'rl', 'nlp', 'nn', 'ann'];['regression', 'train', 'model', 'deep learning', 'layer', 'label', 'logistic regression', 'predict', 'linear regression', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.739;0.4;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];LR ~ 0.978744;Python script;7818.0;29;;
2016-08-21 12:00:08;;Apache 2.0;https://www.kaggle.com/yassinealouini/features-processing;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['test data', 'train', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.715;0.346;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];Features processing;Python script;4290.0;16;;
2016-08-25 08:19:24;;Apache 2.0;https://www.kaggle.com/yassinealouini/hyperopt-the-xgboost-model;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ml'];['machine learning', 'test data', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.781;0.379;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];Hyperopt the Xgboost model;Python script;24627.0;23;;
2016-08-02 10:48:21;;Apache 2.0;https://www.kaggle.com/zfturbo/xredboost;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/predicting-red-hat-business-value;0.755;0.477;2020-12-13 16:02:15;Predicting Red Hat Business Value;[];XRedBoost;Python script;11882.0;74;;
2020-04-25 14:51:19;;Apache 2.0;https://www.kaggle.com/tarunpaparaju/panda-challenge-resnet-multitask-8-fold-on-tpu;1.0;['sklearn', 'pillow', 'pytorch', 'albumentations', 'tensorflow', 'spacy', 'keras', 'skimage'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['machine learning', 'train', 'model', 'epoch', 'loss', 'label', 'predict', 'relu', 'resnet', 'propagation'];https://www.kaggle.com/c/prostate-cancer-grade-assessment;0.706;0.467;2020-12-13 16:03:25;multiple data sources;['tpu'];PANDA Challenge: ResNet multitask 8-fold on TPU ðŸ”¥;Python notebook;3512.0;65;;
2020-04-22 17:07:47;Dataset available here: https://www.kaggle.com/xhlulu/panda-resized-train-data-512x512;Apache 2.0;https://www.kaggle.com/xhlulu/panda-resize-and-save-train-data;1.0;['skimage'];['ai', 'cv'];['train', 'label'];https://www.kaggle.com/c/prostate-cancer-grade-assessment;0.716;0.488;2020-12-13 16:03:25;Prostate cANcer graDe Assessment (PANDA) Challenge;[];PANDA: Resize and Save Train Data;Python notebook;4443.0;86;;
2020-04-22 22:10:27;About this notebook;Apache 2.0;https://www.kaggle.com/yasufuminakama/panda-se-resnext50-classification-baseline;1.0;['caffe', 'sklearn', 'pytorch', 'albumentations', 'skimage'];['ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'loss', 'relu', 'resnet', 'classification'];https://www.kaggle.com/c/prostate-cancer-grade-assessment;0.72;0.488;2020-12-13 16:03:25;multiple data sources;['gpu'];PANDA / se_resnext50 classification baseline;Python notebook;4811.0;86;0.62958;0.59546
2020-05-05 19:02:31;About this notebook;Apache 2.0;https://www.kaggle.com/yasufuminakama/panda-se-resnext50-regression-baseline;1.0;['caffe', 'sklearn', 'pytorch', 'albumentations', 'skimage'];['ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/prostate-cancer-grade-assessment;0.727;0.49;2020-12-13 16:03:25;multiple data sources;['gpu'];PANDA / se_resnext50 regression baseline;Python notebook;5686.0;88;0.78836;0.77284
2015-11-25 16:23:46;;Apache 2.0;https://www.kaggle.com/abhilashawasthi/xgboost-example-0-61171;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['linear regression', 'test data', 'gru', 'regression', 'train', 'deep learning', 'loss', 'label', 'predict', 'rank', 'understanding', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.746;0.268;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];XGBoost Example (0.62656 LB Score);R script;9346.0;7;0.62876;0.62655
2015-11-30 03:59:08;;Apache 2.0;https://www.kaggle.com/benhamner/xgboost-example-1;1.0;['xgboost'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn'];['test data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.763;0.34;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];XGBoost Example (0.61249);R script;14927.0;15;0.62236;0.61249
2015-12-17 16:13:29;;Apache 2.0;https://www.kaggle.com/bobcz3/journey-through-prudential;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.693;0.281;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Journey Through Prudential;Python notebook;2648.0;8;;
2016-01-29 23:05:46;;Apache 2.0;https://www.kaggle.com/casalicchio/use-the-mlr-package-scores-0-649;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['machine learning', 'training data', 'regression', 'test data', 'train', 'random forest', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.795;0.469;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Use the mlr Package (scores 0.649);R script;38933.0;67;;
2015-12-21 16:27:39;;Apache 2.0;https://www.kaggle.com/chechir/features-predictive-power;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'linear regression', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.746;0.34;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Features predictibility;Rmarkdown script;9347.0;15;;
2016-01-10 02:40:16;;Apache 2.0;https://www.kaggle.com/inspector/keras-hyperopt-example-sketch;1.0;['keras', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['regression', 'train', 'model', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.762;0.319;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Keras Hyperopt Example Sketch;Python script;14432.0;12;;
2016-01-25 02:42:21;;Apache 2.0;https://www.kaggle.com/inversion/digitize;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.733;0.375;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Digitize;Python script;6695.0;22;;
2015-12-15 15:22:12;;Apache 2.0;https://www.kaggle.com/jonathanslomka/python-xgboost-starter;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.76;0.268;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Python XGBoost Starter;Python script;13666.0;7;0.63759;0.63624
2017-11-28 11:12:06;Import Libraries;Apache 2.0;https://www.kaggle.com/kkondo/ridge-regression-score-0-55443;1.0;['sklearn'];['ner', 'ai', 'dl', 'nn', 'ann'];['test data', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.693;0.302;2020-12-13 16:12:52;multiple data sources;[];Ridge Regression (Score: 0.55443);Python notebook;2620.0;10;;
2016-02-02 13:24:35;;Apache 2.0;https://www.kaggle.com/mariopasquato/linear-model;1.0;['pattern'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['linear regression', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'recommend', 'classification', 'bayesian'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.757;0.427;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Linear model 0.65 in lb;R script;12447.0;40;0.65394;0.65002
2015-11-30 11:35:33;;Apache 2.0;https://www.kaggle.com/omarelgabry/prudential-insurance-risk-predictions;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'random forest', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.74;0.311;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Prudential Insurance Risk Predictions;Python notebook;7923.0;11;0.55984;0.55394
2016-02-15 17:14:20;;Apache 2.0;https://www.kaggle.com/pchitta/caret-cv;1.0;['xgboost'];['ner', 'ai', 'cv', 'nlp', 'nn'];['regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.765;0.34;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];caret_cv;R script;15686.0;15;0.64952;0.65345
2015-11-23 16:14:55;;Apache 2.0;https://www.kaggle.com/pruadmin/starter-script;0.5;[];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ann'];['training data', 'test data', 'train', 'fitting', 'model', 'deep learning', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.765;0.431;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Starter Script;R script;15672.0;42;0.36361;0.36309
2016-01-21 13:00:29;;Apache 2.0;https://www.kaggle.com/scirpus/genetic-programming-lb-0-64;1.0;['xgboost', 'sklearn'];['nlp', 'ai', 'nn', 'ner'];['generation', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.727;0.334;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Genetic Programming LB ~ 0.64;Python script;5770.0;14;0.64544;0.64013
2015-12-15 04:51:37;;Apache 2.0;https://www.kaggle.com/tdevries/neural-network-example;1.0;['h2o', 'xgboost', 'sklearn', 'keras'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['test data', 'regression', 'neuron', 'train', 'model', 'input layer', 'neural network', 'epoch', 'deep learning', 'layer', 'gradient descent', 'loss', 'label', 'validation data', 'predict', 'relu', 'classification', 'hidden layer'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.762;0.375;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Neural Network Example;Python script;14373.0;22;0.58704;0.57546
2015-12-01 15:46:59;;Apache 2.0;https://www.kaggle.com/threecourse/plotting-histograms;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.729;0.292;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];plotting histograms;Python script;6028.0;9;;
2015-12-02 10:23:52;;Apache 2.0;https://www.kaggle.com/wittmaan/exploring-the-data;0.5;[];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['gru', 'test data', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.769;0.429;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];Exploring the Data;Rmarkdown script;17467.0;41;;
2016-02-07 02:49:47;;Apache 2.0;https://www.kaggle.com/zeroblue/xgboost-with-optimized-offsets;1.0;['xgboost'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['regression', 'train', 'fitting', 'model', 'understanding', 'deep learning', 'label', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/prudential-life-insurance-assessment;0.792;0.453;2020-12-13 16:12:52;Prudential Life Insurance Assessment;[];XGBoost with optimized offsets;Python script;35313.0;55;0.66652;0.66285
2018-10-13 11:33:40;;Apache 2.0;https://www.kaggle.com/anycode/simple-nn-baseline;1.0;['keras', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ml'];['regression', 'train', 'fitting', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.732;0.447;2020-12-13 16:17:21;PUBG Finish Placement Prediction (Kernels Only);[];Simple NN Baseline;Python script;6528.0;51;0.00000;0.00000
2018-10-22 14:23:55;;Apache 2.0;https://www.kaggle.com/anycode/simple-nn-baseline-3;1.0;['keras', 'sklearn'];['ner', 'ai', 'nlu', 'rl', 'nlp', 'nn', 'ml'];['activation function', 'training data', 'train', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.716;0.433;2020-12-13 16:17:21;PUBG Finish Placement Prediction (Kernels Only);[];Simple NN Baseline 3;Python script;4446.0;43;0.02472;0.02472
2018-11-28 08:16:06;;Apache 2.0;https://www.kaggle.com/anycode/simple-nn-baseline-4;1.0;['sklearn', 'mxnet', 'pytorch', 'tensorflow', 'keras'];['ner', 'ai', 'dl', 'gbm', 'ml', 'nlp', 'nn', 'ann'];['activation function', 'regression', 'train', 'recognition', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'relu', 'loss', 'predict', 'rank', 'understanding', 'resnet', 'classification', 'hidden layer'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.718;0.403;2020-12-13 16:17:21;PUBG Finish Placement Prediction (Kernels Only);[];Simple NN Baseline 4;Python script;4578.0;30;0.02459;0.02459
2018-12-20 14:52:35;Thanks for viewing my Kernel! If you like my work and find it useful, please leave an upvote! :) Player Unknown's BattleGrounds aka PUBG is an online multiplayer game where up to 100 players parachute onto an island where they scavange for weapons and equipment to kill others in a last man standing battle royale. Players can choose to enter the match solo, or with a small team of up to four people. In either case, the last person or team left alive wins the match. Key insights:  Highly correlated variables with winPlacePerc: walkDistance (0.8), killPlace (-0.7), boosts (0.6) and weaponsAcquired (0.6) 57% of players didn't have a single kill. 88% of players have only less than 3 kills. If a player gets 3 or more health boosting boosts and heals, the chances are high that he/she gets more than 0.5 as winPlacePerc Less than 5 players in a team is ideal for an average winPlacePerc of more than 0.5;Apache 2.0;https://www.kaggle.com/arunsankar/key-insights-from-pubg-data;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['test data', 'train', 'layer', 'label', 'predict', 'rank'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.728;0.418;2020-12-13 16:17:21;multiple data sources;['data visualization, exploratory data analysis'];Key Insights from PUBG data;Python notebook;5943.0;36;;
2018-11-04 16:14:24;;Apache 2.0;https://www.kaggle.com/ceshine/a-simple-post-processing-trick-lb-0237-0204;1.0;['pytorch'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'layer', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.72;0.452;2020-12-13 16:17:21;multiple data sources;[];A Simple Post-Processing Trick (LB .0237 -> .0204);Python script;4848.0;54;0.02049;0.02049
2018-11-12 00:08:03;;Apache 2.0;https://www.kaggle.com/chocozzz/lightgbm-baseline;1.0;['lightgbm', 'sklearn'];['ai', 'rl', 'nn', 'gbm'];['filter', 'regression', 'train', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.739;0.464;2020-12-13 16:17:21;PUBG Finish Placement Prediction (Kernels Only);[];LightGBM Baseline;Python notebook;7770.0;63;0.02059;0.02059
2018-10-09 13:18:30;EDA for the popular battle royale game PUBG;Apache 2.0;https://www.kaggle.com/deffro/eda-is-fun;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['filter', 'train', 'layer', 'label', 'predict', 'rank'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.811;0.653;2020-12-13 16:17:21;PUBG Finish Placement Prediction (Kernels Only);['data visualization, exploratory data analysis, feature engineering'];EDA is Fun!;Python notebook;66647.0;1157;;
2019-01-30 06:10:48;;Apache 2.0;https://www.kaggle.com/jkraju/1st-place-solution;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'layer', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.74;0.413;2020-12-13 16:17:21;multiple data sources;[];DAGFeatures  + TopSort;R script;7992.0;34;0.01385;0.01385
2019-01-31 16:56:38;;Apache 2.0;https://www.kaggle.com/kamalchhirang/5th-place-solution-0-0184-score;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn', 'ml'];['machine learning', 'regression', 'train', 'model', 'deep learning', 'layer', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.71;0.421;2020-12-13 16:17:21;PUBG Finish Placement Prediction (Kernels Only);[];5th place Solution - 0.0184 Score;Python script;3869.0;37;0.01849;0.01849
2019-09-05 04:18:04;Load data;Apache 2.0;https://www.kaggle.com/plasticgrammer/pubg-finish-placement-prediction-playground;1.0;['tensorflow', 'lightgbm', 'sklearn'];['ai', 'nn', 'ann', 'gbm'];['filter', 'train', 'model', 'layer', 'label', 'predict', 'rank'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.79;0.544;2020-12-13 16:17:21;PUBG Finish Placement Prediction (Kernels Only);['gpu, exploratory data analysis'];PUBG Finish Placement Prediction: playground;Python notebook;33311.0;188;0.02029;0.02029
2018-10-09 15:24:46;;Apache 2.0;https://www.kaggle.com/slmf1995/exploring-pubg-match-statistics-rankings;1.0;['xgboost'];['ai', 'dl', 'rl'];['training data', 'test data', 'train', 'model', 'layer', 'label', 'predict', 'rank'];https://www.kaggle.com/c/pubg-finish-placement-prediction;0.718;0.405;2020-12-13 16:17:21;PUBG Finish Placement Prediction (Kernels Only);['beginner, exploratory data analysis, xgboost'];Exploring PUBG Match Statistics & Rankings;Rmarkdown script;4658.0;31;0.00000;0.00000
2018-11-29 08:05:04;;Apache 2.0;https://www.kaggle.com/amneves/quick-draw-keras-cnn-model;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ml'];['train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/quickdraw-doodle-recognition;0.744;0.408;2020-12-13 16:18:48;multiple data sources;['beginner, deep learning, classification'];Quick, Draw, Keras! CNN model;Python notebook;8888.0;32;0.76149;0.76138
2018-11-09 11:05:23;Motivation;Apache 2.0;https://www.kaggle.com/gaborfodor/how-to-draw-an-owl-lb-0-002;0.5;[];['ai'];['train', 'understanding', 'model', 'neural network'];https://www.kaggle.com/c/quickdraw-doodle-recognition;0.743;0.491;2020-12-13 16:18:48;Quick, Draw! Doodle Recognition Challenge;['data visualization, deep learning'];ðŸ¦‰ How to draw an owl? [LB=0.002];Python notebook;8592.0;89;;
2018-11-14 09:33:16;;Apache 2.0;https://www.kaggle.com/gaborfodor/shuffle-csvs;0.5;[];['ai', 'cv'];['train'];https://www.kaggle.com/c/quickdraw-doodle-recognition;0.74;0.527;2020-12-13 16:18:48;Quick, Draw! Doodle Recognition Challenge;[];Shuffle CSVs;Python notebook;7870.0;146;;
2018-10-24 19:53:18;Let's check the first 100 owls;Apache 2.0;https://www.kaggle.com/gaborfodor/un-recognized-drawings;0.5;[];['ai'];['train'];https://www.kaggle.com/c/quickdraw-doodle-recognition;0.679;0.397;2020-12-13 16:18:48;Quick, Draw! Doodle Recognition Challenge;['beginner, data visualization'];(Un)recognized drawings âœ… âŒ;Python notebook;1964.0;28;;
2018-09-29 16:23:13;Lets Play With Quick Draw!!;Apache 2.0;https://www.kaggle.com/harunshimanto/lets-play-with-quick-draw;1.0;['pattern', 'tensorflow', 'sklearn', 'keras'];['ai', 'dl', 'cnn', 'cv', 'rl', 'ml', 'nn', 'rnn', 'ann'];['k-nearest neighbor', 'predict', 'relu', 'machine learning', 'neuron', 'train', 'classification', 'labeled', 'model', 'neural network', 'layer', 'loss', 'hidden layer', 'test data', 'fitting', 'output layer', 'label', 'random forest', 'convolutional neural network'];https://www.kaggle.com/c/quickdraw-doodle-recognition;0.739;0.479;2020-12-13 16:18:48;Quick, Draw! Doodle Recognition Challenge;['deep learning, classification, feature engineering'];Lets Play With Quick Draw!! ðŸŽ®;Python notebook;7713.0;76;0.00000;0.00000
2018-11-09 07:18:09;Bidirectional LSTM;Apache 2.0;https://www.kaggle.com/huyenvyvy/bidirectional-lstm-using-data-generator-lb-0-825;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'nn', 'rnn', 'ml'];['filter', 'train', 'recognition', 'model', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/quickdraw-doodle-recognition;0.711;0.416;2020-12-13 16:18:48;multiple data sources;['gpu'];Bidirectional LSTM Using Data Generator LB 0.825;Python notebook;3970.0;35;;
2018-09-26 19:47:25;This Notebook Demonstrates: Reading the data in python, preparing it for analysis, and adjusting the labels to contain underscores The code that simplfies a Raw drawing to the Simplified drawing How to make a submission file with predictions in the required format;Apache 2.0;https://www.kaggle.com/inversion/getting-started-viewing-quick-draw-doodles-etc;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['training data', 'filter', 'train', 'label', 'predict'];https://www.kaggle.com/c/quickdraw-doodle-recognition;0.735;0.485;2020-12-13 16:18:48;Quick, Draw! Doodle Recognition Challenge;[];Getting Started: Viewing Quick, Draw! Doodles, etc;Python notebook;7019.0;82;;
2018-11-27 01:45:06;;Apache 2.0;https://www.kaggle.com/paulorzp/ensemble-weighted-voting;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['training data', 'test data', 'train', 'recognition', 'model', 'deep learning', 'layer', 'label', 'predict', 'classification'];https://www.kaggle.com/c/quickdraw-doodle-recognition;0.728;0.459;2020-12-13 16:18:48;multiple data sources;['ensembling'];Ensemble Weighted Voting;Python script;5889.0;59;0.90437;0.90536
2018-10-13 14:24:52;Find outliers with unusual entropyInspired by the pre-processing step from the paper http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2763.pdf Noise Removal with Image Entropy Key ingredient to a successful sketch center loss is the guarantee of non-noisy data (outliers), as it will significantly affect the class feature centers. However, sketch data collected with crowdsourcing are inevitable to noise, where we propose a noisy data removal technique to alleviate such issue by resorting to image entropy. Given a category of sketch, we can get entropy for each sketch and the overall entropy distribution on a category basis. We empirically find that keeping the middle 90% of each category as normal samples gives us best results. In Figure 4, we visualize the entropy histogram of star samples in our training set. If we choose the middle 90% samples as normal samples for star category, we can calculate and get the 0.05 and 0.95 percentiles of star images entropy as 0.1051 and 0.1721, respectively. We then treat the remaining samples as outliers or noise points (entropy âˆˆ [0, 0.1051)   (0.1721, 1]). It can be observed that low entropy sketches tend to be overly-abstract, yet high entropy ones being messy, sometimes with meaningless scribbles. Nevertheless, sketch data falling into middle entropy range present more consistent and reasonable drawings.;Apache 2.0;https://www.kaggle.com/sorokin/sketch-entropy;0.5;[];['ai', 'dl', 'cv', 'rl', 'nn', 'ann'];['train', 'label', 'loss'];https://www.kaggle.com/c/quickdraw-doodle-recognition;0.69;0.413;2020-12-13 16:18:48;Quick, Draw! Doodle Recognition Challenge;[];Sketch Entropy;Python notebook;2485.0;34;;
2018-12-30 11:05:58;;Apache 2.0;https://www.kaggle.com/hung96ad/pytorch-starter;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'ann', 'cv'];['gru', 'predict', 'train', 'model', 'epoch', 'lstm', 'loss', 'relu'];https://www.kaggle.com/c/quora-insincere-questions-classification;0.775;0.563;2020-12-13 16:20:42;Quora Insincere Questions Classification;['gpu'];PyTorch starter;Python notebook;20995.0;247;0.69233;0.69233
2018-11-06 22:57:02;Setup;Apache 2.0;https://www.kaggle.com/mihaskalic/lstm-is-all-you-need-well-maybe-embeddings-also;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nn', 'ml'];['predict', 'train', 'model', 'epoch', 'layer', 'lstm', 'loss'];https://www.kaggle.com/c/quora-insincere-questions-classification;0.768;0.572;2020-12-13 16:20:42;Quora Insincere Questions Classification;['gpu'];LSTM is all you need! well, maybe embeddings also.;Python notebook;17116.0;286;0.64251;0.64251
2019-02-05 12:23:28;;Apache 2.0;https://www.kaggle.com/wowfattie/3rd-place;1.0;['nltk', 'gensim', 'tensorflow', 'spacy', 'vocabulary', 'keras'];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['gru', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'lstm', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/quora-insincere-questions-classification;0.8;0.576;2020-12-13 16:20:42;Quora Insincere Questions Classification;['gpu'];kernel49facda48a;Python script;45341.0;302;0.71157;0.70599
2017-05-26 16:29:39;;Apache 2.0;https://www.kaggle.com/act444/lb-0-158-xgb-handcrafted-leaky;1.0;['xgboost', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/quora-question-pairs;0.725;0.476;2020-12-13 16:22:58;Quora Question Pairs;[];[LB 0.158] XGB_handcrafted_leaky;Python script;5468.0;73;;
2017-04-18 23:32:57;Duplicates of DuplicatesIn this notebook, we will take a look at connected groups of questions that are marked as duplicates of each other. What we will find is that, at least in the training set, duplicates tend to occur in clusters of related questions.;Apache 2.0;https://www.kaggle.com/davidthaler/duplicates-of-duplicates;0.5;[];['dl', 'ai', 'nn', 'rl'];['train', 'label', 'filter'];https://www.kaggle.com/c/quora-question-pairs;0.709;0.458;2020-12-13 16:22:58;Quora Question Pairs;[];Duplicates of Duplicates;Python notebook;3751.0;58;;
2017-05-03 18:27:25;Adding the following meta features allowed my model to go from 0,252 to 0.217. I suspect that more gain can be squeezed from similar features as well, I just found this last night. The magic features are based on question frequency.;Apache 2.0;https://www.kaggle.com/jturkewitz/magic-features-0-03-gain;0.5;[];['ai'];['train', 'model'];https://www.kaggle.com/c/quora-question-pairs;0.754;0.531;2020-12-13 16:22:58;Quora Question Pairs;[];Magic Features (0.03 gain);Python notebook;11564.0;155;;
2017-05-28 06:57:44;;Apache 2.0;https://www.kaggle.com/lystdo/lb-0-18-lstm-with-glove-and-magic-features;1.0;['tensorflow', 'sklearn', 'keras', 'nltk'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/quora-question-pairs;0.758;0.478;2020-12-13 16:22:58;Quora Question Pairs;[];[ LB 0.18+ ] LSTM with GloVe and magic features;Python script;12675.0;75;;
2017-04-28 13:17:04;;Apache 2.0;https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings;1.0;['nltk', 'gensim', 'theano', 'tensorflow', 'keras'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'label', 'lstm', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/quora-question-pairs;0.821;0.569;2020-12-13 16:22:58;Quora Question Pairs;[];LSTM with word2vec embeddings;Python script;94079.0;273;;
2017-06-07 12:16:31;Below a discussion of team Waffle convolutions inc.'s implementation of the 1D convolutional model (place 45 on the leaderboard), show our biased comparison to LSTMs, quickly describe its performance and explain some of our 'design' choices. Comparison to LSTMA quick overview of its benefits with respect to the LSTM approach:  Doesn't overfit as much  Much shorter run times, 1 epoch ~ 130 sec on a 1050 GPU, as opposed to 400 sec epochs for a 250 unit LSTM  Intuitively much clearer what is happening (might just be me though)  Seems to perform better (with only magic features, 0.160 single model on lb)   Downsides of the 1D CNN:  Sentences which are 'equal' but have different sequence of words (eg. 'Is bacon the best thing since sliced bread?' or 'Since sliced bread is bacon the best thing?') flunk easily  No concept of word importance (such TFIDF, can be added with a dense layer though)  Our implementation cannot switch question1 with question2 to double the data   However in all fairness, we played around more with the 1D convolutional models than LSTMs, we simply couldn't get the latter to have similar performance. The actual model;Apache 2.0;https://www.kaggle.com/rethfro/1d-cnn-single-model-score-0-14-0-16-or-0-23;0.5;[];['ai', 'cnn', 'ml', 'nn', 'ann'];['filter', 'train', 'fitting', 'model', 'epoch', 'layer', 'lstm', 'loss', 'relu'];https://www.kaggle.com/c/quora-question-pairs;0.758;0.482;2020-12-13 16:22:58;Quora Question Pairs;['cnn'];1D CNN (single model score: 0.14, 0.16 or 0.23);Python notebook;12763.0;79;;
2017-05-21 02:51:40;Feature as described by Krzysztof Dziedzic;Apache 2.0;https://www.kaggle.com/tour1st/magic-feature-v2-0-045-gain;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/quora-question-pairs;0.722;0.487;2020-12-13 16:22:58;Quora Question Pairs;[];Magic feature v2(0.045 gain);Python notebook;5092.0;85;;
2015-05-18 18:43:26;;Apache 2.0;https://www.kaggle.com/benhamner/exploratory-plots-with-julia-and-gadfly;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/random-acts-of-pizza;0.644;0.099;2020-12-13 16:26:53;Random Acts of Pizza;[];Exploratory Plots With Julia and Gadfly;Julia script;974.0;1;;
2015-05-18 23:20:04;;Apache 2.0;https://www.kaggle.com/benhamner/rmarkdown-default-text;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'train', 'deep learning', 'label', 'classification', 'propagation'];https://www.kaggle.com/c/random-acts-of-pizza;0.657;0.152;2020-12-13 16:26:53;Random Acts of Pizza;[];Default RMarkdown Script;Rmarkdown script;1259.0;2;;
2015-05-18 23:25:58;;Apache 2.0;https://www.kaggle.com/benhamner/simple-julia-benchmark;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/random-acts-of-pizza;0.697;0.214;2020-12-13 16:26:53;Random Acts of Pizza;[];Simple Julia Benchmark;Julia script;2872.0;4;0.61632;0.61632
2015-05-18 23:09:03;;Apache 2.0;https://www.kaggle.com/benhamner/wordclouds-1;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'training data', 'train', 'deep learning', 'label', 'classification', 'propagation'];https://www.kaggle.com/c/random-acts-of-pizza;0.647;0.099;2020-12-13 16:26:53;Random Acts of Pizza;[];Wordclouds;Rmarkdown script;1041.0;1;;
2016-12-09 07:00:16;;Apache 2.0;https://www.kaggle.com/chqngh/random-pizza;1.0;['sklearn'];['ai', 'nn', 'cv'];['regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'classification', 'naive bayes'];https://www.kaggle.com/c/random-acts-of-pizza;0.639;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];Random Pizza;Python notebook;883.0;0;;
2019-03-06 19:04:51;Download the corpus from nltk;Apache 2.0;https://www.kaggle.com/dpamgautam/nlp-basic-text-data-preprocessing-1;1.0;['nltk'];['ner', 'ai', 'dl', 'nn', 'ann'];['recommend'];https://www.kaggle.com/c/random-acts-of-pizza;0.577;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];NLP : basic text data  preprocessing 1;Python notebook;310.0;0;;
2018-12-04 22:55:26;;Apache 2.0;https://www.kaggle.com/elias22/kerneld8b44bc755;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'label', 'loss', 'classification'];https://www.kaggle.com/c/random-acts-of-pizza;0.532;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];kerneld8b44bc755;Python script;158.0;0;;
2016-11-03 19:39:41;;Apache 2.0;https://www.kaggle.com/haroonc/notebook26e44b5093;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/random-acts-of-pizza;0.56;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];Notebook26e44b5093;R notebook;236.0;0;;
2016-11-03 20:35:52;;Apache 2.0;https://www.kaggle.com/haroonc/notebookaa32534f23;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/random-acts-of-pizza;0.564;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];Notebookef342232f2;R notebook;251.0;0;;
2016-12-21 11:09:57;;Apache 2.0;https://www.kaggle.com/heyheyivan/random-pizza;1.0;['sklearn'];['ai', 'nn', 'cv'];['regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'classification', 'naive bayes'];https://www.kaggle.com/c/random-acts-of-pizza;0.567;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];Random Pizza;Python notebook;264.0;0;;
2017-12-28 20:26:14;;Apache 2.0;https://www.kaggle.com/jatinraina/random-acts-of-pizza-xgboost;1.0;['xgboost'];['ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'test data', 'train', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/random-acts-of-pizza;0.671;0.236;2020-12-13 16:26:53;Random Acts of Pizza;['data visualization, xgboost'];Random acts of pizza xgboost;R notebook;1638.0;5;;
2018-05-08 19:33:52;;Apache 2.0;https://www.kaggle.com/jihyeseo/pizza-eda;1.0;['sklearn'];['ai'];['train'];https://www.kaggle.com/c/random-acts-of-pizza;0.579;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];pizza eda;Python notebook;319.0;0;;
2017-03-15 18:31:28;RaoF;Apache 2.0;https://www.kaggle.com/mbp14mtp/raopizza;1.0;['sklearn', 'nltk'];['ann', 'ai', 'nn', 'ml'];['train', 'model', 'predict'];https://www.kaggle.com/c/random-acts-of-pizza;0.603;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];RaoPizza;Python notebook;472.0;0;;
2017-04-12 15:33:34;;Apache 2.0;https://www.kaggle.com/rashmijrao/ml-project;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/random-acts-of-pizza;0.634;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];ML project;Python script;818.0;0;;
2017-03-28 08:59:36;;Apache 2.0;https://www.kaggle.com/tonybyte/notebook4a30ab2da1;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/random-acts-of-pizza;0.512;0.0;2020-12-13 16:26:53;Random Acts of Pizza;[];Notebook4a30ab2da1;R notebook;118.0;0;;
2019-05-29 06:19:33;;Apache 2.0;https://www.kaggle.com/adityajn105/getting-started-with-vggface-0-787-lb;1.0;['tensorflow', 'sklearn', 'keras', 'pillow'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nn', 'ml'];['train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.681;0.327;2020-12-13 16:27:41;Northeastern SMILE Lab - Recognizing Faces in the Wild;['gpu, beginner, data visualization'];Getting Started with VGGFace[0.79 LB];Python notebook;2041.0;13;0.785;0.787
2019-05-31 16:37:17;;Apache 2.0;https://www.kaggle.com/caseyworks/open-kimono-kinship-notebook-0-792lb;1.0;['caffe', 'xgboost', 'imutils', 'sklearn', 'pillow', 'tensorflow', 'keras', 'skimage'];['ner', 'ai', 'dl', 'cv', 'rl', 'nn', 'ml'];['training data', 'test data', 'train', 'recognition', 'model', 'natural language processing', 'layer', 'clustering', 'vgg', 'label', 'relu', 'predict', 'computer vision', 'classification', 'natural language'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.703;0.34;2020-12-13 16:27:41;multiple data sources;['gpu, beginner, feature engineering, +1 moredata cleaning'];Open Kimono Kinship Notebook [0.792LB];Python notebook;3247.0;15;;
2019-06-18 18:16:18;ObjectiveObjective of this kernel is to  Quickly get an idea of the dataset and the images Tricky coding that helps one to consummate all image info in a data frame. Render some insights well ahead of getting into technical stuff Exploration of the dataset and distribution using intuitive plots;Apache 2.0;https://www.kaggle.com/gowrishankarin/eda-with-plotly-smart-cute-and-pretty-people;0.5;[];['ai', 'nn'];['train', 'label', 'predict'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.649;0.302;2020-12-13 16:27:41;Northeastern SMILE Lab - Recognizing Faces in the Wild;['gpu'];EDA with Plotly-Smart, Cute and Pretty People;Python notebook;1076.0;10;;
2019-05-15 10:54:18;load train images;Apache 2.0;https://www.kaggle.com/hengzheng/simple-autoencoder-keras-and-kmean-cluster;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'nn', 'ann'];['autoencoder', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'k-means', 'predict', 'relu'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.69;0.346;2020-12-13 16:27:41;Northeastern SMILE Lab - Recognizing Faces in the Wild;['gpu'];Simple Autoencoder [Keras] and KMean Cluster ;Python notebook;2477.0;16;;
2019-05-15 03:36:07;;Apache 2.0;https://www.kaggle.com/hominlee/show-image;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.713;0.352;2020-12-13 16:27:41;Northeastern SMILE Lab - Recognizing Faces in the Wild;['gpu'];show_image;Python notebook;4120.0;17;;
2019-05-14 19:20:06;importing packages;Apache 2.0;https://www.kaggle.com/justlookinxd/starter-code;0.5;[];['ner', 'ai', 'nn'];['train', 'model'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.643;0.292;2020-12-13 16:27:41;multiple data sources;['gpu'];starter code;Python notebook;956.0;9;;
2019-06-26 18:20:45;;Apache 2.0;https://www.kaggle.com/nikhil1011/blood-relationship;1.0;['tensorflow', 'keras'];['ai', 'nn', 'ann'];['train', 'model', 'layer', 'vgg', 'predict', 'relu'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.592;0.292;2020-12-13 16:27:41;multiple data sources;['gpu'];Blood_Relationship;Python notebook;391.0;9;;
2019-07-28 07:35:25;https://www.tensorflow.org/beta/tutorials/generative/style_transfer;Apache 2.0;https://www.kaggle.com/rblcoder/faces-in-the-wild-compare-related-n-unrelated;1.0;['tensorflow'];['ner', 'ai', 'cv', 'ml', 'nn', 'ann'];['train'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.539;0.281;2020-12-13 16:27:41;Northeastern SMILE Lab - Recognizing Faces in the Wild;['gpu'];Faces in the Wild compare related n unrelated;Python notebook;173.0;8;;
2019-06-23 12:26:06;;Apache 2.0;https://www.kaggle.com/shivamsarawagi/wildimagedetection-0-875;1.0;['tensorflow', 'keras', 'pillow'];['ner', 'ai', 'cv', 'nn', 'ml'];['train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.686;0.302;2020-12-13 16:27:41;Northeastern SMILE Lab - Recognizing Faces in the Wild;['gpu, beginner'];WildImageDetection;Python notebook;2262.0;10;0.872;0.875
2019-07-31 09:22:51;1 Train;Apache 2.0;https://www.kaggle.com/tenffe/vggface-cv-focal-loss;1.0;['tensorflow', 'keras', 'pillow'];['ner', 'ai', 'cv', 'rl', 'nn', 'ml'];['train', 'model', 'epoch', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.654;0.292;2020-12-13 16:27:41;Northeastern SMILE Lab - Recognizing Faces in the Wild;['gpu'];VGGFace + CV + focal loss;Python notebook;1186.0;9;;
2019-06-19 08:03:12;;Apache 2.0;https://www.kaggle.com/thanatoz/lets-generate-more-faces-gan;1.0;['tensorflow', 'keras'];['ner', 'ai', 'dl', 'nn', 'ann'];['train', 'model', 'epoch', 'layer', 'loss', 'predict', 'relu', 'ground truth'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.674;0.334;2020-12-13 16:27:41;Northeastern SMILE Lab - Recognizing Faces in the Wild;['gpu, image data, gan'];Lets generate more faces [GAN];Python notebook;1746.0;14;;
2019-07-23 10:53:19;THIS KERNAL IS BLEND OF VGGs kernal used for blends are:-  https://www.kaggle.com/shivamsarawagi/wildimagedetection-0-875 https://www.kaggle.com/hsinwenchang/vggface-baseline-197x197 https://www.kaggle.com/arjunrao2000/kinship-detection-with-vgg16 https://www.kaggle.com/leonbora/kinship-recognition-transfer-learning-vggface https://www.kaggle.com/janpreets/just-another-feature-extractor-0-824-lb  I will be blending same model with diffrent scores (somewhat) rather then diffrent models blends.;Apache 2.0;https://www.kaggle.com/vaishvik25/blend-of-smiles;0.5;[];['ai', 'nn'];['vgg', 'recognition', 'model', 'label'];https://www.kaggle.com/c/recognizing-faces-in-the-wild;0.683;0.334;2020-12-13 16:27:41;multiple data sources;[];blend of  SMILEs;Python notebook;2119.0;14;0.902;0.899
2018-01-27 14:07:41;Introduction;Apache 2.0;https://www.kaggle.com/asindico/a-japanese-journey;1.0;['sklearn'];['ai', 'nn', 'ann', 'rl'];['training data', 'predict', 'train', 'label', 'clustering'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.746;0.49;2020-12-13 16:32:50;multiple data sources;[];A Japanese Journey;Python notebook;9295.0;88;;
2017-12-07 22:01:25;;Apache 2.0;https://www.kaggle.com/breakfastpirate/weeks-before-after-golden-week-2016;0.5;[];['ai', 'nn', 'ann'];['test data'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.675;0.427;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;[];Weeks Before/After Golden Week 2016;Rmarkdown script;1807.0;40;;
2017-12-10 04:04:55;;Apache 2.0;https://www.kaggle.com/captcalculator/a-very-extensive-recruit-exploratory-analysis;1.0;['pattern'];['ai', 'dl', 'ml'];['train', 'label', 'training data'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.738;0.486;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;[];A Very Extensive Recruit Exploratory Analysis;Rmarkdown script;7635.0;83;;
2017-12-06 04:14:32;;Apache 2.0;https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.725;0.435;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;[];Mean Mix - Math, Geo, Harmonic (LB 0.493);Python script;5507.0;44;0.53293;0.49373
2017-12-24 16:10:54;"Recruit Restaurant EDAFabien Daniel (December 2017)  This notebook aims at providing a first glance at the data provided for the ""Recruit Restaurant Visitor Forecasting"" challenge. The purpose of this competition is to predict the number of people that will make reservations during 5 weeks, from April 2017 to May 2017. Historical data for a period âˆ¼âˆ¼1 year prior to April 2017 is provided in order get some insights on customers' habits.  1. Load the data  2. Restaurant locations  3. Reservations against visits   3.1 A global view of all air restaurants 3.2 A spot check 3.2.1 case 1: the ideal case 3.2.2 case 2   3.3 Test set reservations   1. Load the dataFirst, I load all the packages that will be used throughout this notebook and set a few parameters related to display:";Apache 2.0;https://www.kaggle.com/fabiendaniel/recruit-restaurant-eda;0.5;[];['ai', 'nn', 'ml', 'rl'];['model', 'label', 'filter', 'predict'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.72;0.446;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;[];Recruit Restaurant EDA;Python notebook;4806.0;50;;
2020-09-26 22:39:48;;Apache 2.0;https://www.kaggle.com/headsortails/be-my-guest-recruit-restaurant-eda;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['filter', 'training data', 'regression', 'test data', 'train', 'fitting', 'model', 'validation data', 'layer', 'clustering', 'label', 'logistic regression', 'predict', 'linear regression', 'classification', 'labeled'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.818;0.645;2020-12-13 16:32:50;multiple data sources;['beginner, data visualization, exploratory data analysis, +1 moregeospatial analysis'];Be my guest - Recruit Restaurant EDA;Rmarkdown script;86894.0;1000;;
2018-01-17 22:48:24;IntroductionThank you all for your support and time. My goal with this kernel is to provide a broad first look at the weather stations. If you want a more exhaustive and detailed look at the data and how to use it, please check out my new kernel here: https://www.kaggle.com/huntermcgushion/exhaustive-weather-eda-file-overview  I would love to hear your thoughts on both this kernel and the one linked above. Thanks again for your time!;Apache 2.0;https://www.kaggle.com/huntermcgushion/weather-station-location-eda;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['filter'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.705;0.418;2020-12-13 16:32:50;multiple data sources;[];Weather Station Location EDA;Python notebook;3387.0;36;;
2018-07-15 21:20:55;;Apache 2.0;https://www.kaggle.com/ievgenvp/lstm-encoder-decoder-via-keras-lb-0-5;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nlp', 'nn', 'ml'];['autoencoder', 'training data', 'test data', 'train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'label', 'lstm', 'predict', 'relu', 'understanding', 'classification'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.771;0.421;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;['neural networks, lstm'];LSTM encoder-decoder via Keras (LB 0.5);Python script;18282.0;37;;
2018-02-07 05:08:40;;Apache 2.0;https://www.kaggle.com/plantsgo/solution-public-0-471-private-0-505;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.743;0.478;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;[];solution(public:0.471,private:0.505 );Python script;8486.0;75;;
2017-12-30 03:25:13;;Apache 2.0;https://www.kaggle.com/pranav84/surprise-me-h2o-automl-version;1.0;['xgboost', 'sklearn', 'h2o'];['ner', 'ai', 'automl', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ml'];['regression', 'train', 'model', 'validation data', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.747;0.46;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;['ensembling'];Surprise me! H2O autoML version;Python script;9416.0;60;0.52178;0.48057
2018-02-07 02:18:42;;Apache 2.0;https://www.kaggle.com/pureheart/1st-place-lgb-model-public-0-470-private-0-502;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.774;0.542;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;[];1st Place LGB Model(public:0.470, private:0.502);Python script;20261.0;181;;
2018-02-10 03:13:26;;Apache 2.0;https://www.kaggle.com/rsakata/16th-place-solution-simple-ver-private-0-509;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.731;0.459;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;[];16th place solution (simple ver.) [private: 0.509];R script;6273.0;59;0.50914;0.47248
2017-12-02 11:00:40;"Recruit Holdings : Forecasting VisitorsThis is an exploration of ARIMA  modeling techniques using R's built-in time series packages for the Recruit Restaurant Visitor Forecasting. The goal of this challenge is to forecast future visitor demand at various restaurants. The dataset fits on a simple computer and is ideal to play around with.  While the competition provides a number of structured datasets, for this exploration, we will only be using the following two datasets: 1. air_visit_data.csv : historical data for the air restaurants, this is the main dataset 2. air_store_info.csv : details about the restaurants such as location and genre  Time Series PrimerA time series is a structured dataset that can be annual, monthly, or in this case daily. To begin to understand the models, lets outline some key concepts about time-series.  Time series has to be stationary There are 3 conditions that must be fulfilled, otherwise a time series cannot be modeled  1. Constant Mean: AKA no overall trend   2. Constant Variance: the magnitude of waviness should remain constant   3. Frequency: the waves should be evenly spaced   Another consideration: Seasonailty <img src ='https://searchengineland.com/figz/wp-content/seloads/2012/09/High_Seasonality_Pic1.png' /> Seasonality can be on a weekly, monthly, quarterly, or annual basis.  Example:the sales of products around Christmas every year Example: the number of public transit riders during different days of the week.  Simple example: Airplane passengers Note above, the above time series has both a linear upward trend and a seasonal cycle. Luckily the upward trend and seasonality can be addressed with time series techniques  Forecasting in R: ARIMA Models The main technique being explored is ARIMA which stands for Auto Regressive Integrated Moving Average. Wikipedia defines ARIMA as: ""stands for Autoregressive Integrated Moving Average models. Univariate (single vector) ARIMA is a forecasting technique that projects the future values of a series based entirely on its own inertia. Its main application is in the area of short term forecasting requiring at least 40 historical data points."" The auto regressive part of the model determines the dependence on previous time points. The moving average part of the model determines the dependence on errors at different points";Apache 2.0;https://www.kaggle.com/timolee/feeling-hungry-a-beginner-s-guide-to-arima-models;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.745;0.459;2020-12-13 16:32:50;multiple data sources;[];Feeling Hungry? A Beginner's Guide to ARIMA models;R notebook;9067.0;59;;
2018-02-06 16:44:56;;Apache 2.0;https://www.kaggle.com/tunguz/surprise-me-2;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.768;0.514;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;[];Surprise Me 2!;Python script;17108.0;122;;
2017-11-30 03:22:59;;Apache 2.0;https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting;0.711;0.447;2020-12-13 16:32:50;Recruit Restaurant Visitor Forecasting;[];weighted mean comparisons, LB 0.497, 1ST;Python script;3911.0;51;0.53889;0.49733
2019-07-31 19:48:43;Set ParametersIn GCP, work is organized into projects. gcs_path is the path that images and the training csv will be uploaded to. AutoML Vision requires the images to be stored in a directory with the root directory being gs://{project_id}-vcm. train_filename is the name of the csv file that's uploaded to GCS. Doesn't matter what you choose here. gcp_service_account_json is the path to the service account key. Service accounts allow you to authenticate with GCP using a JSON key (rather than typing in a password). I uploaded my service account key as a private dataset. Read more about setting one up at https://cloud.google.com/iam/docs/understanding-service-accounts train_budget is the number of node hours. 1 is min. 24 is maximum. I believe AutoML Vision Classifaction costs $20 per node hour. dataset_name is the name of the dataset that's loaded into AutoML. Doesn't matter what you choose here. model_name is the name of the model in AutoML. Doesn't matter what you choose here. I use the convention of dataset_name underscore train_budget;Apache 2.0;https://www.kaggle.com/antgoldbloom/training-using-google-automl;0.5;[];['ai', 'dl', 'automl', 'gan', 'rl', 'ml', 'nn', 'ann'];['training data', 'train', 'model', 'label', 'understanding', 'classification'];https://www.kaggle.com/c/recursion-cellular-image-classification;0.655;0.383;2020-12-13 16:34:09;multiple data sources;[];Training using Google AutoML;Python notebook;1215.0;24;;
2019-07-17 10:47:17;Let's see what is going on here!;Apache 2.0;https://www.kaggle.com/apap950419/visualizing-the-effect-of-sirna-treatment;0.5;[];['ner', 'ai', 'nn', 'ann'];['train'];https://www.kaggle.com/c/recursion-cellular-image-classification;0.629;0.357;2020-12-13 16:34:09;Recursion Cellular Image Classification;[];Visualizing the effect of siRNA treatment;Python notebook;747.0;18;;
2020-01-28 20:29:21;;Apache 2.0;https://www.kaggle.com/hmendonca/kaggle-pytorch-utility-script;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'loss'];https://www.kaggle.com/c/recursion-cellular-image-classification;0.733;0.497;2020-12-13 16:34:09;multiple data sources;['deep learning, utility script'];Kaggle pytorch utility script;Python script;6737.0;96;;
2019-07-01 03:03:18;Recursion Cellular Image ClassificationCellSignal: Disentangling biological signal from experimental noise in cellular images;Apache 2.0;https://www.kaggle.com/jesucristo/quick-visualization-eda;0.5;[];['ai', 'nn', 'ann'];['image classification', 'train', 'label', 'predict', 'classification'];https://www.kaggle.com/c/recursion-cellular-image-classification;0.767;0.555;2020-12-13 16:34:09;Recursion Cellular Image Classification;['data visualization, exploratory data analysis, deep learning'];Quick Visualization + EDA;Python notebook;16328.0;219;;
2019-07-01 22:23:51;Load libs;Apache 2.0;https://www.kaggle.com/leighplt/densenet121-pytorch;1.0;['pytorch'];['ai', 'nn', 'ann'];['filter', 'predict', 'train', 'model', 'epoch', 'loss', 'relu'];https://www.kaggle.com/c/recursion-cellular-image-classification;0.743;0.484;2020-12-13 16:34:09;Recursion Cellular Image Classification;['gpu'];DenseNet121 | pytorch;Python notebook;8502.0;81;0.15459;0.08643
2019-07-12 10:36:08;Recursion Cellular Image Classification</span>Disentangling biological signal from experimental noise in cellular images</span>;Apache 2.0;https://www.kaggle.com/pheaboo/a-journey-through-the-experiment-design;0.5;[];['ai'];['train', 'image classification', 'classification'];https://www.kaggle.com/c/recursion-cellular-image-classification;0.678;0.446;2020-12-13 16:34:09;Recursion Cellular Image Classification;[];A journey through the experiment design!;Python notebook;1895.0;50;;
2019-08-05 00:51:05;"Intro This kernel lets you efficiently convert all images from their tensor format into RGB images, then save them as 400x400 JPEGs inside two zip files (train and test). Feel free to customize this kernel as you wish. You can change the shape and extension of the final output image by changing the input arguments to convert_to_rgb and build_new_df.  Notes In a previous version (V11) of the kernel, I claimed that the rxrx.io.load_site_as_rgb function was inefficient, and tried to provide a faster solution. It turns out I did not input the correct argument, so it was instead fetching the images directly from Google Storage; with the correct argument, the speed was comparable. My sincere apologies for misleading everyone.  Updates V13: Changed output image size to 400 px instead of 224.  Sources Found out about the loading functions from this kernel: https://www.kaggle.com/jesucristo/quick-visualization-eda";Apache 2.0;https://www.kaggle.com/xhlulu/recursion-2019-load-resize-and-save-images;0.5;[];['ai', 'cv'];['train', 'label'];https://www.kaggle.com/c/recursion-cellular-image-classification;0.73;0.478;2020-12-13 16:34:09;Recursion Cellular Image Classification;['beginner, classification, feature engineering, +2 moredata cleaning, computer vision'];Recursion 2019: Load, Resize and Save Images;Python notebook;6210.0;75;;
2019-07-31 23:18:05;Load libraries;Apache 2.0;https://www.kaggle.com/yhn112/resnet18-baseline-pytorch-ignite;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'predict', 'relu', 'resnet'];https://www.kaggle.com/c/recursion-cellular-image-classification;0.727;0.463;2020-12-13 16:34:09;Recursion Cellular Image Classification;['gpu, beginner, deep learning'];ResNet18 baseline | PyTorch+Ignite;Python notebook;5751.0;62;0.17647;0.10088
2019-03-18 07:13:19;Reducing Commercial Aviation Fatalities;Apache 2.0;https://www.kaggle.com/aditya100/reducing-commercial-aviation-fatalities;1.0;['lightgbm', 'sklearn'];['ai', 'rl', 'gbm'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.579;0.188;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];Reducing Commercial Aviation Fatalities;Python notebook;320.0;3;0.00000;0.00000
2019-02-06 17:01:33;Reducing Commercial Aviation Fatalities;Apache 2.0;https://www.kaggle.com/brunoguilhermeg/eda-and-simple-unsupervised-analysis-1;1.0;['sklearn'];['ai', 'nn', 'rl'];['train', 'test data', 'label', 'filter'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.635;0.214;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];EDA  and  simple Unsupervised analysis 1;Python notebook;824.0;4;;
2018-12-20 15:53:20;Export for time series;Apache 2.0;https://www.kaggle.com/danofer/getting-started-baseline;0.5;[];['ai'];['train'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.614;0.188;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;['beginner, data cleaning'];Getting started - Baseline;Python notebook;569.0;3;;
2019-02-05 20:31:56;;Apache 2.0;https://www.kaggle.com/docxian/first-look-preparation-save-as-rdata;0.5;[];['ai', 'nn'];['train', 'test data', 'training data', 'filter'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.628;0.253;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];First look / preparation / save as RData;R notebook;727.0;6;;
2019-01-29 10:53:27;;Apache 2.0;https://www.kaggle.com/econdata/reducing-commercial-aviation-fatalities;1.0;['lightgbm', 'sklearn'];['ai', 'rl', 'nn', 'gbm'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.603;0.152;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];Reducing Commercial Aviation Fatalities;Python notebook;475.0;2;0.78378;0.54608
2019-02-17 15:26:38;;Apache 2.0;https://www.kaggle.com/kamalchhirang/forgot-to-shuffle-the-data-while-splitting-boom;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.641;0.152;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];Forgot to shuffle the data while splitting & boom;Python script;918.0;2;;
2018-12-29 12:44:33;This notebook is a small dive into looking at the data provided for this comp.  The main focus is on looking at the EEG and ECG data.  The data was much noisier and difficult to interpret than I hoped, so this evaluation kind of went nowhere.  It was still interesting to look at some EEG, ECG, and RR data.;Apache 2.0;https://www.kaggle.com/mrbruce/aviation-fatalities-data-insight;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['train', 'label', 'filter'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.603;0.236;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];kernelbd8cf6eed5;Python notebook;469.0;5;;
2019-02-08 19:03:18;Data Analysis;Apache 2.0;https://www.kaggle.com/opoliakova/data-analysis-and-manipulation-sklearn-modeling;1.0;['pattern', 'sklearn'];['ner', 'ai', 'rl'];['regression', 'train', 'model', 'loss', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.611;0.188;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];Data analysis and manipulation, sklearn modeling;Python notebook;543.0;3;;
2020-04-10 14:52:26;Import Packages;Apache 2.0;https://www.kaggle.com/robbiebeane/commercial-aviation-v01;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv'];['regression', 'train', 'model', 'loss', 'label', 'logistic regression', 'gradient boosting', 'predict'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.533;0.099;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];Commercial Aviation;Python notebook;159.0;1;1.21507;0.88456
2019-01-09 14:09:42;Imbalance learning;Apache 2.0;https://www.kaggle.com/sarmat/sklearn-lgbm-ensemble-baseline;1.0;['tensorflow', 'lightgbm', 'sklearn'];['ai', 'nn', 'gbm'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.643;0.188;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];Sklearn + LGBM ensemble baseline;Python notebook;963.0;3;0.83321;0.45393
2019-02-24 17:42:43;Reducing Commercial Aviation Fatalities;Apache 2.0;https://www.kaggle.com/shahaffind/reducing-commercial-aviation-fatalities-11th;1.0;['xgboost', 'sklearn', 'lightgbm'];['ai', 'nn', 'ann', 'gbm'];['filter', 'test data', 'train', 'fitting', 'model', 'loss', 'label', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.664;0.268;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];Reducing Commercial Aviation Fatalities (11th);Python notebook;1451.0;7;0.56702;0.41864
2018-12-26 06:43:54;A look through these plots show that there are some errors in the experiments readings. Below is a list of errors:  Crew: 1, Seat: 0, error in ecg readings for DA. Crew: 2, Seat: 1, error in all gsr readings. Crew: 5, Seat: 1, error in gsr readings for DA. Crew: 7, Seat: 0, error in gsr readings for CA and DA.;Apache 2.0;https://www.kaggle.com/teemingyi/plots-of-experiments;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.58;0.152;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];Plots of experiments;Python notebook;326.0;2;;
2018-12-26 07:27:20;IntroductionIn this kernel we will create a quick baseline by simply predicting the optimum constant for each class. The metric for this competition is multi class logloss. The best constant to predict for this metric is the frequency of each class. This simple and fast solution will serve as a common sense baseline that we can use to see  if using complex and computation intensive machine learning techniques will actually produce real benefits.;Apache 2.0;https://www.kaggle.com/vbookshelf/baseline-best-constant-to-predict-for-logloss;0.5;[];['ai', 'nn', 'ann'];['machine learning', 'predict', 'test data', 'train', 'deep learning', 'loss'];https://www.kaggle.com/c/reducing-commercial-aviation-fatalities;0.671;0.253;2020-12-13 16:35:26;Reducing Commercial Aviation Fatalities;[];Baseline - Best Constant to Predict for Logloss;Python notebook;1659.0;6;;
2020-07-28 03:40:57;;Apache 2.0;https://www.kaggle.com/abdalazez/restaurant-revenue-predictive;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'cv'];['train', 'fitting', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.611;0.319;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];Restaurant Revenue Predictive;Python notebook;539.0;12;2039266.12881;2289862.63261
2016-12-21 09:24:17;;Apache 2.0;https://www.kaggle.com/ani310/restaurant-revenue;1.0;['sklearn'];['ai'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.768;0.453;2020-12-13 16:39:10;Restaurant Revenue Prediction;[];Restaurant Revenue;Python notebook;17057.0;55;1959757.13592;1790719.48441
2015-05-09 01:24:59;;Apache 2.0;https://www.kaggle.com/arsenal/geomap-for-average-revenue;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.743;0.367;2020-12-13 16:39:10;Restaurant Revenue Prediction;[];Geomap for Average Revenue;R script;8541.0;20;;
2020-10-09 20:18:20;Data Profiling;Apache 2.0;https://www.kaggle.com/ayushikaushik/regression-analysis;1.0;['xgboost', 'sklearn'];['ai', 'dl', 'cv', 'nn', 'ml'];['linear regression', 'filter', 'training data', 'regression', 'test data', 'train', 'fitting', 'model', 'support vector machines', 'logistic regression', 'k-nearest neighbor', 'predict', 'decision tree', 'random forest'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.646;0.371;2020-12-13 16:39:10;Restaurant Revenue Prediction;['regression, linear regression, xgboost'];regression analysis;Python notebook;1016.0;21;1918274.54455;1773052.81160
2015-04-23 08:16:40;;Apache 2.0;https://www.kaggle.com/benhamner/boruta-random-forest-benchmark;0.5;[];['nlp', 'ai', 'nn', 'ner'];['predict', 'train', 'deep learning', 'random forest', 'classification'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.71;0.281;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];Boruta & Random Forest Benchmark;R script;3801.0;8;1837308.36181;1728980.47409
2015-04-21 22:45:49;;Apache 2.0;https://www.kaggle.com/benhamner/t-sne-restaurant-visualization;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.705;0.281;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];t-SNE Restaurant Visualization;R script;3407.0;8;;
2015-04-30 22:28:47;;Apache 2.0;https://www.kaggle.com/benhamner/top-10-leaderboard-performance-over-time-1;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'predict', 'classification', 'propagation'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.697;0.268;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];Top 10 Leaderboard Performance Over Time;Rmarkdown script;2851.0;7;;
2020-10-22 15:13:25;first modifying The OpenDate column;Apache 2.0;https://www.kaggle.com/jatta3399/revenuerrestr;1.0;['tensorflow', 'xgboost', 'sklearn', 'lightgbm'];['ai', 'gbm', 'cv', 'nn', 'ann'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.505;0.319;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];REVENUERrestr.;Python notebook;107.0;12;1890117.15580;1931514.66806
2018-11-13 19:40:27;RMSE 1.92 M (Rank 1600);Apache 2.0;https://www.kaggle.com/jquesadar/restaurant-revenue-1st-place-solution;1.0;['sklearn'];['ai', 'cv'];['train', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.702;0.292;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];Restaurant Revenue - 1st place solution;Python notebook;3196.0;9;1763333.52127;1696289.57718
2020-08-26 19:51:39;Purpose;Apache 2.0;https://www.kaggle.com/matt4byu/restaurant-revenue-prediction-analysis;1.0;['xgboost'];['ai', 'rl', 'cv', 'nn', 'ann'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'label', 'gradient boosting', 'predict', 'random forest'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.615;0.281;2020-12-13 16:39:11;Restaurant Revenue Prediction;['feature engineering, random forest, regression, +1 morexgboost'];Restaurant Revenue Prediction Analysis;R notebook;578.0;8;1742159.80550;1608850.68350
2020-09-03 22:22:20;Feature preprocessing;Apache 2.0;https://www.kaggle.com/meridk/ms-dos;1.0;['sklearn'];['ai', 'nn', 'cv'];['filter', 'test data', 'regression', 'train', 'model', 'label', 'predict', 'linear regression', 'random forest', 'bayesian'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.552;0.214;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];MeriDK fork;Python notebook;210.0;4;1808336.81969;1781826.24357
2015-04-28 14:41:43;;Apache 2.0;https://www.kaggle.com/qiujiqiong/correlationship-matrix;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.7;0.302;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];Correlationship Matrix;R script;3037.0;10;;
2020-09-10 18:52:52;;Apache 2.0;https://www.kaggle.com/spoorthiuk/restaurant-revenue-prediction-withdifferentmodels;1.0;['xgboost', 'sklearn'];['ai', 'nn'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.531;0.253;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];Restaurant_revenue_prediction_WithDifferentModels;Python notebook;154.0;6;1883099.54122;1929245.11373
2020-06-19 05:25:10;TEST;Apache 2.0;https://www.kaggle.com/subbhashit/resturant-revenue-predicition;1.0;['xgboost', 'sklearn', 'lightgbm'];['ai', 'gbm', 'cv', 'nn', 'ann'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.667;0.39;2020-12-13 16:39:10;Restaurant Revenue Prediction;[];resturant revenue predicition;Python notebook;1533.0;26;;
2015-04-28 18:03:03;;Apache 2.0;https://www.kaggle.com/tomtillo/histogram-plot-for-1-37-variables;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.731;0.352;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];Histogram plot for 1-37 variables;R script;6336.0;17;;
2020-06-30 23:28:18;Data preprocessing;Apache 2.0;https://www.kaggle.com/vibeeshk/restaurant-revenue-prediction;1.0;['sklearn'];['ai', 'nn'];['test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/restaurant-revenue-prediction;0.568;0.268;2020-12-13 16:39:11;Restaurant Revenue Prediction;[];Restaurant Revenue Prediction;Python notebook;270.0;7;1872426.66053;1752629.94503
2015-11-17 17:40:52;;Apache 2.0;https://www.kaggle.com/abhilashawasthi/xgb-rossmann;1.0;['xgboost'];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn', 'ann'];['training data', 'test data', 'train', 'fitting', 'model', 'understanding', 'deep learning', 'label', 'predict', 'rank', 'recommend', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.796;0.379;2020-12-13 16:46:28;Rossmann Store Sales;[];xGB_RossMann;R script;39647.0;23;;
2015-11-21 16:44:56;;Apache 2.0;https://www.kaggle.com/cast42/xgboost-in-python-with-rmspe-v2;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ann'];['training data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.8;0.468;2020-12-13 16:46:28;Rossmann Store Sales;[];XGBoost Feature Importance;Python script;44966.0;66;0.14148;0.12820
2015-10-07 14:09:29;;Apache 2.0;https://www.kaggle.com/dvasyukova/predict-sales-with-pandas-py;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'machine learning', 'regression', 'test data', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.744;0.357;2020-12-13 16:46:29;Rossmann Store Sales;[];predict_sales_with_pandas.py;Python script;8857.0;18;0.15208;0.14066
2015-12-15 04:18:39;;Apache 2.0;https://www.kaggle.com/ggep22/how-does-new-competition-affect-sales;1.0;['pattern'];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['linear regression', 'filter', 'training data', 'regression', 'train', 'model', 'deep learning', 'label', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.737;0.352;2020-12-13 16:46:29;Rossmann Store Sales;[];How Does New Competition Affect Sales?;Rmarkdown script;7346.0;17;;
2018-08-05 00:39:23;Structured and time series data;Apache 2.0;https://www.kaggle.com/hortonhearsafoo/fast-ai-lesson-3;1.0;['sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['predict', 'train', 'model', 'neural network', 'epoch', 'loss'];https://www.kaggle.com/c/rossmann-store-sales;0.737;0.408;2020-12-13 16:46:28;multiple data sources;['gpu'];fast.ai lesson 3;Python notebook;7365.0;32;;
2015-11-01 18:08:31;;Apache 2.0;https://www.kaggle.com/khozzy/xgboost-parameter-tuning-template;1.0;['xgboost'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'deep learning', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.788;0.393;2020-12-13 16:46:28;Rossmann Store Sales;[];XGBoost parameter tuning template;Rmarkdown script;30598.0;27;;
2015-10-06 16:25:17;;Apache 2.0;https://www.kaggle.com/michaelpawlus/obligatory-xgboost-example;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['test data', 'random forest', 'regression', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.774;0.387;2020-12-13 16:46:28;Rossmann Store Sales;[];Random Forest Example (0.12579);R script;19956.0;25;0.14546;0.13366
2015-10-08 16:21:06;;Apache 2.0;https://www.kaggle.com/mlandry/random-forest-example;1.0;['h2o'];['ner', 'ai', 'gbm', 'ml', 'nlp', 'nn', 'ann'];['training data', 'test data', 'regression', 'random forest', 'train', 'fitting', 'model', 'neural network', 'deep learning', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.805;0.485;2020-12-13 16:46:28;Rossmann Store Sales;[];H2O Random Forest Example (0.11578);R script;54627.0;82;0.12750;0.11519
2017-04-29 17:06:13;Submission_Novikova;Apache 2.0;https://www.kaggle.com/novikovanastya/submission-novikova;1.0;['sklearn'];['ai', 'cv'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/rossmann-store-sales;0.681;0.362;2020-12-13 16:46:28;Rossmann Store Sales;[];Submission_Novikova;Python notebook;2034.0;19;;
2015-10-21 16:23:05;;Apache 2.0;https://www.kaggle.com/nsecord/filling-gaps-in-the-training-set;1.0;['pattern'];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.773;0.453;2020-12-13 16:46:28;Rossmann Store Sales;[];Filling Gaps in the Training Set;Rmarkdown script;19530.0;55;;
2018-11-29 20:14:20;;Apache 2.0;https://www.kaggle.com/omarelgabry/a-journey-through-rossmann-stores;1.0;['xgboost', 'sklearn'];['ai', 'gbm', 'ml', 'nn', 'ann'];['machine learning', 'regression', 'train', 'model', 'label', 'predict', 'linear regression'];https://www.kaggle.com/c/rossmann-store-sales;0.774;0.481;2020-12-13 16:46:28;Rossmann Store Sales;[];A Journey through Rossmann Stores;Python notebook;20166.0;78;;
2015-10-07 11:43:47;;Apache 2.0;https://www.kaggle.com/paso84/xgboost-in-python-with-rmspe;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.767;0.379;2020-12-13 16:46:28;Rossmann Store Sales;[];XGBoost in python with RMSPE;Python script;16478.0;23;0.13269;0.11948
2015-10-29 19:29:23;;Apache 2.0;https://www.kaggle.com/shearerp/interactive-sales-visualization;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.79;0.509;2020-12-13 16:46:28;Rossmann Store Sales;[];Interactive Sales Visualization!;Rmarkdown script;32680.0;114;;
2018-06-14 20:15:16;;Apache 2.0;https://www.kaggle.com/stefanozakher94/eda-and-forecasting-with-rfregressor-final-updated;1.0;['pattern', 'xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nn', 'ann'];['linear regression', 'machine learning', 'filter', 'regression', 'test data', 'train', 'fitting', 'model', 'random forest', 'understanding', 'neural network', 'label', 'predict', 'rank', 'decision tree', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.713;0.362;2020-12-13 16:46:28;Rossmann Store Sales;['data visualization, feature engineering, data cleaning, +2 morerandom forest, dailychallenge'];EDA and forecasting with RFRegressor_FINAL_UPDATED;Python notebook;4097.0;19;0.17139;0.15851
2015-12-14 00:23:13;;Apache 2.0;https://www.kaggle.com/thie1e/exploratory-analysis-rossmann;0.5;[];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['test data', 'train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.823;0.589;2020-12-13 16:46:28;Rossmann Store Sales;['data visualization, exploratory data analysis'];Exploratory Analysis Rossmann;Rmarkdown script;104233.0;374;;
2015-10-27 04:46:26;;Apache 2.0;https://www.kaggle.com/vinhnguyen/evolutionary-algorithms-for-param-tuning;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['training data', 'generation', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/rossmann-store-sales;0.743;0.362;2020-12-13 16:46:28;Rossmann Store Sales;[];Evolutionary Algorithms for param tuning;Python script;8613.0;19;0.20925;0.21512
2018-07-13 12:40:02;;Apache 2.0;https://www.kaggle.com/xwxw2929/rossmann-sales-top1;1.0;['xgboost'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nn', 'ann'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/rossmann-store-sales;0.728;0.362;2020-12-13 16:46:28;Rossmann Store Sales;['beginner'];Rossmann Sales;Python notebook;5825.0;19;0.11171;0.11257
2015-10-18 21:53:25;;Apache 2.0;https://www.kaggle.com/zygmunt/predict-sales-with-pandas-py;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/rossmann-store-sales;0.775;0.403;2020-12-13 16:46:28;Rossmann Store Sales;[];predict_sales_with_pandas.py;Python script;21065.0;30;0.14612;0.13888
2019-10-02 10:58:13;Introduction to DICOM & Voxels What is a DICOM?Digital Imaging and Communications in Medicine (DICOM) - an international standard related to the exchange, storage and communication of digital medical images. Prior to this format, there was no standardized way to transfer medical scans. So loading up a single patient's study outside the hospital, in older formats took about 10-30 minutes for a single scan! While DICOM 16-bit images (with values ranging from -32768..32767), other 8-bit greyscale images store values 0 - 255. These value ranges in DICOM are useful, as they correlate with the Hounsfield Scale. Each voxel can store a large amount of information. NB: If you want to get right to the code & image example, click here;Apache 2.0;https://www.kaggle.com/dcstang/see-like-a-radiologist-with-systematic-windowing;0.5;[];['ann', 'ai', 'nn', 'ml'];['train', 'model', 'label', 'loss'];https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection;0.757;0.532;2020-12-13 16:47:30;RSNA Intracranial Hemorrhage Detection;[];See like a Radiologist with Systematic Windowing;Python notebook;12584.0;158;;
2020-01-28 20:29:21;;Apache 2.0;https://www.kaggle.com/hmendonca/kaggle-pytorch-utility-script;1.0;['pytorch', 'sklearn'];['ner', 'ai', 'dl', 'nn', 'ann'];['filter', 'train', 'model', 'epoch', 'layer', 'label', 'loss'];https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection;0.733;0.497;2020-12-13 16:47:30;multiple data sources;['deep learning, utility script'];Kaggle pytorch utility script;Python script;6738.0;96;;
2019-10-24 20:27:50;Let's try to clean up the dataset as best as we can, and create something we can do some model prototyping with! For prototyping we'll want something that's small, and ideally fits within Kaggle's 20GB limit so we can upload it as a dataset for others to easily work with. (If you're just looking for the dataset created with this notebook, you can find it here.) After cleaning up the data and create a prototyping dataset, we will use it to train and submit a model. You can see the followup notebook that uses this data here: From Prototyping To Submission Here are the issues that we will address.  Fix images with incorrect RescaleIntercept Remove some images if they have little useful information (e.g. they don't actually contain brain tissue) Resample this dataset to 2/1 split of with/without haemorrhage, so we have a smaller dataset for quick prototyping Crop the images to just contain the brain, and save the size of the crop in case it's important Do histogram rescaling and then save JPEG 256x256 px images  We'll be using the fastai.medical.imaging library here - for more information about this see the notebook Some DICOM gotchas to be aware of. We'll also use the same basic setup that's in the notebook.;Apache 2.0;https://www.kaggle.com/jhoward/cleaning-the-data-for-rapid-prototyping-fastai;0.5;[];['ai', 'dl', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'label'];https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection;0.756;0.521;2020-12-13 16:47:30;multiple data sources;[];Cleaning the data for rapid prototyping (fastai);Python notebook;12097.0;134;;
2019-10-30 22:07:22;"In the notebook ""Cleaning the data for rapid prototyping"" I showed how to create a small, fast, ready-to-use dataset for prototyping our models. The dataset created in that notebook, along with the metadata files it uses, are now available here. So let's use them to create a model! In this notebook we'll see the whole journey from pre-training using progressive resizing on our prototyping sample, through to fine-tuning on the full dataset, and then submitting to the competition. I'm intentionally not doing any tricky modeling in this notebook, because I want to show the power of simple techniques and simples architectures. You should take this as a starting point and experiment! e.g. try data augmentation methods, architectures, preprocessing approaches, using the DICOM metadata, and so forth... We'll be using the fastai.medical.imaging library here - for more information about this see the notebook Some DICOM gotchas to be aware of. We'll also use the same basic setup that's in the notebook. Update: I'm out of GPU hours and Kaggle isn't freezing when running the current version of the notebook. To see a complete run, see this version. I've commented out the GPU calls in this run so I can run it end to end.";Apache 2.0;https://www.kaggle.com/jhoward/from-prototyping-to-submission-fastai;0.5;[];['ner', 'ai', 'dl', 'cnn', 'rl', 'nlp', 'nn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'resnet'];https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection;0.76;0.512;2020-12-13 16:47:30;multiple data sources;['gpu'];From prototyping to submission (fastai);Python notebook;13512.0;119;;
2019-11-14 04:56:05;Load data;Apache 2.0;https://www.kaggle.com/mathormad/5th-place-solution-stacking-pipeline;1.0;['tensorflow', 'lightgbm', 'sklearn', 'keras'];['ner', 'ai', 'cnn', 'gbm', 'rl', 'nn'];['train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection;0.692;0.45;2020-12-13 16:47:30;multiple data sources;['gpu, deep learning, ensembling'];[5th place solution] stacking pipeline;Python notebook;2572.0;53;0.04553;0.64131
2019-09-19 11:57:08;Viewing Dicom CT images with correct  windowingCT image values correspond to Hounsfield units (HU).  But the values stored in CT Dicoms are not Hounsfield units, but instead a scaled version.  To extract the Hounsfield units we need to apply a linear transformation, which can be deduced from the Dicom tags. Once we have transformed the pixel values to Hounsfield units, we can apply a windowing: the usual values for a head CT are a center of 40 and a width of 80, but we can also extract this from the Dicom headers.;Apache 2.0;https://www.kaggle.com/omission/eda-view-dicom-images-with-correct-windowing;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection;0.759;0.522;2020-12-13 16:47:30;RSNA Intracranial Hemorrhage Detection;['beginner, data visualization, exploratory data analysis, +1 moredata cleaning'];EDA: View dicom images with correct windowing;Python notebook;13108.0;136;;
2019-10-23 07:44:58;Gradient & Sigmoid WindowingI've been exploring a bunch of different ways to window the DICOM images and I thought I'd share a few ideas and results. Huge thanks to David Tang, Marco, Nanashi, and Richard McKinley and for their amazing Kernels that I totally borrowed code and ideas from. Contents  No Windowing Brain Windowing Metadata Windowing One Window, Three Channels Gradient Windowing Brain + Subdural + Bone Windowing Exclusive Windowing Gradient (Brain + Subdural + Bone) Windowing Sigmoid Windowing Sigmoid (Brain + Subdural + Bone) Windowing Sigmoid Gradient (Brain + Subdural + Bone) Windowing Acknowledgements;Apache 2.0;https://www.kaggle.com/reppic/gradient-sigmoid-windowing;0.5;[];['ai', 'dl', 'ml', 'nn', 'ann'];['recurrent neural network', 'train', 'model', 'neural network', 'deep learning', 'label'];https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection;0.715;0.507;2020-12-13 16:47:30;RSNA Intracranial Hemorrhage Detection;['data visualization'];Gradient & Sigmoid Windowing;Python notebook;4273.0;110;;
2018-08-30 14:31:32;Practical EDA on numerical data;Apache 2.0;https://www.kaggle.com/aantonova/practical-eda-on-numerical-data;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nn', 'ann'];['filter', 'regression', 'train', 'model', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;0.729;0.47;2020-12-13 16:48:20;RSNA Pneumonia Detection Challenge;[];Practical EDA on numerical data;Python notebook;5973.0;68;;
2018-08-28 22:09:18;This is an attempt to implament the LB metric: mean avarage precision at different IoU threshold. It might be helpful for local validation. The map_iou function evaluates the metric on one image. Idea borrowed from https://www.kaggle.com/raresbarbantan/f2-metric and is modified for this competition. I haven't thoroughly tested it so please comment if you found any bugs!;Apache 2.0;https://www.kaggle.com/chenyc15/mean-average-precision-metric;0.3;[];[];['ground truth', 'predict'];https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;0.731;0.456;2020-12-13 16:48:20;RSNA Pneumonia Detection Challenge;[];Mean Average Precision Metric;Python notebook;6270.0;57;;
2018-09-09 19:44:40;Lung Radiograph ImagesThe folder stage_1_train_images contains one image per patient, for a total of 25684 images.;Apache 2.0;https://www.kaggle.com/giuliasavorgnan/start-here-beginner-intro-to-lung-opacity-s1;0.5;[];['ai', 'nn', 'ann', 'rl'];['training data', 'train', 'model', 'label', 'loss', 'classification'];https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;0.723;0.459;2020-12-13 16:48:20;RSNA Pneumonia Detection Challenge;['beginner, data visualization, exploratory data analysis, +1 moredata cleaning'];START HERE: Beginner Intro to Lung Opacity S1;Python notebook;5199.0;59;;
2020-01-12 20:34:53;RSNA Pneumonia Detection EDAContent Introduction  Prepare the data analysis   -Load packages    -Load the data  Data exploration   -Missing data   -Merge train and class info data   -Explore DICOM data   -Add meta information from DICOM data   -Modality   -Body Part Examined   -View Position   -Conversion Type   -Rows and Columns   -Patient Age   -Patient Sex  Conclusions  References;Apache 2.0;https://www.kaggle.com/gpreda/rsna-pneumonia-detection-eda;0.5;[];['ner', 'ai', 'rl', 'nn', 'ml'];['predict', 'training data', 'test data', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;0.754;0.498;2020-12-13 16:48:20;RSNA Pneumonia Detection Challenge;['data visualization, exploratory data analysis, deep learning, +1 moremedicine'];RSNA Pneumonia Detection EDA;Python notebook;11443.0;98;;
2018-10-27 19:19:21;OverviewThe notebook aims to get a better feeling for the data and more importantly the distributions of values. We take the labels and combine them with the detailed class info and try and determine what the biggest challenges of the prediction might be.;Apache 2.0;https://www.kaggle.com/kmader/lung-opacity-overview;0.5;[];['ner', 'ai', 'nn', 'rl'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;0.769;0.559;2020-12-13 16:48:20;RSNA Pneumonia Detection Challenge;[];Lung Opacity Overview;Python notebook;17323.0;232;;
2018-08-27 16:52:21;OverviewWelcome to the 2018 RSNA Challenge co-hosted by Kaggle. In this competition, the primary endpoint will be the detection of bounding boxes corresponding to the diagnosis of pneumonia (e.g. lung infection) on chest radiographs, a special 2D high resolution grayscale medical image. Note that pnuemonia is just one of many possible disease processes that can occur on a chest radiograph, and that any given single image may contain 0, 1 or many boxes corresponding to possible pneumonia locations. My name is Peter Chang, MD. I am both a radiologist physician and a data scientist / software engineer with machine learning experience. Today, in this Jupyter notebook, we will explore the 2018 RSNA Challenge dataset including underlying data structures, imaging file formats and label types.;Apache 2.0;https://www.kaggle.com/peterchang77/exploratory-data-analysis;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['image segmentation', 'machine learning', 'generation', 'train', 'label', 'loss', 'classification'];https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;0.79;0.571;2020-12-13 16:48:20;RSNA Pneumonia Detection Challenge;[];Exploratory Data Analysis;Python notebook;33509.0;281;;
2018-09-17 15:20:41;"Pneumonia Detection CompetitionData ExplorationWhat is pneumonia? ""Chest X-rays are currently the best available method for diagnosing pneumonia, playing a crucial role in clinical care and epidemiological studies. Pneumonia is responsible for more than 1 million hospitalizations and 50,000 deaths per year in the US alone."" - Link to Stanford ML Group Paper";Apache 2.0;https://www.kaggle.com/robikscube/eda-lets-detect-pneumonia-explore-lung-images;0.5;[];['ai', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;0.714;0.413;2020-12-13 16:48:20;RSNA Pneumonia Detection Challenge;['data visualization, exploratory data analysis, image data'];[EDA] Lets Detect Pneumonia! - Explore Lung Images;Python notebook;4171.0;34;;
2018-09-15 11:13:49;"OverviewI love this competition! But the title ""Pneumonia Detection"" for the competition is misleading because you actually have to do ""Lung Opacities Detection"", and lung opacities are not the same as pneumonia. Lung opacities are vague, fuzzy clouds of white in the darkness of the lungs, which makes detecting them a real challenge. This kernel is for people who want to understand what lung opacities are and how come there are ""No Lung Opacity / Not Normal"" images. In this kernel I will show images from different classes (Normal; No Lung Opacity / Not Normal; Lung Opacity) and write my interpretation of the opacities in the image. I will cover some basics of chest radiography and focus only on the lungs because ""Lung Opacities"" appear only in the lungs. Write your questions in the comments section and I'll answer them as best that I can. My name is Guy Zahavi, I am a physician (anesthesiology resident) and graduate bioinformatics student. I am not affiliated with the competition hosts or Kaggle, I'm just excited about this competition :) I took the code for this kernel from Peter Chang's awesome Exploratory Data Analysis. Thank you Peter!";Apache 2.0;https://www.kaggle.com/zahaviguy/what-are-lung-opacities;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['filter', 'train', 'model', 'neural network', 'loss', 'label', 'u-net', 'predict', 'understanding', 'labeled'];https://www.kaggle.com/c/rsna-pneumonia-detection-challenge;0.828;0.598;2020-12-13 16:48:20;RSNA Pneumonia Detection Challenge;['data visualization, medicine'];What are lung opacities? ;Python notebook;125134.0;432;;
2020-10-12 23:38:35;setup;Apache 2.0;https://www.kaggle.com/boliu0/monai-3d-cnn-inference;1.0;['albumentations', 'sklearn'];['ai', 'cv', 'ml', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.728;0.535;2020-12-13 16:49:33;multiple data sources;['gpu'];MONAI 3D CNN - Inference;Python notebook;5826.0;164;0.309;0.295
2020-10-13 17:25:58;setup;Apache 2.0;https://www.kaggle.com/boliu0/monai-3d-cnn-training;1.0;['pytorch', 'albumentations', 'sklearn'];['ai', 'dl', 'cnn', 'cv', 'ml', 'nn', 'ann'];['train', 'model', 'epoch', 'label', 'loss'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.72;0.531;2020-12-13 16:49:33;multiple data sources;['gpu'];MONAI 3D CNN - Training;Python notebook;4863.0;154;;
2020-10-09 23:35:50;Pulmonary Embolism Detection. Data Analysis and visualization.;Apache 2.0;https://www.kaggle.com/isaienkov/pulmonary-embolism-detection-eda;0.5;[];['ner', 'ai', 'ml', 'gan'];['train', 'model', 'label', 'loss'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.728;0.488;2020-12-13 16:49:33;RSNA STR Pulmonary Embolism Detection;['beginner, data visualization, exploratory data analysis, +2 morecomputer vision, medicine'];Pulmonary Embolism Detection EDA;Python notebook;5925.0;86;;
2020-10-19 03:16:44;;Apache 2.0;https://www.kaggle.com/khyeh0719/cnn-gru-baseline-stage2-train-inference;1.0;['pytorch', 'albumentations', 'skimage', 'sklearn'];['ai', 'cnn', 'cv', 'rl', 'nn', 'ann'];['gru', 'filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.741;0.526;2020-12-13 16:49:33;multiple data sources;['gpu, computer vision, time series analysis'];CNN-GRU Baseline- Stage2 Train+Inference;Python notebook;8154.0;145;0.232;0.233
2020-10-11 06:25:51;;Apache 2.0;https://www.kaggle.com/nitindatta/pulmonary-embolism-dicom-preprocessing-eda;1.0;['skimage'];['ner', 'ai', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'test data', 'train', 'model', 'label', 'understanding', 'classification'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.713;0.514;2020-12-13 16:49:33;RSNA STR Pulmonary Embolism Detection;['beginner, data visualization, image data, +2 morecomputer vision, medicine'];ðŸ©ºPulmonary Embolism Dicom preprocessing & EDAðŸ©º;Python notebook;4093.0;121;;
2020-09-29 18:48:24;;Apache 2.0;https://www.kaggle.com/orkatz2/cnn-lstm-pytorch-train;1.0;['pytorch', 'albumentations', 'skimage', 'sklearn'];['ner', 'ai', 'cnn', 'cv', 'nn', 'ann'];['predict', 'train', 'model', 'epoch', 'lstm', 'label', 'loss', 'relu'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.711;0.444;2020-12-13 16:49:33;multiple data sources;['gpu'];CNN + LSTM - Pytorch [Train] ;Python notebook;3926.0;49;;
2020-09-25 11:19:52;;Apache 2.0;https://www.kaggle.com/orkatz2/resnext-pulmonary-embolism-inference;1.0;['pytorch', 'albumentations'];['ai', 'nn', 'cv'];['filter', 'train', 'model', 'epoch', 'label', 'predict'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.716;0.449;2020-12-13 16:49:33;multiple data sources;['gpu'];ResNext - Pulmonary Embolism[inference];Python notebook;4371.0;52;0.784;0.725
2020-09-22 23:00:56;;Apache 2.0;https://www.kaggle.com/paulorzp/mean-baseline;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.703;0.465;2020-12-13 16:49:33;RSNA STR Pulmonary Embolism Detection;[];Mean Baseline;Python notebook;3269.0;64;0.437;0.434
2020-09-19 18:46:52;Gradient & Sigmoid WindowingI've been exploring a bunch of different ways to window the DICOM images and I thought I'd share a few ideas and results. Huge thanks to David Tang, Marco, Nanashi, Richard McKinley and Ryan Epp for their amazing Kernels that I totally borrowed code and ideas from. Though the original notebook and idea of gradient and sigmoid windowing was applied for RSNA Intracranial Hemorrhage Detection Challenge, in my view these techniques can put serious impact on the competition's LB. Contents â¦¿ 1. No Windowing â¦¿ 2. Lung Windowing â¦¿ 3. Metadata Windowing â¦¿ 4. One Window, Three Channels â¦¿ 5. Gradient Windowing â¦¿ 6. Mapping Multiple Windows in Multiple Channels â¦¿ 7. Exclusive Windowing â¦¿ 8. Gradient Ensemble Windowing â¦¿ 9. Sigmoid Windowing â¦¿ 10. Sigmoid Ensemble Windowing â¦¿ 11. Sigmoid Gradient Ensemble Windowing â¦¿ 12. Acknowledgements;Apache 2.0;https://www.kaggle.com/redwankarimsony/rsna-str-pe-gradient-sigmoid-windowing;0.5;[];['ai', 'dl', 'ml', 'nn', 'ann'];['recurrent neural network', 'train', 'model', 'neural network', 'deep learning', 'predict'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.65;0.452;2020-12-13 16:49:33;multiple data sources;['exploratory data analysis, feature engineering, image data, +2 morediseases, advanced'];RSNA-STR-PE [Gradient & Sigmoid Windowing];Python notebook;1099.0;54;;
2020-09-28 14:10:42;Table of contents Introduction Motivation Reconstruction versus Windows Reusable code     DICOM-CT data 3D visualizations Reading data Windows and LUTs 3D reconstruction with manual windowing and MarchingCubes algorithm Lungs segmentation Other stuff: Vtk to numpy support     Gentle introduction to VTK VTK in standalone Kaggle notebooks Cylinder = Hello World example Cylinder rotation with animation Kitchen example Frog example      Appendix: Slicer3D in Kaggle notebooks Installation MRHead-Example CTChest-Example OSIC training data Example;Apache 2.0;https://www.kaggle.com/wrrosa/advanced-dicom-ct-3d-visualizations-with-vtk;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'understanding', 'propagation'];https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection;0.671;0.45;2020-12-13 16:49:33;multiple data sources;['data visualization, medicine'];Advanced DICOM-CT 3D visualizations with VTK;Python notebook;1638.0;53;;
2020-01-02 11:51:29;Welcome to Santa's Workshop Tour Bazaar 2019: Revenge of the AccountantsThis is a starter code and easy to understand. We consider a linear optimization problem to minimize a cost function subject to constraints. The cost function contains the family (preference) and accounding (penalty) costs. The constraints are: For each day the total number of people attending the workshop must be between 125 and 300. The algorithm: We create a simple feasible start solution for the first iteration step. For this we say that every day get the same number fo families. This number is 50. After that the families go to the bazaar to reduce the total costs. Each familie with preference costs greater than 0 tries to swap the day with another family or swap the assigned day. But the swap is only valid if it changes the preference or acoounding costs. The convergence of the algorithm:   iteration step objective value     0 12.778.763   1 766.069   2 462.110   3 416.496   4 400.576   5 395.614   6    7   ... ...   x 145.111   x+1 144.594   x+2 144.413   x+3    x+4    x+5    We can see that the improvement of the objective value is small in the later steps. Alternatively you can also start with another start solution which is closer to the optimum. For this version we start with a precalculated start solution.;Apache 2.0;https://www.kaggle.com/drcapa/santa-tour-revenge-bazaar;0.5;[];['ai'];['train', 'label'];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.589;0.099;2020-12-13 16:50:31;multiple data sources;['beginner'];Santa_Tour_Revenge_Bazaar;Python notebook;376.0;1;;
2019-12-28 12:48:22;C++ code:;Apache 2.0;https://www.kaggle.com/golubev/baseline;0.2;[];['ai', 'dl'];[];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.685;0.408;2020-12-13 16:50:31;multiple data sources;[];Baseline?!;Python notebook;2188.0;32;115384.85;115384.85
2019-12-27 20:15:46;In this kernel, we will optimize only preference cost in submission file from C++ Stochastic Product Search MIP(Mix Integer Programming) it's a great decision for the current task. We will use not commercial library ortools from google. It's not fast like commercial libraries, but still very fast. If use all variables(6000*100) it would take a lot of time. We setup MAX_BEST_CHOICE = 5 and NUM_SWAP = 3000, how result we get 5*3000 variables and it is enough for get some result in resonable time:;Apache 2.0;https://www.kaggle.com/golubev/mip-optimization-preference-cost-santa2019revenge;0.5;[];['ai'];['train', 'model', 'predict'];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.655;0.375;2020-12-13 16:50:31;multiple data sources;[];MIP: Optimization preference cost Santa2019Revenge;Python notebook;1216.0;22;115427.50;115427.50
2019-12-24 01:39:09;;Apache 2.0;https://www.kaggle.com/grapestone5321/santa-2019-sample-submission;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.582;0.099;2020-12-13 16:50:31;Santa 2019 - Revenge of the Accountants;[];Santa 2019-sample_submission;Python notebook;333.0;1;12824478.24;12824478.24
2019-12-25 05:52:21;;Apache 2.0;https://www.kaggle.com/hs999518/santa-clause;1.0;['sklearn'];['ner', 'ai', 'nn'];['regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.513;0.0;2020-12-13 16:50:31;Santa 2019 - Revenge of the Accountants;[];Santa clause;Python notebook;120.0;0;;
2019-12-24 14:40:54;;Apache 2.0;https://www.kaggle.com/jazivxt/santa-opt-v2;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.612;0.236;2020-12-13 16:50:31;Santa 2019 - Revenge of the Accountants;[];Santa Opt v2;Python script;552.0;5;130536.08;130536.08
2020-05-29 06:32:40;"CP2410 Assignment 2 - Jonache HiltonThis assignment implements two diffrent algorithms; a very fast cost function, which uses Numba JIT from http://numba.pydata.org/ and Stochastic Product search algorithm, which is based off of the Stochastic optimization approach. The assignment also uses two different data structures, lists (Chapter 5.4, page 207 from Data Structures & Algorithms in Python textbook) and Arrays (Chapter 5, page 184 from Data Structures & Algorithms in Python textbook). These algorithms and data structures are used in combination to solve the ""Santa's Workshop Tour 2019"" and ""Santa 2019: Revenge of the Accountants"" competitions.";Apache 2.0;https://www.kaggle.com/jonachehilton/cp2410-assignment-2-jonache-hilton;0.5;[];['ner', 'ai', 'gan', 'rl', 'nn', 'ml'];['train', 'label', 'predict'];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.504;0.0;2020-12-13 16:50:31;Santa 2019 - Revenge of the Accountants;[];CP2410 Assignment 2 - Jonache Hilton;Python notebook;106.0;0;;
2020-06-07 12:40:46;"CP2410 Assignment 2 Introduction This assignment aims to investigate different search tree structures and their applications in Python. The following tests were conducted using the data from the ""Santa 2019: Revenge of the Accountants"" Kaggle challenge ( https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants ). The goal of this challenge was to assign families to a day for a tour of Santa's workshop without overbooking/underbooking specific days and with minimal accounting fees. To do so, the challenge supplies a list of families and their preferred days, the penalty costs for not giving families their first preference, and a function to cover any other incurred overcrowding costs. Data Structure 1 â€“ Binary Search Trees (from chapter 11.1 of textbook) Binary search trees consist of various nodes connected through parent/children relationships. Each node has a left and right child that hold a key/value pair or a blank. The trait that defines a search tree is that any left child (and all its descendants) should be less than its parent (and all of its ancestors), and any right child (and all its descendants) should be more than its parent (and all of its ancestors). When inserting into this data type, the node will go through a series of less than/greater checks (starting at the root node), then descend the tree through the left or right child accordingly until it reaches a blank node, at which point it will override the blank. This makes searching through this datatype potentially faster than an array, as shown by its big-Oh notation of O(h), where 'h' is the height of the tree. However, if nodes are inserted in descedning/ascending order, the tree will construct with only one type of child, making the height of the tree equal to the number of elements, resulting in O(n) search times. In the case of this project, binary search trees will be employed in a for loop to conveniently store score values for each family to see which of their preferences results in the lowest overall costs. Data Structure 2 â€“ AVL Trees (from chapter 11.3 of textbook) AVL trees are binary search trees that restructure themselves to ensure the depth of external nodes do not differ by 2 or more. It does so using trinode restructuring, which takes a trio of problem nodes in a line and rearranges them to a triangle shape. It continues to do so until the conditons of the binary search tree and AVL tree are met. Each trinode restructure takes O(1) time, but the restructuring of an entire tree for the case of an insert/remove can take up to O(lg n). AVL trees have an advantage over binary search trees in that the height of a tree will not expand inconveniently. For example, if the elements were added in ascending order to a binary search tree, the tree would have a height of n, which means that accessing elements would take O(n) time. However, if the same happened in an AVL tree, the tree would be re-arranged as it was developing, leading to a max height of lg n and an O(lg n) time for accessing elements. In the case of this project, AVL trees will be employed in the same way that binary search trees are, but will hopefully reduce the time taken to access elements. Algorithm 1 â€“ Finding minimum via Inorder Traversal Inorder traversal is a way of exploring binary trees, alongside preorder and postorder traversal. Inorder traversal will recursively visit the left child of a node, then the node itself, then the right child. In the case of a search tree, inorder traversal will visit nodes in ascending order, which is an easy way to identify the minimum. Inorder traversal has to evaluate every node in the binary serch tree, and hence, it takes O(n) time In the case of this project, inorder traversal will be used to return a sorted list of the penalties as to find the minimum. Algorithm 2 - Finding minimum directly Using the properties of search trees and inorder traversal, we can specialise/shorten the inorder travesal algorithm to find only the minimum value of the tree instead of a sorted list. In any search tree, the left most element will always be the smallest value. Hence, we can employ the inorder traversal algorithm until the first node is evaluated, at which point we can stop the traversal. In the worst case, the minimum element is stored at the maximum depth, meaning it will have to traverse across the same number of elements as the height of the tree. This will take O(n) for a binary search tree, or O(lg n) for a AVL tree. In the case of this project, this method will be used to shorten the amount of time taken to find the minimum of the search tree.";Apache 2.0;https://www.kaggle.com/lparker7/assignment-2;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['label', 'predict'];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.462;0.0;2020-12-13 16:50:31;Santa 2019 - Revenge of the Accountants;[];Assignment 2;Python notebook;61.0;0;;
2019-12-24 00:49:34;from https://www.kaggle.com/capiru/santa-s-workshop-eda-sorting-visualization Santa 2019: Revenge of the Accountants This competition is a scheduling optimization challenge where you have two major factors to consider:  The preferred day for a family to visit has costs associated to it. Where the costs are as follows: choice_0: no consolation gifts choice_1: one 50 gift card to Santa's Gift Shop choice_2: one 50 gift card, and 25% off Santa's Buffet (value 9) for each family member choice_3: one 100 gift card, and 25% off Santa's Buffet (value 9) for each family member choice_4: one 200 gift card, and 25% off Santa's Buffet (value 9) for each family member choice_5: one 200 gift card, and 50% off Santa's Buffet (value 18) for each family member choice_6: one 300 gift card, and 50% off Santa's Buffet (value 18) for each family member choice_7: one 300 gift card, and free Santa's Buffet (value 36) for each family member choice_8: one 400 gift card, and free Santa's Buffet (value 36) for each family member choice_9: one 500 gift card, and free Santa's Buffet (value 36) for each family member, and 50% off North Pole Helicopter Ride tickets (value 199) for each family member otherwise: one 500 gift card, and free Santa's Buffet (value 36) for each family member, and free North Pole Helicopter Ride tickets (value 398) for each family member;Apache 2.0;https://www.kaggle.com/seshurajup/eda-for-santa-2019-revenge-of-the-accountants;0.5;[];['ai', 'rl'];['label'];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.582;0.236;2020-12-13 16:50:31;Santa 2019 - Revenge of the Accountants;[];EDA for Santa 2019: Revenge of the Accountants;Python notebook;333.0;5;;
2019-12-27 08:13:02;Santa Business!;Apache 2.0;https://www.kaggle.com/shrutimechlearn/santa-returns-workshop-explorers-wave-1-vs-wave-2;0.5;[];['ai', 'nn', 'ann'];['train', 'loss', 'label', 'predict'];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.602;0.268;2020-12-13 16:50:31;multiple data sources;[];Santa Returns! Workshop Explorers Wave 1 vs Wave 2;Python notebook;464.0;7;;
2019-12-24 15:44:03;References https://www.kaggle.com/vipito/santa-ip https://www.kaggle.com/inversion/santa-s-2019-starter-notebook https://www.kaggle.com/sekrier/fast-scoring-using-c-52-usec https://www.kaggle.com/nickel/250x-faster-cost-function-with-numba-jit https://www.kaggle.com/ilu000/greedy-dual-and-tripple-shuffle-with-fast-scoring https://www.kaggle.com/xhlulu/santa-s-2019-stochastic-product-search https://www.kaggle.com/hengzheng/santa-s-seed-seeker;Apache 2.0;https://www.kaggle.com/vipito/fork-of-santa-ip;0.5;[];['ai', 'ml'];['train', 'model', 'predict'];https://www.kaggle.com/c/santa-2019-revenge-of-the-accountants;0.634;0.319;2020-12-13 16:50:31;Santa 2019 - Revenge of the Accountants;[];Fork of Santa IP;Python notebook;812.0;12;115863.49;115863.49
2017-12-13 02:44:25;;Apache 2.0;https://www.kaggle.com/astraldawn/favour-choice-0-83909;0.5;[];['nlp', 'ai', 'nn', 'ner'];['rank', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/santa-gift-matching;0.641;0.334;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];Favour choice [0.83909];Python script;922.0;14;0.00000;0.00000
2018-01-07 07:20:17;This kernel is aimed to improve the solution quality of your current submission via Hungarian method. This kernel is largely inspired by beluga's previous kernel on the original problem (before the competition relaunch) Improve with the Hungarian method [0.9375] https://www.kaggle.com/gaborfodor/improve-with-the-hungarian-method-0-9375  For demonstration purpose, I also used the1owl's kernel - Santas ACME Optimizer Needs Optimizing, which has a relatively good score of 0.8922217472 to start improving upon.         https://www.kaggle.com/the1owl/santas-acme-optimizer-needs-optimizing     Kernel Updates: For demonstration purpose, I also used ZFTurbo's recent kernel - Max Flow with Min Cost v2 [0.9267], which has a great  score of 0.9264476351 to start improving upon. https://www.kaggle.com/zfturbo/max-flow-with-min-cost-v2-0-9267;Apache 2.0;https://www.kaggle.com/dongxu027/path-to-improve-score-hungarian-slowness;0.5;[];['ai', 'nn', 'rl'];['rank', 'predict'];https://www.kaggle.com/c/santa-gift-matching;0.666;0.357;2020-12-13 16:57:44;multiple data sources;[];Path to Improve Score - Hungarian Slowness;Python notebook;1483.0;18;0.92647;0.92647
2017-12-16 14:47:02;IntroductionWe will show how to use the  Hungarian method to improve an existing solution. Let's forget the twins for a moment and look at the vast majority (99.6%) of the rest. Each kid should receive one gift so it is clearly an assigment problem and the objective function is linear. It is not even an NP hard problem we could use polinomial algorithms to solve it. The only problem is that we have 10^6 points so an O(n^3) algorithm might not be feasible. Scipy.optimize has linear_sum_assignment let's use that on small random subsets of the large input space. We will use Vlad Golubev's submission as the baseline submission to improve. You could start with a better solution with Selfish Gene's nice heuristics.;Apache 2.0;https://www.kaggle.com/gaborfodor/improve-with-the-hungarian-method-0-9375;0.5;[];['rl'];['rank', 'label'];https://www.kaggle.com/c/santa-gift-matching;0.724;0.442;2020-12-13 16:57:44;multiple data sources;[];Improve with the Hungarian method [0.9375];Python notebook;5276.0;48;;
2018-01-09 17:28:04;;Apache 2.0;https://www.kaggle.com/glenslade/baseline-python-ortools-algo-0-933795;0.2;[];['ner', 'ai', 'nlu', 'nn'];[];https://www.kaggle.com/c/santa-gift-matching;0.647;0.367;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];Efficient python/ortools algo. [0.9337...+];Python notebook;1028.0;20;0.93379;0.93379
2017-12-15 07:02:41;;Apache 2.0;https://www.kaggle.com/golubev/1-iteration-py-after-night-c-0-9337;0.5;[];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['classification', 'deep learning'];https://www.kaggle.com/c/santa-gift-matching;0.672;0.39;2020-12-13 16:57:44;multiple data sources;[];+1 iteration Py after night C++ [0.9337];Python script;1692.0;26;0.00000;0.00000
2018-01-12 05:59:15;;Apache 2.0;https://www.kaggle.com/golubev/simple-example-min-cost-flow;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santa-gift-matching;0.725;0.421;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];Simple example Min Cost Flow;Python script;5434.0;37;;
2018-01-12 06:11:54;;Apache 2.0;https://www.kaggle.com/golubev/simple-example-mip-ortools-cbc;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santa-gift-matching;0.669;0.371;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];Simple example MIP(ortools:CBC);Python script;1582.0;21;;
2017-12-12 21:10:20;;Apache 2.0;https://www.kaggle.com/inversion/inversion-s-been-nice-benchmark;0.2;[];['ai', 'nn', 'rl'];[];https://www.kaggle.com/c/santa-gift-matching;0.643;0.334;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];"""inversion's been nice"" benchmark";Python notebook;957.0;14;;
2017-12-16 10:00:09;;Apache 2.0;https://www.kaggle.com/jacksapper/faster-hungarian-1-4it-sec-3-1it-sec-0-9372;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['rank', 'classification', 'deep learning'];https://www.kaggle.com/c/santa-gift-matching;0.657;0.367;2020-12-13 16:57:44;multiple data sources;['optimization'];Faster Hungarian[1.4it/sec -> 3.1it/sec][0.9372++];Python script;1251.0;20;0.00000;0.00000
2017-12-14 10:26:12;"SummaryThis is not a typical machine learning problem, but it's a very interesting optimization one. The possibility to evaluate fast the solutions it's going to be very important. The following implementation of the Average Normalized Happiness function was originally provided by organizers here but with small changes it can run in almost 70% less time update: I have added a more extended analysis of the time execution comparison and changed the name of the kernel. The original one was confusing: ""70% Faster Average Normalized Happiness Function"". The optimized script takes 70% less time to execute than the original one, but that means it is about 300% faster.";Apache 2.0;https://www.kaggle.com/lbronchal/300-faster-average-normalized-happiness-function;0.5;[];['ai', 'gan'];['rank', 'machine learning'];https://www.kaggle.com/c/santa-gift-matching;0.68;0.371;2020-12-13 16:57:44;Santa Gift Matching Challenge;['optimization'];300% Faster Average Normalized Happiness Function;Python notebook;1981.0;21;;
2017-12-14 16:52:56;;Apache 2.0;https://www.kaggle.com/lebroschar/scoring-in-0-2-sec-with-lookup-table-4gb;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santa-gift-matching;0.674;0.371;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];Scoring in 0.2 sec with lookup table (~4GB);Python script;1760.0;21;;
2017-12-15 08:45:14;;Apache 2.0;https://www.kaggle.com/sekrier/50ms-scoring-just-with-numpy;0.0;[];[];[];https://www.kaggle.com/c/santa-gift-matching;0.654;0.379;2020-12-13 16:57:44;multiple data sources;['optimization'];50ms scoring just with numpy;Python notebook;1176.0;23;;
2017-12-18 02:00:10;;Apache 2.0;https://www.kaggle.com/selfishgene/nothing-fancy-just-some-heuristics-0-9374;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning', 'predict'];https://www.kaggle.com/c/santa-gift-matching;0.705;0.416;2020-12-13 16:57:44;Santa Gift Matching Challenge;['optimization'];Nothing fancy, just some heuristics [0.9374];Python script;3460.0;35;;
2017-12-23 20:19:56;;Apache 2.0;https://www.kaggle.com/wendykan/average-normalized-happiness-demo;0.5;[];['nlp', 'ai', 'nn', 'ner'];['rank', 'classification', 'deep learning'];https://www.kaggle.com/c/santa-gift-matching;0.722;0.411;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];Average Normalized Happiness Demo;Python script;5089.0;33;;
2017-12-14 17:51:05;;Apache 2.0;https://www.kaggle.com/zeemeen/uniform-filling-of-gift-buckets;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['rank', 'classification', 'deep learning'];https://www.kaggle.com/c/santa-gift-matching;0.65;0.411;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];uniform filling of gift buckets;Python script;1091.0;33;0.00000;0.00000
2017-12-24 17:14:49;;Apache 2.0;https://www.kaggle.com/zfturbo/greedy-children-baseline-v2-0-7455;0.5;[];['nlp', 'ai', 'nn', 'ner'];['rank', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/santa-gift-matching;0.681;0.411;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];Greedy children baseline v2 [0.7455];Python script;2045.0;33;0.74551;0.74551
2017-12-24 20:49:09;;Apache 2.0;https://www.kaggle.com/zfturbo/happiness-vs-gift-popularity-v2-0-89;0.5;[];['nlp', 'ai', 'nn', 'ner'];['rank', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/santa-gift-matching;0.734;0.477;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];Happiness vs Gift popularity v2 [0.89];Python script;6858.0;74;;
2018-01-10 09:16:57;;Apache 2.0;https://www.kaggle.com/zfturbo/infinite-probabilistic-improver-0-931;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['classification', 'deep learning'];https://www.kaggle.com/c/santa-gift-matching;0.716;0.473;2020-12-13 16:57:44;multiple data sources;[];Infinite Probabilistic Improver [>0.931];Python script;4372.0;71;0.93496;0.93496
2018-01-05 22:41:26;;Apache 2.0;https://www.kaggle.com/zfturbo/max-flow-with-min-cost-v2-0-9267;0.5;[];['nlp', 'ai', 'nn', 'ner'];['rank', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/santa-gift-matching;0.707;0.468;2020-12-13 16:57:44;Santa Gift Matching Challenge;[];Max Flow with Min Cost v2 [0.9267];Python script;3621.0;66;0.92644;0.92644
2019-12-17 14:14:34;Santa's Workshop Kaggle Competition This competition is a scheduling optimization challenge where you have two major factors to consider:  The preferred day for a family to visit has costs associated to it. Where the costs are as follows: choice_0: no consolation gifts choice_1: one 50 gift card to Santa's Gift Shop choice_2: one 50 gift card, and 25% off Santa's Buffet (value 9) for each family member choice_3: one 100 gift card, and 25% off Santa's Buffet (value 9) for each family member choice_4: one 200 gift card, and 25% off Santa's Buffet (value 9) for each family member choice_5: one 200 gift card, and 50% off Santa's Buffet (value 18) for each family member choice_6: one 300 gift card, and 50% off Santa's Buffet (value 18) for each family member choice_7: one 300 gift card, and free Santa's Buffet (value 36) for each family member choice_8: one 400 gift card, and free Santa's Buffet (value 36) for each family member choice_9: one 500 gift card, and free Santa's Buffet (value 36) for each family member, and 50% off North Pole Helicopter Ride tickets (value 199) for each family member otherwise: one 500 gift card, and free Santa's Buffet (value 36) for each family member, and free North Pole Helicopter Ride tickets (value 398) for each family member   The daily occupancy from a previous day affects the next day's costs. By the following equation:;Apache 2.0;https://www.kaggle.com/capiru/santa-s-workshop-eda-sorting-visualization;0.5;[];['ai', 'ml', 'rl'];['fitting', 'label', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.685;0.423;2020-12-13 16:59:00;Santa's Workshop Tour 2019;['data visualization, exploratory data analysis'];Santa's Workshop EDA + Sorting Visualization;Python notebook;2222.0;38;;
2019-11-28 04:17:00;How to Schedule Cost Effective Santa Workshop Tours?Seems like the Santa underestimated the families enthusiasm! In order to make everyone happy and not blow a hole in his pocket, lets show Santa some plots to help him understanding his financial costs! Key Discussions : How big are the families interested in the tour? How much would the consolation gifts cost the Santa, if families can't get their preferred choices? How does the accounting cost behave, given different combination of tour plans?  Important Update: Fixed the formula for accounting cost, in the previous version, the absolute of difference was not taken. Fixed the exponent formula (+ instead of -), seems like the evaluation metric page got updated.;Apache 2.0;https://www.kaggle.com/chewzy/santa-finances-a-closer-look-at-the-costs;0.5;[];['ai', 'nn', 'ann', 'rl'];['understanding', 'label'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.747;0.561;2020-12-13 16:58:59;Santa's Workshop Tour 2019;['data visualization, exploratory data analysis'];Santa Finances - A Closer Look at the Costs;Python notebook;9632.0;240;;
2019-12-27 17:30:16;;Apache 2.0;https://www.kaggle.com/chudak/another-pytorch-implementation;0.5;[];['ai', 'nn'];['predict', 'model', 'epoch', 'loss', 'relu'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.716;0.482;2020-12-13 16:58:59;multiple data sources;[];another pytorch implementation;Python notebook;4422.0;79;69983.82;69983.82
2019-12-02 00:55:21;Version 2 I have improved the plotting to make it into function that show the cost as well. This was all started by an interesting suggestion by @hengck23.  https://www.kaggle.com/c/santa-workshop-tour-2019/discussion/119654#latest-684807;Apache 2.0;https://www.kaggle.com/ghostskipper/visualising-results;0.5;[];['ai', 'nn', 'ann'];['train', 'label', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.712;0.495;2020-12-13 16:58:59;multiple data sources;['data visualization'];Visualising Results;Python notebook;3979.0;94;;
2020-11-20 12:37:25;C++ code:;Apache 2.0;https://www.kaggle.com/golubev/c-stochastic-product-search-65ns;0.2;[];['ai', 'dl'];[];https://www.kaggle.com/c/santa-workshop-tour-2019;0.736;0.525;2020-12-13 16:58:59;multiple data sources;[];C++ Stochastic Product Search (65ns);Python notebook;7209.0;142;;
2019-12-26 16:58:18;"Please check the trend ""Best choices"" and as we know most the families prefer to attending at weekends or the last day before Christmas. In the current submission file(for example) as we can see occupation in 65,72,79,86,93 days equal exactly 125 its because for daily occupancy 125 we don't have accounting penalty for prev day. And it's a great trick for minimizing occupation on Monday(""bad"" day) and maximize on Sunday(good day). But we have 2 reasons why we don't use that trick every Monday:  Days what closer to Christmas actually better and better and 2nd day before Christmas we totally cant use that trick If we use a simple algorithm with swap families/days usually we can't get exactly 125 from 200+(for example) and if we slowly changing to 125 we increase accounting penalty almost to infinity.  Lets try use that trick for days: 44, 51, 58";Apache 2.0;https://www.kaggle.com/golubev/manual-to-improve-submissions;0.5;[];['ai'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.684;0.456;2020-12-13 16:59:00;multiple data sources;[];Manual to improve submissions;Python notebook;2148.0;57;;
2020-01-02 07:09:20;In this kernel, we will optimize only preference cost in submission file from C++ Stochastic Product Search MIP(Mix Integer Programming) it's a great decision for the current task. We will use not commercial library ortools from google. It's not fast like commercial libraries, but still very fast. If use all variables(5000*100) it would take a lot of time. We setup MAX_BEST_CHOICE = 5 and NUM_SWAP = 2500, how result we get 5*2500 variables and it is enough for get some result in resonable time:;Apache 2.0;https://www.kaggle.com/golubev/mip-optimization-preference-cost;0.5;[];['ai'];['train', 'model', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.713;0.49;2020-12-13 16:58:59;multiple data sources;[];MIP: Optimization preference cost ;Python notebook;4148.0;88;69827.70;69827.70
2020-01-09 05:08:08;About Min Cost Flow (MCF) you can read there: OR-Tools, Wiki Don't have simple(!) way to get optimum preference cost from MCF because we can't split families, but if we could I recomend this notebook: Lower bound In this example we will optimize preference cost between families with same numbers members without changing daily occupation:;Apache 2.0;https://www.kaggle.com/golubev/optimization-preference-cost-mincostflow;0.5;[];['ai'];['train', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.712;0.486;2020-12-13 16:58:59;multiple data sources;[];Optimization preference cost (MinCostFlow);Python notebook;3988.0;84;69761.84;69761.84
2019-12-01 18:22:23;Credit for fast scoring and the dual shuffle goes to https://www.kaggle.com/sekrier/fast-scoring-using-c-52-usec Credit for the starting values goes to https://www.kaggle.com/isaienkov/genetic-algorithm-basics For ealier versions: Credit for the starting values goes to https://www.kaggle.com/jazivxt/using-a-baseline;Apache 2.0;https://www.kaggle.com/ilu000/greedy-dual-and-tripple-shuffle-with-fast-scoring;0.2;[];['ai', 'dl'];[];https://www.kaggle.com/c/santa-workshop-tour-2019;0.691;0.423;2020-12-13 16:59:00;multiple data sources;[];Greedy dual and tripple shuffle with fast scoring;Python notebook;2532.0;38;78350.24;78350.24
2019-11-26 17:17:45;Read in the family information and sample submission;Apache 2.0;https://www.kaggle.com/inversion/santa-s-2019-starter-notebook;0.5;[];['ai', 'nn'];['train', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.774;0.544;2020-12-13 16:58:59;Santa's Workshop Tour 2019;[];Santa's 2019 Starter Notebook;Python notebook;20327.0;188;672254.02;672254.02
2019-12-01 14:45:36;Here is simple implementation of Genetic Algorithm. It is based on current best public solution https://www.kaggle.com/jazivxt/using-a-baseline. Initial population generates from only one solution (that is not good, but enought for this example).;Apache 2.0;https://www.kaggle.com/isaienkov/genetic-algorithm-basics;0.5;[];['ner', 'ai', 'nn'];['train', 'generation', 'epoch', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.727;0.481;2020-12-13 16:58:59;multiple data sources;[];Genetic Algorithm Basics;Python notebook;5762.0;78;78997.98;78997.98
2019-12-15 05:49:27;;Apache 2.0;https://www.kaggle.com/jazivxt/using-a-baseline;0.2;[];['ai', 'nn', 'ann'];[];https://www.kaggle.com/c/santa-workshop-tour-2019;0.738;0.492;2020-12-13 16:58:59;multiple data sources;[];Using a baseline;Python notebook;7513.0;90;72046.78;72046.78
2019-12-04 02:00:21;In this notebook, we'll go through some exploratory data analysis, visualize these data points to gain deeper insights, and then build the cost function according to the problem formulation and do a bit of a search for the optimum. The Problem Santa's worhshop runs like a well-oiled machine and the accountants run a tight ship. Let's have a look at how we can accommodate as many families as possible without breaking the bank entirely. The Deets: 5,000 families 100 days 125 to 300 per day (hard limit)  The Costs:Consolation Gifts:This should be interesting to look at. Every choice has a cost and the lower we go in priority, the more gratuitous we get with the families. Look at 9 and ++ getting helicopter rides for each person. Ooph!   choice Gift Card Buffet Helicopter      (fixed) (per P) (per P)   0 0 0 0   1 50 0 0   2 50 9 0   3 100 9 0   4 200 9 0   5 200 18 0   6 300 18 0   7 300 36 0   8 400 36 0   9 500 36 199   ++ 500 36 398    Accounting:Accounting runs a tight, recursive ship. Nd is the number of occupants on the day d. Starting 100 days before, working your way towards Christmas. accounting penalty=1âˆ‘d=100Ndâˆ’125400â‹…N(0.5+|Ndâˆ’Nd+1|50)dSources:Blatantly copying stuff from these notebooks. Give them an upvote!  https://www.kaggle.com/inversion/santa-s-2019-starter-notebook (Get a good initial idea) https://www.kaggle.com/chewzy/santa-finances-a-closer-look-at-the-costs (Nice EDA) https://www.kaggle.com/nickel/santa-s-2019-fast-pythonic-cost-23-s (Really fast Pythonic cost, which I adapted) https://www.kaggle.com/xhlulu/santa-s-2019-stochastic-product-search (Great impementation of random search, which I adapted) https://www.kaggle.com/ilu000/greedy-dual-and-tripple-shuffle-with-fast-scoring (Better data, which I'm using in the newer kernels.);Apache 2.0;https://www.kaggle.com/jesperdramsch/intro-to-santa-s-2019-viz-costs-22-s-and-search;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['label', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.682;0.425;2020-12-13 16:59:00;multiple data sources;['beginner, data visualization, exploratory data analysis'];Intro to Santa's 2019: Viz, Costs[22Î¼s] and Search;Python notebook;2056.0;39;;
2020-01-05 09:38:35;Santa's Workshop Tour 2019How sharing of notebooks with high-scoring submissions affects competition dynamicsThis post was inspired by the discussion Optium for Silver Medal.  Below you can see the progression of scores in each medal zone throughout December. It looks like some kagglers rightfully claimed that the movement on the leaderboard after Christmas was largerly caused by published notebooks with scores in the bronze zone.;Apache 2.0;https://www.kaggle.com/kustin/leaderboard-and-high-scoring-submission-sharing;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['rank', 'label', 'filter'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.662;0.429;2020-12-13 16:59:00;multiple data sources;['data visualization'];Leaderboard and high-scoring submission sharing;Python notebook;1384.0;41;;
2019-12-02 16:24:22;MotivationAlthough many cost functions are more than fast enough already, I wanted to a simple pythonic version, while still being blazing fast. The results are somewhere between 23 and 24 Âµs. Also, I updated the cost function so it reports how many high and low days there are, instead of only out of bounds. The original function returns 4 values: family cost, accounting cost, number of days under 125 or over 300. I also show how to use closures to combine costs and penalties for out of bounds days.;Apache 2.0;https://www.kaggle.com/nickel/santa-s-2019-fast-pythonic-cost-23-s;0.5;[];['ai'];['predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.706;0.449;2020-12-13 16:59:00;multiple data sources;['utility script'];Santa's 2019: Fast Pythonic Cost (23 Âµs);Python notebook;3480.0;52;77246.77;77246.77
2019-11-30 09:44:41;Let's use a C implementation of the score function to speed up evaluation;Apache 2.0;https://www.kaggle.com/sekrier/fast-scoring-using-c-42-usec;0.2;[];['ai', 'dl'];[];https://www.kaggle.com/c/santa-workshop-tour-2019;0.682;0.431;2020-12-13 16:59:00;Santa's Workshop Tour 2019;['utility script'];fast scoring using C (42 usec);Python notebook;2065.0;42;127758.94;127758.94
2019-12-10 18:49:16;References https://www.kaggle.com/inversion/santa-s-2019-starter-notebook https://www.kaggle.com/sekrier/fast-scoring-using-c-52-usec https://www.kaggle.com/nickel/250x-faster-cost-function-with-numba-jit https://www.kaggle.com/ilu000/greedy-dual-and-tripple-shuffle-with-fast-scoring https://www.kaggle.com/xhlulu/santa-s-2019-stochastic-product-search https://www.kaggle.com/hengzheng/santa-s-seed-seeker;Apache 2.0;https://www.kaggle.com/vipito/santa-ip;0.5;[];['ai', 'ml'];['train', 'model', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.74;0.517;2020-12-13 16:58:59;Santa's Workshop Tour 2019;[];Santa IP;Python notebook;8007.0;127;72057.06;72057.06
2019-12-23 00:06:08;;Apache 2.0;https://www.kaggle.com/xhlulu/santa-s-2019-faster-cost-function-24-s;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/santa-workshop-tour-2019;0.728;0.479;2020-12-13 16:59:00;Santa's Workshop Tour 2019;['utility script'];Santa's 2019: Faster Cost Function (24Âµs);Python script;5835.0;76;;
2019-12-10 09:22:51;About this kernelIn this kernel I propose a simple algorithm that I came up with to improve the current best LB score. Here's a simple description of the algorithm, which is defined as stochastic_product_search:  Sample a small number of families (e.g. 6 families), call this fam_size, or f. Given their top-k choices (e.g. two most preferred days), you find the cartesian product of those choices. That will give you k^f (or in the example above 2^6 = 64) possible changes that you can make to your current assignment. For each of those proposed change, you create a new assignment by updating your current best assignment. If the new_score is better than the current best_score, update both of those values. Repeat that for the number of iterations desired, given by parameter n_iter.  References (Previous) Best LB score: https://www.kaggle.com/ilu000/greedy-dual-and-tripple-shuffle-with-fast-scoring Current Best LB Score: https://www.kaggle.com/vipito/santa-ip Fast cost function: https://www.kaggle.com/xhlulu/santa-s-2019-faster-cost-function-24-s;Apache 2.0;https://www.kaggle.com/xhlulu/santa-s-2019-stochastic-product-search;0.2;[];['ml'];[];https://www.kaggle.com/c/santa-workshop-tour-2019;0.727;0.472;2020-12-13 16:59:00;multiple data sources;[];Santa's 2019: Stochastic Product Search;Python notebook;5696.0;70;72107.38;72107.38
2020-04-14 06:16:42;XGBoost;Apache 2.0;https://www.kaggle.com/akshat0007/santander-customer-satisfaction-xgboost;1.0;['xgboost', 'sklearn'];['ai'];['filter', 'train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.556;0.383;2020-12-13 17:05:46;Santander Customer Satisfaction;[];santander-customer-satisfaction - XGBoost;Python notebook;222.0;24;;
2016-04-23 15:47:30;var3: nationality of the customer;Apache 2.0;https://www.kaggle.com/cast42/exploring-features;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'ml', 'rl'];['test data', 'filter', 'train', 'label', 'clustering'];https://www.kaggle.com/c/santander-customer-satisfaction;0.813;0.582;2020-12-13 17:05:46;Santander Customer Satisfaction;['data visualization, exploratory data analysis'];Exploring features;Python notebook;72235.0;334;;
2016-04-18 20:41:50;;Apache 2.0;https://www.kaggle.com/cast42/xgboost-with-early-stopping;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.792;0.435;2020-12-13 17:05:46;Santander Customer Satisfaction;[];XGBoost with early stopping;Python script;34872.0;44;0.81527;0.82886
2016-04-24 15:00:12;;Apache 2.0;https://www.kaggle.com/darraghdog/under-23-year-olds-are-always-happy;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.728;0.327;2020-12-13 17:05:46;Santander Customer Satisfaction;[];Under 23 year olds are always happy;R script;5883.0;13;0.82690;0.84185
2016-03-03 04:12:42;;Apache 2.0;https://www.kaggle.com/ericcouto/using-10x-less-ram;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/santander-customer-satisfaction;0.722;0.379;2020-12-13 17:05:46;Santander Customer Satisfaction;[];Using 10x less RAM;Python script;5122.0;23;;
2016-04-21 05:01:57;;Apache 2.0;https://www.kaggle.com/kobakhit/0-84-score-with-36-features-only;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.762;0.393;2020-12-13 17:05:46;Santander Customer Satisfaction;[];~0.83 score with 36 features only;Python script;14312.0;27;0.81977;0.83408
2016-03-02 21:45:16;;Apache 2.0;https://www.kaggle.com/santyago/xgboost-starter-script-0-823-on-lb;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.738;0.367;2020-12-13 17:05:46;Santander Customer Satisfaction;[];XGBOOST Starter script (~0.823 on LB);R script;7516.0;20;0.81641;0.83300
2016-04-12 14:18:54;;Apache 2.0;https://www.kaggle.com/scirpus/python-xgb-lb-41047;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'rl', 'nlp', 'nn'];['predict', 'train', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.752;0.327;2020-12-13 17:05:46;Santander Customer Satisfaction;[];Python XGB (LB 0.841047);Python script;10773.0;13;0.81017;0.82451
2016-04-03 21:12:59;Advanced Feature Exploration;Apache 2.0;https://www.kaggle.com/selfishgene/advanced-feature-exploration;1.0;['pattern', 'sklearn'];['ner', 'ai', 'rl', 'cv', 'nn'];['predict', 'train', 'model', 'label', 'k-means', 'loss'];https://www.kaggle.com/c/santander-customer-satisfaction;0.762;0.456;2020-12-13 17:05:46;Santander Customer Satisfaction;['feature engineering, intermediate'];Advanced Feature Exploration;Python notebook;14467.0;57;;
2016-03-08 19:36:42;;Apache 2.0;https://www.kaggle.com/sionek/reverse-feature-engineering;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/santander-customer-satisfaction;0.756;0.397;2020-12-13 17:05:46;Santander Customer Satisfaction;[];Reverse Feature Engineering;R script;12131.0;28;;
2016-03-12 02:33:47;;Apache 2.0;https://www.kaggle.com/srodriguex/model-and-feature-selection-with-python;1.0;['xgboost', 'sklearn'];['ai', 'nn', 'ann', 'cv'];['train', 'model', 'test data'];https://www.kaggle.com/c/santander-customer-satisfaction;0.745;0.371;2020-12-13 17:05:46;Santander Customer Satisfaction;[];Model and feature selection with Python;Python notebook;8978.0;21;;
2016-03-03 02:35:09;;Apache 2.0;https://www.kaggle.com/tks0123456789/data-exploration;1.0;['sklearn'];['ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/santander-customer-satisfaction;0.759;0.423;2020-12-13 17:05:46;Santander Customer Satisfaction;[];Data exploration;Python notebook;13252.0;38;;
2016-03-08 11:46:25;;Apache 2.0;https://www.kaggle.com/tuomastik/pca-visualization;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['training data', 'train', 'fitting', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.782;0.442;2020-12-13 17:05:46;Santander Customer Satisfaction;[];PCA visualization;Python script;25388.0;48;;
2016-04-17 08:45:06;;Apache 2.0;https://www.kaggle.com/xiziling/roc-curve;1.0;['xgboost', 'sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.687;0.319;2020-12-13 17:05:46;Santander Customer Satisfaction;[];roc_curve;Python script;2313.0;12;;
2016-03-07 21:52:42;;Apache 2.0;https://www.kaggle.com/yuansun/lb-0-84-for-starters;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'fitting', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.771;0.429;2020-12-13 17:05:46;Santander Customer Satisfaction;[];LB ~0.84 for starters;Python script;18508.0;41;0.82536;0.83905
2016-05-03 00:24:37;;Apache 2.0;https://www.kaggle.com/zfturbo/to-the-top-v3;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.771;0.411;2020-12-13 17:05:46;Santander Customer Satisfaction;[];To the TOP v3;R script;18460.0;33;0.82433;0.84231
2016-03-31 07:59:54;;Apache 2.0;https://www.kaggle.com/zfturbo/xgb-lalala;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-satisfaction;0.746;0.367;2020-12-13 17:05:46;Santander Customer Satisfaction;[];XGB (R);R script;9190.0;20;0.82624;0.84133
2019-04-09 07:23:16;;Apache 2.0;https://www.kaggle.com/bogorodvo/starter-code-saving-and-loading-lgb-xgb-cb;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ml'];['predict', 'test data', 'train', 'fitting', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/santander-customer-transaction-prediction;0.776;0.566;2020-12-13 17:07:47;Santander Customer Transaction Prediction;[];Starter Code. Saving and Loading LGB, XGB, CB;Python script;21571.0;262;;
2019-03-07 23:13:06;;Apache 2.0;https://www.kaggle.com/brandenkmurray/nothing-works;1.0;['lightgbm'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ann'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-customer-transaction-prediction;0.763;0.558;2020-12-13 17:07:47;Santander Customer Transaction Prediction;[];Nothing Works;R script;14734.0;229;0.89903;0.90031
2019-03-29 06:34:05;In this kernel, I implement vectorized PDF caculation (without for loop) to get their correlation matrix. This is helpful to study feature grouping. credits to @sibmike https://www.kaggle.com/sibmike/are-vars-mixed-up-time-intervals;Apache 2.0;https://www.kaggle.com/jiweiliu/fast-pdf-calculation-with-correlation-matrix;0.5;[];['ai', 'nn'];['train', 'label', 'loss'];https://www.kaggle.com/c/santander-customer-transaction-prediction;0.75;0.554;2020-12-13 17:07:48;Santander Customer Transaction Prediction;['data visualization, exploratory data analysis'];Fast PDF calculation with correlation matrix;Python notebook;10438.0;216;;
2019-03-15 18:46:58;;Apache 2.0;https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nn'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/santander-customer-transaction-prediction;0.789;0.611;2020-12-13 17:07:47;Santander Customer Transaction Prediction;[];LGB 2 leaves + augment;Python notebook;32106.0;541;0.89983;0.90111
2019-03-23 01:27:32;"The statistics of training set and test set are very similar. However, one thing that caught my eye was the fact that the distribution of the number of unique values (across features) is significantly different between training set and test set. It seems that the test set consists of real samples as well as synthetic samples that were generated by sampling the real samples feature distributions (These are probably the ""rows which are not included in scoring""). If this is correct, then finding out which sample is synthetic, and which is real should be relatively easy task: Given a sample, we can go over its features and check if the feature value is unique. If at least one of the sample's features is unique, then the sample must be a real sample. It turns out that if a given sample has no unique values then it is a synthetic sample. (It doesn't have to be like that, but in this dataset the probability is seemingly to low that this would not be the case).";Apache 2.0;https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split;0.5;[];['ner', 'ai', 'nn'];['train'];https://www.kaggle.com/c/santander-customer-transaction-prediction;0.778;0.615;2020-12-13 17:07:47;Santander Customer Transaction Prediction;[];List of Fake Samples and Public/Private LB split;Python notebook;23068.0;575;;
2016-11-29 04:37:54;Comprehensive exploration and visualization;Apache 2.0;https://www.kaggle.com/alabsinatheer/comprehensive-exploration-and-visualization-1-1;0.5;[];['ner', 'ai', 'dl', 'nn', 'ann'];['train', 'label'];https://www.kaggle.com/c/santander-product-recommendation;0.757;0.453;2020-12-13 17:13:04;Santander Product Recommendation;[]; Comprehensive exploration and visualization-1;Python notebook;12591.0;55;;
2016-10-27 13:00:34;;Apache 2.0;https://www.kaggle.com/anokas/collaborative-filtering-btb-lb-0-01691;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'regression', 'train', 'fitting', 'model', 'deep learning', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/santander-product-recommendation;0.771;0.478;2020-12-13 17:13:04;Santander Product Recommendation;[];Collaborative Filtering BTB (LB 0.01691);Python script;18463.0;75;0.01709;0.01689
2016-12-22 14:23:01;;Apache 2.0;https://www.kaggle.com/apryor6/detailed-cleaning-visualization;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'nn'];['filter', 'training data', 'train', 'model', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/santander-product-recommendation;0.788;0.533;2020-12-13 17:13:04;Santander Product Recommendation;[];Detailed Cleaning/Visualization;Rmarkdown script;31048.0;160;;
2016-12-22 06:45:00;Detailed Data Cleaning/VisualizationA blog post about the final end-to-end solution (21st place) is available here, and the source code is on my github This is a Python version of a kernel I wrote in R for this dataset found here. There are some slight differences between how missing values are treated in Python and R, so the two kernels are not exactly the same, but I have tried to make them as similar as possible. This was done as a convenience to anybody who wanted to use my cleaned data as a starting point but prefers Python to R. It also is educational to compare how the same task can be accomplished in either language. The goal of this competition is to predict which new Santander products, if any, a customer will purchase in the following month. Here, I will do some data cleaning, adjust some features, and do some visualization to get a sense of what features might be important predictors. I won't be building a predictive model in this kernel, but I hope this gives you some insight/ideas and gets you excited to build your own model. Let's get to it First GlanceLimit the number of rows read in to avoid memory crashes with the kernel;Apache 2.0;https://www.kaggle.com/apryor6/detailed-cleaning-visualization-python;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['filter', 'training data', 'train', 'model', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/santander-product-recommendation;0.808;0.569;2020-12-13 17:13:04;Santander Product Recommendation;[];Detailed Cleaning/Visualization (Python);Python notebook;59522.0;272;;
2016-10-27 21:48:29;;Apache 2.0;https://www.kaggle.com/delemeator/xgb-starter;1.0;['xgboost'];['ner', 'ai', 'nlp', 'nn', 'ml'];['test data', 'train', 'model', 'deep learning', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/santander-product-recommendation;0.739;0.352;2020-12-13 17:13:05;Santander Product Recommendation;[];XGB starter;R script;7776.0;17;;
2017-09-29 12:34:29;;Apache 2.0;https://www.kaggle.com/donyoe/santander-quick-first-view;0.5;[];['ai', 'rl'];['rank', 'recommend', 'label', 'train'];https://www.kaggle.com/c/santander-product-recommendation;0.763;0.476;2020-12-13 17:13:04;Santander Product Recommendation;[];Santander quick first view;Rmarkdown script;14733.0;73;;
2016-11-24 20:11:00;;Apache 2.0;https://www.kaggle.com/jturkewitz/reduce-size-of-dataset-to-1-gb;0.5;[];['ai', 'dl', 'ml', 'cv'];['train', 'filter'];https://www.kaggle.com/c/santander-product-recommendation;0.728;0.371;2020-12-13 17:13:04;Santander Product Recommendation;[];Reduce Size of Dataset to 1 GB;Python notebook;5916.0;21;;
2016-11-12 21:33:23;It's one more data exploratory and visualization notebook. This is my first R notebook I start from demographic data  The product data are analyzed in Know your data Part 2 Products  Part 3 is about the similarity in the data;Apache 2.0;https://www.kaggle.com/katerynad/know-your-data;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['train', 'label', 'filter', 'predict'];https://www.kaggle.com/c/santander-product-recommendation;0.698;0.327;2020-12-13 17:13:05;Santander Product Recommendation;[];Know your data;R notebook;2911.0;13;;
2016-11-19 12:50:38;;Apache 2.0;https://www.kaggle.com/mayukh18/basic-svd-using-6-data-lb-0-01961;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'deep learning', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/santander-product-recommendation;0.732;0.367;2020-12-13 17:13:04;Santander Product Recommendation;[];Basic SVD using 6% data! ( LB 0.01961);Python script;6454.0;20;;
2016-11-27 07:40:43;;Apache 2.0;https://www.kaggle.com/sudalairajkumar/ensembling-code-python;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'recommend', 'classification', 'deep learning'];https://www.kaggle.com/c/santander-product-recommendation;0.699;0.319;2020-12-13 17:13:05;Santander Product Recommendation;[];Ensembling code - python;Python script;3002.0;12;;
2016-11-11 09:50:04;;Apache 2.0;https://www.kaggle.com/sudalairajkumar/keras-starter-script;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn', 'ml'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'relu', 'recommend', 'classification'];https://www.kaggle.com/c/santander-product-recommendation;0.738;0.34;2020-12-13 17:13:05;Santander Product Recommendation;[];Keras starter script ;Python script;7579.0;15;0.01953;0.01935
2016-11-27 05:22:58;Looking at the leaderboard, I was wondering what is the maximum possible score one would be able to achieve since the top score currently is 0.0303 (on 13th Nov, 2016). I thought if we could get a ball park value for the maximum possible score, it will be nice to know. So in this notebook, let us do some computation for the same. Objective: We have train data from 2015-01-28 to 2016-05-28 and the test data is from 2016-06-28. The objective is to predict what additional products a customer will get in the last month, 2016-06-28, in addition to what they already have at 2016-05-28. Evaluation: Evaluation metric is Mean Average Precision @ 7 (MAP@7) as seen from this evaluation page. If the number of added products for the given user at that time point is 0 (which is from May 2016 to June 2016), then the precision is defined to be 0. Maximum possible score: All those customers who did not buy a product in the given one month will have a map@7 score of 0 though they will be counted in the mean calculation.  So the maximum possible score for the competition is not 1 and a value lesser than 1. We do not know the additional products bought in June 2016 to compute the maximum possible score for June. So let us compute the maximum possible score for May 2016 by taking in to account what the customers already have in April 2016. This will get us a fairly good idea on the maximum possible score one would be able to achieve in this competition.;Apache 2.0;https://www.kaggle.com/sudalairajkumar/maximum-possible-score;0.5;[];['ai', 'rl'];['train', 'test data', 'predict'];https://www.kaggle.com/c/santander-product-recommendation;0.742;0.411;2020-12-13 17:13:04;Santander Product Recommendation;[];Maximum Possible Score;Python notebook;8429.0;33;;
2016-11-20 19:23:00;;Apache 2.0;https://www.kaggle.com/sudalairajkumar/when-less-is-more;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/santander-product-recommendation;0.78;0.497;2020-12-13 17:13:04;Santander Product Recommendation;[];When Less is More.!;Python script;24455.0;96;0.02727;0.02686
2016-12-19 10:27:45;;Apache 2.0;https://www.kaggle.com/tezdhar/when-less-is-more-extended;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/santander-product-recommendation;0.726;0.357;2020-12-13 17:13:05;Santander Product Recommendation;[];When Less is More ... extended;Python script;5602.0;18;0.02924;0.02883
2016-11-20 10:11:02;;Apache 2.0;https://www.kaggle.com/zfturbo/mass-hashes;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'fitting', 'deep learning', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/santander-product-recommendation;0.738;0.405;2020-12-13 17:13:04;Santander Product Recommendation;[];Mass Hashes;Python script;7566.0;31;0.02608;0.02585
2016-11-01 09:28:02;;Apache 2.0;https://www.kaggle.com/zfturbo/santander-battle;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'deep learning', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/santander-product-recommendation;0.751;0.421;2020-12-13 17:13:04;Santander Product Recommendation;[];Santander Battle;Python script;10586.0;37;;
2018-06-30 19:20:14;;Apache 2.0;https://www.kaggle.com/asydorchuk/save-98-of-ram;0.5;[];['nlp', 'ai', 'nn', 'ner'];['test data', 'train', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/santander-value-prediction-challenge;0.77;0.491;2020-12-13 17:15:37;Santander Value Prediction Challenge;[];Save 98% of RAM with this trick;Python script;18126.0;89;;
2018-06-30 00:59:01;;Apache 2.0;https://www.kaggle.com/headsortails/breaking-bank-santander-eda;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'training data', 'test data', 'train', 'clustering', 'label', 'predict', 'rank', 'understanding'];https://www.kaggle.com/c/santander-value-prediction-challenge;0.791;0.591;2020-12-13 17:15:36;Santander Value Prediction Challenge;['beginner, data visualization, exploratory data analysis, +1 morefeature engineering'];Breaking Bank - Santander EDA;Rmarkdown script;34020.0;389;;
2018-07-19 12:46:12;Giba's Property https://www.kaggle.com/titericz/the-property-by-giba (kernel) https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/61329 (post)  This kernel is just to extend giba's result in a stupid brute-force wayThe updated part is based on S D's comment;Apache 2.0;https://www.kaggle.com/johnfarrell/giba-s-property-extended-extended-result;0.5;[];['ner', 'ai'];['train', 'predict'];https://www.kaggle.com/c/santander-value-prediction-challenge;0.768;0.551;2020-12-13 17:15:37;Santander Value Prediction Challenge;[];Giba's Property (Extended Extended Result);Python notebook;16804.0;206;;
2018-07-13 18:11:14;;Apache 2.0;https://www.kaggle.com/ogrellier/santander-46-features;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-value-prediction-challenge;0.736;0.507;2020-12-13 17:15:37;Santander Value Prediction Challenge;[];Santander_46_features;Python script;7115.0;111;1.35602;1.39667
2019-07-13 15:26:34;Load Required Libraries;Apache 2.0;https://www.kaggle.com/samratp/lightgbm-xgboost-catboost;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ai', 'rl', 'gbm'];['filter', 'test data', 'regression', 'train', 'fitting', 'model', 'label', 'predict'];https://www.kaggle.com/c/santander-value-prediction-challenge;0.788;0.555;2020-12-13 17:15:37;Santander Value Prediction Challenge;[];LightGBM + XGBoost + Catboost;Python notebook;30936.0;219;1.49821;1.54118
2019-05-25 13:30:07;;Apache 2.0;https://www.kaggle.com/sggpls/santander-pipeline-kernel-xgb-fe-lb1-38;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/santander-value-prediction-challenge;0.738;0.519;2020-12-13 17:15:37;Santander Value Prediction Challenge;['feature engineering, regression, xgboost'];Santander Pipeline Kernel, xgb + fe [LB1.38];Python script;7600.0;130;;
2018-07-13 00:46:22;;Apache 2.0;https://www.kaggle.com/titericz/giba-countvectorizer-d-lb-1-43;1.0;['sklearn'];['ai', 'cv'];['train', 'model', 'predict'];https://www.kaggle.com/c/santander-value-prediction-challenge;0.725;0.477;2020-12-13 17:15:37;Santander Value Prediction Challenge;['beginner, feature engineering, data cleaning, +1 morerandom forest'];Giba CountVectorizer :-D;Python notebook;5431.0;74;;
2018-07-18 01:15:38;;Apache 2.0;https://www.kaggle.com/titericz/the-property-by-giba;0.5;[];['ai'];['train'];https://www.kaggle.com/c/santander-value-prediction-challenge;0.784;0.592;2020-12-13 17:15:36;Santander Value Prediction Challenge;['data visualization, data cleaning'];The Property by Giba;R notebook;27600.0;390;;
2015-12-01 04:06:24;;Apache 2.0;https://www.kaggle.com/davidshinn/where-are-all-the-gifts;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.733;0.403;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Where Are All the Gifts?;Python script;6613.0;30;;
2015-12-11 10:55:43;;Apache 2.0;https://www.kaggle.com/gaborfodor/you-ll-shoot-your-eye-out-kid;0.5;[];['nlp', 'ai', 'nn', 'ner'];['label', 'classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.73;0.403;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];VisualizeYou'll shoot your eye out, kid;Python script;6245.0;30;;
2016-01-26 15:55:58;;Apache 2.0;https://www.kaggle.com/hxd1011/cost-function-wrw-in-r-fast-version;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.671;0.188;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Cost Function (WRW) in R (fast version);R script;1658.0;3;;
2015-12-01 23:26:59;;Apache 2.0;https://www.kaggle.com/inventology/random-christmas-tree;0.5;[];['nlp', 'ai', 'nn', 'ner'];['label', 'classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.625;0.214;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Random Christmas Tree :);R script;685.0;4;;
2015-12-01 11:29:10;;Apache 2.0;https://www.kaggle.com/jamesdlawrence/happy-christmas;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.659;0.319;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Happy Christmas!;R script;1307.0;12;;
2015-12-24 00:07:41;;Apache 2.0;https://www.kaggle.com/justfor/you-ll-shoot-your-eye-out-kid;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.722;0.188;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];YsyeoK;Python script;5062.0;3;;
2015-12-09 16:26:23;;Apache 2.0;https://www.kaggle.com/kotomord/lower-bound;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.689;0.268;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Lower bound;Python script;2400.0;7;;
2015-12-02 23:37:03;;Apache 2.0;https://www.kaggle.com/lplewa/gifts-distribution-by-weight;0.5;[];['nlp', 'ai', 'nn', 'ner'];['label', 'classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.675;0.268;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Gifts Distribution by Weight;Python script;1793.0;7;;
2015-12-11 23:21:29;;Apache 2.0;https://www.kaggle.com/madcap/you-ll-shoot-your-eye-out-kid-r-ver;1.0;['pattern'];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.665;0.253;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];You'll shoot your eye out, kid (R ver);R script;1458.0;6;12630694130.00390;12630694130.00390
2015-12-02 03:20:47;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/nearest-neighbours;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.674;0.214;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Nearest Neighbours;R script;1750.0;4;;
2016-07-15 14:19:26;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/santa-exploration;0.5;[];['ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['recommend', 'model'];https://www.kaggle.com/c/santas-stolen-sleigh;0.767;0.481;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Santa exploration;Rmarkdown script;16467.0;78;;
2015-12-01 22:37:48;;Apache 2.0;https://www.kaggle.com/raddar/wrw-metric-in-r;1.0;['pattern'];['nlp', 'ai', 'nn', 'ner'];['classification', 'filter', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.704;0.292;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];WRW metric in R;R script;3337.0;9;;
2015-12-22 19:10:01;;Apache 2.0;https://www.kaggle.com/rdslater/you-ll-shoot-your-eye-out-kid;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.718;0.319;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];You'll shoot your eye out, kid;Python script;4613.0;12;;
2015-12-09 19:48:48;;Apache 2.0;https://www.kaggle.com/sergeylebedev/benchmark-r;0.5;[];['nlp', 'ai', 'nn', 'ner'];['clustering', 'classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.608;0.214;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];benchmark.R;R script;516.0;4;12663383501.22880;12663383501.22880
2015-12-21 23:23:06;;Apache 2.0;https://www.kaggle.com/stochastic/santa-exploration;1.0;['h2o'];['ner', 'ai', 'nlu', 'dl', 'cnn', 'gbm', 'gan', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['gru', 'deep learning', 'vgg', 'label', 'classification'];https://www.kaggle.com/c/santas-stolen-sleigh;0.692;0.319;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Visualizing your submission in 3D;Rmarkdown script;2578.0;12;;
2015-12-18 02:00:01;;Apache 2.0;https://www.kaggle.com/sushize/btb-ssz;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.69;0.188;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];btb_ssz;Python script;2449.0;3;;
2015-12-04 08:29:04;;Apache 2.0;https://www.kaggle.com/thakurrajanand/beat-the-benchmark-14358424217;0.5;[];['nlp', 'ai', 'nn', 'ner'];['model', 'classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.731;0.362;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Beat the Benchmark - 14358424217;R script;6403.0;19;13729012657.45100;13729012657.45100
2015-12-14 02:39:25;;Apache 2.0;https://www.kaggle.com/wendykan/computing-weighted-reindeer-weariness;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.759;0.449;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Computing Weighted Reindeer Weariness;Python script;13030.0;52;;
2015-12-13 11:25:40;;Apache 2.0;https://www.kaggle.com/williamlucia/santa-s-stolen-sleigh-simple-heuristic;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['classification', 'filter', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.677;0.188;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];Santa's Stolen Sleigh Simple Heuristic;R script;1861.0;3;0.00000;0.00000
2016-01-08 14:01:39;;Apache 2.0;https://www.kaggle.com/xchent/test1aaa;0.5;[];['nlp', 'ai', 'nn', 'ner'];['classification', 'deep learning'];https://www.kaggle.com/c/santas-stolen-sleigh;0.637;0.214;2020-12-13 17:25:56;Santa's Stolen Sleigh;[];test1aaa;Python script;857.0;4;;
2019-07-25 20:02:00;Motivation;Apache 2.0;https://www.kaggle.com/rlagrois/macropca-with-unknown-features;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['filter', 'test data', 'regression', 'train', 'model', 'label', 'predict', 'rank'];https://www.kaggle.com/c/the-winton-stock-market-challenge;0.7;0.268;2020-12-13 17:38:52;The Winton Stock Market Challenge;['finance, pca, dimensionality reduction'];MacroPCA with Unknown Features;R notebook;3045.0;7;;
2019-04-05 21:44:35;Traning the model;Apache 2.0;https://www.kaggle.com/akkibansal/box-office-prediction;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nn', 'ann'];['test data', 'regression', 'train', 'model', 'label', 'predict', 'linear regression', 'random forest'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.725;0.357;2020-12-13 17:40:32;TMDB Box Office Prediction;[];Box Office Prediction;Python notebook;5528.0;18;;
2019-02-14 08:58:50;Now you see me...TMDB!!!;Apache 2.0;https://www.kaggle.com/ashishpatel26/now-you-see-me;1.0;['xgboost', 'sklearn'];['dl', 'ai', 'nn', 'ann'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.744;0.484;2020-12-13 17:40:32;TMDB Box Office Prediction;['beginner, exploratory data analysis, statistical analysis, +1 moreensembling'];Now you see me...!!!;Python notebook;8876.0;81;;
2020-06-17 11:54:40;"TMDB - Detailed EDA and Time Series   TMDB (The Movie Database) is a community-built portal containing data about movies and TV shows thus becoming a direct competition of IMDb. As the film industry grows every year, more and more movies are being released - and as the taste of cinema-goers changes with time, film indutry has to adapt as well. Amount of money flowing to and from this market is immense so the interest in trends prediction is growing. On other side, for a common person it would be difficult to follow all the news from this world and websites like TMDB allow them to get a good overview of what is currently played. The database given to us in this competition is a movie database containing a vast amount of information about movies dating back to 70s. This will give us a great opportunity to learn how the film industry developed over the years, who are the biggest players and what trends can we observe. And of course finally, we can hone our data analysis and machine learning skills. Let's start our investigation!  (screenshot of the main page of TMDB) Content: 1-EDA a-Number of titles per year b-Revenue per year c-Budget per year d-Runtime per year    2-Correlations a-Overview of numerical variables   3-Categorical variables   SOME INSIGHTS  Average length of a movie is about 107 minutes The longest movie: ""Carlos"" is 5hrs 38min long The most popular movies: ""Wonder Woman"" and ""Beauty and the Beast"" Movies with the biggest budget: ""Pirates of the Caribbean: On Stranger Tides"" and ""Pirates of the Caribbean: At World's End"" The biggest movie producers: Warner Bros. and Universal Pictures The most popular genres: Drama and Comedy  Much more you will find in the analysis itself so enjoy!  First loading of required libraries:";Apache 2.0;https://www.kaggle.com/datark1/tmdb-detailed-eda-and-time-series;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ann'];['machine learning', 'train', 'layer', 'label', 'predict', 'rank', 'recommend'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.717;0.411;2020-12-13 17:40:32;TMDB Box Office Prediction;['data visualization, exploratory data analysis, feature engineering'];TMDB - Detailed EDA and Time Series;Python notebook;4554.0;33;;
2019-04-28 16:58:39;The data;Apache 2.0;https://www.kaggle.com/dway88/feature-eng-feature-importance-random-forest;1.0;['sklearn'];['ner', 'ai', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'test data', 'train', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.668;0.346;2020-12-13 17:40:32;TMDB Box Office Prediction;['feature engineering, random forest'];Feature Eng., Feature Importance, Random Forest;Python notebook;1570.0;16;2.14980;2.14980
2019-02-21 08:16:33;MDB Box Office Prediction;Apache 2.0;https://www.kaggle.com/enric1296/complete-guide-eda-feat-model;1.0;['catboost', 'xgboost', 'sklearn'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'test data', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.7;0.421;2020-12-13 17:40:32;TMDB Box Office Prediction;['beginner, data visualization, exploratory data analysis, +1 morexgboost'];Complete Guide +EDA+FEAT+MODEL;Python notebook;3067.0;37;;
2019-02-20 02:51:12;;Apache 2.0;https://www.kaggle.com/holar9/xgboost-starter;1.0;['xgboost'];['ner', 'ai', 'nlp', 'nn', 'ml'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.7;0.383;2020-12-13 17:40:32;TMDB Box Office Prediction;['beginner, feature engineering, xgboost'];Xgboost Starter;R script;3031.0;24;2.01046;2.01046
2019-04-02 17:46:44;Let's Get Started;Apache 2.0;https://www.kaggle.com/kamalchhirang/eda-feature-engineering-lgb-xgb-cat;1.0;['catboost', 'xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.779;0.544;2020-12-13 17:40:32;multiple data sources;['beginner, data visualization, exploratory data analysis, +1 morefeature engineering'];EDA, Feature Engineering, LGB+XGB+CAT;Python notebook;23515.0;186;;
2019-05-31 17:45:10;Movie Visualization, Recommendation, Prediction;Apache 2.0;https://www.kaggle.com/sjj118/movie-visualization-recommendation-prediction;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'cv', 'rl', 'nn', 'ann'];['gru', 'filter', 'train', 'fitting', 'model', 'label', 'predict', 'recommend'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.742;0.397;2020-12-13 17:40:32;multiple data sources;[];Movie Visualization, Recommendation, Prediction;Python notebook;8321.0;28;;
2019-04-18 21:56:10;;Apache 2.0;https://www.kaggle.com/somang1418/eda-lgb-xgb-modelings-with-a-cute-panda-meme;1.0;['xgboost', 'lightgbm', 'nltk', 'catboost', 'sklearn'];['ner', 'ai', 'gbm', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['filter', 'test data', 'regression', 'train', 'fitting', 'model', 'loss', 'label', 'gradient boosting', 'predict', 'recommend'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.72;0.462;2020-12-13 17:40:32;multiple data sources;['beginner, data visualization, exploratory data analysis'];EDA + LGB/XGB Modelings with a Cute(!) Panda Meme;Python notebook;4891.0;61;;
2019-02-28 03:47:35;;Apache 2.0;https://www.kaggle.com/stearnsio/what-makes-a-movie-profitable;0.5;[];['dl', 'ai', 'nn', 'rl'];['train', 'training data', 'filter'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.678;0.34;2020-12-13 17:40:32;TMDB Box Office Prediction;['data visualization, exploratory data analysis, feature engineering, +1 moredata cleaning'];What Makes A Movie Profitable?;Rmarkdown script;1920.0;15;;
2019-07-19 11:54:32;;Apache 2.0;https://www.kaggle.com/tavoosi/predicting-box-office-revenue-with-random-forest;1.0;['pattern'];['ner', 'ai', 'rl'];['filter', 'machine learning', 'regression', 'test data', 'train', 'random forest', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/tmdb-box-office-prediction;0.768;0.522;2020-12-13 17:40:32;TMDB Box Office Prediction;['beginner, data visualization, exploratory data analysis, +2 moreclassification, feature engineering'];Predicting box office revenue with Random Forest;Rmarkdown script;16959.0;137;1.95841;1.95841
2018-08-14 08:56:05;Motivation: Measurements in the Pixel detectorThe innermost componets of tracking detectors (volume_id = 7,8,9)  for high energy physics are often Silicon Pixel detector, as it is is modelled in this dataset.  A pixelated readout structure allows to detect the single or group of pixels that are traversed by the particle and thus receive a signal. Evidently, this is strongly dependent on the incident angle of the particle into the detection sensor, as illustrated below:  The markers here are:  black dots : pixel center positions magenta dot: reconstructed cluster position red dot: true intersection of particle with sensor mid-surface  Each single module has thus a channel system which is two-dimensional to the local coordinates. These are the channel indizes ch0 and ch1, which indicate which pixel(s) on the module have been crossed by a particle. When a particle crosses a pixel, it induces charge by ionisation, this does not alter the charge of the traversing particle, but reduces its energy (and thus momentum) slightly. The value of the charge can be read out in the pixel detector and is stored for each channel identifiec by (ch0,ch1) as the approprated value. The following shows such a pixel module channel schema with the came cluster as above, the coloring of the traversed pixels indicates their value (charge).  Position informationIn a first stage, pixels that are adjunct are grouped together into clusters, an operation that has already been done in the presented dataset. The particle intersection with the module can then be rather precisely determined by taking the pixel position - and, as it it the case in the dataset - using the charge information of the individual pixels that contribute to the cluster.;Apache 2.0;https://www.kaggle.com/asalzburger/pixel-detector-cells;0.5;[];['ner', 'ai', 'dl', 'ml', 'nn', 'ann'];['train', 'layer', 'model', 'predict'];https://www.kaggle.com/c/trackml-particle-identification;0.7;0.453;2020-12-13 17:43:28;TrackML Particle Tracking Challenge;[];Pixel Detector: Cells;Python notebook;3036.0;55;;
2018-05-04 12:10:18;;Apache 2.0;https://www.kaggle.com/codename007/a-very-extensive-eda-of-physics-particles-plotly;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['filter', 'machine learning', 'train', 'layer', 'label', 'ground truth'];https://www.kaggle.com/c/trackml-particle-identification;0.733;0.465;2020-12-13 17:43:27;TrackML Particle Tracking Challenge;['beginner, data visualization, exploratory data analysis, +1 morephysics'];A Very Extensive EDA of Physics Particles : Plotly;Python notebook;6675.0;64;;
2018-05-21 17:40:10;"Particle trajectoriesI will derive particle trajectories wrt time for a particle that is not acted upon by any force and for a particle that is interacting with a constant magnetic field. This might be an idealized view of what goes on in the detector, neglecting particle-particle ineractions, particle- detector interactions, inhomogenous magnetic field and other things I cannot think of :). Might be a bit confusing, but hope that you will find use for the final equations at least. They are boxed. Special relativitySince the particles have high energies and thus speeds, one should use special relativity to describe the trajectories. As you might be aware of, time is relative in special relativity. This means that, starting off, one parametrizes the trajectories in terms of ""proper time"" Ï„Ï„ instead of ""regular time"" tt. Proper time is time measured in a coordinate system moving with the same speed and direction as the particle. I.e. the particle is at rest in this coordinate system. Regular time (time in our coordinate frame, where the detector is at rest) is treated in many ways as a spatial coordinate. Lets get started. I will use â†’overheadarrowoverheadarrowâ†’ for regular coordinate vectors and Boldface for four-vecors. That is vectors that also contains this new ""time dimension"" as the first entry. Some examples of four-vecors: f - four force p - four momentum u - four velocity Regular vectors: â†’r=âŸ¨x,y,zâŸ© No forcef=âˆ‚pâˆ‚Ï„=0 This means four-momentum is conserved: mâŸ¨âˆ‚ctâˆ‚Ï„,âˆ‚xâˆ‚Ï„,âˆ‚yâˆ‚Ï„,âˆ‚zâˆ‚Ï„âŸ©=mâŸ¨âˆ‚ctâˆ‚Ï„,âˆ‚â†’râˆ‚Ï„âŸ©=p0 By intagrating we can conclude that: â†’r=â†’p0mÏ„+â†’r0 From the first ""time component "" of the four-vector we have: mcâˆ‚tâˆ‚Ï„=Ecâ†’Ï„=mc2Et, assuming t(0)=0. Where E is the energy of the particle (The time-component of the four-momentum is energy divided by c). Also note that if the energy is not much higher than the resting energy mc2, time ticks at the same rate in both coordinate systems. Lets plug this into our parametrized trajectory: â†’r=â†’p0c2Et+â†’r0=â†’p0mâˆš1+|â†’p0|2(mc)2t+â†’r0 Note that for small momentums(much smaller than mc), we get back the non-relativistic result â†’r=â†’pmt+â†’r0 Constant magnetic field (z-direction)The forces acting upon a chared particle by electromagnetism is called the lorentz force. Or four-force in special relativity. For constant magnetic field in z-dir, we have the lorentz four-force: âˆ‚pâˆ‚Ï„=q[000000âˆ’B00B000000][âˆ‚tâˆ‚Ï„âˆ‚xâˆ‚Ï„âˆ‚yâˆ‚Ï„âˆ‚zâˆ‚Ï„]=qBâŸ¨0,âˆ’âˆ‚yâˆ‚Ï„,âˆ‚xâˆ‚Ï„,0âŸ©, where q is the particle's charge and B is the magnitude of the magnetic field. Lets look at the x and y coordinatesof the particle, since there now is a force acting in the x and y directions. We have: mâˆ‚2xâˆ‚Ï„2=âˆ’qBâˆ‚yâˆ‚Ï„mâˆ‚2yâˆ‚Ï„2=qBâˆ‚xâˆ‚Ï„ Integrating:     mâˆ‚xâˆ‚Ï„=âˆ’qB(yâˆ’y0)+p0,xmâˆ‚yâˆ‚Ï„=qB(xâˆ’x0)+p0,y This set of first-order differential equations has the solution:   x(Ï„)=p0,yqBcos[qBmÏ„]+p0,xqBsin[qBmÏ„]âˆ’p0,yqB+x0y(Ï„)=p0,yqBsin[qBmÏ„]âˆ’p0,xqBcos[qBmÏ„]+p0,xqB+y0 In terms of t:      x(t)=p0,yqBcos[qBmkt]+p0,xqBsin[qBmkt]âˆ’p0,yqB+x0y(t)=p0,yqBsin[qBmkt]âˆ’p0,xqBcos[qBmkt]+p0,xqB+y0z(t)=p0,zmkt+z0     With k=1âˆš1+|â†’p0|2(mc)2";Apache 2.0;https://www.kaggle.com/makahana/particle-trajectory-equations-from-a-physicist;0.2;[];['ner', 'ai', 'nn', 'ann'];[];https://www.kaggle.com/c/trackml-particle-identification;0.701;0.411;2020-12-13 17:43:28;TrackML Particle Tracking Challenge;[];Particle Trajectory Equations from a Physicist;Python notebook;3137.0;33;;
2018-05-01 22:42:19;Quick plot for particle trajectories from training data.;Apache 2.0;https://www.kaggle.com/makahana/quick-trajectory-plot;0.5;[];['ai'];['train', 'label', 'training data'];https://www.kaggle.com/c/trackml-particle-identification;0.689;0.393;2020-12-13 17:43:28;TrackML Particle Tracking Challenge;[];Quick trajectory plot;Python notebook;2398.0;27;;
2018-05-05 03:19:40;In this notebook, I'm looking at a few things in the small training set to try to understand what is in the data. If you want to understand more about how detectors work and how particles interact with them, I would strongly suggest at least going over the relevant review articles from the Particle Data Group (PDG), which are all available for free online. There are also several textbooks on detectors available if you're really interested in knowing more details. Unfortunately, there aren't so many easily accessible resources for learning in detail how things like track reconstruction (the topic of this competition) are implemented in practice.;Apache 2.0;https://www.kaggle.com/muonneutrino/basic-eda-from-a-physicist;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'model', 'layer', 'label', 'loss', 'understanding'];https://www.kaggle.com/c/trackml-particle-identification;0.68;0.437;2020-12-13 17:43:28;TrackML Particle Tracking Challenge;['data visualization, exploratory data analysis, physics'];Basic  EDA from a Physicist;Python notebook;1981.0;45;;
2018-05-09 18:42:34;;Apache 2.0;https://www.kaggle.com/pranav84/a-beginner-s-guide-to-cern-s-trackml-challenge;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['training data', 'machine learning', 'test data', 'train', 'layer', 'clustering', 'ground truth'];https://www.kaggle.com/c/trackml-particle-identification;0.786;0.547;2020-12-13 17:43:27;TrackML Particle Tracking Challenge;['data visualization, physics'];A Beginner's Guide to CERN's TrackML Challenge;Rmarkdown script;29078.0;195;;
2018-05-03 13:03:13;"Simple  Events EDA  - TrackML Particle TrackingIn this notebook, I have explored the dataset provied in the TrackML Particle Tracking Challenge. The dataset comprises of multiple independent events, where each event contains simulated measurements (essentially 3D points) of particles generated in a collision between proton bunches at the Large Hadron Collider at CERN. The goal of this challenge is to group the recorded measurements or hits for each event into tracks, sets of hits that belong to the same initial particle. A solution must uniquely associate each hit to one track. The training dataset contains the recorded hits, their ground truth counterpart and their association to particles, and the initial parameters of those particles. Contents   Read Event Files  Read - Cells, Particles, Truth, Hits Files   Exploring Cells Dataset 3.1 Distribution of cells.ch0 3.2 Distribution of cells.ch1 3.3 Distribution of cells.value 3.4 mean of ""cells.value"" by ch0 and ch1   Exploring Hits Data 4.1 X, Y, Z global coordinates of particles 4.2 Layer Id for every hit 4.3 Module Id for every hit 4.4 Volume Id for every hit 4.5 Distance of particles from Origin 4.6 Initial Position of particles in 3d space   Exploring Particles Data 5.1 Animated trajectory of a positively charged particle 5.2 Animated trajectory of a negatively charged particle 5.3 Plotting the initial x, y, and z coordinates of particles 5.4 Distribution of positively and negatively charged particles 5.5 Distribution of number of hits for different particles   Exploring Truths Data 6.1 Truth Data weight per id";Apache 2.0;https://www.kaggle.com/shivamb/trajectory-animation-eda;0.5;[];['ner', 'ai', 'ml', 'nn', 'ann'];['training data', 'train', 'layer', 'label', 'ground truth'];https://www.kaggle.com/c/trackml-particle-identification;0.7;0.449;2020-12-13 17:43:28;TrackML Particle Tracking Challenge;['beginner, data visualization, exploratory data analysis'];Trajectory Animation + EDA;Python notebook;3043.0;52;;
2018-06-14 08:37:05;;Apache 2.0;https://www.kaggle.com/sionek/bayesian-optimization;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'clustering', 'rank', 'understanding', 'classification', 'bayesian'];https://www.kaggle.com/c/trackml-particle-identification;0.739;0.47;2020-12-13 17:43:27;TrackML Particle Tracking Challenge;[];Bayesian Optimization;R script;7737.0;68;0.49261;0.49530
2018-06-21 18:04:38;;Apache 2.0;https://www.kaggle.com/sionek/mod-dbscan-x-100-parallel;1.0;['sklearn'];['ner', 'ai', 'dl', 'cnn', 'gan', 'rl', 'nlp', 'nn', 'rnn', 'ml'];['filter', 'train', 'fitting', 'model', 'supervised learning', 'deep learning', 'gradient descent', 'clustering', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/trackml-particle-identification;0.753;0.502;2020-12-13 17:43:27;TrackML Particle Tracking Challenge;[];mod DBSCAN x 100 (parallel);R script;11250.0;104;0.45350;0.45600
2018-06-09 22:42:38;"Quick Problem DescriptionLink every track to one hit. Disclaimer: I'm oversimplifying physics to explain this problem. Every particle leaves a track behind it, like a car leaving tire marks in the sand.  We did not catch the particle in action.  Now we want to link every track (tire mark) to one hit that the particle created. In every event, a large number of particles are released.  They move along a path leaving behind their tracks.  They eventually hit a particle detector surface on the other end. In the training data we have the following information on each event:  Hits: x,y,zx,y,z coordinates of each hit on the particle detector Particles: Each particle's initial position (vx,vy,vzvx,vy,vz), momentum (px,py,pzpx,py,pz), charge (qq) and number of hits Truth: Mapping between hits and generating particles; the particle's trajectory, momentum and the hit weight Cells: Precise location of where each particle hit the detector and how much energy it deposited";Apache 2.0;https://www.kaggle.com/wesamelshamy/trackml-problem-explanation-and-data-exploration;0.5;[];['ner', 'ai', 'ml'];['train', 'label', 'training data', 'layer'];https://www.kaggle.com/c/trackml-particle-identification;0.792;0.589;2020-12-13 17:43:27;TrackML Particle Tracking Challenge;['data visualization, exploratory data analysis, statistical analysis'];TrackML Problem Explanation and Data Exploration;Python notebook;35399.0;377;;
2019-05-21 19:21:37;;Apache 2.0;https://www.kaggle.com/tarunaryyan/imputation-for-missing-values-in-features;1.0;['sklearn'];['ai', 'nn', 'ml'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/tradeshift-text-classification;0.639;0.152;2020-12-13 17:43:34;Tradeshift Text Classification;[];Imputation for missing values in features;Python notebook;890.0;2;;
2016-11-08 18:01:25;;Apache 2.0;https://www.kaggle.com/anokas/frequent-words-model-v2;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['model', 'classification', 'filter', 'deep learning'];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.66;0.281;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];Frequent Words Model v2;Python script;1336.0;8;0.07171;0.07171
2016-12-21 23:15:37;Python Script to visualize our test. We will use word clouds to see the frequency fo our words in the titles and the contents.;Apache 2.0;https://www.kaggle.com/eliotbarr/word-clouds;0.2;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];[];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.705;0.34;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];Word Clouds ;Python notebook;3426.0;15;;
2016-11-27 15:49:17;;Apache 2.0;https://www.kaggle.com/katarz/tags-exploration;0.7;['nltk'];['ai', 'nn', 'ann'];[];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.636;0.188;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];Tags exploration;Python notebook;841.0;3;;
2017-02-24 12:41:19;;Apache 2.0;https://www.kaggle.com/narae78/predict-tags-with-tf-idf-method;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['gru', 'filter', 'test data', 'train', 'natural language processing', 'label', 'predict', 'natural language'];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.746;0.327;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];Predict tags with TF-IDF method;Rmarkdown script;9379.0;13;;
2016-12-12 06:32:36;;Apache 2.0;https://www.kaggle.com/nlothian/adversarial-validation;1.0;['pattern', 'sklearn'];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn', 'ml'];['training data', 'test data', 'train', 'model', 'understanding', 'validation data', 'deep learning', 'label', 'predict', 'recommend', 'classification'];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.679;0.188;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];Adversarial Validation;Python script;1959.0;3;;
2017-01-13 14:53:21;Load Data;Apache 2.0;https://www.kaggle.com/sagado/n-gram-tf-idf-lb-0-08412;1.0;['sklearn'];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'test data'];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.71;0.152;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];n-gram tf-idf (LB: 0.08412);Python notebook;3817.0;2;;
2016-11-22 13:50:37;;Apache 2.0;https://www.kaggle.com/sudalairajkumar/frequent-words-model-v3;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['training data', 'train', 'model', 'deep learning', 'classification'];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.703;0.327;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];Frequent Words Model V3;Python script;3261.0;13;0.08453;0.08453
2016-11-03 14:54:16;Simple exploration notebook;Apache 2.0;https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook;1.0;['nltk'];['ner', 'nn', 'ml'];['model'];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.705;0.319;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];Simple Exploration Notebook;Python notebook;3413.0;12;;
2016-12-09 19:11:47;;Apache 2.0;https://www.kaggle.com/tjvananne/tag-to-title-content-matching-in-r-first-attempt;1.0;['pattern'];['ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['recommend', 'model', 'label'];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.713;0.375;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];Tag to Title/Content Matching in R - First Attempt;Rmarkdown script;4124.0;22;;
2016-12-02 00:37:21;;Apache 2.0;https://www.kaggle.com/tomderuijter/tf-idf;0.5;[];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['model', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.719;0.281;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];tf-idf;Python script;4742.0;8;0.05670;0.05670
2016-11-30 18:06:15;;Apache 2.0;https://www.kaggle.com/ymcdull/frequent-words-model;0.5;[];['ner', 'ai', 'ml', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'recommend', 'classification'];https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags;0.699;0.253;2020-12-13 17:46:50;Transfer Learning on Stack Exchange Tags;[];Frequent Words Model;Python script;3002.0;6;;
2018-12-07 14:21:35;Object Oriented Santa's RouteSanta Claus lives in a wonderfull world! Let's implement his world with object oriented programming? This migh help us develop cleaner code when searching for the best path for his deliveries. The CityThe lowest level we can get in his world are the cities. They have some properties:  x: the horizontal location on a map y: the vertical location on a map id: the city id, corresponds to the city name  From those basic properties we may create another one:  coord:  the tuple (x, y)  Then we may chek if a city is prime or not:  is_prime:  Boolean indication if is prime  Let's implement all that and see how it works:;Apache 2.0;https://www.kaggle.com/andresionek/object-oriented-santa-s-route-concorde-solver;0.2;[];['ner', 'ai', 'rl', 'nn', 'ml'];[];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.726;0.48;2020-12-13 17:50:09;Traveling Santa 2018 - Prime Paths;['beginner'];ðŸŒŸ Object Oriented Santa's Route + Concorde Solver;Python notebook;5598.0;77;;
2019-01-10 16:04:46;;Apache 2.0;https://www.kaggle.com/bicotsp/pmtest1;0.5;[];['ner', 'ai', 'dl', 'cv', 'rl', 'ml', 'nn', 'ann'];['gru'];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.72;0.497;2020-12-13 17:50:09;Traveling Santa 2018 - Prime Paths;[];PMtest1;Python notebook;4867.0;97;1514325.09034;1514325.09034
2018-11-27 22:51:34;This notebook shows how to build and run concorde TSP solver directly, without using a rather underfeatured wrapper like pyconcorde.;Apache 2.0;https://www.kaggle.com/blacksix/concorde-for-5-hours;0.2;[];['ai', 'dl', 'rl', 'nn', 'ml'];[];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.721;0.5;2020-12-13 17:50:09;Traveling Santa 2018 - Prime Paths;[];Concorde for 5 hours;Python notebook;5000.0;101;1516912.37511;1516912.37511
2018-12-08 21:14:35;This notebook shows how to build and run concorde TSP solver directly, without using a rather underfeatured wrapper like pyconcorde.;Apache 2.0;https://www.kaggle.com/blacksix/concorde-for-5-min;0.2;[];['ai', 'dl', 'rl', 'nn', 'ml'];[];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.729;0.498;2020-12-13 17:50:09;Traveling Santa 2018 - Prime Paths;[];Concorde for 5 min;Python notebook;6050.0;98;1518479.68945;1518479.68945
2019-01-04 05:54:49;IntroductionIn the past multiple kernels such as Riffling for fine selection and CPU shuffle have demonstrated one finetuning technique of checking small rolling windows of the tour, trying all permutation of k nodes in them and stitching the best scoring permutation back into the tour. This works but the number of permutations grows so fast that it becomes impractical after around k=11..12 even on GPUs:;Apache 2.0;https://www.kaggle.com/blacksix/dp-shuffle;0.2;[];['ner', 'ai', 'dl', 'rl'];[];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.726;0.527;2020-12-13 17:50:09;multiple data sources;[];DP shuffle;Python notebook;5560.0;147;1515572.48448;1515572.48448
2018-12-09 17:44:36;Synopsis;Apache 2.0;https://www.kaggle.com/blacksix/flip-n-roll-fast-python-scorer;0.7;['pattern'];['ai', 'rl'];[];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.674;0.435;2020-12-13 17:50:10;multiple data sources;[];Flip 'n Roll (+fast python scorer);Python notebook;1766.0;44;;
2018-12-05 17:04:05;LKH 2.0.9LKH is an effective implementation of the Lin-Kernighan heuristic and is considered one of the best algorithms available for the TSP problem. Even though the algorithm is approximate, optimal solutions are produced with an impressively high frequency. It has been implemented in the programming language C and is distributed for academic and non-commercial use. In this notebook I will show how to run LKH for this competition in a kernel environment. For more information please refer to the official page or the original paper. Notes:  I'm not considering the prime penalty. Internet must be enabled.  Acknowledgments:  LKH author, Keld Helsgaun.;Apache 2.0;https://www.kaggle.com/jsaguiar/lkh-solver;0.2;[];['ner', 'ai', 'dl', 'nn', 'ann'];[];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.721;0.473;2020-12-13 17:50:10;Traveling Santa 2018 - Prime Paths;['optimization'];LKH Solver;Python notebook;4915.0;71;1516780.69265;1516780.69265
2018-12-11 05:48:28;;Apache 2.0;https://www.kaggle.com/nagadomi/cpu-shuffle-numba;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.674;0.437;2020-12-13 17:50:10;multiple data sources;[];riffle riffle (numba);Python script;1768.0;45;1516705.08336;1516705.08336
2018-12-14 20:12:24;;Apache 2.0;https://www.kaggle.com/nagadomi/gpu-shuffle-numba-cuda;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.704;0.48;2020-12-13 17:50:09;multiple data sources;['gpu'];GPU shuffle (numba.cuda);Python script;3351.0;77;1516693.58125;1516693.58125
2018-11-21 20:50:30;Greedy Reindeer - A Starter code to the Traveling Santa ProblemUsing a greedy algorithm to solve the problemPrime cities are taken into account;Apache 2.0;https://www.kaggle.com/theoviel/greedy-reindeer-starter-code;0.2;[];['ai'];[];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.725;0.484;2020-12-13 17:50:09;Traveling Santa 2018 - Prime Paths;['beginner'];Greedy Reindeer  - Starter code;Python notebook;5461.0;81;;
2018-11-23 20:29:46;0. Setup + ExplorationUsing sympy for primality test and prime finding functions. Installed package from GitHub repo jvkersch/pyconcorde for the Concorde TSP Solver. numpy and pandas for basic numerical and data processing needs.;Apache 2.0;https://www.kaggle.com/thexyzt/xyzt-s-visualizations-and-various-tsp-solvers;0.5;[];['dl', 'nn', 'ml', 'rl'];['label'];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.73;0.472;2020-12-13 17:50:10;Traveling Santa 2018 - Prime Paths;[];XYZT's Visualizations and Various TSP Solvers;Python notebook;6252.0;70;;
2018-11-21 15:59:42;This kernel hands off the cities to the very fast Concorde TSP solver Ignores the prime twist on this problem You must have https://github.com/jvkersch/pyconcorde installed in Kernels to run this;Apache 2.0;https://www.kaggle.com/wcukierski/concorde-solver;0.2;[];['ai'];[];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.774;0.543;2020-12-13 17:50:09;Traveling Santa 2018 - Prime Paths;[];Concorde solver ;Python notebook;19944.0;183;1533474.68695;1533474.68695
2018-11-22 13:30:38;;Apache 2.0;https://www.kaggle.com/zfturbo/scoring-function;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['classification', 'deep learning'];https://www.kaggle.com/c/traveling-santa-2018-prime-paths;0.702;0.447;2020-12-13 17:50:10;Traveling Santa 2018 - Prime Paths;[];Scoring function;Python script;3188.0;51;;
2020-05-31 05:47:56;;Apache 2.0;https://www.kaggle.com/grapestone5321/trec-covid-information-retrieval-sample-submission;0.2;[];['ai', 'nn'];[];https://www.kaggle.com/c/trec-covid-information-retrieval;0.642;0.214;2020-12-13 17:51:06;TREC-COVID Information Retrieval;[];TREC-COVID Information Retrieval-sample_submission;Python notebook;942.0;4;0.00000;0.00000
2020-06-01 01:46:37;;Apache 2.0;https://www.kaggle.com/khotijahs1/trec-covid-information-retrieval;1.0;['sklearn', 'h2o'];['ner', 'ai', 'nlu', 'dl', 'cnn', 'gbm', 'gan', 'nlg', 'cv', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['gru', 'filter', 'train', 'model', 'vgg', 'label'];https://www.kaggle.com/c/trec-covid-information-retrieval;0.66;0.236;2020-12-13 17:51:06;TREC-COVID Information Retrieval;[];TREC-COVID Information Retrieval;Python notebook;1315.0;5;0.00000;0.01808
2020-05-30 05:38:18;;Apache 2.0;https://www.kaggle.com/n3n77i/aggreg;0.2;[];['dl', 'ai', 'nn'];[];https://www.kaggle.com/c/trec-covid-information-retrieval;0.51;0.0;2020-12-13 17:51:06;multiple data sources;[];aggreg;Python script;116.0;0;;
2020-05-29 14:14:44;;Apache 2.0;https://www.kaggle.com/n3n77i/title-read;0.2;[];['dl', 'ai', 'nn'];[];https://www.kaggle.com/c/trec-covid-information-retrieval;0.523;0.099;2020-12-13 17:51:06;TREC-COVID Information Retrieval;[];title_read;Python script;139.0;1;;
2020-04-26 11:47:57;Intoduction;Apache 2.0;https://www.kaggle.com/eswarchandt/nilearn-visualizations-and-nn-baseline;0.5;[];['ner', 'ai', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/trends-assessment-prediction;0.655;0.411;2020-12-13 17:52:38;TReNDS Neuroimaging;[];Nilearn Visualizations and NN Baseline;Python notebook;1201.0;33;;
2020-04-29 14:06:57;Reading using h5py;Apache 2.0;https://www.kaggle.com/mks2192/reading-matlab-mat-files-and-eda;0.5;[];['ner', 'ai', 'nn'];['train', 'label', 'predict'];https://www.kaggle.com/c/trends-assessment-prediction;0.696;0.452;2020-12-13 17:52:38;TReNDS Neuroimaging;[];Reading Matlab (.mat) Files and EDA;Python notebook;2800.0;54;;
2020-05-11 16:20:57;TReNDS Neuroimaging - Data exploration;Apache 2.0;https://www.kaggle.com/rftexas/trends-eda-lightgbm-rapids-ai-svm;1.0;['lightgbm', 'h2o', 'sklearn', 'tensorflow', 'keras'];['ner', 'ai', 'gbm', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['machine learning', 'regression', 'neuron', 'train', 'model', 'layer', 'label', 'predict', 'understanding', 'labeled'];https://www.kaggle.com/c/trends-assessment-prediction;0.749;0.497;2020-12-13 17:52:38;multiple data sources;['gpu'];TReNDS: EDA, LightGBM, Rapids.AI SVM;Python notebook;10142.0;96;0.15995;0.15983
2020-09-19 17:22:53;Fetch brain development functional datasets;Apache 2.0;https://www.kaggle.com/soham1024/visualization-using-nilearn;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/trends-assessment-prediction;0.709;0.507;2020-12-13 17:52:38;multiple data sources;['gpu, beginner, data visualization'];visualization using nilearn;Python notebook;3781.0;111;;
2020-04-28 14:36:57;;Apache 2.0;https://www.kaggle.com/tarunpaparaju/trends-neuroimaging-2d-slice-resnet-pytorch;1.0;['pytorch', 'skimage'];['ner', 'ai', 'cv', 'rl', 'nn', 'ml'];['machine learning', 'test data', 'regression', 'neuron', 'train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'predict', 'relu', 'resnet', 'propagation'];https://www.kaggle.com/c/trends-assessment-prediction;0.726;0.471;2020-12-13 17:52:38;TReNDS Neuroimaging;[];TReNDS Neuroimaging: 2D Slice ResNet PyTorch ðŸ”¥ ;Python notebook;5656.0;69;0.18548;0.18648
2020-04-24 01:00:10;Library;Apache 2.0;https://www.kaggle.com/yasufuminakama/trends-lgb-baseline;1.0;['lightgbm', 'sklearn'];['ai', 'dl', 'gbm', 'cv', 'rl', 'nn'];['filter', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/trends-assessment-prediction;0.706;0.459;2020-12-13 17:52:38;TReNDS Neuroimaging;[];TReNDS / LGB baseline;Python notebook;3529.0;59;0.16292;0.16264
2020-04-10 00:00:16;;Apache 2.0;https://www.kaggle.com/abhishek/roberta-inference-5-folds;0.5;[];['ai', 'nn'];['filter', 'train', 'model', 'epoch', 'layer'];https://www.kaggle.com/c/tweet-sentiment-extraction;0.791;0.59;2020-12-13 17:53:52;multiple data sources;['gpu'];roberta inference 5 folds;Python notebook;34308.0;380;0.71470;0.71275
2020-04-16 22:38:23;Data Processing;Apache 2.0;https://www.kaggle.com/abhishek/super-duper-fast-pytorch-tpu-kernel;1.0;['pytorch', 'spacy', 'sklearn', 'pillow'];['ai', 'dl', 'rl', 'nlp', 'nn'];['filter', 'train', 'model', 'epoch', 'layer', 'loss'];https://www.kaggle.com/c/tweet-sentiment-extraction;0.75;0.551;2020-12-13 17:53:52;multiple data sources;['tpu'];Super-duper fast pytorch tpu kernel... ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥;Python notebook;10359.0;206;;
2020-05-05 06:20:29;A Quick Look at Selected Text Noise In this notebook, I am exploring the noise in the dataset. I analysed the selected_text column in the original dataset. I compared selected_text with the prediction of a reference model to find out why I am getting low performance in positive and negative sentiment predictions.  Reference ModelI am using prediction of a trained model (5 fold ensemble of a RoBERTa-Base with public LB 0.712) to find out anamolies. I can identify many of them by performing error analysis. In particular, when my Jaccard score is very low, many times it's due to the inherent noise in the data.  Can We Use It For Our Benefits?   If you find it useful, please upvote! Thank you! ðŸ”¥ N.B. I just listed a few noise. At the end of the notebook you can generate random samples to find more variety of noise.;Apache 2.0;https://www.kaggle.com/debanga/what-the-no-ise;0.5;[];['ner', 'ai', 'rl', 'nn', 'ann'];['train', 'model', 'ground truth', 'predict'];https://www.kaggle.com/c/tweet-sentiment-extraction;0.689;0.502;2020-12-13 17:53:52;multiple data sources;['beginner, exploratory data analysis'];What The No*ise? ðŸ˜†;Python notebook;2413.0;104;;
2020-04-23 20:38:19;Private test not that private afterall ðŸ™ˆHaving access to any information regarding the private test dataset is always useful. In this competition even more since semi-supervised learning may play a decisive role. After some Googling, I came across the (probably) initial dataset that has been used by the author of this (atypical) Kaggle challenge. In this notebook, starting from the found dataset, I will attempt to create the private test dataset and will propose some ideas on how we can use this data to enhance our models. Clean data I believe that this is quite an important discovery, as the found dataset has not been processed. In the Exploratory Data Analysis part, we will see how salient information such as hashtags and uncensured words are present in that dataset. Also, and maybe even more important, all original 13 sentiments are still present (empty, sadness, enthusiasm, neutral, worry, surprise, love, fun, hate, happiness, boredom, relief, anger). ðŸ’¡Ideas Among others, one of the main advantages we can have by using this dataset is to find a subset of the training data that better match the private dataset set and try to overfit a model to that data. Also, as we will see later, the extra informations such as hahtags, tweet authors and sentiment can be used during pre and post-processing, as they play an important role in this challenge. Data leakage As some of you correctly specified, we cannot says that this is data leakage as the original dataset is mentioned in the challenge description: The dataset is titled Sentiment Analysis: Emotion in Text tweets. Nontheless, the link (https://www.figure-eight.com/data-for-everyone/) they refers to is broken. The dataset is also available here in Kaggle: https://www.kaggle.com/icw123/emotion;Apache 2.0;https://www.kaggle.com/jonathanbesomi/private-test-not-that-private-afterall;0.5;[];['ner', 'ai', 'gan', 'rl', 'ml', 'nn', 'ann'];['test data', 'training data', 'train', 'model', 'sentiment analysis', 'supervised learning'];https://www.kaggle.com/c/tweet-sentiment-extraction;0.701;0.505;2020-12-13 17:53:52;Tweet Sentiment Extraction;[];Private test not that **private** afterall ;Python notebook;3103.0;108;;
2020-04-10 10:52:32;ðŸ“ Question-Answering Starter pack with ðŸ¤—transformers     In this notebook, by making use of  transformers we express the learning problem as a question-answering system.        The code and the notebook-format have been designed to be easy-to-understand for beginners but hopefully also useful for advanced Kagglers.        Any comment/feedback is very appreciated. Disclaimer: work in progress, I will add new resources and comments soon.;Apache 2.0;https://www.kaggle.com/jonathanbesomi/question-answering-starter-pack;0.5;[];['nlp', 'ai', 'nn', 'ner'];['predict', 'training data', 'train', 'model', 'epoch', 'deep learning', 'natural language processing', 'loss', 'understanding', 'natural language'];https://www.kaggle.com/c/tweet-sentiment-extraction;0.749;0.53;2020-12-13 17:53:52;multiple data sources;['beginner, nlp, learn'];ðŸ“ðŸ¤— Question-Answering Starter pack;Python notebook;10161.0;153;0.70050;0.69461
2020-03-29 10:36:36;;Apache 2.0;https://www.kaggle.com/ratan123/sentiment-extraction-understanding-metric-eda;1.0;['pytorch', 'spacy', 'nltk'];['ner', 'ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['filter', 'test data', 'generation', 'train', 'model', 'understanding', 'epoch', 'layer', 'predict', 'recommend'];https://www.kaggle.com/c/tweet-sentiment-extraction;0.722;0.518;2020-12-13 17:53:52;multiple data sources;['beginner, data visualization, text data'];Sentiment Extraction:Understanding metric+EDA;Python notebook;5094.0;128;0.00000;0.00000
2020-06-17 13:20:11;NLP Cheatsheet: Master NLP;Apache 2.0;https://www.kaggle.com/rftexas/nlp-cheatsheet-master-nlp;1.0;['pattern', 'vocabulary'];['ner', 'ai', 'dl', 'cnn', 'gan', 'rl', 'ml', 'nlp', 'nn', 'rnn', 'ann'];['unlabeled', 'filter', 'recurrent neural network', 'machine translation', 'predict', 'unsupervised learning', 'relu', 'gru', 'training data', 'neuron', 'train', 'lstm', 'activation function', 'recommend', 'classification', 'labeled', 'propagation', 'model', 'neural network', 'layer', 'loss', 'hidden layer', 'supervised learning', 'generation', 'deep learning', 'label', 'computer vision', 'natural language'];https://www.kaggle.com/c/tweet-sentiment-extraction;0.724;0.501;2020-12-13 17:53:52;multiple data sources;[];NLP Cheatsheet: Master NLP;Python notebook;5347.0;102;;
2020-06-05 18:18:47;Libraries;Apache 2.0;https://www.kaggle.com/shoheiazuma/tweet-sentiment-roberta-pytorch;1.0;['pytorch', 'sklearn'];['ai', 'nn'];['filter', 'train', 'model', 'epoch', 'loss', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/tweet-sentiment-extraction;0.759;0.546;2020-12-13 17:53:52;multiple data sources;['gpu'];Tweet Sentiment RoBERTa PyTorch;Python notebook;13260.0;191;0.71705;0.71493
2020-08-11 00:04:25;Tweet Sentiment Extraction;Apache 2.0;https://www.kaggle.com/vbmokin/tse2020-roberta-0-7175-prlb-outlier-analysis;1.0;['pattern', 'tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'cnn', 'cv', 'rl', 'nlp', 'nn', 'ann'];['training data', 'train', 'model', 'epoch', 'validation data', 'layer', 'clustering', 'loss', 'label', 'predict', 'rank', 'relu'];https://www.kaggle.com/c/tweet-sentiment-extraction;0.776;0.542;2020-12-13 17:53:52;multiple data sources;['gpu, data visualization, exploratory data analysis, +1 moreclassification'];TSE2020 - RoBERTa - 0.7175 PrLB, Outlier Analysis;Python notebook;21606.0;182;0.71446;0.71192
2017-02-19 19:46:55;;Apache 2.0;https://www.kaggle.com/aikinogard/random-forest-starter-with-numerical-features;1.0;['sklearn'];['ai', 'nn'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.772;0.536;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];Random Forest Starter with numerical features;Python notebook;19116.0;165;;
2017-03-08 09:22:39;;Apache 2.0;https://www.kaggle.com/brandenkmurray/it-is-lit;1.0;['xgboost'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.76;0.501;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];it is lit;R script;13386.0;102;0.53971;0.54001
2017-02-09 14:54:57;How To Correctly Load Data into RThere's a number of people asking how to load the listing data into R. There's a few incorrect solutions posted, so I want to clear this up. The mistake everyone is making when parsing the JSON is that they are expecting each variable in each row to contain just one object and are trying to parse the JSON as such, but this is not true. Each listing has only one price and one street_address, but can have many photos and features. This kernel will correctly load the data into a tibble using list-columns for the photos and features variables. The purrr package is extremely well suited for working with data like this and I recommend anyone who has not seen it before to check it out.;Apache 2.0;https://www.kaggle.com/danjordan/how-to-correctly-load-data-into-r;0.5;[];['dl', 'ai', 'nn'];['train', 'recommend'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.745;0.482;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];How to *Correctly* Load Data into R;R notebook;8967.0;79;;
2017-02-15 00:46:12;"Hello there, everyone.  I did a brief analysis on the ""managers"" since at first glance the average ""interest level"" seemed to differ substantially from one to another . Anyway, let me know what you think about it and like this notebook if you enjoyed reading it (it's my 1st one, be nice :D)";Apache 2.0;https://www.kaggle.com/den3b81/do-managers-matter-some-insights-on-manager-id;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['training data', 'test data', 'train', 'model', 'predict', 'rank'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.708;0.453;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];"Do managers matter? Some insights on ""manager_id""";Python notebook;3652.0;55;;
2017-02-11 21:48:32;;Apache 2.0;https://www.kaggle.com/enrique1500/rental-listing-ny-map;0.5;[];['ner', 'ai', 'nlp', 'ml'];['training data', 'regression', 'train', 'computer vision', 'recommend', 'sentiment analysis'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.734;0.492;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];Rental Listing NY Map;Rmarkdown script;6803.0;90;;
2017-08-14 06:42:03;This's notebook for CV statistics;Apache 2.0;https://www.kaggle.com/guoday/cv-statistics-better-parameters-and-explaination;1.0;['xgboost', 'sklearn'];['ai', 'dl', 'rl', 'cv', 'ml'];['predict', 'train', 'fitting', 'model', 'label', 'loss'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.766;0.511;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];CV statisticsï¼ˆBetter parameters and  explainationï¼‰;Python notebook;15953.0;117;0.53583;0.53591
2017-03-10 19:07:04;Processing and deduplicating Features;Apache 2.0;https://www.kaggle.com/jxnlco/deduplicating-features;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl'];['train'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.711;0.449;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];Deduplicating  Features ;Python notebook;3937.0;52;;
2017-10-05 09:11:17;Data Exploration and Feature Engeenering StarterTwo Sigma Connect   Renthop CompetitionSergei Neviadomski;Apache 2.0;https://www.kaggle.com/neviadomski/data-exploration-two-sigma-renthop;0.5;[];['ner', 'ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.747;0.494;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];Data Exploration Two Sigma Renthop;Python notebook;9593.0;93;;
2017-05-02 21:42:50;IntroductionPoonam Ligade 9th Feb 2017 In this competition we are trying to classify interest levels of rental listings into high , low or medium. In this mainly we will look at data exploration and visulization part EDA is often most tedious and boring job. But the more time you spend here on understanding, cleaning and preparing data the better fruits your predictive model will bare!! Lets start. 1) Introduction  Import Libraries Load data Variable Identification Run Statistical summaries Correlations  2) Visualisation  Univariate Analysis Bivariate Analysis  3) Feature engineering;Apache 2.0;https://www.kaggle.com/poonaml/two-sigma-renthop-eda;0.5;[];['ner', 'ai', 'nn', 'ml'];['train', 'model', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.767;0.525;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];Two Sigma RentHop EDA;Python notebook;16509.0;142;;
2017-04-02 13:22:51;;Apache 2.0;https://www.kaggle.com/rakhlin/another-python-version-of-it-is-lit-by-branden;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.728;0.44;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];another Python version of It is lit by Branden;Python script;5914.0;47;;
2017-03-16 10:08:21;;Apache 2.0;https://www.kaggle.com/robertoruiz/feature-engineering-1-sentiment-analysis;0.5;[];['ai', 'dl'];['machine learning', 'regression', 'train', 'model', 'linear regression', 'sentiment analysis', 'classification'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.739;0.475;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;['linguistics'];Feature engineering 1: Sentiment analysis;Rmarkdown script;7775.0;72;;
2017-03-17 20:02:43;;Apache 2.0;https://www.kaggle.com/stanislavushakov/python-version-of-it-is-lit-by-branden;1.0;['vocabulary', 'xgboost', 'sklearn'];['ner', 'ai', 'dl', 'cv', 'nlp', 'nn', 'ml'];['train', 'fitting', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.739;0.437;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];Python version of It is lit by Branden;Python script;7675.0;45;0.54179;0.54122
2017-02-10 04:04:06;In this exploration notebook, we shall try to uncover the basic information about the dataset which will help us build our models / features. Let us first import the necessary modules.;Apache 2.0;https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-2-connect;0.5;[];['ner', 'ai', 'rl'];['test data', 'training data', 'train', 'model', 'label'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.792;0.601;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;['data visualization, exploratory data analysis'];Simple Exploration Notebook - 2Ïƒ Connect;Python notebook;35176.0;455;;
2017-02-10 00:58:31;Browsing the data, I think the 'description', 'feature' and 'photo' columns should contain pretty valuable information, but they are a little bit hard to cope with. Here I tried to deal with 'feature' Let's play with the renthop data.;Apache 2.0;https://www.kaggle.com/ygtcrt/how-to-deal-with-features-in-renthop-data;0.5;[];['ai', 'rl'];['train', 'filter'];https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries;0.717;0.446;2020-12-13 17:56:15;Two Sigma Connect: Rental Listing Inquiries;[];How to deal with features in Renthop data?;R notebook;4551.0;50;;
2016-12-02 17:17:40;;Apache 2.0;https://www.kaggle.com/bguberfain/univariate-model-with-clip;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'regression', 'train', 'model', 'reward', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.735;0.421;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];Univariate model with clip;Python script;7010.0;37;;
2016-11-29 16:48:53;Introduction to the kagglegym API;Apache 2.0;https://www.kaggle.com/jeffmoser/kagglegym-api-overview;0.5;[];['ai', 'rl'];['training data', 'train', 'model', 'reward', 'reinforcement learning', 'predict', 'ground truth'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.797;0.5;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];Kaggle Gym API Overview;Python notebook;41659.0;100;;
2017-02-25 21:41:24;;Apache 2.0;https://www.kaggle.com/optimism/two-sigma-playground-clarified-and-criticised;1.0;['sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn', 'ml'];['regression', 'train', 'fitting', 'model', 'reward', 'deep learning', 'layer', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.703;0.418;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];Two sigma playground clarified and criticised;Python script;3287.0;36;;
2016-12-12 15:33:09;Correlations only make sense when both time series are stationary. This analysis shows two things:  features are non stationary and require differencing. correlations are different for different ids.;Apache 2.0;https://www.kaggle.com/priyaranade/differencing-and-stationarity;0.5;[];['dl', 'ai', 'nn'];['train'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.715;0.425;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];Differencing and stationarity;Python notebook;4264.0;39;;
2016-12-27 13:42:36;This notebook is dedicated to visualizing variables distribution shift in time. Distribution shift is measured with correlation between overall ant timestamp distribution of each variable. I hope this might be useful for someone, as it is clear that distribution of variables change in time. Some timestamp outliers in training dataset can be seen as well.;Apache 2.0;https://www.kaggle.com/raddar/variables-shifting-distributions-in-time;0.5;[];['ai'];['training data', 'regression', 'train', 'model', 'predict', 'rank'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.724;0.421;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];Variables shifting distributions in time;R notebook;5394.0;37;;
2016-12-04 11:17:30;;Apache 2.0;https://www.kaggle.com/slothouber/kagglegym-emulation;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'reward', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.749;0.515;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];kagglegym_emulation;Python script;10090.0;123;;
2016-12-09 10:55:42;Simple notebook to explore the variables present in the dataset. Please upvote if you find it useful :);Apache 2.0;https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-5;0.5;[];['dl', 'ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.793;0.571;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;['data visualization, exploratory data analysis'];Simple Exploration Notebook;Python notebook;36008.0;279;;
2017-03-02 22:32:44;;Apache 2.0;https://www.kaggle.com/titericz/team-rocket-13;1.0;['xgboost', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'nlp', 'nn'];['train', 'fitting', 'model', 'reward', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.757;0.479;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];Team Rocket #13;Python script;12306.0;76;;
2017-03-01 17:01:29;;Apache 2.0;https://www.kaggle.com/tks0123456789/xgb-500-600-001;1.0;['xgboost'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'reward', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.718;0.421;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];xgb_500_600_001;Python script;4665.0;37;0.02637;0.00608
2017-02-24 18:59:48;;Apache 2.0;https://www.kaggle.com/trottefox/two-sigma-playground;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['test data', 'regression', 'generation', 'train', 'fitting', 'model', 'reward', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.758;0.497;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];Two sigma playground;Python script;12985.0;96;0.01812;0.01403
2017-03-05 00:21:26;;Apache 2.0;https://www.kaggle.com/willieliao/et1-ridge3-med-adj;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'regression', 'train', 'model', 'reward', 'deep learning', 'predict', 'linear regression', 'classification'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.71;0.418;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];et1_ridge3_med_adj;Python script;3807.0;36;0.03127;0.01837
2016-12-20 17:42:07;;Apache 2.0;https://www.kaggle.com/ymcdull/ridge-lb-0-0100659;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'reward', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-financial-modeling;0.757;0.45;2020-12-13 18:01:01;Two Sigma Financial Modeling Challenge;[];Ridge (LB: 0.0100659);Python script;12489.0;53;-0.00793;0.00865
2018-09-26 17:45:04;;Apache 2.0;https://www.kaggle.com/chocozzz/two-sigma-news-simple-eda-prophet-nlp;1.0;['statsmodels', 'lightgbm', 'sklearn', 'pattern', 'vocabulary'];['ner', 'ai', 'dl', 'gbm', 'gan', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'generation', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/two-sigma-financial-news;0.757;0.491;2020-12-13 18:02:27;Two Sigma: Using News to Predict Stock Movements;['beginner, exploratory data analysis'];Two Sigma News : Simple EDA + Prophet + NLP;Python notebook;12311.0;89;;
2018-11-14 20:13:26;Cleaning up market data;Apache 2.0;https://www.kaggle.com/danielson/cleaning-up-market-data-errors-and-stock-splits;1.0;['pattern'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['filter', 'predict', 'training data', 'train', 'fitting', 'model', 'loss'];https://www.kaggle.com/c/two-sigma-financial-news;0.746;0.519;2020-12-13 18:02:27;Two Sigma: Using News to Predict Stock Movements;[];Cleaning Up Market Data Errors (and Stock Splits);Python notebook;9258.0;131;;
2018-10-03 21:41:20;"August 2019 Update: this competition is closed and is no longer accepting submissions. Thanks for participating!  Two Sigma Financial News Competition Official Getting Started KernelIntroductionIn this competition you will predict how stocks will change based on the market state and news articles.  You will loop through a long series of trading days; for each day, you'll receive an updated state of the market, and a series of news articles which were published since the last trading day, along with impacted stocks and sentiment analysis.  You'll use this information to predict whether each stock will have increased or decreased ten trading days into the future.  Once you make these predictions, you can move on to the next trading day. This competition is different from most Kaggle Competitions in that:  You can only submit from Kaggle Kernels, and you may not use other data sources, GPU, or internet access. This is a two-stage competition.  In Stage One you can edit your Kernels and improve your model, where Public Leaderboard scores are based on their predictions relative to past market data.  At the beginning of Stage Two, your Kernels are locked, and we will re-run your Kernels over the next six months, scoring them based on their predictions relative to live data as those six months unfold. You must use our custom kaggle.competitions.twosigmanews Python module.  The purpose of this module is to control the flow of information to ensure that you are not using future data to make predictions for the current trading day.  In this Starter Kernel, we'll show how to use the twosigmanews module to get the training data, get test features and make predictions, and write the submission file.TL;DR: End-to-End Usage Example from kaggle.competitions import twosigmanews env = twosigmanews.make_env()  (market_train_df, news_train_df) = env.get_training_data() train_my_model(market_train_df, news_train_df)  for (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days():   predictions_df = make_my_predictions(market_obs_df, news_obs_df, predictions_template_df)   env.predict(predictions_df)  env.write_submission_file() Note that train_my_model and make_my_predictions are functions you need to write for the above example to work.";Apache 2.0;https://www.kaggle.com/dster/two-sigma-news-official-getting-started-kernel;0.5;[];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['training data', 'train', 'model', 'predict', 'sentiment analysis'];https://www.kaggle.com/c/two-sigma-financial-news;0.834;0.645;2020-12-13 18:02:27;Two Sigma: Using News to Predict Stock Movements;['beginner, finance'];Two Sigma News Official Getting Started Kernel;Python notebook;156013.0;987;0.02315;0.02315
2018-10-23 16:15:05;Import packages;Apache 2.0;https://www.kaggle.com/kazuokiriyama/tuning-hyper-params-in-lgbm-achieve-0-66-in-lb;1.0;['xgboost', 'sklearn', 'lightgbm'];['ai', 'dl', 'gbm', 'rl', 'nn'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/two-sigma-financial-news;0.739;0.464;2020-12-13 18:02:27;Two Sigma: Using News to Predict Stock Movements;['beginner'];Tuning hyper-params in LGBM achieve >0.66 in LB;Python notebook;7766.0;63;0.65787;0.65787
2018-09-27 22:23:43;In this notebook I'll try to explore the basic information about the dataset to help us build our models / features. Data descriptionEach asset is identified by an assetCode (note that a single company may have multiple assetCodes). Depending on what you wish to do, you may use the assetCode, assetName, or time as a way to join the market data to news data. The marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:  Returns are always calculated either open-to-open (from the opening time of one trading day to the open of another) or close-to-close (from the closing time of one trading day to the open of another). Returns are either raw, meaning that the data is not adjusted against any benchmark, or market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument. Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons. Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking.;Apache 2.0;https://www.kaggle.com/pestipeti/simple-eda-two-sigma;0.5;[];['dl', 'ai', 'nn', 'ml'];['filter', 'training data', 'train', 'model', 'label'];https://www.kaggle.com/c/two-sigma-financial-news;0.757;0.52;2020-12-13 18:02:27;Two Sigma: Using News to Predict Stock Movements;['beginner, exploratory data analysis'];Simple EDA - Two Sigma;Python notebook;12616.0;133;;
2018-11-19 04:37:43;;Apache 2.0;https://www.kaggle.com/qqgeogor/eda-script-67;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'nn'];['regression', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/two-sigma-financial-news;0.779;0.558;2020-12-13 18:02:27;Two Sigma: Using News to Predict Stock Movements;[];eda script 67;Python notebook;23512.0;231;;
2018-11-11 01:54:31;;Apache 2.0;https://www.kaggle.com/rabaman/0-64-in-100-lines;1.0;['lightgbm'];['ner', 'ai', 'dl', 'gbm', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'model', 'validation data', 'deep learning', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/two-sigma-financial-news;0.72;0.46;2020-12-13 18:02:27;Two Sigma: Using News to Predict Stock Movements;[];TS News LightGBM;Python script;4820.0;60;0.64888;0.64888
2018-09-27 19:24:21;Background;Apache 2.0;https://www.kaggle.com/youhanlee/simple-quant-features-using-python;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'nn', 'ann'];['train', 'model', 'filter'];https://www.kaggle.com/c/two-sigma-financial-news;0.772;0.527;2020-12-13 18:02:27;Two Sigma: Using News to Predict Stock Movements;[];Simple quant features using python;Python notebook;19106.0;147;;
2019-03-20 10:08:34;;Apache 2.0;https://www.kaggle.com/zikazika/predicting-stock-movement;1.0;['xgboost', 'lightgbm', 'nltk', 'sklearn', 'tensorflow'];['ner', 'ai', 'dl', 'gbm', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['train', 'model', 'reward', 'label', 'gradient boosting', 'predict', 'understanding', 'bayesian'];https://www.kaggle.com/c/two-sigma-financial-news;0.72;0.504;2020-12-13 18:02:27;Two Sigma: Using News to Predict Stock Movements;[];Predicting stock movement;Python notebook;4841.0;106;;
2017-04-01 20:57:28;;Apache 2.0;https://www.kaggle.com/aagundez/tensorflow-cnn;1.0;['tensorflow'];['ai', 'nn', 'cv'];['train', 'loss', 'relu', 'layer'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.724;0.253;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];TensorFlow CNN;Python notebook;5314.0;6;;
2016-07-25 20:35:02;;Apache 2.0;https://www.kaggle.com/agalea91/mislabeled-training-images;0.5;[];['ai', 'cv'];['train', 'label'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.74;0.379;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Mislabeled training images;Python notebook;7935.0;23;;
2016-05-20 16:11:09;;Apache 2.0;https://www.kaggle.com/alexlzzz/rl-encoding;0.5;[];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.734;0.379;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Run-Length Encoding in Python;Python script;6866.0;23;;
2016-05-19 21:08:43;All our training data is in one column, titled pixels. The Data page states: train_masks.csv gives the training image masks in run-length encoded format. So, we need to segment this data into something we can plot!;Apache 2.0;https://www.kaggle.com/anokas/quick-data-analysis;0.5;[];['ai', 'cv', 'ml', 'nn', 'ann'];['train', 'label', 'training data'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.729;0.387;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Exploratory data analysis;Python notebook;6029.0;25;;
2016-07-06 20:24:01;This notebook presents an image augmentation method that uses both local distortion and random affine transformation. These transformations uses anti-aliasing for high-quality output.;Apache 2.0;https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation;0.5;[];['ai', 'nn', 'ann', 'cv'];['filter', 'train', 'recognition', 'neural network', 'convolutional neural network'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.797;0.476;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Elastic Transform for Data Augmentation;Python notebook;41653.0;73;;
2016-05-25 14:41:23;;Apache 2.0;https://www.kaggle.com/bguberfain/ellipse-fit;0.5;[];['ai', 'cv'];['train'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.728;0.352;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Ellipse fit;Python notebook;5948.0;17;;
2016-06-01 00:25:30;;Apache 2.0;https://www.kaggle.com/chefele/all-masks-average;0.5;[];['ai'];['train'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.646;0.268;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Average Mask for each Patient;Python notebook;1022.0;7;;
2016-05-25 19:34:53;;Apache 2.0;https://www.kaggle.com/chefele/animated-images-with-outlined-nerve-area;0.5;[];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.777;0.482;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Animated Images with Outlined Nerve Area;Python script;22324.0;79;;
2016-05-21 00:55:52;;Apache 2.0;https://www.kaggle.com/chefele/plot-images-overlaid-with-mask;0.5;[];['ner', 'ai', 'cv', 'nn', 'ann'];['train'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.765;0.431;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Training Images plus Nerve Area Outline;Python notebook;15632.0;42;;
2016-05-27 05:07:37;;Apache 2.0;https://www.kaggle.com/dchen71/overlay-nerve-visualization;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.718;0.268;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Overlay Nerve Visualization;R script;4649.0;7;;
2016-08-06 16:20:18;;Apache 2.0;https://www.kaggle.com/hexietufts/easy-to-use-keras-imagedatagenerator;1.0;['keras', 'theano'];['ner', 'ai', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['image segmentation', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.809;0.429;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Easy to use Keras ImageDataGenerator;Python script;61379.0;41;;
2016-08-11 09:05:56;;Apache 2.0;https://www.kaggle.com/rakhlin/fast-run-length-encoding-python;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.777;0.458;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Fast Run-Length Encoding (Python);Python script;21850.0;58;;
2019-04-25 01:16:29;;Apache 2.0;https://www.kaggle.com/tanlikesmath/ultrasound-nerve-segmentation-with-fastai;1.0;['pytorch'];['ner', 'ai', 'dl', 'ml'];['filter', 'train', 'model', 'epoch', 'label', 'loss', 'rank', 'resnet'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.705;0.311;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;['gpu'];Ultrasound Nerve Segmentation with fastai;Python notebook;3458.0;11;;
2016-05-25 14:35:50;;Apache 2.0;https://www.kaggle.com/vagrawal/all-masks-average;0.5;[];['ai'];['train'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.716;0.327;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];All masks average;Python notebook;4431.0;13;;
2016-05-19 18:36:26;;Apache 2.0;https://www.kaggle.com/wcukierski/example-run-length-encoding-function;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.712;0.311;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Example run-length encoding function;R script;4060.0;11;;
2016-05-19 15:50:42;;Apache 2.0;https://www.kaggle.com/wcukierski/plot-images-1;0.5;[];['ai'];['train'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.696;0.281;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Plot images;Python notebook;2789.0;8;;
2016-06-07 00:32:30;;Apache 2.0;https://www.kaggle.com/zfturbo/keras-is-there-any-nerve;1.0;['keras', 'sklearn', 'theano'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn'];['test data', 'train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/ultrasound-nerve-segmentation;0.77;0.379;2020-12-13 18:06:51;Ultrasound Nerve Segmentation;[];Keras (Is there any nerve?);Python script;18199.0;23;0.56776;0.55894
2019-11-02 15:50:02;;Apache 2.0;https://www.kaggle.com/aleksandradeis/understanding-clouds-eda;1.0;['albumentations', 'opencv-python', 'pillow'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['training data', 'test data', 'train', 'label', 'understanding', 'labeled'];https://www.kaggle.com/c/understanding_cloud_organization;0.742;0.53;2020-12-13 18:07:53;Understanding Clouds from Satellite Images;['data visualization, exploratory data analysis, image data'];Understanding Clouds EDA;Python notebook;8400.0;152;;
2018-11-21 12:49:43;;Apache 2.0;https://www.kaggle.com/arpitsinha/predict-grant-applications;1.0;['sklearn'];['ai', 'nn'];['regression', 'train', 'model', 'predict', 'classification'];https://www.kaggle.com/c/unimelb;0.624;0.0;2020-12-13 18:08:21;Predict Grant Applications;[];Predict Grant Applications;Python notebook;684.0;0;;
2018-11-21 13:21:45;;Apache 2.0;https://www.kaggle.com/enaveenkumar/grant;1.0;['sklearn'];['ner', 'ai', 'nn', 'ml'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/unimelb;0.6;0.0;2020-12-13 18:08:21;Predict Grant Applications;[];Grant;Python notebook;451.0;0;;
2018-11-23 07:14:49;;Apache 2.0;https://www.kaggle.com/hari141091/predict-grant-applications-1121;1.0;['sklearn'];['ai', 'nn'];['filter', 'test data', 'regression', 'train', 'model', 'predict'];https://www.kaggle.com/c/unimelb;0.607;0.0;2020-12-13 18:08:21;Predict Grant Applications;[];Predict Grant Applications 1121;Python notebook;505.0;0;;
2020-06-11 07:48:10;;Apache 2.0;https://www.kaggle.com/oleg348/grant-predict;1.0;['sklearn'];['ai', 'nn', 'ann', 'cv'];['regression', 'train', 'model', 'loss', 'predict', 'classification'];https://www.kaggle.com/c/unimelb;0.499;0.0;2020-12-13 18:08:21;Predict Grant Applications;[];grant_predict;Python notebook;99.0;0;;
2018-12-25 15:39:21;my first edaPlease tell me if I make mistake Contents metadata_train/test.csv  Overview check null check target check metadata   train/test.parquet train.parquet Overview check waves   test.parquet;Apache 2.0;https://www.kaggle.com/go1dfish/basic-eda;0.5;[];['ann', 'ai', 'nn', 'ml'];['train', 'model', 'label', 'test data'];https://www.kaggle.com/c/vsb-power-line-fault-detection;0.724;0.48;2020-12-13 18:09:07;VSB Power Line Fault Detection;['beginner, exploratory data analysis'];basic EDA;Python notebook;5354.0;77;;
2019-01-08 23:41:21;;Apache 2.0;https://www.kaggle.com/jackvial/dwt-signal-denoising;1.0;['statsmodels', 'pattern'];['dl', 'ai', 'nn'];['train', 'model', 'filter'];https://www.kaggle.com/c/vsb-power-line-fault-detection;0.768;0.518;2020-12-13 18:09:07;VSB Power Line Fault Detection;['data visualization, data cleaning, signal processing'];DWT Signal Denoising;Python notebook;16992.0;129;;
2018-12-20 18:30:35;This kernel will show you how to load complete parquet files into Pandas and how to read just a subset of the data.;Apache 2.0;https://www.kaggle.com/sohier/reading-the-data-with-python;0.5;[];['ner', 'ai', 'dl'];['train'];https://www.kaggle.com/c/vsb-power-line-fault-detection;0.762;0.513;2020-12-13 18:09:07;VSB Power Line Fault Detection;[];Reading the Data with Python;Python notebook;14452.0;120;;
2019-01-28 11:27:29;Fast Fourier Transform & DenoisingIn this kernel, I briefly introduces two ways to perform denoising :  Using an averaging  smoothing technique Using the FFT  Enjoy!;Apache 2.0;https://www.kaggle.com/theoviel/fast-fourier-transform-denoising;0.5;[];['ai'];['train', 'label', 'filter'];https://www.kaggle.com/c/vsb-power-line-fault-detection;0.757;0.489;2020-12-13 18:09:07;VSB Power Line Fault Detection;[];Fast Fourier Transform & Denoising;Python notebook;12400.0;87;;
2018-12-24 04:55:50;Signal processing with ScipyBelow, we will explore different methods for processing and cleaning signal with Scipy. I haven't worked extensively with signal processing, so if there is any expert out there who would have insight or suggestion in improving this work, please let me know! About ScipySciPy (pronounced â€œSigh Pieâ€) is open-source software for mathematics, science, and engineering.   Scipy is an extremely useful library for scientific and numerical computing in Python. It contains very useful submodules for Optimization, Fast Fourier Transform, Linear Algebra, Matrix Encoding, and Image Processing. In fact, Scikit-learn uses it extensively for maniputating large sparse matrix, and for algorithms such as Ordinary Least Square! We will focus on the signal processing submodule for this exploration notebook. This is a work in progress, so please tune in for more updates! References: Loading data (signal and target): https://www.kaggle.com/theoviel/fast-fourier-transform-denoising Read data: https://www.kaggle.com/sohier/reading-the-data-with-python  Recommendations:FFT Kernel (We won't cover this since the following notebook cover it extensively):  https://www.kaggle.com/theoviel/fast-fourier-transform-denoising  Official Scipy Tutorial on Signal Processing:  https://docs.scipy.org/doc/scipy/reference/tutorial/signal.html;Apache 2.0;https://www.kaggle.com/xhlulu/exploring-signal-processing-with-scipy;0.5;[];['ai', 'ml'];['filter', 'train', 'model', 'label', 'recommend'];https://www.kaggle.com/c/vsb-power-line-fault-detection;0.721;0.397;2020-12-13 18:09:07;VSB Power Line Fault Detection;[];Exploring Signal Processing with Scipy;Python notebook;4945.0;28;;
2018-12-28 13:58:18;Analyze Power Line Signal Like a PhysicistWe read and explore the data, especially the labels. Then, we focus on the frequency domain. Digital filtering with infinitely fast roll-off (sharp cutoff) is demonstrated.  Statistician likes to use moving average and other smoothing techniques, which are basically low-pass filters with very slow roll-off. Engineers prefer more realistic filters with finite roll-off, because they have to implement filter in the real world. That is why scipy.signal provides an array of different filters. If you are a physicist who doesn't care about real world, why don't we just filter with infinitely fast roll-off? Before we begin, I would like to thank https://www.kaggle.com/xhlulu/exploring-signal-processing-with-scipy and the host: https://www.kaggle.com/sohier/reading-the-data-with-python;Apache 2.0;https://www.kaggle.com/zoujie/analyze-power-line-signals-like-a-physicist;0.5;[];['ner', 'ai', 'rl', 'ml', 'nn', 'ann'];['train', 'label', 'filter'];https://www.kaggle.com/c/vsb-power-line-fault-detection;0.692;0.425;2020-12-13 18:09:07;VSB Power Line Fault Detection;['data visualization, exploratory data analysis, data cleaning'];Analyze Power Line Signals Like a Physicist;Python notebook;2589.0;39;;
2019-05-02 19:07:27;;Apache 2.0;https://www.kaggle.com/abhishekshambhu/walmartsales1;1.0;['sklearn'];['ai', 'nn', 'ml', 'cv'];['regression', 'train', 'model', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.667;0.281;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;[];Walmartsales1;Python notebook;1526.0;8;;
2018-11-16 02:42:28;Data exploration;Apache 2.0;https://www.kaggle.com/andredornas/tp2-walmart-sales-forecast;1.0;['sklearn'];['ai', 'nn', 'ml', 'cv'];['train', 'model', 'layer', 'label', 'predict', 'relu'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.799;0.511;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;[];TP2 - Walmart Sales Forecast;Python notebook;44602.0;117;3406.24879;3269.67473
2020-02-10 11:00:07;Checking the given files;Apache 2.0;https://www.kaggle.com/aswathikv/walmartrecruiting;1.0;['sklearn'];['ner', 'ai', 'nn'];['regression', 'train', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.683;0.357;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;[];WalmartRecruiting;Python notebook;2103.0;18;;
2020-04-03 03:50:30;Author: Caio Avelino LinkedIn Kaggle;Apache 2.0;https://www.kaggle.com/avelinocaio/walmart-store-sales-forecasting;1.0;['sklearn'];['ner', 'ai', 'cv', 'nn', 'ann'];['filter', 'machine learning', 'test data', 'train', 'fitting', 'model', 'layer', 'label', 'predict', 'decision tree', 'random forest'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.759;0.462;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;['data visualization, random forest, regression'];Walmart - Store Sales Forecasting;Python notebook;13165.0;61;2699.17571;2684.15209
2019-12-03 15:11:28;;Apache 2.0;https://www.kaggle.com/balaji03/walmart-store-sale;1.0;['catboost', 'xgboost', 'sklearn'];['ner', 'ai', 'cv', 'nn', 'ann'];['train', 'model', 'regression', 'predict'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.645;0.236;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;['gpu'];Walmart Store Sale;Python notebook;1004.0;5;3269.08325;3062.45208
2019-02-07 13:05:47;Let's explore temperature;Apache 2.0;https://www.kaggle.com/bnorbert/eda-walmart;0.5;[];['ai', 'nn', 'ml'];['train', 'label'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.685;0.236;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;[];eda_walmart;Python notebook;2195.0;5;;
2020-06-12 22:13:07;Imports;Apache 2.0;https://www.kaggle.com/caesarlupum/walmart-store-sales-forecasting-anomaly-analysis;1.0;['statsmodels', 'xgboost', 'lightgbm', 'sklearn', 'pattern'];['ner', 'ai', 'dl', 'gbm', 'rl', 'ml', 'nn', 'ann'];['filter', 'test data', 'regression', 'train', 'model', 'label', 'predict', 'decision tree', 'random forest'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.669;0.34;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;['beginner'];Walmart-Store Sales Forecasting& Anomaly Analysis;Python notebook;1587.0;15;;
2020-04-11 09:19:54;Overview;Apache 2.0;https://www.kaggle.com/cmcoutosilva/zedelivery-python;1.0;['pattern', 'sklearn'];['ner', 'ai', 'gan', 'cv', 'ml', 'nn', 'ann'];['linear regression', 'filter', 'machine learning', 'regression', 'training data', 'train', 'test data', 'model', 'random forest', 'understanding', 'label', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.612;0.214;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;[];zedelivery_Python;Python notebook;552.0;4;19003.32665;18604.82365
2019-01-08 19:52:29;;Apache 2.0;https://www.kaggle.com/naresh31/walmart-recruiting-store-sales-forecasting;0.5;[];['ai', 'nn'];['train', 'filter'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.74;0.371;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;[];Walmart Recruiting - Store Sales Forecasting;Python notebook;7972.0;21;;
2019-12-13 18:41:48;;Apache 2.0;https://www.kaggle.com/sabinhashmi/walmart-sales-forecast;1.0;['sklearn'];['ner', 'ai', 'nn', 'cv'];['regression', 'train', 'model', 'label', 'predict', 'random forest'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.65;0.319;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;['gpu'];Walmart Sales Forecast;Python notebook;1098.0;12;;
2020-07-10 12:13:16;Walmart Recruiting Store Sales Forecastinghttps://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;Apache 2.0;https://www.kaggle.com/talestsp/forecasting;1.0;['sklearn'];['ner', 'ai', 'nn', 'rl'];['filter', 'machine learning', 'regression', 'test data', 'train', 'fitting', 'model', 'validation data', 'predict', 'linear regression'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.552;0.188;2020-12-13 18:10:03;multiple data sources;[];Forecasting;Python notebook;210.0;3;;
2019-09-09 07:35:29;;Apache 2.0;https://www.kaggle.com/yepp2411/walmart-prediction-1-eda-with-time-and-space;0.5;[];['ner', 'ai', 'nn', 'rl'];['test data', 'filter', 'train', 'label', 'predict'];https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting;0.718;0.397;2020-12-13 18:10:03;Walmart Recruiting - Store Sales Forecasting;[];Walmart prediction - (1) EDA with time and space;Python notebook;4638.0;28;;
2020-08-07 15:42:40;Walmart: Trip Type Classification;Apache 2.0;https://www.kaggle.com/aniketvishnu/wallmart-trip-type-classification-complete-model;1.0;['sklearn'];['ner', 'ai', 'rl', 'cv', 'nn'];['random forest', 'train', 'model', 'validation data', 'clustering', 'loss', 'label', 'predict', 'classification', 'ground truth'];https://www.kaggle.com/c/walmart-recruiting-trip-type-classification;0.652;0.188;2020-12-13 18:10:16;Walmart Recruiting: Trip Type Classification;['exploratory data analysis, random forest, multiclass classification, +1 morestatistical analysis'];Wallmart: trip type classification(complete model);Python notebook;1144.0;3;;
2020-09-22 11:31:57;;Apache 2.0;https://www.kaggle.com/dishask99/notebook8bb2428ec0;1.0;['sklearn'];['ai', 'dl', 'rl', 'nn', 'ml'];['train', 'model', 'layer', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/walmart-recruiting-trip-type-classification;0.536;0.0;2020-12-13 18:10:16;Walmart Recruiting: Trip Type Classification;[];notebook8bb2428ec0;Python notebook;165.0;0;;
2017-09-05 21:41:35;;Apache 2.0;https://www.kaggle.com/arjunsurendran/using-lstm-on-training-data;1.0;['tensorflow', 'sklearn', 'keras'];['ai', 'nn', 'rnn'];['train', 'fitting', 'model', 'input layer', 'output layer', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.781;0.456;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];Using LSTM on Training Data;Python notebook;24834.0;57;;
2017-07-17 14:32:01;Time series forecast of only one page with Facebook Prophet library. I quickly put this together,  my Pandas skills are not very good, so it may be improved.;Apache 2.0;https://www.kaggle.com/attollos/time-series-forecast-example-with-prophet;0.5;[];['ai', 'dl', 'ml', 'rl'];['train', 'label', 'predict'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.74;0.367;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];Time series forecast example with Prophet;Python notebook;7924.0;20;;
2017-09-12 13:25:17;;Apache 2.0;https://www.kaggle.com/chechir/weekend-flag-median-with-wiggle;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.713;0.383;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];weekend flag median with wiggle;Python script;4107.0;24;196.39284;196.39284
2017-07-18 13:51:29;;Apache 2.0;https://www.kaggle.com/clustifier/weekend-weekdays;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.724;0.418;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];weekend / weekdays;Python script;5398.0;36;;
2017-07-25 10:13:59;Let's understand how to minimize smape with constant values, like what is done in the best public kernels.  Is the median the right value? First let us define the smape function.  It is quite straightforward, the only caveat is to treat nan correctly.  Thanks to the official answers on the forum, we know we can use this code.  It handles the case where there are nan in the y_true array, but it assumes there are no nan in the y_pred array.;Apache 2.0;https://www.kaggle.com/cpmpml/smape-weirdness;0.5;[];['ai', 'dl', 'ml', 'nn', 'ann'];['gradient descent', 'predict'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.764;0.523;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;['beginner'];SMAPE Weirdness;Python notebook;15295.0;139;;
2017-08-03 16:19:08;;Apache 2.0;https://www.kaggle.com/gvyshnya/parallel-operations-over-a-pandas-df;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'training data', 'deep learning'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.737;0.367;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];Parallel operations over a Pandas DF;Python script;7428.0;20;;
2017-08-01 08:20:03;;Apache 2.0;https://www.kaggle.com/gvyshnya/prophet-class-wrapper;1.0;['sklearn'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.707;0.379;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];Prophet Class Wrapper;Python script;3572.0;23;;
2020-02-11 08:54:14;;Apache 2.0;https://www.kaggle.com/headsortails/wiki-traffic-forecast-exploration-wtf-eda;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'nn', 'ml'];['filter', 'training data', 'regression', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.816;0.602;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;['data visualization, exploratory data analysis'];Wiki Traffic Forecast Exploration - WTF EDA;Rmarkdown script;79682.0;463;;
2017-07-16 23:43:08;;Apache 2.0;https://www.kaggle.com/merckel/preliminary-investigation-holtwinters-arima;0.5;[];['ai', 'nn'];['filter', 'predict'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.73;0.4;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;['linguistics'];Preliminary Investigation: HoltWinters & ARIMA;Rmarkdown script;6215.0;29;;
2017-07-14 02:59:28;;Apache 2.0;https://www.kaggle.com/nigelcarpenter/page-median;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'training data', 'deep learning'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.698;0.405;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];Page median;R script;2939.0;31;;
2017-07-16 12:20:44;;Apache 2.0;https://www.kaggle.com/opanichev/simple-model;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'classification', 'deep learning'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.74;0.456;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];Simple Model;Python script;8010.0;57;0.00000;0.00000
2017-09-12 16:17:42;;Apache 2.0;https://www.kaggle.com/paulorzp/one-line-solution-2nd-stage-final;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.73;0.453;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];One Line Solution 2nd stage Final;Python script;6174.0;55;41.94324;41.94324
2017-09-07 15:29:34;This kernel started as a demonstration of the impact of rounding the prediction to integers using some realistic examples (in line with the opinions stated in the Discussion (https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/38727). In this version I used this cross-validation framework to explore the popular ideas (such as the use of the median of medians (MM) by Ehsan and the use of weekly seasonality by Clustifier (WK). The example as published here - a simple combination of both ideas - leads to LB 44.5 and with one small unpublished modification to 44.0 (top 3% as of yesterday). Both MM and WK are reasonably robust on all sets where I tested it  though I would not be suprised to see the score deteriorate to 46 in the future as it happened on some previous 60 days sets.;Apache 2.0;https://www.kaggle.com/rshally/web-traffic-cross-valid-round-and-wk-lb-44-5;0.5;[];['ai', 'cv'];['train', 'predict'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.704;0.4;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];Web Traffic: Cross-valid, Round and Wk LB 44.5;Python notebook;3374.0;29;;
2017-09-01 09:22:07;;Apache 2.0;https://www.kaggle.com/safavieh/median-estimation-by-fibonacci-et-al-lb-44-9;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.732;0.429;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;[];Median Estimation by Fibonacci et al. [LB 44.9];Python script;6429.0;41;0.00000;0.00000
2017-07-19 18:21:17;Time series forecast of only one page with Facebook Prophet library. I quickly put this together,  my Pandas skills are not very good, so it may be improved.;Apache 2.0;https://www.kaggle.com/tunguz/forecast-example-w-prophet-median;0.5;[];['ai', 'dl', 'ml', 'rl'];['train', 'label', 'predict'];https://www.kaggle.com/c/web-traffic-time-series-forecasting;0.726;0.421;2020-12-13 18:15:01;Web Traffic Time Series Forecasting;['beginner'];Forecast example w/ Prophet - median ;Python notebook;5535.0;37;;
2018-01-13 06:56:55;"Who can identify whale species in images?In this competition, we are given a multilabel classification problem, which is basically a problem where we have to decide, given an image, which labels does it belong to? Our task is as follows: ""For each Image in the test set, you may predict up to 5 labels for the whale Id""  In this notebook, we will:  Look at the actual images to get a first impression of the data Cluster the images according to their pixel intensities to find potentially formed groups Generate a baseline bernoulli sample submission  A standard approach to multilabel classification is to learn as many OVA (one vs all) models as there are distinct labels and then assign labels by the classifier output of each of the models, we'll get to that later. If this notebook earns your upvote, please upvote this :)";Apache 2.0;https://www.kaggle.com/andersy005/getting-started;0.5;[];['ner', 'ai', 'nn', 'ml'];['test data', 'train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/whale-categorization-playground;0.694;0.334;2020-12-13 18:17:20;Humpback Whale Identification Challenge;[];getting-started;Python notebook;2672.0;14;0.02730;0.02730
2018-01-14 12:02:31;Color or Gray;Apache 2.0;https://www.kaggle.com/cristianpb/using-similarity-of-train-and-test-for-predictions;0.5;[];['ai', 'nn', 'ann', 'cv'];['train', 'label', 'predict'];https://www.kaggle.com/c/whale-categorization-playground;0.656;0.236;2020-12-13 18:17:20;Humpback Whale Identification Challenge;['data visualization, animals'];Using similarity of train and test for predictions;Python notebook;1217.0;5;;
2018-01-15 22:24:11;;Apache 2.0;https://www.kaggle.com/CVxTz/beating-the-baseline-keras-lb-0-38;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'predict', 'recommend', 'resnet', 'classification'];https://www.kaggle.com/c/whale-categorization-playground;0.724;0.375;2020-12-13 18:17:20;Humpback Whale Identification Challenge;[];Beating the baseline - Keras ( lb 0.38);Python script;5286.0;22;;
2018-03-23 01:43:19;Importing the data;Apache 2.0;https://www.kaggle.com/gimunu/data-augmentation-with-keras-into-cnn;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai'];['filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu'];https://www.kaggle.com/c/whale-categorization-playground;0.794;0.485;2020-12-13 18:17:20;Humpback Whale Identification Challenge;['classification'];Data augmentation with keras into CNN;Python notebook;36997.0;82;0.32754;0.32754
2018-03-20 13:55:14;;Apache 2.0;https://www.kaggle.com/gimunu/training-augmentation-and-pretrained-vgg16-model;1.0;['tensorflow', 'sklearn', 'keras'];['nlp', 'ai', 'nn', 'ner'];['filter', 'regression', 'train', 'model', 'epoch', 'deep learning', 'layer', 'vgg', 'loss', 'label', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/whale-categorization-playground;0.646;0.214;2020-12-13 18:17:20;Humpback Whale Identification Challenge;[];Training augmentation and pretrained VGG16 model;Python script;1010.0;4;;
2018-01-10 17:41:23;;Apache 2.0;https://www.kaggle.com/inversion/most-common-id-benchmark;1.0;['skimage'];['ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/whale-categorization-playground;0.648;0.268;2020-12-13 18:17:20;Humpback Whale Identification Challenge;[];Most Common Id Benchmark;Python notebook;1052.0;7;;
2018-01-17 05:26:22;;Apache 2.0;https://www.kaggle.com/jhamer90811/identifying-whales-using-pca-logistic-regression;1.0;['sklearn'];['ner', 'ai', 'dl', 'cv', 'rl', 'nlp', 'nn'];['training data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'logistic regression', 'predict', 'classification'];https://www.kaggle.com/c/whale-categorization-playground;0.642;0.214;2020-12-13 18:17:20;Humpback Whale Identification Challenge;[];Identifying Whales Using PCA/Logistic Regression;Python script;941.0;4;;
2018-01-13 22:01:40;WhalesWhales are cool! Mostly because I think animals are generally awesome, but Whales are truly badasses of the great blue sea. Let's try to identify them! Summary Statistics;Apache 2.0;https://www.kaggle.com/mmrosenb/whales-an-exploration;0.5;[];['ner', 'ai', 'rl', 'cv', 'nn'];['training data', 'test data', 'train', 'model', 'label', 'predict', 'rank', 'recommend', 'classification'];https://www.kaggle.com/c/whale-categorization-playground;0.711;0.383;2020-12-13 18:17:20;Humpback Whale Identification Challenge;['data visualization, exploratory data analysis, computer vision'];Whales: An Exploration;Python notebook;3942.0;24;;
2018-03-29 20:18:45;While exploring the data I recognized that there are many duplicate images in the training set.  I used the function phash from the package imagehash to detect them. The following code plots some examples.;Apache 2.0;https://www.kaggle.com/stehai/duplicate-images;0.5;[];['ai', 'nn'];['train'];https://www.kaggle.com/c/whale-categorization-playground;0.687;0.405;2020-12-13 18:17:20;Humpback Whale Identification Challenge;[];Duplicate Images;Python notebook;2318.0;31;;
2015-10-19 00:37:54;;Apache 2.0;https://www.kaggle.com/amhchiu/bag-of-ingredients-in-r;0.5;[];['ner', 'ai', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'train', 'model', 'deep learning', 'label', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/whats-cooking;0.78;0.4;2020-12-13 18:21:34;What's Cooking?;[];Bag of Ingredients in R;Rmarkdown script;24194.0;29;;
2016-09-18 02:07:40;;Apache 2.0;https://www.kaggle.com/apapiu/linear-svc-on-bag-of-words;1.0;['sklearn', 'nltk'];['cv', 'ai', 'rl', 'gan'];['regression', 'train', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/whats-cooking;0.683;0.214;2020-12-13 18:21:34;What's Cooking?;[];Linear SVC on Bag of Words;Python notebook;2113.0;4;;
2017-01-08 17:44:59;;Apache 2.0;https://www.kaggle.com/b2nice/what-s-cooking-my-first-random-forest;1.0;['sklearn'];['ai', 'dl'];['test data', 'random forest', 'train', 'fitting', 'model', 'label', 'predict', 'classification'];https://www.kaggle.com/c/whats-cooking;0.692;0.292;2020-12-13 18:21:34;What's Cooking?;[];What's cooking? My first Random Forest...;Python notebook;2568.0;9;;
2015-10-05 22:11:40;;Apache 2.0;https://www.kaggle.com/dipayan/whatscooking-python;1.0;['pattern', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/whats-cooking;0.773;0.411;2020-12-13 18:21:34;What's Cooking?;[];WhatsCooking(Python);Python script;19773.0;33;;
2015-09-30 05:58:46;;Apache 2.0;https://www.kaggle.com/khyh00/deep-cooking;1.0;['sklearn', 'theano'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'label', 'loss', 'classification'];https://www.kaggle.com/c/whats-cooking;0.752;0.352;2020-12-13 18:21:34;What's Cooking?;[];Deep Cooking;Python script;10832.0;17;;
2019-04-11 06:33:06;Import Basic Libraries;Apache 2.0;https://www.kaggle.com/limkaraik/predicting-cuisine;1.0;['sklearn'];['ai', 'gan', 'rl', 'nn', 'ml'];['machine learning', 'test data', 'random forest', 'train', 'model', 'predict', 'decision tree', 'classification'];https://www.kaggle.com/c/whats-cooking;0.636;0.214;2020-12-13 18:21:34;multiple data sources;[];Predicting Cuisine;Python notebook;847.0;4;;
2015-09-29 16:03:47;;Apache 2.0;https://www.kaggle.com/manuelatadvice/noname;1.0;['nltk'];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/whats-cooking;0.75;0.379;2020-12-13 18:21:34;What's Cooking?;[];10 Most Used Ingredients;Python script;10282.0;23;;
2015-09-21 14:17:33;;Apache 2.0;https://www.kaggle.com/michelblaauw/h2o-bag-of-words-template;1.0;['h2o'];['ner', 'ai', 'gbm', 'nlp', 'nn'];['predict', 'training data', 'train', 'model', 'deep learning', 'loss', 'classification'];https://www.kaggle.com/c/whats-cooking;0.69;0.253;2020-12-13 18:21:34;What's Cooking?;[];H2O Bag Of Words Template (0.19268);R script;2458.0;6;0.11645;0.11645
2016-07-30 18:23:31;;Apache 2.0;https://www.kaggle.com/tuanndd/random-forest-tfidf;1.0;['sklearn'];['ai', 'nn', 'cv'];['train', 'model', 'predict'];https://www.kaggle.com/c/whats-cooking;0.723;0.253;2020-12-13 18:21:34;What's Cooking?;[];Random Forest (TFIDF);Python notebook;5253.0;6;;
2017-08-31 20:22:11;;Apache 2.0;https://www.kaggle.com/vincentclaes/clustering-food-by-cuisine;1.0;['sklearn'];['ner', 'ai', 'nn'];['train', 'label', 'k-means', 'predict'];https://www.kaggle.com/c/whats-cooking;0.711;0.253;2020-12-13 18:21:34;What's Cooking?;[];clustering food by cuisine;Python notebook;3885.0;6;;
2015-10-24 14:53:58;;Apache 2.0;https://www.kaggle.com/yilihome/simple-xgboost-in-r;1.0;['xgboost'];['ner', 'ai', 'gbm', 'cv', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/whats-cooking;0.759;0.311;2020-12-13 18:21:34;What's Cooking?;[];starter xgboost in R;R script;13211.0;11;0.78962;0.78962
2015-11-28 04:57:37;;Apache 2.0;https://www.kaggle.com/yuanmeng/whatscooking-python;1.0;['pattern', 'sklearn', 'nltk'];['ner', 'ai', 'cv', 'nlp', 'nn'];['regression', 'train', 'model', 'deep learning', 'predict', 'classification'];https://www.kaggle.com/c/whats-cooking;0.641;0.253;2020-12-13 18:21:34;What's Cooking?;[];WhatsCooking(Python);Python script;923.0;6;0.78861;0.78861
2018-06-26 21:18:13;;Apache 2.0;https://www.kaggle.com/ambarish/data-cook-eda-modelling-wordembeddings;1.0;['pattern', 'vocabulary'];['ai', 'rl', 'nn', 'cv'];['filter', 'test data', 'regression', 'train', 'model', 'label', 'logistic regression', 'predict', 'unsupervised learning', 'supervised learning'];https://www.kaggle.com/c/whats-cooking-kernels-only;0.684;0.367;2020-12-13 18:23:40;What's Cooking? (Kernels Only);['beginner, data visualization, nlp, +1 moretext mining'];Data Cook - EDA+Modelling+WordEmbeddings;Rmarkdown script;2151.0;20;;
2018-06-24 14:06:08;;Apache 2.0;https://www.kaggle.com/codename007/cooking-cooking-cooking;1.0;['sklearn'];['ai', 'rl', 'nn', 'gan'];['test data', 'training data', 'train', 'model', 'predict'];https://www.kaggle.com/c/whats-cooking-kernels-only;0.689;0.397;2020-12-13 18:23:40;What's Cooking? (Kernels Only);['beginner, data visualization, multiclass classification'];Cooking Cooking Cooking... âœ“âœ“;Python notebook;2395.0;28;0.79867;0.79867
2018-09-06 13:23:02;What's Cooking? ðŸ˜‹ðŸœ;Apache 2.0;https://www.kaggle.com/gloriahristova/a-walkthrough-eda-vizualizations-unigram-model;1.0;['vocabulary', 'sklearn'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'nn', 'ml'];['filter', 'machine learning', 'regression', 'generation', 'train', 'training data', 'model', 'test data', 'random forest', 'label', 'logistic regression', 'predict', 'understanding', 'classification'];https://www.kaggle.com/c/whats-cooking-kernels-only;0.714;0.476;2020-12-13 18:23:40;What's Cooking? (Kernels Only);['beginner, data visualization, classification, +1 moretext mining'];A Walkthrough - EDA+Vizualizations+Unigram Model;Python notebook;4196.0;73;0.79927;0.79927
2018-09-09 19:40:45;;Apache 2.0;https://www.kaggle.com/josephgpinto/cooking-is-chemistry-really;1.0;['vocabulary'];['ai', 'dl', 'gan', 'cv', 'rl', 'nn'];['filter', 'train', 'model', 'label', 'predict'];https://www.kaggle.com/c/whats-cooking-kernels-only;0.657;0.362;2020-12-13 18:23:40;What's Cooking? (Kernels Only);[];Cooking is chemistry, really;Rmarkdown script;1253.0;19;0.72274;0.72274
2018-07-16 14:50:00;;Apache 2.0;https://www.kaggle.com/nulldata/unsupervised-cuisine-ingredients-creation;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn', 'ml'];['train', 'model', 'filter'];https://www.kaggle.com/c/whats-cooking-kernels-only;0.659;0.34;2020-12-13 18:23:40;What's Cooking? (Kernels Only);[];Unsupervised Cuisine - Ingredients Creation;Rmarkdown script;1313.0;15;;
2019-11-14 08:52:39;;Apache 2.0;https://www.kaggle.com/shivamb/tf-idf-with-ovr-svm-what-s-cooking;1.0;['sklearn'];['ner', 'ai', 'dl', 'gan', 'rl', 'nlp', 'nn', 'ann'];['train', 'model', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/whats-cooking-kernels-only;0.772;0.543;2020-12-13 18:23:40;What's Cooking? (Kernels Only);['text data, multiclass classification, svm'];TF-IDF with OvR SVM : What's Cooking;Python script;19029.0;183;;
2018-09-19 02:07:33;What's cooking kernel !;Apache 2.0;https://www.kaggle.com/tejaeduc/whats-cooking-neural-nets-log-reg-svc;1.0;['lightgbm', 'nltk', 'sklearn', 'tensorflow', 'keras'];['ai', 'rl', 'nn', 'gbm'];['regression', 'train', 'model', 'neural network', 'epoch', 'layer', 'loss', 'label', 'logistic regression', 'predict', 'relu'];https://www.kaggle.com/c/whats-cooking-kernels-only;0.679;0.327;2020-12-13 18:23:40;What's Cooking? (Kernels Only);['gpu'];Whats Cooking! (Neural  nets, log reg, svc);Python notebook;1956.0;13;0.77916;0.77916
2018-06-29 00:05:54;;Apache 2.0;https://www.kaggle.com/umeshnarayanappa/recipes-tf-idf-and-bigrams;1.0;['pattern'];['ner', 'ai', 'dl', 'gan', 'rl', 'ml', 'nn', 'ann'];['speech recognition', 'gru', 'filter', 'recognition', 'model', 'natural language processing', 'layer', 'label', 'natural language'];https://www.kaggle.com/c/whats-cooking-kernels-only;0.694;0.357;2020-12-13 18:23:40;What's Cooking? (Kernels Only);[];Recipes tf-idf and Bigrams;Rmarkdown script;2679.0;18;;
2018-06-22 19:50:47;Explore the data a little: what are the most common ingredients?;Apache 2.0;https://www.kaggle.com/wendykan/what-s-cooking-eda;0.5;[];['ai', 'rl', 'nn', 'gan'];['train', 'filter'];https://www.kaggle.com/c/whats-cooking-kernels-only;0.63;0.327;2020-12-13 18:23:41;What's Cooking? (Kernels Only);[];what's cooking EDA;Python notebook;760.0;13;;
2018-03-16 14:06:18;;Apache 2.0;https://www.kaggle.com/amlanpraharaj/simple-lr-using-only-seeds;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['regression', 'train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.594;0.214;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];simple lr using only seeds;Python script;403.0;4;0.48824;0.48824
2018-03-04 08:34:24;The idea of this script was just to practice my python skills of manipulating data, though without any model building. It will show you how to get the 'Predict' result from the historical data, which is 100% accurate as expected.....;Apache 2.0;https://www.kaggle.com/anqitu/how-i-get-full-mark;0.5;[];['ai'];['model', 'filter', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.564;0.152;2020-12-13 18:26:55;multiple data sources;[];How I get FULL mark;Python notebook;251.0;2;;
2018-03-27 05:44:10;;Apache 2.0;https://www.kaggle.com/brianfong192/how-far-can-i-get-with-nba-stats-wip-lb-0-452;0.5;[];['dl', 'ner', 'ai', 'nn'];['rank', 'layer', 'filter', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.652;0.253;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];How far can I get with NBA stats? (WIP) (LB:0.452);Rmarkdown script;1140.0;6;;
2018-03-15 18:39:46;;Apache 2.0;https://www.kaggle.com/kevinpan/minimal-code-for-fivethirtyeight-s-elo-predictions;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['deep learning', 'classification', 'filter', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.613;0.236;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];Minimal Code for FiveThirtyEight's Elo Predictions;R script;562.0;5;0.48547;0.48547
2018-02-23 15:29:38;Loading Libraries;Apache 2.0;https://www.kaggle.com/kvirasat/women-s-ncaa-data-visualization;1.0;['sklearn'];['ner', 'ai', 'gan', 'cv', 'nn'];['model', 'label', 'loss'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.559;0.152;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];Women's NCAA - Data Visualization;Python notebook;233.0;2;;
2018-02-21 11:15:45;;Apache 2.0;https://www.kaggle.com/maxphilipp/creating-tidy-dataset-with-dplyr;1.0;['pattern'];['ner', 'nn', 'gan'];['filter', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.663;0.311;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];Creating Tidy Dataset with Dplyr;Rmarkdown script;1406.0;11;;
2018-03-10 11:49:34;FEATURE I;Apache 2.0;https://www.kaggle.com/naturebalance/trying-predict-something-ncaa-dataset;1.0;['sklearn'];['ai', 'nn', 'cv'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.626;0.152;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];Trying predict something -NCAA Dataset;Python notebook;705.0;2;0.00000;0.00000
2018-02-23 19:20:54;Preprocessing code to join all the tables;Apache 2.0;https://www.kaggle.com/rhajou/table-joining-adding-tourney-games-round-number;0.5;[];['ner', 'ai', 'nn', 'rl'];['model', 'label'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.539;0.152;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];Table joining |adding tourney games round number;Python notebook;173.0;2;;
2018-02-21 16:54:15;EDA of the NCAA Women's Basketball Data;Apache 2.0;https://www.kaggle.com/robikscube/eda-of-women-s-ncaa-bracket-data-in-progress;0.5;[];['ai', 'nn'];['label', 'loss'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.653;0.352;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];EDA of Women's NCAA Bracket Data (in progress);Python notebook;1160.0;17;;
2018-02-22 12:53:06;;Apache 2.0;https://www.kaggle.com/senkin13/lgbm-starter;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn', 'ml'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.631;0.253;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];lgbm_starter;Python script;766.0;6;0.00000;0.00000
2018-02-20 20:51:57;;Apache 2.0;https://www.kaggle.com/taffeylewis/for-fun-my-original-synthetic-spreads-womens;0.5;[];['ner', 'ai', 'dl', 'cv', 'ml'];['loss', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.67;0.371;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];"For Fun: My original ""Synthetic Spreads"" (Womens)";Rmarkdown script;1606.0;21;0.00000;0.00000
2018-03-20 06:24:01;Preprocessing code to join all the tables;Apache 2.0;https://www.kaggle.com/tejasrinivas/preprocessing-code-to-join-all-the-tables-eda;0.5;[];['ner', 'ai', 'nn', 'rl'];['model', 'label'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.673;0.403;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];Preprocessing code to join all the tables & EDA;Python notebook;1729.0;30;;
2018-03-08 00:46:51;;Apache 2.0;https://www.kaggle.com/thawatt/calculate-elo-and-rpi;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['rank', 'classification', 'deep learning'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.607;0.214;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;['feature engineering'];Calculate ELO and RPI;Python script;505.0;4;;
2018-02-21 06:08:09;;Apache 2.0;https://www.kaggle.com/wacaxx/wncaa-elo-based-submission-r-2018;0.5;[];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['layer', 'classification', 'deep learning', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2018;0.623;0.281;2020-12-13 18:26:55;Google Cloud & NCAAÂ® ML Competition 2018-Women's;[];WNCAA Elo Based Submission - R - 2018;R script;670.0;8;0.00000;0.00000
2019-03-02 20:48:14;;Apache 2.0;https://www.kaggle.com/akash14/google-cloud-ncaa-ml-competition-2019-women-s;1.0;['sklearn'];['ai', 'nn', 'cv'];['loss', 'model', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.641;0.346;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];Google Cloud & NCAAÂ® ML Competition 2019-Women's;Python notebook;916.0;16;0.00000;0.00000
2019-03-20 21:08:23;;Apache 2.0;https://www.kaggle.com/akash14/google-march-madness;1.0;['sklearn'];['ai', 'cv'];['loss', 'model', 'fitting', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.586;0.352;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;['gpu, tpu, beginner, +2 moreexploratory data analysis, deep learning'];google march madness;Python notebook;358.0;17;19.73677;19.73677
2019-02-16 19:25:00;;Apache 2.0;https://www.kaggle.com/ateplyuk/lgbm-str-w;1.0;['lightgbm', 'sklearn'];['ai', 'rl', 'gbm'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.662;0.413;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];Lgbm_str_w;Python notebook;1371.0;34;0.00000;0.00000
2019-03-21 06:23:04;;Apache 2.0;https://www.kaggle.com/hamidhaghshenas/public-score-0-000000;1.0;['sklearn'];['ai', 'cv'];['loss', 'model', 'fitting', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.635;0.268;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];Public Score 0.000000;Python notebook;820.0;7;0.00000;0.00000
2019-02-24 07:32:37;;Apache 2.0;https://www.kaggle.com/jaseziv83/a-very-deep-look-at-the-women-s-ncaa;0.5;[];['ner', 'ai', 'dl', 'rl', 'nn'];['model', 'label', 'filter', 'loss'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.636;0.319;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];A Very Deep Look at the Women's NCAA;Rmarkdown script;847.0;12;;
2019-03-21 02:47:40;;Apache 2.0;https://www.kaggle.com/jazivxt/courtside-seat-2019w-competitiveness;1.0;['sklearn'];['ai'];['predict', 'regression', 'fitting', 'model', 'loss'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.674;0.367;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];Courtside Seat 2019W #Competitiveness;Python notebook;1747.0;20;4.16680;4.16680
2019-04-11 18:51:35;;Apache 2.0;https://www.kaggle.com/jleecook/1st-place-regression-to-projected-mov;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['filter', 'regression', 'model', 'deep learning', 'classification'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.665;0.268;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];1st Place - Regression to Projected MOV;R script;1465.0;7;;
2019-03-22 18:28:51;;Apache 2.0;https://www.kaggle.com/lavanyadml/google-cloud-ncaa-ml-competition;1.0;['xgboost', 'sklearn', 'lightgbm'];['ai', 'dl', 'gan', 'gbm', 'cv'];['filter', 'train', 'model', 'loss', 'label', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.557;0.0;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;['gpu'];Google Cloud & NCAAÂ® ML Competition;Python notebook;227.0;0;;
2019-03-12 11:48:07;;Apache 2.0;https://www.kaggle.com/lavanyadml/google-cloud-ncaa-ml-competition-2019-women-s;1.0;['sklearn'];['ai', 'cv'];['loss', 'model', 'fitting', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.596;0.188;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];Google Cloud & NCAAÂ® ML Competition 2019-Women's ;Python notebook;422.0;3;0.00000;0.00000
2019-03-28 08:23:09;;Apache 2.0;https://www.kaggle.com/littlesmart/elo-rating-system-for-ncaa-predictions;0.5;[];['ai', 'nn'];['label', 'filter', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.52;0.152;2020-12-13 18:28:45;multiple data sources;[];Elo rating system f;Python notebook;132.0;2;;
2019-03-04 17:08:08;Women Ballers - Google Cloud & NCAAÂ® ML Competition 2019-Women'sBy Maya Shaked;Apache 2.0;https://www.kaggle.com/mshaked/women-ballers;0.5;[];['dl', 'ai', 'nn', 'ml'];['filter'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.588;0.188;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];Women Ballers;R notebook;369.0;3;;
2019-03-22 18:22:10;Use Pandas to read the predictions and data files.;Apache 2.0;https://www.kaggle.com/peacefultransfer/simulate-the-tournament-based-on-your-predictions;0.5;[];['ner', 'ai', 'nn', 'ml'];['predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.534;0.0;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];Simulate the Tournament Based on Your Predictions;Python notebook;161.0;0;;
2019-04-11 12:19:35;;Apache 2.0;https://www.kaggle.com/scirpus/post-facto;1.0;['sklearn'];['ai', 'nn'];['train', 'loss', 'label', 'predict'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.52;0.099;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];Post Facto;Python notebook;132.0;1;0.40714;0.40714
2019-03-19 22:43:25;;Apache 2.0;https://www.kaggle.com/thawatt/get-tourney-round;0.5;[];['ner', 'ai', 'nlp', 'nn', 'ml'];['classification', 'deep learning'];https://www.kaggle.com/c/womens-machine-learning-competition-2019;0.565;0.214;2020-12-13 18:28:45;Google Cloud & NCAAÂ® ML Competition 2019-Women's;[];Get Tourney Round;Python script;258.0;4;;
2019-04-29 15:38:20;Import modules;Apache 2.0;https://www.kaggle.com/alexcherniuk/imdb-review-word2vec-bilstm-99-acc;1.0;['nltk', 'gensim', 'sklearn', 'tensorflow', 'vocabulary', 'keras'];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['unlabeled', 'filter', 'test data', 'train', 'fitting', 'model', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'labeled'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.763;0.449;2020-12-13 18:30:04;multiple data sources;['beginner, deep learning, classification, +1 moredata cleaning'];IMDB review Word2Vec & BiLSTM;Python notebook;14761.0;52;0.99000;0.99000
2018-10-28 11:58:37;Reading Data;Apache 2.0;https://www.kaggle.com/amitkvikram/bag-of-words;1.0;['pattern', 'vocabulary', 'sklearn', 'nltk'];['ner', 'ai', 'cv', 'rl', 'nn', 'ml'];['unlabeled', 'training data', 'test data', 'regression', 'train', 'model', 'label', 'logistic regression', 'naive bayes', 'predict', 'random forest', 'labeled'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.629;0.253;2020-12-13 18:30:04;Bag of Words Meets Bags of Popcorn;[];bag_of_words;Python notebook;740.0;6;;
2019-01-29 20:23:33;;Apache 2.0;https://www.kaggle.com/jatinmittal0001/word2vec;1.0;['nltk', 'gensim', 'sklearn', 'tensorflow', 'keras'];['ner', 'ai', 'gan', 'rl', 'nlp', 'nn'];['unlabeled', 'filter', 'training data', 'train', 'fitting', 'model', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'labeled'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.701;0.268;2020-12-13 18:30:04;multiple data sources;['gpu'];Word2Vec;Python notebook;3133.0;7;0.96892;0.96892
2020-11-11 13:40:48;Loading the training data set;Apache 2.0;https://www.kaggle.com/jayantmathur/imdb-review-data;1.0;['sklearn', 'nltk'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['unlabeled', 'training data', 'test data', 'generation', 'train', 'model', 'label', 'naive bayes', 'predict', 'decision tree', 'labeled'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.58;0.346;2020-12-13 18:30:04;Bag of Words Meets Bags of Popcorn;['nlp, decision tree, naive bayes'];IMDB Review Data;Python notebook;325.0;16;;
2018-11-21 12:04:35;;Apache 2.0;https://www.kaggle.com/nilanml/imdb-review-deep-model-94-89-accuracy;1.0;['tensorflow', 'sklearn', 'keras', 'nltk'];['ner', 'ai', 'rl', 'nlp', 'nn'];['gru', 'filter', 'train', 'fitting', 'model', 'epoch', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'labeled'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.795;0.534;2020-12-13 18:30:04;multiple data sources;['gpu, deep learning, nlp, +2 moretext data, lstm'];IMDB Review  - Deep Model ~ 93.51% Accuracy;Python notebook;38474.0;161;;
2019-08-01 03:34:42;Contraction Mapping;Apache 2.0;https://www.kaggle.com/noi031/sentiment-analysis-with-self-attention;1.0;['tensorflow', 'keras', 'gensim'];['nlp', 'ai', 'nn'];['unlabeled', 'filter', 'train', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'relu', 'labeled'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.681;0.253;2020-12-13 18:30:04;multiple data sources;['gpu'];Sentiment Analysis with Self-Attention;Python notebook;2017.0;6;1.00000;1.00000
2020-06-10 12:40:02;Word Cloud;Apache 2.0;https://www.kaggle.com/pratikbarua/bag-of-words-meets-bags-of-popcorn;1.0;['vocabulary', 'sklearn', 'nltk'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['unlabeled', 'regression', 'train', 'model', 'label', 'logistic regression', 'naive bayes', 'predict', 'classification', 'labeled'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.573;0.292;2020-12-13 18:30:04;Bag of Words Meets Bags of Popcorn;[];Bag of Words Meets Bags of Popcorn;Python notebook;292.0;9;;
2018-10-09 20:43:28;;Apache 2.0;https://www.kaggle.com/theainerd/bag-of-words-meets-bag-of-popcorn;1.0;['pattern', 'textblob', 'sklearn', 'nltk'];['ner', 'ai', 'cv', 'rl', 'nlp', 'nn', 'ml'];['unlabeled', 'test data', 'regression', 'train', 'fitting', 'model', 'supervised learning', 'deep learning', 'label', 'logistic regression', 'predict', 'classification', 'labeled'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.685;0.253;2020-12-13 18:30:04;Bag of Words Meets Bags of Popcorn;['beginner, feature engineering, data cleaning, +1 morenlp'];Bag of Words Meets Bag of Popcorn;Python script;2222.0;6;0.89644;0.89644
2018-10-08 20:56:45;;Apache 2.0;https://www.kaggle.com/vamsi1251/bag-of-words-model;1.0;['sklearn', 'nltk'];['ai', 'ml'];['train', 'label', 'labeled', 'predict'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.683;0.253;2020-12-13 18:30:04;Bag of Words Meets Bags of Popcorn;['deep learning, nlp'];Bag_of_Words_model;Python notebook;2108.0;6;;
2020-01-05 13:41:51;;Apache 2.0;https://www.kaggle.com/yepp2411/baseline-model-using-nn-for-movie-review;1.0;['tensorflow', 'sklearn', 'keras', 'nltk'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['unlabeled', 'test data', 'train', 'fitting', 'model', 'epoch', 'layer', 'loss', 'label', 'predict', 'labeled'];https://www.kaggle.com/c/word2vec-nlp-tutorial;0.59;0.268;2020-12-13 18:30:04;Bag of Words Meets Bags of Popcorn;['gpu'];Baseline model using NN for movie review;Python notebook;378.0;7;0.87352;0.87352
2016-04-09 20:48:12;;Apache 2.0;https://www.kaggle.com/anokas/patch-features-rfr;0.0;[];[];[];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.634;0.188;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Patch Features RFR;Python notebook;806.0;3;;
2020-01-08 19:03:05;;Apache 2.0;https://www.kaggle.com/aparajit0511/yelp-restaurant;1.0;['caffe'];['ai', 'nn', 'cv'];['train', 'label', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.523;0.0;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;['gpu'];Yelp Restaurant;Python notebook;139.0;0;;
2018-11-28 14:12:56;;Apache 2.0;https://www.kaggle.com/ashbunny/kernel909af544ff;1.0;['xgboost'];['ai', 'nn'];['filter', 'train', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.551;0.0;2020-12-13 18:36:57;multiple data sources;[];kernel909af544ff;R notebook;206.0;0;;
2016-02-04 01:41:53;;Apache 2.0;https://www.kaggle.com/benhamner/sample-photos;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'classification', 'deep learning'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.662;0.099;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Sample Photos;R script;1378.0;1;;
2016-04-11 17:39:46;;Apache 2.0;https://www.kaggle.com/dmytrolystopad/imagine-an-image1;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.657;0.188;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Imagine an Image1;Python script;1255.0;3;0.63952;0.63790
2018-07-30 01:00:32;Data Exploration for Yelp Image Classification Challenge Exploring the image data and labels, visualizing random samples of images, and plotting image shape distributions.;Apache 2.0;https://www.kaggle.com/enerrio/data-exploration-yelp-classification;0.5;[];['ner', 'ai', 'cv', 'ml', 'nn', 'ann'];['image classification', 'machine learning', 'training data', 'train', 'model', 'label', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.678;0.268;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;['data visualization, exploratory data analysis'];Data Exploration Yelp Classification ;Python notebook;1905.0;7;;
2016-04-02 19:35:00;;Apache 2.0;https://www.kaggle.com/innerproduct/imagine-an-image;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.63;0.152;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Imagine an Image;Python script;757.0;2;0.61922;0.61263
2016-03-28 22:47:47;;Apache 2.0;https://www.kaggle.com/innerproduct/more-naivete;0.5;[];['ai'];['train', 'label', 'predict'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.633;0.188;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];more naivete;Python notebook;793.0;3;0.64037;0.63791
2016-04-08 09:16:52;;Apache 2.0;https://www.kaggle.com/innerproduct/trying-to-process-images;1.0;['sklearn'];['ner', 'ai', 'cv', 'nlp', 'nn', 'ml'];['test data', 'random forest', 'train', 'fitting', 'deep learning', 'layer', 'label', 'predict', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.721;0.214;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];random forests on pixels;Python script;4916.0;4;;
2018-05-08 20:25:24;;Apache 2.0;https://www.kaggle.com/jihyeseo/photo-classify;1.0;['sklearn'];['ai'];['train'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.527;0.0;2020-12-13 18:36:57;Yelp Restaurant Photo Classification;[];photo classify;Python notebook;147.0;0;;
2016-11-22 22:47:19;;Apache 2.0;https://www.kaggle.com/luckygemini/notebook5d5fc80c9f;0.5;[];['ner', 'ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.541;0.099;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Notebook67cad851f5;Python notebook;180.0;1;;
2016-04-11 03:09:44;;Apache 2.0;https://www.kaggle.com/millerintllc/imagine-an-image1;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.622;0.188;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Imagine an Image1;Python script;654.0;3;0.64037;0.63844
2016-02-29 05:00:49;;Apache 2.0;https://www.kaggle.com/rafaellopes/load-data-with-get-dummies;0.5;[];['ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.639;0.152;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];load_data with get_dummies;Python notebook;893.0;2;;
2018-09-01 12:34:29;Data Analysis In this kernel we try to do some simple insights into the training data on categorization of the resutaurant classification. I padded the labels to 9 categories and transformed them into individual columns in the dataframe with a value of N, if that category is not present in the input label and Y if that category is peresent in the lable. This rearrangement helps us to answer various questions:  How many restaurants serve lunch and dinner How many restaurants serve dinner with alcohol in the outdoors and are kid friendly How mant restaurants serve dinner only and need reservations;Apache 2.0;https://www.kaggle.com/sgovindu/yelp-data-analysis;0.5;[];['dl', 'ner', 'ai', 'nn'];['train', 'label', 'training data', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.602;0.099;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Yelp Data Analysis;Python notebook;461.0;1;;
2016-04-12 20:52:19;;Apache 2.0;https://www.kaggle.com/titericz/imagine-an-image3;1.0;['sklearn'];['nlp', 'ai', 'nn', 'ner'];['filter', 'train', 'model', 'deep learning', 'label', 'predict', 'rank', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.65;0.099;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Imagine an Image3;Python script;1085.0;1;0.63754;0.63527
2016-04-05 00:55:48;;Apache 2.0;https://www.kaggle.com/triskelion/shallow-learning;1.0;['sklearn'];['ner', 'ai', 'nlp', 'nn', 'ml'];['train', 'deep learning', 'layer', 'label', 'predict', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.654;0.152;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Shallow Learning;Python script;1184.0;2;;
2016-02-04 04:01:58;;Apache 2.0;https://www.kaggle.com/valueq/util-for-data-exploration;0.5;[];['nlp', 'ai', 'nn', 'ner'];['train', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.702;0.281;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];util for data exploration;Python script;3185.0;8;;
2016-02-04 02:41:02;;Apache 2.0;https://www.kaggle.com/wendykan/expensive-restaurants-look-like-this;0.5;[];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn', 'ml'];['filter', 'train', 'deep learning', 'label', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.756;0.405;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Expensive restaurants look like this;Python notebook;12201.0;31;;
2016-02-10 09:02:18;;Apache 2.0;https://www.kaggle.com/wongjingping/naive-benchmark-0-61;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn', 'ml'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.689;0.236;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Naive Benchmark (0.61);Python notebook;2419.0;5;0.60529;0.60973
2016-04-09 07:22:45;Yelp Data Crawler Class;Apache 2.0;https://www.kaggle.com/yrevar/yelp-training-data-helper;0.5;[];['ner', 'ai', 'nn'];['filter', 'training data', 'test data', 'train', 'model', 'epoch', 'vgg', 'label'];https://www.kaggle.com/c/yelp-restaurant-photo-classification;0.648;0.152;2020-12-13 18:36:56;Yelp Restaurant Photo Classification;[];Yelp Training Data Helper;Python notebook;1052.0;2;0.00000;0.00000
2017-04-16 10:21:52;;Apache 2.0;https://www.kaggle.com/adamringhede/rnn-test;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nlp', 'nn', 'rnn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'understanding', 'classification'];https://www.kaggle.com/c/youtube8m;0.693;0.099;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];RNN test;Python script;2635.0;1;;
2017-04-17 19:55:39;;Apache 2.0;https://www.kaggle.com/amunategui/download-tfrecords-for-case-insensitive-systems;0.5;[];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['understanding', 'label', 'classification', 'deep learning'];https://www.kaggle.com/c/youtube8m;0.638;0.302;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];Download tfrecords for case-insensitive systems;Python script;877.0;10;;
2017-03-31 00:32:25;;Apache 2.0;https://www.kaggle.com/darrellulm/google-cloud-youtube-8m-video-testrun;0.5;[];['ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/youtube8m;0.594;0.099;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];Google Cloud & YouTube-8M Video TestRun;Python notebook;403.0;1;;
2017-05-26 13:18:58;;Apache 2.0;https://www.kaggle.com/drn01z3/keras-baseline-on-video-features-0-7941-lb;1.0;['tensorflow', 'keras'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'understanding', 'classification'];https://www.kaggle.com/c/youtube8m;0.697;0.413;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];keras baseline on video features [0.7941 LB];Python script;2842.0;34;;
2017-05-15 15:40:11;;Apache 2.0;https://www.kaggle.com/kuixui/rnn-test;1.0;['tensorflow', 'sklearn', 'keras'];['ner', 'ai', 'nlp', 'nn', 'rnn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'lstm', 'predict', 'relu', 'understanding', 'classification'];https://www.kaggle.com/c/youtube8m;0.639;0.099;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];RNN test;Python script;889.0;1;;
2017-03-09 06:52:17;;Apache 2.0;https://www.kaggle.com/mikell/most-popular-labels;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'deep learning', 'label', 'understanding', 'classification'];https://www.kaggle.com/c/youtube8m;0.584;0.152;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];Most Popular Labels;Python script;347.0;2;;
2019-02-16 11:43:08;;Apache 2.0;https://www.kaggle.com/mnds18/eda-explore-youtube8m-sample-data;1.0;['tensorflow', 'sklearn'];['dl', 'ai', 'nn'];['train', 'label', 'filter'];https://www.kaggle.com/c/youtube8m;0.564;0.099;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];EDA: Explore Youtube8M sample data;Python notebook;253.0;1;;
2017-02-17 05:39:54;;Apache 2.0;https://www.kaggle.com/neilzhang/youtube-8m;0.5;[];['ai', 'nn'];['label'];https://www.kaggle.com/c/youtube8m;0.72;0.188;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];Youtube-8M;Python notebook;4859.0;3;;
2017-06-01 16:39:24;;Apache 2.0;https://www.kaggle.com/nik0lashka/0-7941-lb-outofmemory-fix-generator-fix;1.0;['tensorflow', 'keras'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'model', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'predict', 'relu', 'understanding', 'classification'];https://www.kaggle.com/c/youtube8m;0.601;0.268;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];[0.7941 LB] + outOfMemory fix + generator fix;Python script;460.0;7;;
2017-04-17 01:44:53;;Apache 2.0;https://www.kaggle.com/pmw9440/notebook1ee341b1c9;0.5;[];['ai', 'nn'];['label'];https://www.kaggle.com/c/youtube8m;0.491;0.099;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];Notebook1ee341b1c9;Python notebook;89.0;1;;
2017-02-21 12:53:44;;Apache 2.0;https://www.kaggle.com/vlarine/most-popular-labels;0.5;[];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'deep learning', 'label', 'understanding', 'classification'];https://www.kaggle.com/c/youtube8m;0.692;0.311;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];Most Popular Labels;Python script;2571.0;11;0.04209;0.04214
2017-06-03 19:36:57;;Apache 2.0;https://www.kaggle.com/xpeuler/split-video-for-data-augmentation;1.0;['pattern', 'tensorflow'];['ner', 'ai', 'dl', 'rl', 'nlp', 'nn'];['train', 'deep learning', 'label', 'understanding', 'classification'];https://www.kaggle.com/c/youtube8m;0.663;0.214;2020-12-13 18:41:49;Google Cloud & YouTube-8M Video Understanding Challenge;[];Split_video for data augmentation;Python script;1411.0;4;;
2018-06-26 12:26:59;;Apache 2.0;https://www.kaggle.com/abimannan/youtube-2m;1.0;['tensorflow', 'sklearn'];['ai', 'nn', 'ann'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/youtube8m-2018;0.562;0.0;2020-12-13 18:43:34;The 2nd YouTube-8M Video Understanding Challenge;['gpu'];Abimannan Youtube ;Python notebook;245.0;0;0.00000;0.00000
2018-06-06 19:09:39;;Apache 2.0;https://www.kaggle.com/amitkumarjaiswal/google-youtube-1st-try;1.0;['vocabulary'];['ner', 'ai', 'dl', 'nlp', 'nn'];['train', 'deep learning', 'label', 'understanding', 'classification'];https://www.kaggle.com/c/youtube8m-2018;0.611;0.099;2020-12-13 18:43:34;The 2nd YouTube-8M Video Understanding Challenge;[];Google Youtube 1st try;Python script;537.0;1;0.05130;0.05160
2019-05-01 12:20:56;Reference: https://www.kaggle.com/amansrivastava/exploration-bi-lstm-model;Apache 2.0;https://www.kaggle.com/anirudh257/exploration-bi-lstm-model-by-aman-srivasthava;1.0;['pattern', 'vocabulary', 'tensorflow', 'keras'];['ner', 'ai', 'dl', 'gan', 'cv', 'rl', 'ml', 'nn', 'ann'];['training data', 'train', 'model', 'input layer', 'epoch', 'layer', 'lstm', 'label', 'loss', 'relu', 'classification'];https://www.kaggle.com/c/youtube8m-2018;0.563;0.152;2020-12-13 18:43:34;The 2nd YouTube-8M Video Understanding Challenge;[];Exploration + Bi-LSTM model by Aman Srivasthava;Python notebook;248.0;2;;
2018-06-05 01:34:03;;Apache 2.0;https://www.kaggle.com/artyomp/a-fixed-download-script;1.0;['vocabulary'];['ner', 'ai', 'rl', 'nlp', 'nn'];['train', 'deep learning', 'label', 'understanding', 'classification'];https://www.kaggle.com/c/youtube8m-2018;0.67;0.292;2020-12-13 18:43:34;The 2nd YouTube-8M Video Understanding Challenge;[];A fixed download script;Python script;1615.0;9;;
2018-06-12 11:07:50;;Apache 2.0;https://www.kaggle.com/ashishpatel26/youtube-8m-dataset-using-extratreesclassifier;1.0;['tensorflow', 'sklearn'];['ai', 'rl'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/youtube8m-2018;0.675;0.311;2020-12-13 18:43:34;The 2nd YouTube-8M Video Understanding Challenge;[];YouTube-8M dataset using ExtraTreesClassifier;Python notebook;1808.0;11;0.05029;0.05052
2019-01-15 20:42:40;;Apache 2.0;https://www.kaggle.com/ayoubchebbi/youtube-8m-analytics;1.0;['vocabulary'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'label', 'regression'];https://www.kaggle.com/c/youtube8m-2018;0.758;0.481;2020-12-13 18:43:34;The 2nd YouTube-8M Video Understanding Challenge;['data visualization, classification'];youtube-8M analytics;Python notebook;12764.0;78;;
2020-06-15 08:30:15;Load packages;Apache 2.0;https://www.kaggle.com/anshuljdhingra/analysis-youtube8m-2019;1.0;['vocabulary', 'tensorflow'];['dl', 'ner', 'ai', 'nn'];['filter', 'training data', 'regression', 'test data', 'train', 'model', 'lstm', 'label', 'predict', 'understanding'];https://www.kaggle.com/c/youtube8m-2019;0.46;0.0;2020-12-13 18:45:07;The 3rd YouTube-8M Video Understanding Challenge;[];Analysis YouTube8m 2019 ðŸ“¹;Python notebook;60.0;0;;
2019-06-28 12:06:25;;Apache 2.0;https://www.kaggle.com/aqeelnawaz44/analyse-kernal;1.0;['tensorflow'];['ner', 'ai', 'nn'];['train', 'label'];https://www.kaggle.com/c/youtube8m-2019;0.631;0.236;2020-12-13 18:45:07;multiple data sources;[];kernel2a50e84252;Python notebook;771.0;5;0.00000;0.00010
2019-10-12 17:39:31;;Apache 2.0;https://www.kaggle.com/artyomp/stronger-baseline-with-pytorch;1.0;['pytorch', 'vocabulary', 'tensorflow', 'sklearn'];['ner', 'ai', 'dl', 'rl', 'ml', 'nlp', 'nn', 'ann'];['filter', 'training data', 'test data', 'neuron', 'train', 'model', 'epoch', 'deep learning', 'layer', 'validation data', 'loss', 'label', 'predict', 'understanding', 'classification', 'labeled'];https://www.kaggle.com/c/youtube8m-2019;0.699;0.367;2020-12-13 18:45:07;multiple data sources;['gpu'];Stronger baseline with PyTorch;Python script;3023.0;20;0.74967;0.75962
2019-07-02 23:45:38;;Apache 2.0;https://www.kaggle.com/ayoubchebbi/the-3rd-youtube-8m-video-understanding-challenge;1.0;['vocabulary'];['ner', 'ai', 'dl', 'rl', 'nn', 'ann'];['train', 'label', 'regression'];https://www.kaggle.com/c/youtube8m-2019;0.681;0.253;2020-12-13 18:45:07;The 3rd YouTube-8M Video Understanding Challenge;[];The 3rd YouTube-8M Video Understanding Challenge;Python notebook;2018.0;6;;
2019-07-24 03:00:19;;Apache 2.0;https://www.kaggle.com/isikkuntay/vid-to-seg-predictions;0.5;[];['dl', 'ai', 'nn'];['train', 'model', 'label', 'predict'];https://www.kaggle.com/c/youtube8m-2019;0.676;0.099;2020-12-13 18:45:07;multiple data sources;['gpu'];vid to seg predictions;Python notebook;1814.0;1;0.00004;0.00012
2019-09-02 15:27:41;;Apache 2.0;https://www.kaggle.com/lixiawei15/hotpot-discuss;1.0;['vocabulary'];['ai', 'nn'];['train'];https://www.kaggle.com/c/youtube8m-2019;0.518;0.0;2020-12-13 18:45:07;The 3rd YouTube-8M Video Understanding Challenge;[];hotpot discuss;Python notebook;129.0;0;;
2019-08-24 09:47:35;;Apache 2.0;https://www.kaggle.com/plengnakdee/draft-1;1.0;['vocabulary'];['ai', 'rl'];['train', 'label'];https://www.kaggle.com/c/youtube8m-2019;0.525;0.099;2020-12-13 18:45:07;The 3rd YouTube-8M Video Understanding Challenge;[];Draft-1;Python notebook;143.0;1;;
2019-10-15 10:12:20;;Apache 2.0;https://www.kaggle.com/sekrier/bronze-with-last-year-sub-and-video-length;0.5;[];['dl', 'ai', 'nn'];['label'];https://www.kaggle.com/c/youtube8m-2019;0.565;0.099;2020-12-13 18:45:07;multiple data sources;[];Bronze with last year sub and video length;Python notebook;257.0;1;0.22951;0.24267
2019-07-31 20:45:54;Load Data;Apache 2.0;https://www.kaggle.com/sinchanbhattacharya/eda-word-cloud;1.0;['vocabulary'];['ner', 'ai', 'nn', 'rl'];['train'];https://www.kaggle.com/c/youtube8m-2019;0.538;0.0;2020-12-13 18:45:07;The 3rd YouTube-8M Video Understanding Challenge;[];EDA-Word Cloud;Python notebook;171.0;0;;
2019-07-06 09:57:30;;Apache 2.0;https://www.kaggle.com/tbmoon/how-to-save-tfrecord-into-npy-too-slow;1.0;['pytorch', 'tensorflow'];['ai'];['train', 'label'];https://www.kaggle.com/c/youtube8m-2019;0.668;0.099;2020-12-13 18:45:07;The 3rd YouTube-8M Video Understanding Challenge;['gpu'];How to save tfrecord into npy (Too Slow!);Python notebook;1570.0;1;;
2019-09-21 16:02:33;/m/04ctx knife;Apache 2.0;https://www.kaggle.com/thehandler/kernel3a0d441f2e;1.0;['vocabulary', 'tensorflow'];['ai', 'dl', 'cv', 'rl', 'nn', 'ml'];['train', 'label'];https://www.kaggle.com/c/youtube8m-2019;0.546;0.0;2020-12-13 18:45:07;The 3rd YouTube-8M Video Understanding Challenge;['gpu'];kernel3a0d441f2e;Python notebook;192.0;0;;
2017-10-14 16:19:56;;Apache 2.0;https://www.kaggle.com/abdelwahedassklou/only-cat-boost-lb-0-0641-939;1.0;['catboost'];['nlp', 'ai', 'nn', 'ner'];['train', 'model', 'deep learning', 'loss', 'label', 'predict', 'classification'];https://www.kaggle.com/c/zillow-prize-1;0.74;0.469;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);[];Only_Cat boost (LB : 0.0641 939);Python script;7937.0;67;0.07521;0.06419
2017-07-18 04:21:21;;Apache 2.0;https://www.kaggle.com/aharless/xgb-w-o-outliers-lgb-with-outliers-combined;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'rl', 'nlp', 'nn'];['training data', 'test data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/zillow-prize-1;0.775;0.496;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);['xgboost'];XGB w/o outliers & LGB with outliers combined;Python script;20784.0;95;0.07546;0.06442
2017-09-22 14:40:43;;Apache 2.0;https://www.kaggle.com/aharless/xgboost-lightgbm-and-ols;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['training data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/zillow-prize-1;0.773;0.473;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);['xgboost, ensembling, gradient boosting'];XGBoost, LightGBM, and OLS;Python script;19459.0;71;0.07543;0.06440
2017-09-30 02:40:08;;Apache 2.0;https://www.kaggle.com/aharless/xgboost-lightgbm-and-ols-and-nn;1.0;['xgboost', 'lightgbm', 'sklearn', 'tensorflow', 'keras'];['ner', 'ai', 'gbm', 'cv', 'rl', 'ml', 'nlp', 'nn', 'ann'];['training data', 'regression', 'train', 'fitting', 'model', 'neural network', 'epoch', 'deep learning', 'layer', 'loss', 'label', 'gradient boosting', 'predict', 'relu', 'classification'];https://www.kaggle.com/c/zillow-prize-1;0.763;0.456;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);['neural networks, ensembling, gradient boosting'];XGBoost, LightGBM, and OLS and NN;Python script;14815.0;57;0.07545;0.06436
2017-05-24 20:56:28;;Apache 2.0;https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655;1.0;['xgboost'];['ner', 'ai', 'rl', 'nlp', 'nn', 'ml'];['train', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/zillow-prize-1;0.783;0.548;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);['xgboost'];Simple XGBoost Starter (~0.0655);Python script;26566.0;197;;
2017-08-06 21:33:19;This notebook shows how I reduce the size of the properties dataset by selecting smaller datatypes.   I noticed the size of the properties dataset is pretty big for a lower/mid-range laptop so I made a script to make the dataset smaller without losing information.  This notebook uses the following approach:   Iterate over every column  Determine if the column is numeric  Determine if the column can be represented by an integer  Find the min and the max value  Determine and apply the smallest datatype that can fit the range of values    This reduces the dataset from approx. 1.3 GB to 466 MB;Apache 2.0;https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65;0.5;[];['ai'];['train'];https://www.kaggle.com/c/zillow-prize-1;0.805;0.568;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);[];Reducing DataFrame memory size by ~65% ;Python notebook;53537.0;267;;
2017-05-29 00:53:04;;Apache 2.0;https://www.kaggle.com/captcalculator/a-very-extensive-zillow-exploratory-analysis;0.5;[];['ai', 'nn', 'ml'];['test data', 'predict', 'train', 'label', 'clustering'];https://www.kaggle.com/c/zillow-prize-1;0.76;0.502;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);['exploratory data analysis'];A Very Extensive Zillow Exploratory Analysis;Rmarkdown script;13527.0;104;;
2017-08-22 00:04:28;;Apache 2.0;https://www.kaggle.com/davidfumo/boosted-trees-lb-0-0643707;1.0;['xgboost', 'sklearn', 'lightgbm'];['ner', 'ai', 'gbm', 'rl', 'nlp', 'nn', 'ml'];['training data', 'regression', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/zillow-prize-1;0.757;0.458;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);[];Boosted Trees LB ~ 0.0643707;Python script;12426.0;58;;
2017-10-07 00:35:24;Creating some ML pipelinesIntroduction;Apache 2.0;https://www.kaggle.com/evanmiller/pipelines-gridsearch-awesome-ml-pipelines;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'dl', 'gbm', 'cv', 'rl', 'ml'];['train', 'model', 'filter', 'predict'];https://www.kaggle.com/c/zillow-prize-1;0.78;0.435;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);[];Pipelines + GridSearch  = Awesome ML pipelines;Python notebook;24514.0;44;;
2017-05-25 13:28:29;;Apache 2.0;https://www.kaggle.com/guolinke/simple-lightgbm-starter-lb-0-06487;1.0;['lightgbm'];['ner', 'ai', 'gbm', 'nlp', 'nn'];['regression', 'train', 'deep learning', 'label', 'gradient boosting', 'predict', 'classification'];https://www.kaggle.com/c/zillow-prize-1;0.747;0.464;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);['gradient boosting'];Simple LightGBM starter. LB 0.06487;Python script;9525.0;63;;
2017-11-19 07:28:22;"Here is what was done on the first Zillow kernel:  Look into the data (ofcourse!!) Visualize the various independent variables visually (links for the same are provided) Check for columns showing non-variance Split columns based on their data types. Carefully perform further analysis on missing values in those columns with respect to the values they can possibly hold; rather than assinging them to '0' blindly!  What we did later:  Created new meaningful features (from 60 to 82 columns) Visualizing the features.(to be updated)  What we did NOW:  Looked into memory consumption of the dataframe and REDUCED it !!  As always let's take a look into the data.";Apache 2.0;https://www.kaggle.com/jeru666/zillow-revamped-with-memory-reduction;0.5;[];['ai', 'dl', 'rl', 'ml', 'nn', 'ann'];['train', 'understanding', 'model', 'label'];https://www.kaggle.com/c/zillow-prize-1;0.716;0.435;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);['beginner, feature engineering, data cleaning'];Zillow Revamped with Memory Reduction!;Python notebook;4427.0;44;;
2017-10-11 19:18:18;;Apache 2.0;https://www.kaggle.com/kamilkk/no-stacking-no-lb-overfitting-0-064337;1.0;['lightgbm', 'sklearn'];['ner', 'ai', 'gan', 'gbm', 'nlp', 'nn'];['test data', 'train', 'fitting', 'model', 'deep learning', 'label', 'predict', 'classification'];https://www.kaggle.com/c/zillow-prize-1;0.725;0.435;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);[];No stacking & no LB overfitting - 0.064337;Python script;5521.0;44;;
2017-05-31 20:38:05;;Apache 2.0;https://www.kaggle.com/philippsp/exploratory-analysis-zillow;0.5;[];['ai', 'dl', 'rl'];['train', 'label', 'filter', 'predict'];https://www.kaggle.com/c/zillow-prize-1;0.828;0.662;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);['beginner, exploratory data analysis'];Exploratory Analysis - Zillow;Rmarkdown script;123137.0;1387;;
2017-10-06 14:34:03;Data loading, we parse transactiondate;Apache 2.0;https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435;1.0;['catboost'];['ai', 'cv'];['predict', 'train', 'model', 'label', 'loss'];https://www.kaggle.com/c/zillow-prize-1;0.736;0.459;2020-12-13 18:50:10;Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate);['finance, ensembling, gradient boosting, +1 morehousing'];Concise catboost starter ensemble (PLB: 0.06435);Python notebook;7259.0;59;;
