date;description;link;ml_libs;ml_score;ml_slugs;ml_tags;ml_terms;text;title
2020-05-09 19:40:35;Let’s consider one piece of the optical character recognition problem in Data Science- The identification of handwritten digits. Here we will take a shortcut and use Scikit-Learn’s set of preformatted digits, which is built into the library.;https://thecleverprogrammer.com/2020/05/09/data-science-project-on-handwritten-digits/;['sklearn'];1.0;[];['ML', 'Supervised Learning', 'Classification', 'Unsupervised learning'];['recogn', 'predict', 'fit', 'supervised learning', 'model', 'machine learning', 'classif', 'train', 'unsupervised learning', 'label'];Let’s consider one piece of the optical character recognition problem in Data Science- The identification of handwritten digits. Here we will take a shortcut and use Scikit-Learn’s set of preformatted digits, which is built into the library. Loading and Visualizing the digits data: We will use Scikit-Learn’s data access interface and take a look at this data: from sklearn.datasets import load_digits digits = load_digits() print(digits.images.shape)​x from sklearn.datasets import load_digitsdigits = load_digits()print(digits.images.shape) #output- (1797, 8, 8) The images is a three-dimensional array: 1797 samples, each consisting of an 8*8 grid of pixels. Let’s visualize the First hundred of these: import matplotlib.pyplot as plt fig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1)) for i, ax in enumerate(axes.flat): ax.imshow(digits.images[i], cmap='binary', interpolation='nearest') ax.text(0.05, 0.05, str(digits.target[i]), transform=ax.transAxes, color='green') plt.show() import matplotlib.pyplot as pltfig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1))for i, ax in enumerate(axes.flat): ax.imshow(digits.images[i], cmap='binary', interpolation='nearest') ax.text(0.05, 0.05, str(digits.target[i]), transform=ax.transAxes, color='green')plt.show() x = digits.data print(x.shape) x = digits.dataprint(x.shape) #Output-(1797, 64) y = digits.target print(y.shape) y = digits.targetprint(y.shape) #Output-(1797,) Unsupervised Learning(classification of Digits): Unsupervised Learning is a Category of Machine Learning, In data science we need to use a lot of machine learning models, This is one of them: from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0) model = GaussianNB() model.fit(xtrain, ytrain) ymodel = model.predict(xtest) from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_splitfrom sklearn.naive_bayes import GaussianNB xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0)model = GaussianNB() model.fit(xtrain, ytrain) ymodel = model.predict(xtest) from sklearn.metrics import accuracy_score print(accuracy_score(ytest, ymodel)) print(accuracy_score(ytest, ymodel)) #Output- 0.8333333333333334 Creating Structure and Plotting the Data from sklearn.metrics import confusion_matrix import seaborn as sns mat = confusion_matrix(ytest, ymodel) sns.heatmap(mat, square=True, annot=True, cbar=False) plt.xlabel('predicted value') plt.ylabel('true value') plt.show() from sklearn.metrics import confusion_matriximport seaborn as snsmat = confusion_matrix(ytest, ymodel)sns.heatmap(mat, square=True, annot=True, cbar=False)plt.xlabel('predicted value')plt.ylabel('true value')plt.show();Data Science Project on – Handwritten Digits
2020-05-11 13:57:50;In this Data Science Project we will create a Linear Regression model and a Decision Tree Regression Model to Predict Apple’s Stock Price using Machine Learning and Python.Import pandas to import a CSV file:To get the number of training days:#Output- training days = (251, 7)To Visualize the close price Data:To get the close price:Creating a variable to predict ‘X’ days in the future:Create a new target column shifted ‘X’ units/days up:To create a feature dataset (x) and convert into a numpy array and remove last ‘x’ rows/days:#Output- [[185.720001][188.660004][190.919998][190.080002][189. ][183.089996][186.600006][182.779999][179.660004][178.970001][178.229996][177.380005][178.300003][175.070007][173.300003][179.639999][182.539993][185.220001][190.149994][192.580002][194.809998][194.190002][194.149994][192.740005][193.889999][198.449997][197.869995][199.460007][198.779999][198.580002][195.570007][199.800003][199.740005][197.919998][201.550003][202.729996][204.410004][204.229996][200.020004][201.240005][203.229996][201.75 ][203.300003][205.210007][204.5 ][203.350006][205.660004][202.589996][207.220001][208.839996][208.669998][207.020004][207.740005][209.679993][208.779999][213.039993][208.429993][204.020004][193.339996][197. ][199.039993][203.429993][200.990005][200.479996][208.970001][202.75 ][201.740005][206.5 ][210.350006][210.360001][212.639999][212.460007][202.639999][206.490005][204.160004][205.529999][209.009995][208.740005][205.699997][209.190002][213.279999][213.259995][214.169998][216.699997][223.589996][223.089996][218.75 ][219.899994][220.699997][222.770004][220.960007][217.729996][218.720001][217.679993][221.029999][219.889999][218.820007][223.970001][224.589996][218.960007][220.820007][227.009995][227.059998][224.399994][227.029999][230.089996][236.210007][235.869995][235.320007][234.369995][235.279999][236.410004][240.509995][239.960007][243.179993][243.580002][246.580002][249.050003][243.289993][243.259995][248.759995][255.820007][257.5 ][257.130005][257.23999 ][259.429993][260.140015][262.200012][261.959991][264.470001][262.640015][265.76001 ][267.100006][266.290009][263.190002][262.01001 ][261.779999][266.369995][264.290009][267.839996][267.25 ][264.160004][259.450012][261.73999 ][265.579987][270.709991][266.920013][268.480011][270.769989][271.459991][275.149994][279.859985][280.410004][279.73999 ][280.019989][279.440002][284. ][284.269989][289.910004][289.799988][291.519989][293.649994][300.350006][297.429993][299.799988][298.390015][303.190002][309.630005][310.329987][316.959991][312.679993][311.339996][315.23999 ][318.730011][316.570007][317.700012][319.230011][318.309998][308.950012][317.690002][324.339996][323.869995][309.51001 ][308.660004][318.850006][321.450012][325.209991][320.029999][321.549988][319.609985][327.200012][324.869995][324.950012][319. ][323.619995][320.299988][313.049988][298.179993][288.079987][292.649994][273.519989][273.359985][298.809998][289.320007][302.73999 ][292.920013][289.029999][266.170013][285.339996][275.429993][248.229996][277.970001][242.210007][252.860001][246.669998][244.779999][229.240005][224.369995][246.880005][245.520004][258.440002][247.740005][254.809998][254.289993][240.910004][244.929993]]To create a target dataset (y) and convert it to a numpy array and get all of the target values except the last ‘x’ rows days:#Output-[198.449997 197.869995 199.460007 198.779999 198.580002 195.570007199.800003 199.740005 197.919998 201.550003 202.729996 204.410004204.229996 200.020004 201.240005 203.229996 201.75 203.300003205.210007 204.5 203.350006 205.660004 202.589996 207.220001208.839996 208.669998 207.020004 207.740005 209.679993 208.779999213.039993 208.429993 204.020004 193.339996 197. 199.039993203.429993 200.990005 200.479996 208.970001 202.75 201.740005206.5 210.350006 210.360001 212.639999 212.460007 202.639999206.490005 204.160004 205.529999 209.009995 208.740005 205.699997209.190002 213.279999 213.259995 214.169998 216.699997 223.589996223.089996 218.75 219.899994 220.699997 222.770004 220.960007217.729996 218.720001 217.679993 221.029999 219.889999 218.820007223.970001 224.589996 218.960007 220.820007 227.009995 227.059998224.399994 227.029999 230.089996 236.210007 235.869995 235.320007234.369995 235.279999 236.410004 240.509995 239.960007 243.179993243.580002 246.580002 249.050003 243.289993 243.259995 248.759995255.820007 257.5 257.130005 257.23999 259.429993 260.140015262.200012 261.959991 264.470001 262.640015 265.76001 267.100006266.290009 263.190002 262.01001 261.779999 266.369995 264.290009267.839996 267.25 264.160004 259.450012 261.73999 265.579987270.709991 266.920013 268.480011 270.769989 271.459991 275.149994279.859985 280.410004 279.73999 280.019989 279.440002 284.284.269989 289.910004 289.799988 291.519989 293.649994 300.350006297.429993 299.799988 298.390015 303.190002 309.630005 310.329987316.959991 312.679993 311.339996 315.23999 318.730011 316.570007317.700012 319.230011 318.309998 308.950012 317.690002 324.339996323.869995 309.51001 308.660004 318.850006 321.450012 325.209991320.029999 321.549988 319.609985 327.200012 324.869995 324.950012;https://thecleverprogrammer.com/2020/05/11/data-science-project-stock-price-prediction-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Decision Tree', 'Linear Regression', 'Regression'];['regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'train', 'label', 'decision tree'];"In this Data Science Project we will create a Linear Regression model and a Decision Tree Regression Model to Predict Apple’s Stock Price using Machine Learning and Python. Import pandas to import a CSV file: import pandas as pd apple = pd.read_csv(""AAPL.csv"") print(apple.head()) import pandas as pdapple = pd.read_csv(""AAPL.csv"")print(apple.head()) To get the number of training days: print(""trainging days ="",apple.shape) print(""trainging days ="",apple.shape) #Output- training days = (251, 7) To Visualize the close price Data: import matplotlib.pyplot as plt import seaborn as sns sns.set() plt.figure(figsize=(10, 4)) plt.title(""Apple's Stock Price"") plt.xlabel(""Days"") plt.ylabel(""Close Price USD ($)"") plt.plot(apple[""Close Price""]) plt.show() import matplotlib.pyplot as pltimport seaborn as snssns.set()plt.figure(figsize=(10, 4))plt.title(""Apple's Stock Price"")plt.xlabel(""Days"")plt.ylabel(""Close Price USD ($)"")plt.plot(apple[""Close Price""])plt.show() To get the close price: apple = apple[[""Close Price""]] print(apple.head()) apple = apple[[""Close Price""]]print(apple.head()) Creating a variable to predict ‘X’ days in the future: futureDays = 25 futureDays = 25 Create a new target column shifted ‘X’ units/days up: apple[""Prediction""] = apple[[""Close Price""]].shift(-futureDays) print(apple.head()) print(apple.tail()) apple[""Prediction""] = apple[[""Close Price""]].shift(-futureDays)print(apple.head())print(apple.tail()) To create a feature dataset (x) and convert into a numpy array and remove last ‘x’ rows/days: import numpy as np x = np.array(apple.drop([""Prediction""], 1))[:-futureDays] print(x) import numpy as npx = np.array(apple.drop([""Prediction""], 1))[:-futureDays]print(x) #Output- [[185.720001][188.660004][190.919998][190.080002][189. ][183.089996][186.600006][182.779999][179.660004][178.970001][178.229996][177.380005][178.300003][175.070007][173.300003][179.639999][182.539993][185.220001][190.149994][192.580002][194.809998][194.190002][194.149994][192.740005][193.889999][198.449997][197.869995][199.460007][198.779999][198.580002][195.570007][199.800003][199.740005][197.919998][201.550003][202.729996][204.410004][204.229996][200.020004][201.240005][203.229996][201.75 ][203.300003][205.210007][204.5 ][203.350006][205.660004][202.589996][207.220001][208.839996][208.669998][207.020004][207.740005][209.679993][208.779999][213.039993][208.429993][204.020004][193.339996][197. ][199.039993][203.429993][200.990005][200.479996][208.970001][202.75 ][201.740005][206.5 ][210.350006][210.360001][212.639999][212.460007][202.639999][206.490005][204.160004][205.529999][209.009995][208.740005][205.699997][209.190002][213.279999][213.259995][214.169998][216.699997][223.589996][223.089996][218.75 ][219.899994][220.699997][222.770004][220.960007][217.729996][218.720001][217.679993][221.029999][219.889999][218.820007][223.970001][224.589996][218.960007][220.820007][227.009995][227.059998][224.399994][227.029999][230.089996][236.210007][235.869995][235.320007][234.369995][235.279999][236.410004][240.509995][239.960007][243.179993][243.580002][246.580002][249.050003][243.289993][243.259995][248.759995][255.820007][257.5 ][257.130005][257.23999 ][259.429993][260.140015][262.200012][261.959991][264.470001][262.640015][265.76001 ][267.100006][266.290009][263.190002][262.01001 ][261.779999][266.369995][264.290009][267.839996][267.25 ][264.160004][259.450012][261.73999 ][265.579987][270.709991][266.920013][268.480011][270.769989][271.459991][275.149994][279.859985][280.410004][279.73999 ][280.019989][279.440002][284. ][284.269989][289.910004][289.799988][291.519989][293.649994][300.350006][297.429993][299.799988][298.390015][303.190002][309.630005][310.329987][316.959991][312.679993][311.339996][315.23999 ][318.730011][316.570007][317.700012][319.230011][318.309998][308.950012][317.690002][324.339996][323.869995][309.51001 ][308.660004][318.850006][321.450012][325.209991][320.029999][321.549988][319.609985][327.200012][324.869995][324.950012][319. ][323.619995][320.299988][313.049988][298.179993][288.079987][292.649994][273.519989][273.359985][298.809998][289.320007][302.73999 ][292.920013][289.029999][266.170013][285.339996][275.429993][248.229996][277.970001][242.210007][252.860001][246.669998][244.779999][229.240005][224.369995][246.880005][245.520004][258.440002][247.740005][254.809998][254.289993][240.910004][244.929993]] To create a target dataset (y) and convert it to a numpy array and get all of the target values except the last ‘x’ rows days: y = np.array(apple[""Prediction""])[:-futureDays] print(y) y = np.array(apple[""Prediction""])[:-futureDays]print(y) #Output- [198.449997 197.869995 199.460007 198.779999 198.580002 195.570007199.800003 199.740005 197.919998 201.550003 202.729996 204.410004204.229996 200.020004 201.240005 203.229996 201.75 203.300003205.210007 204.5 203.350006 205.660004 202.589996 207.220001208.839996 208.669998 207.020004 207.740005 209.679993 208.779999213.039993 208.429993 204.020004 193.339996 197. 199.039993203.429993 200.990005 200.479996 208.970001 202.75 201.740005206.5 210.350006 210.360001 212.639999 212.460007 202.639999206.490005 204.160004 205.529999 209.009995 208.740005 205.699997209.190002 213.279999 213.259995 214.169998 216.699997 223.589996223.089996 218.75 219.899994 220.699997 222.770004 220.960007217.729996 218.720001 217.679993 221.029999 219.889999 218.820007223.970001 224.589996 218.960007 220.820007 227.009995 227.059998224.399994 227.029999 230.089996 236.210007 235.869995 235.320007234.369995 235.279999 236.410004 240.509995 239.960007 243.179993243.580002 246.580002 249.050003 243.289993 243.259995 248.759995255.820007 257.5 257.130005 257.23999 259.429993 260.140015262.200012 261.959991 264.470001 262.640015 265.76001 267.100006266.290009 263.190002 262.01001 261.779999 266.369995 264.290009267.839996 267.25 264.160004 259.450012 261.73999 265.579987270.709991 266.920013 268.480011 270.769989 271.459991 275.149994279.859985 280.410004 279.73999 280.019989 279.440002 284.284.269989 289.910004 289.799988 291.519989 293.649994 300.350006297.429993 299.799988 298.390015 303.190002 309.630005 310.329987316.959991 312.679993 311.339996 315.23999 318.730011 316.570007317.700012 319.230011 318.309998 308.950012 317.690002 324.339996323.869995 309.51001 308.660004 318.850006 321.450012 325.209991320.029999 321.549988 319.609985 327.200012 324.869995 324.950012 323.619995 320.299988 313.049988 298.179993 288.079987292.649994 273.519989 273.359985 298.809998 289.320007 302.73999292.920013 289.029999 266.170013 285.339996 275.429993 248.229996277.970001 242.210007 252.860001 246.669998 244.779999 229.240005224.369995 246.880005 245.520004 258.440002 247.740005 254.809998254.289993 240.910004 244.929993 241.410004 262.470001 259.429993266.070007 267.98999 273.25 287.049988 284.429993 286.690002282.799988 276.929993 268.369995 276.100006 275.029999 282.970001283.170013 278.579987 287.730011 293.799988 289.070007 293.160004297.559998 300.630005 303.73999 310.130005] Split the data into 75% training and 25% testing from sklearn.model_selection import train_test_split xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25) from sklearn.model_selection import train_test_splitxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25) Creating Models # Creating the decision tree regressor model from sklearn.tree import DecisionTreeRegressor tree = DecisionTreeRegressor().fit(xtrain, ytrain) # creating the Linear Regression model from sklearn.linear_model import LinearRegression linear = LinearRegression().fit(xtrain, ytrain) # Creating the decision tree regressor modelfrom sklearn.tree import DecisionTreeRegressortree = DecisionTreeRegressor().fit(xtrain, ytrain)​# creating the Linear Regression modelfrom sklearn.linear_model import LinearRegressionlinear = LinearRegression().fit(xtrain, ytrain) To get the last ‘x’ rows/days of the feature dataset: xfuture = apple.drop([""Prediction""], 1)[:-futureDays] xfuture = xfuture.tail(futureDays) xfuture = np.array(xfuture) print(xfuture) xfuture = apple.drop([""Prediction""], 1)[:-futureDays]xfuture = xfuture.tail(futureDays)xfuture = np.array(xfuture)print(xfuture) #Output- [[273.359985][298.809998][289.320007][302.73999 ][292.920013][289.029999][266.170013][285.339996][275.429993][248.229996][277.970001][242.210007][252.860001][246.669998][244.779999][229.240005][224.369995][246.880005][245.520004][258.440002][247.740005][254.809998][254.289993][240.910004][244.929993]] To see the model tree prediction treePrediction = tree.predict(xfuture) print(""Decision Tree prediction ="",treePrediction) treePrediction = tree.predict(xfuture)print(""Decision Tree prediction ="",treePrediction) To see the model linear regression prediction linearPrediction = linear.predict(xfuture) print(""Linear regression Prediction ="",linearPrediction) linearPrediction = linear.predict(xfuture)print(""Linear regression Prediction ="",linearPrediction) Visualize decision tree predictions predictions = treePrediction valid = apple[x.shape[0]:] valid[""Predictions""] = predictions plt.figure(figsize=(10, 6)) plt.title(""Apple's Stock Price Prediction Model(Decision Tree Regressor Model)"") plt.xlabel(""Days"") plt.ylabel(""Close Price USD ($)"") plt.plot(apple[""Close Price""]) plt.plot(valid[[""Close Price"", ""Predictions""]]) plt.legend([""Original"", ""Valid"", ""Predictions""]) plt.show() predictions = treePredictionvalid = apple[x.shape[0]:]valid[""Predictions""] = predictionsplt.figure(figsize=(10, 6))plt.title(""Apple's Stock Price Prediction Model(Decision Tree Regressor Model)"")plt.xlabel(""Days"")plt.ylabel(""Close Price USD ($)"")plt.plot(apple[""Close Price""])plt.plot(valid[[""Close Price"", ""Predictions""]])plt.legend([""Original"", ""Valid"", ""Predictions""])plt.show() Visualize the linear model predictions predictions = linearPrediction valid = apple[x.shape[0]:] valid[""Predictions""] = predictions plt.figure(figsize=(10, 6)) plt.title(""Apple's Stock Price Prediction Model(Linear Regression Model)"") plt.xlabel(""Days"") plt.ylabel(""Close Price USD ($)"") plt.plot(apple[""Close Price""]) plt.plot(valid[[""Close Price"", ""Predictions""]]) plt.legend([""Original"", ""Valid"", ""Predictions""]) plt.show() predictions = linearPredictionvalid = apple[x.shape[0]:]valid[""Predictions""] = predictionsplt.figure(figsize=(10, 6))plt.title(""Apple's Stock Price Prediction Model(Linear Regression Model)"")plt.xlabel(""Days"")plt.ylabel(""Close Price USD ($)"")plt.plot(apple[""Close Price""])plt.plot(valid[[""Close Price"", ""Predictions""]])plt.legend([""Original"", ""Valid"", ""Predictions""])plt.show()";Stock Price Prediction with Machine Learning
2020-05-11 13:57:50;In this Data Science Project we will create a Linear Regression model and a Decision Tree Regression Model to Predict Apple’s Stock Price using Machine Learning and Python.Import pandas to import a CSV file:To get the number of training days:#Output- training days = (251, 7)To Visualize the close price Data:To get the close price:Creating a variable to predict ‘X’ days in the future:Create a new target column shifted ‘X’ units/days up:To create a feature dataset (x) and convert into a numpy array and remove last ‘x’ rows/days:#Output- [[185.720001][188.660004][190.919998][190.080002][189. ][183.089996][186.600006][182.779999][179.660004][178.970001][178.229996][177.380005][178.300003][175.070007][173.300003][179.639999][182.539993][185.220001][190.149994][192.580002][194.809998][194.190002][194.149994][192.740005][193.889999][198.449997][197.869995][199.460007][198.779999][198.580002][195.570007][199.800003][199.740005][197.919998][201.550003][202.729996][204.410004][204.229996][200.020004][201.240005][203.229996][201.75 ][203.300003][205.210007][204.5 ][203.350006][205.660004][202.589996][207.220001][208.839996][208.669998][207.020004][207.740005][209.679993][208.779999][213.039993][208.429993][204.020004][193.339996][197. ][199.039993][203.429993][200.990005][200.479996][208.970001][202.75 ][201.740005][206.5 ][210.350006][210.360001][212.639999][212.460007][202.639999][206.490005][204.160004][205.529999][209.009995][208.740005][205.699997][209.190002][213.279999][213.259995][214.169998][216.699997][223.589996][223.089996][218.75 ][219.899994][220.699997][222.770004][220.960007][217.729996][218.720001][217.679993][221.029999][219.889999][218.820007][223.970001][224.589996][218.960007][220.820007][227.009995][227.059998][224.399994][227.029999][230.089996][236.210007][235.869995][235.320007][234.369995][235.279999][236.410004][240.509995][239.960007][243.179993][243.580002][246.580002][249.050003][243.289993][243.259995][248.759995][255.820007][257.5 ][257.130005][257.23999 ][259.429993][260.140015][262.200012][261.959991][264.470001][262.640015][265.76001 ][267.100006][266.290009][263.190002][262.01001 ][261.779999][266.369995][264.290009][267.839996][267.25 ][264.160004][259.450012][261.73999 ][265.579987][270.709991][266.920013][268.480011][270.769989][271.459991][275.149994][279.859985][280.410004][279.73999 ][280.019989][279.440002][284. ][284.269989][289.910004][289.799988][291.519989][293.649994][300.350006][297.429993][299.799988][298.390015][303.190002][309.630005][310.329987][316.959991][312.679993][311.339996][315.23999 ][318.730011][316.570007][317.700012][319.230011][318.309998][308.950012][317.690002][324.339996][323.869995][309.51001 ][308.660004][318.850006][321.450012][325.209991][320.029999][321.549988][319.609985][327.200012][324.869995][324.950012][319. ][323.619995][320.299988][313.049988][298.179993][288.079987][292.649994][273.519989][273.359985][298.809998][289.320007][302.73999 ][292.920013][289.029999][266.170013][285.339996][275.429993][248.229996][277.970001][242.210007][252.860001][246.669998][244.779999][229.240005][224.369995][246.880005][245.520004][258.440002][247.740005][254.809998][254.289993][240.910004][244.929993]]To create a target dataset (y) and convert it to a numpy array and get all of the target values except the last ‘x’ rows days:#Output-[198.449997 197.869995 199.460007 198.779999 198.580002 195.570007199.800003 199.740005 197.919998 201.550003 202.729996 204.410004204.229996 200.020004 201.240005 203.229996 201.75 203.300003205.210007 204.5 203.350006 205.660004 202.589996 207.220001208.839996 208.669998 207.020004 207.740005 209.679993 208.779999213.039993 208.429993 204.020004 193.339996 197. 199.039993203.429993 200.990005 200.479996 208.970001 202.75 201.740005206.5 210.350006 210.360001 212.639999 212.460007 202.639999206.490005 204.160004 205.529999 209.009995 208.740005 205.699997209.190002 213.279999 213.259995 214.169998 216.699997 223.589996223.089996 218.75 219.899994 220.699997 222.770004 220.960007217.729996 218.720001 217.679993 221.029999 219.889999 218.820007223.970001 224.589996 218.960007 220.820007 227.009995 227.059998224.399994 227.029999 230.089996 236.210007 235.869995 235.320007234.369995 235.279999 236.410004 240.509995 239.960007 243.179993243.580002 246.580002 249.050003 243.289993 243.259995 248.759995255.820007 257.5 257.130005 257.23999 259.429993 260.140015262.200012 261.959991 264.470001 262.640015 265.76001 267.100006266.290009 263.190002 262.01001 261.779999 266.369995 264.290009267.839996 267.25 264.160004 259.450012 261.73999 265.579987270.709991 266.920013 268.480011 270.769989 271.459991 275.149994279.859985 280.410004 279.73999 280.019989 279.440002 284.284.269989 289.910004 289.799988 291.519989 293.649994 300.350006297.429993 299.799988 298.390015 303.190002 309.630005 310.329987316.959991 312.679993 311.339996 315.23999 318.730011 316.570007317.700012 319.230011 318.309998 308.950012 317.690002 324.339996323.869995 309.51001 308.660004 318.850006 321.450012 325.209991320.029999 321.549988 319.609985 327.200012 324.869995 324.950012;https://thecleverprogrammer.com/2020/05/11/stock-price-prediction-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Decision Tree', 'Linear Regression', 'Regression'];['regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'train', 'label', 'decision tree'];"In this Data Science Project we will create a Linear Regression model and a Decision Tree Regression Model to Predict Apple’s Stock Price using Machine Learning and Python. Import pandas to import a CSV file: import pandas as pd apple = pd.read_csv(""AAPL.csv"") print(apple.head())​x import pandas as pdapple = pd.read_csv(""AAPL.csv"")print(apple.head()) To get the number of training days: print(""trainging days ="",apple.shape) print(""trainging days ="",apple.shape) #Output- training days = (251, 7) To Visualize the close price Data: import matplotlib.pyplot as plt import seaborn as sns sns.set() plt.figure(figsize=(10, 4)) plt.title(""Apple's Stock Price"") plt.xlabel(""Days"") plt.ylabel(""Close Price USD ($)"") plt.plot(apple[""Close Price""]) plt.show() import matplotlib.pyplot as pltimport seaborn as snssns.set()plt.figure(figsize=(10, 4))plt.title(""Apple's Stock Price"")plt.xlabel(""Days"")plt.ylabel(""Close Price USD ($)"")plt.plot(apple[""Close Price""])plt.show() To get the close price: apple = apple[[""Close Price""]] print(apple.head()) apple = apple[[""Close Price""]]print(apple.head()) Creating a variable to predict ‘X’ days in the future: futureDays = 25 futureDays = 25 Create a new target column shifted ‘X’ units/days up: apple[""Prediction""] = apple[[""Close Price""]].shift(-futureDays) print(apple.head()) print(apple.tail()) apple[""Prediction""] = apple[[""Close Price""]].shift(-futureDays)print(apple.head())print(apple.tail()) To create a feature dataset (x) and convert into a numpy array and remove last ‘x’ rows/days: import numpy as np x = np.array(apple.drop([""Prediction""], 1))[:-futureDays] print(x) import numpy as npx = np.array(apple.drop([""Prediction""], 1))[:-futureDays]print(x) #Output- [[185.720001][188.660004][190.919998][190.080002][189. ][183.089996][186.600006][182.779999][179.660004][178.970001][178.229996][177.380005][178.300003][175.070007][173.300003][179.639999][182.539993][185.220001][190.149994][192.580002][194.809998][194.190002][194.149994][192.740005][193.889999][198.449997][197.869995][199.460007][198.779999][198.580002][195.570007][199.800003][199.740005][197.919998][201.550003][202.729996][204.410004][204.229996][200.020004][201.240005][203.229996][201.75 ][203.300003][205.210007][204.5 ][203.350006][205.660004][202.589996][207.220001][208.839996][208.669998][207.020004][207.740005][209.679993][208.779999][213.039993][208.429993][204.020004][193.339996][197. ][199.039993][203.429993][200.990005][200.479996][208.970001][202.75 ][201.740005][206.5 ][210.350006][210.360001][212.639999][212.460007][202.639999][206.490005][204.160004][205.529999][209.009995][208.740005][205.699997][209.190002][213.279999][213.259995][214.169998][216.699997][223.589996][223.089996][218.75 ][219.899994][220.699997][222.770004][220.960007][217.729996][218.720001][217.679993][221.029999][219.889999][218.820007][223.970001][224.589996][218.960007][220.820007][227.009995][227.059998][224.399994][227.029999][230.089996][236.210007][235.869995][235.320007][234.369995][235.279999][236.410004][240.509995][239.960007][243.179993][243.580002][246.580002][249.050003][243.289993][243.259995][248.759995][255.820007][257.5 ][257.130005][257.23999 ][259.429993][260.140015][262.200012][261.959991][264.470001][262.640015][265.76001 ][267.100006][266.290009][263.190002][262.01001 ][261.779999][266.369995][264.290009][267.839996][267.25 ][264.160004][259.450012][261.73999 ][265.579987][270.709991][266.920013][268.480011][270.769989][271.459991][275.149994][279.859985][280.410004][279.73999 ][280.019989][279.440002][284. ][284.269989][289.910004][289.799988][291.519989][293.649994][300.350006][297.429993][299.799988][298.390015][303.190002][309.630005][310.329987][316.959991][312.679993][311.339996][315.23999 ][318.730011][316.570007][317.700012][319.230011][318.309998][308.950012][317.690002][324.339996][323.869995][309.51001 ][308.660004][318.850006][321.450012][325.209991][320.029999][321.549988][319.609985][327.200012][324.869995][324.950012][319. ][323.619995][320.299988][313.049988][298.179993][288.079987][292.649994][273.519989][273.359985][298.809998][289.320007][302.73999 ][292.920013][289.029999][266.170013][285.339996][275.429993][248.229996][277.970001][242.210007][252.860001][246.669998][244.779999][229.240005][224.369995][246.880005][245.520004][258.440002][247.740005][254.809998][254.289993][240.910004][244.929993]] To create a target dataset (y) and convert it to a numpy array and get all of the target values except the last ‘x’ rows days: y = np.array(apple[""Prediction""])[:-futureDays] print(y) y = np.array(apple[""Prediction""])[:-futureDays]print(y) #Output- [198.449997 197.869995 199.460007 198.779999 198.580002 195.570007199.800003 199.740005 197.919998 201.550003 202.729996 204.410004204.229996 200.020004 201.240005 203.229996 201.75 203.300003205.210007 204.5 203.350006 205.660004 202.589996 207.220001208.839996 208.669998 207.020004 207.740005 209.679993 208.779999213.039993 208.429993 204.020004 193.339996 197. 199.039993203.429993 200.990005 200.479996 208.970001 202.75 201.740005206.5 210.350006 210.360001 212.639999 212.460007 202.639999206.490005 204.160004 205.529999 209.009995 208.740005 205.699997209.190002 213.279999 213.259995 214.169998 216.699997 223.589996223.089996 218.75 219.899994 220.699997 222.770004 220.960007217.729996 218.720001 217.679993 221.029999 219.889999 218.820007223.970001 224.589996 218.960007 220.820007 227.009995 227.059998224.399994 227.029999 230.089996 236.210007 235.869995 235.320007234.369995 235.279999 236.410004 240.509995 239.960007 243.179993243.580002 246.580002 249.050003 243.289993 243.259995 248.759995255.820007 257.5 257.130005 257.23999 259.429993 260.140015262.200012 261.959991 264.470001 262.640015 265.76001 267.100006266.290009 263.190002 262.01001 261.779999 266.369995 264.290009267.839996 267.25 264.160004 259.450012 261.73999 265.579987270.709991 266.920013 268.480011 270.769989 271.459991 275.149994279.859985 280.410004 279.73999 280.019989 279.440002 284.284.269989 289.910004 289.799988 291.519989 293.649994 300.350006297.429993 299.799988 298.390015 303.190002 309.630005 310.329987316.959991 312.679993 311.339996 315.23999 318.730011 316.570007317.700012 319.230011 318.309998 308.950012 317.690002 324.339996323.869995 309.51001 308.660004 318.850006 321.450012 325.209991320.029999 321.549988 319.609985 327.200012 324.869995 324.950012 323.619995 320.299988 313.049988 298.179993 288.079987292.649994 273.519989 273.359985 298.809998 289.320007 302.73999292.920013 289.029999 266.170013 285.339996 275.429993 248.229996277.970001 242.210007 252.860001 246.669998 244.779999 229.240005224.369995 246.880005 245.520004 258.440002 247.740005 254.809998254.289993 240.910004 244.929993 241.410004 262.470001 259.429993266.070007 267.98999 273.25 287.049988 284.429993 286.690002282.799988 276.929993 268.369995 276.100006 275.029999 282.970001283.170013 278.579987 287.730011 293.799988 289.070007 293.160004297.559998 300.630005 303.73999 310.130005] Split the data into 75% training and 25% testing from sklearn.model_selection import train_test_split xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25) from sklearn.model_selection import train_test_splitxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25) Creating Models # Creating the decision tree regressor model from sklearn.tree import DecisionTreeRegressor tree = DecisionTreeRegressor().fit(xtrain, ytrain) # creating the Linear Regression model from sklearn.linear_model import LinearRegression linear = LinearRegression().fit(xtrain, ytrain) # Creating the decision tree regressor modelfrom sklearn.tree import DecisionTreeRegressortree = DecisionTreeRegressor().fit(xtrain, ytrain)​# creating the Linear Regression modelfrom sklearn.linear_model import LinearRegressionlinear = LinearRegression().fit(xtrain, ytrain) To get the last ‘x’ rows/days of the feature dataset: xfuture = apple.drop([""Prediction""], 1)[:-futureDays] xfuture = xfuture.tail(futureDays) xfuture = np.array(xfuture) print(xfuture) xfuture = apple.drop([""Prediction""], 1)[:-futureDays]xfuture = xfuture.tail(futureDays)xfuture = np.array(xfuture)print(xfuture) #Output- [[273.359985][298.809998][289.320007][302.73999 ][292.920013][289.029999][266.170013][285.339996][275.429993][248.229996][277.970001][242.210007][252.860001][246.669998][244.779999][229.240005][224.369995][246.880005][245.520004][258.440002][247.740005][254.809998][254.289993][240.910004][244.929993]] To see the model tree prediction treePrediction = tree.predict(xfuture) print(""Decision Tree prediction ="",treePrediction) treePrediction = tree.predict(xfuture)print(""Decision Tree prediction ="",treePrediction) To see the model linear regression prediction linearPrediction = linear.predict(xfuture) print(""Linear regression Prediction ="",linearPrediction) linearPrediction = linear.predict(xfuture)print(""Linear regression Prediction ="",linearPrediction) Visualize decision tree predictions predictions = treePrediction valid = apple[x.shape[0]:] valid[""Predictions""] = predictions plt.figure(figsize=(10, 6)) plt.title(""Apple's Stock Price Prediction Model(Decision Tree Regressor Model)"") plt.xlabel(""Days"") plt.ylabel(""Close Price USD ($)"") plt.plot(apple[""Close Price""]) plt.plot(valid[[""Close Price"", ""Predictions""]]) plt.legend([""Original"", ""Valid"", ""Predictions""]) plt.show() predictions = treePredictionvalid = apple[x.shape[0]:]valid[""Predictions""] = predictionsplt.figure(figsize=(10, 6))plt.title(""Apple's Stock Price Prediction Model(Decision Tree Regressor Model)"")plt.xlabel(""Days"")plt.ylabel(""Close Price USD ($)"")plt.plot(apple[""Close Price""])plt.plot(valid[[""Close Price"", ""Predictions""]])plt.legend([""Original"", ""Valid"", ""Predictions""])plt.show() Visualize the linear model predictions predictions = linearPrediction valid = apple[x.shape[0]:] valid[""Predictions""] = predictions plt.figure(figsize=(10, 6)) plt.title(""Apple's Stock Price Prediction Model(Linear Regression Model)"") plt.xlabel(""Days"") plt.ylabel(""Close Price USD ($)"") plt.plot(apple[""Close Price""]) plt.plot(valid[[""Close Price"", ""Predictions""]]) plt.legend([""Original"", ""Valid"", ""Predictions""]) plt.show() predictions = linearPredictionvalid = apple[x.shape[0]:]valid[""Predictions""] = predictionsplt.figure(figsize=(10, 6))plt.title(""Apple's Stock Price Prediction Model(Linear Regression Model)"")plt.xlabel(""Days"")plt.ylabel(""Close Price USD ($)"")plt.plot(apple[""Close Price""])plt.plot(valid[[""Close Price"", ""Predictions""]])plt.legend([""Original"", ""Valid"", ""Predictions""])plt.show()";Stock Price Prediction with Machine Learning
2020-05-14 17:30:05;"One place in Data Science where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified.In this data science project we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories.#Output- [‘alt.atheism’, ‘comp.graphics’, ‘comp.os.ms-windows.misc’, ‘comp.sys.ibm.pc.hardware’, ‘comp.sys.mac.hardware’, ‘comp.windows.x’, ‘misc.forsale’, ‘rec.autos’, ‘rec.motorcycles’, ‘rec.sport.baseball’, ‘rec.sport.hockey’, ‘sci.crypt’, ‘sci.electronics’, ‘sci.med’, ‘sci.space’, ‘soc.religion.christian’, ‘talk.politics.guns’, ‘talk.politics.mideast’, ‘talk.politics.misc’, ‘talk.religion.misc’]For simplicity, we will select just a few of these categories, and download the training and testing set:Here is a representative entry from the data:#Output-From: dmcgee@uluhe.soest.hawaii.edu (Don McGee)Subject: Federal HearingOriginator: dmcgee@uluheOrganization: School of Ocean and Earth Science and TechnologyDistribution: usaLines: 10Fact or rumor….? Madalyn Murray O’Hare an atheist who eliminated theuse of the bible reading and prayer in public schools 15 years ago is nowgoing to appear before the FCC with a petition to stop the reading of theGospel on the airways of America. And she is also campaigning to removeChristmas programs, songs, etc from the public schools. If it is truethen mail to Federal Communications Commission 1919 H Street Washington DC20054 expressing your opposition to her request. Reference Petition number2493.In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers. For this we will use the TF–IDF vectorizer, and create a pipeline that attaches it to a multinomial naive Bayes classifier:With this pipeline, we can apply the model to the training data, and predict labels for the test data:Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator. For example, here is the confusion matrix between the true and predicted labels for the test data:Evidently, even this very simple classifier can successfully separate space talk from computer talk, but it gets confused between talk about religion and talk about Christianity. This is perhaps an expected area of confusion.The very cool thing here is that we now have the tools to determine the category for any string, using the predict() method of this pipeline. Here’s a quick utility function that will return the prediction for a single string:Let’s try it out:sci.spacesoc.religion.christiancomp.graphicsRemember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking. Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective.";https://thecleverprogrammer.com/2020/05/14/data-science-project-on-classification-of-text/;['sklearn'];1.0;[];['ML', 'Text Classification', 'Naive Bayes', 'Classification'];['text classification', 'predict', 'fit', 'model', 'machine learning', 'classif', 'training data', 'naive bayes', 'train', 'label', 'test data'];"One place in Data Science where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified. In this data science project we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories. Let’s download the data and take a look at the target names: from sklearn.datasets import fetch_20newsgroups data = fetch_20newsgroups() print(data.target_names)​x from sklearn.datasets import fetch_20newsgroupsdata = fetch_20newsgroups()print(data.target_names) #Output- [‘alt.atheism’, ‘comp.graphics’, ‘comp.os.ms-windows.misc’, ‘comp.sys.ibm.pc.hardware’, ‘comp.sys.mac.hardware’, ‘comp.windows.x’, ‘misc.forsale’, ‘rec.autos’, ‘rec.motorcycles’, ‘rec.sport.baseball’, ‘rec.sport.hockey’, ‘sci.crypt’, ‘sci.electronics’, ‘sci.med’, ‘sci.space’, ‘soc.religion.christian’, ‘talk.politics.guns’, ‘talk.politics.mideast’, ‘talk.politics.misc’, ‘talk.religion.misc’] For simplicity, we will select just a few of these categories, and download the training and testing set: categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics'] train = fetch_20newsgroups(subset='train', categories=categories) test = fetch_20newsgroups(subset='test', categories=categories) categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']train = fetch_20newsgroups(subset='train', categories=categories)test = fetch_20newsgroups(subset='test', categories=categories) Here is a representative entry from the data: print(train.data[5]) print(train.data[5]) #Output- From: dmcgee@uluhe.soest.hawaii.edu (Don McGee)Subject: Federal HearingOriginator: dmcgee@uluheOrganization: School of Ocean and Earth Science and TechnologyDistribution: usaLines: 10 Fact or rumor….? Madalyn Murray O’Hare an atheist who eliminated theuse of the bible reading and prayer in public schools 15 years ago is nowgoing to appear before the FCC with a petition to stop the reading of theGospel on the airways of America. And she is also campaigning to removeChristmas programs, songs, etc from the public schools. If it is truethen mail to Federal Communications Commission 1919 H Street Washington DC20054 expressing your opposition to her request. Reference Petition number 2493. In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers. For this we will use the TF–IDF vectorizer, and create a pipeline that attaches it to a multinomial naive Bayes classifier: from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import make_pipeline model = make_pipeline(TfidfVectorizer(), MultinomialNB()) from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.pipeline import make_pipeline​model = make_pipeline(TfidfVectorizer(), MultinomialNB()) With this pipeline, we can apply the model to the training data, and predict labels for the test data: model.fit(train.data, train.target) labels = model.predict(test.data) model.fit(train.data, train.target)labels = model.predict(test.data) Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator. For example, here is the confusion matrix between the true and predicted labels for the test data: from sklearn.metrics import confusion_matrix mat = confusion_matrix(test.target, labels) import seaborn as sns import matplotlib.pyplot as plt sns.heatmap(mat.T,square=True,annot=True,fmt='d',cbar=False, xticklabels=train.target_names,yticklabels=train.target_names) plt.xlabel('True Label') plt.ylabel(""Predicted Label"") plt.show() from sklearn.metrics import confusion_matrixmat = confusion_matrix(test.target, labels)import seaborn as snsimport matplotlib.pyplot as pltsns.heatmap(mat.T,square=True,annot=True,fmt='d',cbar=False, xticklabels=train.target_names,yticklabels=train.target_names)plt.xlabel('True Label')plt.ylabel(""Predicted Label"")plt.show() Evidently, even this very simple classifier can successfully separate space talk from computer talk, but it gets confused between talk about religion and talk about Christianity. This is perhaps an expected area of confusion. The very cool thing here is that we now have the tools to determine the category for any string, using the predict() method of this pipeline. Here’s a quick utility function that will return the prediction for a single string: def predict_category(s, train=train,model=model): pred = model.predict([s]) print(train.target_names[pred[0]]) def predict_category(s, train=train,model=model): pred = model.predict([s]) print(train.target_names[pred[0]]) Let’s try it out: predict_category(""sending a payload to the ISS"") predict_category(""sending a payload to the ISS"") sci.space predict_category(""discussing islam vs atheism"") predict_category(""discussing islam vs atheism"") soc.religion.christian predict_category(""determining the screen resolution"") predict_category(""determining the screen resolution"") comp.graphics Remember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking. Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective.";Text Classification with Data Science
2020-05-15 13:10:55;Product reviews are becoming more important with the evolution of traditional brick and mortar retail stores to online shopping. Consumers are posting reviews directly on product pages in real time. With the vast amount of consumer reviews, this creates an opportunity to see how the market reacts to a specific product. We will be attempting to see if we can predict the sentiment of a product review using python and machine learning.You can download this dataset from here.;https://thecleverprogrammer.com/2020/05/15/amazon-product-reviews-sentiment-analysis-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Classification', 'Sentiment Analysis'];['sentiment analysis', 'predict', 'fit', 'model', 'machine learning', 'classif', 'filter', 'training data', 'train'];"Product reviews are becoming more important with the evolution of traditional brick and mortar retail stores to online shopping. Consumers are posting reviews directly on product pages in real time. With the vast amount of consumer reviews, this creates an opportunity to see how the market reacts to a specific product. We will be attempting to see if we can predict the sentiment of a product review using python and machine learning. Let’s Import the necessary Modules and take a look at the data: You can download this dataset from here. import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns import math import warnings warnings.filterwarnings('ignore') # Hides warning warnings.filterwarnings(""ignore"", category=DeprecationWarning) warnings.filterwarnings(""ignore"",category=UserWarning) sns.set_style(""whitegrid"") # Plotting style np.random.seed(7) # seeding random number generator df = pd.read_csv('amazon.csv') print(df.head())​x import matplotlib.pyplot as pltimport pandas as pdimport numpy as npimport seaborn as snsimport mathimport warningswarnings.filterwarnings('ignore') # Hides warningwarnings.filterwarnings(""ignore"", category=DeprecationWarning)warnings.filterwarnings(""ignore"",category=UserWarning)sns.set_style(""whitegrid"") # Plotting stylenp.random.seed(7) # seeding random number generator​df = pd.read_csv('amazon.csv')print(df.head()) Describing the Dataset data = df.copy() data.describe() data = df.copy()data.describe() data.info() data.info() We need to clean up the name column by referencing asins (unique products) since we have 7000 missing values: data[""asins""].unique() data[""asins""].unique() asins_unique = len(data[""asins""].unique()) print(""Number of Unique ASINs: "" + str(asins_unique)) asins_unique = len(data[""asins""].unique())print(""Number of Unique ASINs: "" + str(asins_unique)) #Output– Number of Unique ASINs: 42 Visualizing the distributions of numerical variables: data.hist(bins=50, figsize=(20,15)) plt.show() data.hist(bins=50, figsize=(20,15))plt.show() Outliers in this case are valuable, so we may want to weight reviews that had more than 50+ people who find them helpful. Majority of examples were rated highly (looking at rating distribution). There is twice amount of 5 star ratings than the others ratings combined. Split the data into Train and Test Before we explore the dataset we will split it into training set and test sets. Eventually our goal is to train a sentiment analysis classifier. Since the majority of reviews are positive (5 stars), we will need to do a stratified split on the reviews score to ensure that we don’t train the classifier on imbalanced data. from sklearn.model_selection import StratifiedShuffleSplit print(""Before {}"".format(len(data))) dataAfter = data.dropna(subset=[""reviews.rating""]) # Removes all NAN in reviews.rating print(""After {}"".format(len(dataAfter))) dataAfter[""reviews.rating""] = dataAfter[""reviews.rating""].astype(int) split = StratifiedShuffleSplit(n_splits=5, test_size=0.2) for train_index, test_index in split.split(dataAfter, dataAfter[""reviews.rating""]): strat_train = dataAfter.reindex(train_index) strat_test = dataAfter.reindex(test_index) from sklearn.model_selection import StratifiedShuffleSplitprint(""Before {}"".format(len(data)))dataAfter = data.dropna(subset=[""reviews.rating""])# Removes all NAN in reviews.ratingprint(""After {}"".format(len(dataAfter)))dataAfter[""reviews.rating""] = dataAfter[""reviews.rating""].astype(int)​split = StratifiedShuffleSplit(n_splits=5, test_size=0.2)for train_index, test_index in split.split(dataAfter, dataAfter[""reviews.rating""]): strat_train = dataAfter.reindex(train_index) strat_test = dataAfter.reindex(test_index) #Output- Before 34660 After 34627 We need to see if train and test sets were stratified proportionately in comparison to raw data: print(len(strat_train)) print(len(strat_test)) print(strat_test[""reviews.rating""].value_counts()/len(strat_test)) print(len(strat_train))print(len(strat_test))print(strat_test[""reviews.rating""].value_counts()/len(strat_test)) Data Exploration (Training Set) We will use regular expressions to clean out any unfavorable characters in the dataset, and then preview what the data looks like after cleaning. reviews = strat_train.copy() reviews.head() reviews = strat_train.copy()reviews.head() print(len(reviews[""name""].unique()), len(reviews[""asins""].unique())) print(reviews.info()) print(reviews.groupby(""asins"")[""name""].unique()) print(len(reviews[""name""].unique()), len(reviews[""asins""].unique()))print(reviews.info())print(reviews.groupby(""asins"")[""name""].unique()) Lets see all the different names for this product that have 2 ASINs: different_names = reviews[reviews[""asins""] == ""B00L9EPT8O,B01E6AO69U""][""name""].unique() for name in different_names: print(name) print(reviews[reviews[""asins""] == ""B00L9EPT8O,B01E6AO69U""][""name""].value_counts()) different_names = reviews[reviews[""asins""] == ""B00L9EPT8O,B01E6AO69U""][""name""].unique()for name in different_names: print(name)print(reviews[reviews[""asins""] == ""B00L9EPT8O,B01E6AO69U""][""name""].value_counts()) #Output Echo (White),,, Echo (White),,, Amazon Fire Tv,,, Amazon Fire Tv,,, nan Amazon - Amazon Tap Portable Bluetooth and Wi-Fi Speaker - Black,,, Amazon - Amazon Tap Portable Bluetooth and Wi-Fi Speaker - Black,,, Amazon Fire Hd 10 Tablet, Wi-Fi, 16 Gb, Special Offers - Silver Aluminum,,, Amazon Fire Hd 10 Tablet, Wi-Fi, 16 Gb, Special Offers - Silver Aluminum,,, Amazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, Amazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, Amazon Kindle Fire 5ft USB to Micro-USB Cable (works with most Micro-USB Tablets),,, Amazon Kindle Fire 5ft USB to Micro-USB Cable (works with most Micro-USB Tablets),,, Kindle Dx Leather Cover, Black (fits 9.7 Display, Latest and 2nd Generation Kindle Dxs),, Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,, Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,, Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,, Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,, New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,, Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, Echo (White),,, Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes Special Offers, Tangerine"" Echo (Black),,, Amazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, Echo (Black),,, Echo (Black),,, Amazon Fire Tv,,, Kindle Dx Leather Cover, Black (fits 9.7 Display, Latest and 2nd Generation Kindle Dxs)"",, New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,, Echo (White),,,\r\nEcho (White),,, 2318 Amazon Fire Tv,,,\r\nAmazon Fire Tv,,, 2029 Amazon - Amazon Tap Portable Bluetooth and Wi-Fi Speaker - Black,,,\r\nAmazon - Amazon Tap Portable Bluetooth and Wi-Fi Speaker - Black,,, 259 Amazon Fire Hd 10 Tablet, Wi-Fi, 16 Gb, Special Offers - Silver Aluminum,,,\r\nAmazon Fire Hd 10 Tablet, Wi-Fi, 16 Gb, Special Offers - Silver Aluminum,,, 106 Amazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,\r\nAmazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, 28 Kindle Dx Leather Cover, Black (fits 9.7 Display, Latest and 2nd Generation Kindle Dxs),, 7 Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,\r\nAmazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, 5 Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,,\r\nAmazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,, 5 New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,,\r\nNew Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,, 5 Amazon Kindle Fire 5ft USB to Micro-USB Cable (works with most Micro-USB Tablets),,,\r\nAmazon Kindle Fire 5ft USB to Micro-USB Cable (works with most Micro-USB Tablets),,, 4 Echo (Black),,,\r\nEcho (Black),,, 3 Echo (White),,,\r\nFire Tablet, 7 Display, Wi-Fi, 8 GB - Includes Special Offers, Tangerine"" 1 Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,,\r\nAmazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, 1 Echo (Black),,,\r\nAmazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, 1 New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,,\r\n 1 Amazon Fire Tv,,,\r\nKindle Dx Leather Cover, Black (fits 9.7 Display, Latest and 2nd Generation Kindle Dxs)"",, 1 Name: name, dtype: int64 #OutputEcho (White),,,Echo (White),,,Amazon Fire Tv,,,Amazon Fire Tv,,,nanAmazon - Amazon Tap Portable Bluetooth and Wi-Fi Speaker - Black,,,Amazon - Amazon Tap Portable Bluetooth and Wi-Fi Speaker - Black,,,Amazon Fire Hd 10 Tablet, Wi-Fi, 16 Gb, Special Offers - Silver Aluminum,,,Amazon Fire Hd 10 Tablet, Wi-Fi, 16 Gb, Special Offers - Silver Aluminum,,,Amazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,Amazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,Amazon Kindle Fire 5ft USB to Micro-USB Cable (works with most Micro-USB Tablets),,,Amazon Kindle Fire 5ft USB to Micro-USB Cable (works with most Micro-USB Tablets),,,Kindle Dx Leather Cover, Black (fits 9.7 Display, Latest and 2nd Generation Kindle Dxs),,Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,,Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,,Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,,Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,,New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,,Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,Echo (White),,,Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes Special Offers, Tangerine""Echo (Black),,,Amazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,Echo (Black),,,Echo (Black),,,Amazon Fire Tv,,,Kindle Dx Leather Cover, Black (fits 9.7 Display, Latest and 2nd Generation Kindle Dxs)"",,New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,,​Echo (White),,,\r\nEcho (White),,, 2318Amazon Fire Tv,,,\r\nAmazon Fire Tv,,, 2029Amazon - Amazon Tap Portable Bluetooth and Wi-Fi Speaker - Black,,,\r\nAmazon - Amazon Tap Portable Bluetooth and Wi-Fi Speaker - Black,,, 259Amazon Fire Hd 10 Tablet, Wi-Fi, 16 Gb, Special Offers - Silver Aluminum,,,\r\nAmazon Fire Hd 10 Tablet, Wi-Fi, 16 Gb, Special Offers - Silver Aluminum,,, 106Amazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,\r\nAmazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, 28Kindle Dx Leather Cover, Black (fits 9.7 Display, Latest and 2nd Generation Kindle Dxs),, 7Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,\r\nAmazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, 5Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,,\r\nAmazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,, 5New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,,\r\nNew Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,, 5Amazon Kindle Fire 5ft USB to Micro-USB Cable (works with most Micro-USB Tablets),,,\r\nAmazon Kindle Fire 5ft USB to Micro-USB Cable (works with most Micro-USB Tablets),,, 4Echo (Black),,,\r\nEcho (Black),,, 3Echo (White),,,\r\nFire Tablet, 7 Display, Wi-Fi, 8 GB - Includes Special Offers, Tangerine"" 1Amazon Fire Hd 6 Standing Protective Case(4th Generation - 2014 Release), Cayenne Red,,,\r\nAmazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, 1Echo (Black),,,\r\nAmazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders,,, 1New Amazon Kindle Fire Hd 9w Powerfast Adapter Charger + Micro Usb Angle Cable,,,\r\n 1Amazon Fire Tv,,,\r\nKindle Dx Leather Cover, Black (fits 9.7 Display, Latest and 2nd Generation Kindle Dxs)"",, 1Name: name, dtype: int64 The output confirmed that each ASIN can have multiple names. Therefore we should only really concern ourselves with which ASINs do well, not the product names. fig = plt.figure(figsize=(16,10)) ax1 = plt.subplot(211) ax2 = plt.subplot(212, sharex = ax1) reviews[""asins""].value_counts().plot(kind=""bar"", ax=ax1, title=""ASIN Frequency"") np.log10(reviews[""asins""].value_counts()).plot(kind=""bar"", ax=ax2, title=""ASIN Frequency (Log10 Adjusted)"") plt.show() fig = plt.figure(figsize=(16,10))ax1 = plt.subplot(211)ax2 = plt.subplot(212, sharex = ax1)reviews[""asins""].value_counts().plot(kind=""bar"", ax=ax1, title=""ASIN Frequency"")np.log10(reviews[""asins""].value_counts()).plot(kind=""bar"", ax=ax2, title=""ASIN Frequency (Log10 Adjusted)"")plt.show() Entire training dataset average rating print(reviews[""reviews.rating""].mean()) asins_count_ix = reviews[""asins""].value_counts().index plt.subplots(2,1,figsize=(16,12)) plt.subplot(2,1,1) reviews[""asins""].value_counts().plot(kind=""bar"", title=""ASIN Frequency"") plt.subplot(2,1,2) sns.pointplot(x=""asins"", y=""reviews.rating"", order=asins_count_ix, data=reviews) plt.xticks(rotation=90) plt.show() print(reviews[""reviews.rating""].mean())​asins_count_ix = reviews[""asins""].value_counts().indexplt.subplots(2,1,figsize=(16,12))plt.subplot(2,1,1)reviews[""asins""].value_counts().plot(kind=""bar"", title=""ASIN Frequency"")plt.subplot(2,1,2)sns.pointplot(x=""asins"", y=""reviews.rating"", order=asins_count_ix, data=reviews)plt.xticks(rotation=90)plt.show() Sentiment Analysis Using the features in place, we will build a classifier that can determine a review’s sentiment. def sentiments(rating): if (rating == 5) or (rating == 4): return ""Positive"" elif rating == 3: return ""Neutral"" elif (rating == 2) or (rating == 1): return ""Negative"" # Add sentiments to the data strat_train[""Sentiment""] = strat_train[""reviews.rating""].apply(sentiments) strat_test[""Sentiment""] = strat_test[""reviews.rating""].apply(sentiments) print(strat_train[""Sentiment""][:20]) def sentiments(rating): if (rating == 5) or (rating == 4): return ""Positive"" elif rating == 3: return ""Neutral"" elif (rating == 2) or (rating == 1): return ""Negative""# Add sentiments to the datastrat_train[""Sentiment""] = strat_train[""reviews.rating""].apply(sentiments)strat_test[""Sentiment""] = strat_test[""reviews.rating""].apply(sentiments)print(strat_train[""Sentiment""][:20]) #Output- 4349 Positive 30776 Positive 28775 Neutral 1136 Positive 17803 Positive 7336 Positive 32638 Positive 13995 Positive 6728 Negative 22009 Positive 11047 Positive 22754 Positive 5578 Positive 11673 Positive 19168 Positive 14903 Positive 30843 Positive 5440 Positive 28940 Positive 31258 Positive Name: Sentiment, dtype: object 4349 Positive30776 Positive28775 Neutral1136 Positive17803 Positive7336 Positive32638 Positive13995 Positive6728 Negative22009 Positive11047 Positive22754 Positive5578 Positive11673 Positive19168 Positive14903 Positive30843 Positive5440 Positive28940 Positive31258 PositiveName: Sentiment, dtype: object";Amazon Product Reviews Sentiment Analysis with Machine Learning
2020-05-15 00:33:31;In Data Science the HOG (Histogram of Gradients) is a straightforward feature extraction process that was developed with the idea of identifying pedestrians within images. A fast HOG extractor is built into the Scikit-Image project, and we can try it out relatively quickly and visualize the oriented gradients within each cell:;https://thecleverprogrammer.com/2020/05/15/data-science-project-on-extracting-hog-features/;['skimage'];0.7;[];[];['filter'];"In Data Science the HOG (Histogram of Gradients) is a straightforward feature extraction process that was developed with the idea of identifying pedestrians within images. HOG involves the following steps: Optionally prenormalize images. This leads to features that resist dependence on variations in illumination.Convolve the image with two filters that are sensitive to horizontal and vertical brightness gradients. These capture edge, contour, and texture information.Subdivide the image into cells of a predetermined size, and compute a histogram of the orientations within each cell.Normalize the histogram in each cell by comparing to the block of neighboring cells. This further suppresses the effect of illumination across the image.Construct a one-dimensional feature vector from the information in each cell. A fast HOG extractor is built into the Scikit-Image project, and we can try it out relatively quickly and visualize the oriented gradients within each cell: from skimage import data, color, feature image = color.rgb2gray(data.chelsea()) hogVec, hogVis = feature.hog(image, visualize=True) import matplotlib.pyplot as plt fig, ax = plt.subplots(1, 2, figsize=(12, 6), subplot_kw=dict(xticks=[], yticks=[])) ax[0].imshow(image, cmap='gray') ax[0].set_title('input image') ax[1].imshow(hogVis) ax[1].set_title(""extarcting features from image"") plt.show()​x from skimage import data, color, featureimage = color.rgb2gray(data.chelsea())hogVec, hogVis = feature.hog(image, visualize=True)import matplotlib.pyplot as pltfig, ax = plt.subplots(1, 2, figsize=(12, 6), subplot_kw=dict(xticks=[], yticks=[]))ax[0].imshow(image, cmap='gray')ax[0].set_title('input image')ax[1].imshow(hogVis)ax[1].set_title(""extarcting features from image"")plt.show()";Extracting HOG Features
2020-05-17 01:27:52;In this tutorial, I will show you how to build your very own chatbot using Python. There are broadly two variants of chatbots, Rule-based and Self-learning. A rule-based bot uses some rules on which it is trained, while a self-learning bot uses some machine-learning-based approach to chat.In this tutorial, I will show you how to create a simple and quick chatbot in python using a rule-based approach.;https://thecleverprogrammer.com/2020/05/17/build-your-own-chatbot-with-python/;['pattern', 'nltk'];1.0;[];['Chatbot'];['train', 'recogn', 'chatbot'];"In this tutorial, I will show you how to build your very own chatbot using Python. There are broadly two variants of chatbots, Rule-based and Self-learning. A rule-based bot uses some rules on which it is trained, while a self-learning bot uses some machine-learning-based approach to chat. In this tutorial, I will show you how to create a simple and quick chatbot in python using a rule-based approach. Lets Import the libraries from nltk.chat.util import Chat, reflections​x from nltk.chat.util import Chat, reflections Create the chatbots list of recognizable patterns and it’s a response to those patterns. To do this we will create a variable called pairs. #Pairs is a list of patterns and responses. pairs = [ [ r""(.*)my name is (.*)"", [""Hello %2, How are you today ?"",] ], [ r""(.*)help(.*) "", [""I can help you "",] ], [ r""(.*) your name ?"", [""My name is thecleverprogrammer, but you can just call me robot and I'm a chatbot ."",] ], [ r""how are you (.*) ?"", [""I'm doing very well"", ""i am great !""] ], [ r""sorry (.*)"", [""Its alright"",""Its OK, never mind that"",] ], [ r""i'm (.*) (good|well|okay|ok)"", [""Nice to hear that"",""Alright, great !"",] ], [ r""(hi|hey|hello|hola|holla)(.*)"", [""Hello"", ""Hey there"",] ], [ r""what (.*) want ?"", [""Make me an offer I can't refuse"",] ], [ r""(.*)created(.*)"", [""Aman Kharwal created me using Python's NLTK library "",""top secret ;)"",] ], [ r""(.*) (location|city) ?"", ['New Delhi, India',] ], [ r""(.*)raining in (.*)"", [""No rain in the past 4 days here in %2"",""In %2 there is a 50% chance of rain"",] ], [ r""how (.*) health (.*)"", [""Health is very important, but I am a computer, so I don't need to worry about my health "",] ], [ r""(.*)(sports|game|sport)(.*)"", [""I'm a very big fan of Cricket"",] ], [ r""who (.*) (Cricketer|Batsman)?"", [""Virat Kohli""] ], [ r""quit"", [""Bye for now. See you soon :) "",""It was nice talking to you. See you soon :)""] ], [ r""(.*)"", ['That is nice to hear'] ], ] #Pairs is a list of patterns and responses.pairs = [ [ r""(.*)my name is (.*)"", [""Hello %2, How are you today ?"",] ], [ r""(.*)help(.*) "", [""I can help you "",] ], [ r""(.*) your name ?"", [""My name is thecleverprogrammer, but you can just call me robot and I'm a chatbot ."",] ], [ r""how are you (.*) ?"", [""I'm doing very well"", ""i am great !""] ], [ r""sorry (.*)"", [""Its alright"",""Its OK, never mind that"",] ], [ r""i'm (.*) (good|well|okay|ok)"", [""Nice to hear that"",""Alright, great !"",] ], [ r""(hi|hey|hello|hola|holla)(.*)"", [""Hello"", ""Hey there"",] ], [ r""what (.*) want ?"", [""Make me an offer I can't refuse"",] ], [ r""(.*)created(.*)"", [""Aman Kharwal created me using Python's NLTK library "",""top secret ;)"",] ], [ r""(.*) (location|city) ?"", ['New Delhi, India',] ], [ r""(.*)raining in (.*)"", [""No rain in the past 4 days here in %2"",""In %2 there is a 50% chance of rain"",] ], [ r""how (.*) health (.*)"", [""Health is very important, but I am a computer, so I don't need to worry about my health "",] ], [ r""(.*)(sports|game|sport)(.*)"", [""I'm a very big fan of Cricket"",] ], [ r""who (.*) (Cricketer|Batsman)?"", [""Virat Kohli""] ], [ r""quit"", [""Bye for now. See you soon :) "",""It was nice talking to you. See you soon :)""] ], [ r""(.*)"", ['That is nice to hear'] ],] Okay, so as we finished the patterns and responses, let’s take a look at something called reflections. Reflections is a dictionary file that contains a set of input values and corresponding output values. For example, if the string input was “I am a programmer”, then the output would be “you are a programmer”. print(reflections) print(reflections) #Output {'i am': 'you are', 'i was': 'you were', 'i': 'you', ""i'm"": 'you are', ""i'd"": 'you would', ""i've"": 'you have', ""i'll"": 'you will', 'my': 'your', 'you are': 'I am', 'you were': 'I was', ""you've"": 'I have', ""you'll"": 'I will', 'your': 'my', 'yours': 'mine', 'you': 'me', 'me': 'you'} #Output{'i am': 'you are','i was': 'you were','i': 'you',""i'm"": 'you are',""i'd"": 'you would',""i've"": 'you have',""i'll"": 'you will','my': 'your','you are': 'I am','you were': 'I was',""you've"": 'I have',""you'll"": 'I will','your': 'my','yours': 'mine','you': 'me','me': 'you'} We can also create our own reflections dictionary in the same format as reflections above. Below is an example of how to do this: my_dummy_reflections= { ""go"" : ""gone"", ""hello"" : ""hey there"" } my_dummy_reflections= { ""go"" : ""gone"", ""hello"" : ""hey there""} Now let’s print a default message, and finish our chatbot: #default message at the start of chat print(""Hi, I'm thecleverprogrammer and I like to chat\nPlease type lowercase English language to start a conversation. Type quit to leave "") #Create Chat Bot chat = Chat(pairs, reflections) #default message at the start of chatprint(""Hi, I'm thecleverprogrammer and I like to chat\nPlease type lowercase English language to start a conversation. Type quit to leave "")#Create Chat Botchat = Chat(pairs, reflections) Now, let’s start a conversation #Start conversation chat.converse() #Start conversationchat.converse() Follow us on Instagram for all your Queries Instagram";Build your own Chatbot with Python
2020-05-17 20:14:12;Email spam, are also called as junk emails, are unsolicited messages sent in bulk by email (spamming).In this Data Science Project I will show you how to detect email spam using Machine Learning technique called Natural Language Processing and Python.Import the libraries :Load the data and print the first 5 rows :You can download this data set from here.Now let’s explore the data and get the number of rows & columns :#Output- (5728, 2)To get the column names in the data set :#Output- Index([‘text’, ‘spam’], dtype=’object’)To check for duplicates and remove them :#Output- (5695, 2)To see the number of missing data for each column :#Output- text 0 spam 0 dtype: int64 Stop words in natural language processing, are useless words (data).Now Create a function to clean the text and return the tokens. The cleaning of the text can be done by first removing punctuation and then removing the useless words also known as stop words.Now convert the text into a matrix of token counts :Now we need to split the data into training and testing sets, and then we will use this one row of data for testing to make our prediction later on and test to see if the prediction matches with the actual value.Now we need to create and train the Multinomial Naive Bayes classifier which is suitable for classification with discrete features.To see the classifiers prediction and actual values on the data set :#Output-[0 0 0 … 0 0 0][0 0 0 … 0 0 0]Now let’s see how well our model performed by evaluating the Naive Bayes classifier and the report, confusion matrix & accuracy score.It looks like the model used is 99.71% accurate. Let’s test the model  on the test data set (xtest &  ytest) by printing the predicted value, and the actual value to see if the model can accurately classify the email text.#Output-[1 0 0 … 0 0 0] [1 0 0 … 0 0 0]Now let’s evaluate the model on the test data set :The classifier accurately identified the email messages as spam or not spam with 99.2 % accuracy on the test data.;https://thecleverprogrammer.com/2020/05/17/data-science-project-email-spam-detection-with-machine-learning/;['sklearn', 'nltk'];1.0;[];['ML', 'NLP', 'Naive Bayes', 'Classification'];['detect', 'predict', 'fit', 'model', 'machine learning', 'classif', 'natural language processing', 'training data', 'naive bayes', 'train', 'test data'];"Email spam, are also called as junk emails, are unsolicited messages sent in bulk by email (spamming). In this Data Science Project I will show you how to detect email spam using Machine Learning technique called Natural Language Processing and Python. So this program will detect if an email is spam (1) or not (0) Import the libraries : import numpy as np import pandas as pd import nltk from nltk.corpus import stopwords import string​x import numpy as npimport pandas as pdimport nltkfrom nltk.corpus import stopwordsimport string Load the data and print the first 5 rows : You can download this data set from here. df = pd.read_csv(""emails.csv"") df.head() df = pd.read_csv(""emails.csv"")df.head() #Output text spam 0	Subject: naturally irresistible your corporate...	1 1	Subject: the stock trading gunslinger fanny i...	1 2	Subject: unbelievable new homes made easy im ...	1 3	Subject: 4 color printing special request add...	1 4	Subject: do not have money , get software cds ...	1 #Output text spam0 Subject: naturally irresistible your corporate... 11 Subject: the stock trading gunslinger fanny i... 12 Subject: unbelievable new homes made easy im ... 13 Subject: 4 color printing special request add... 14 Subject: do not have money , get software cds ... 1 Now let’s explore the data and get the number of rows & columns : df.shape df.shape #Output- (5728, 2) To get the column names in the data set : df.columns df.columns #Output- Index([‘text’, ‘spam’], dtype=’object’) To check for duplicates and remove them : df.drop_duplicates(inplace=True) print(df.shape) df.drop_duplicates(inplace=True)print(df.shape) #Output- (5695, 2) To see the number of missing data for each column : print(df.isnull().sum()) print(df.isnull().sum()) #Output- text 0 spam 0 dtype: int64 Now Download the stop words Stop words in natural language processing, are useless words (data). # download the stopwords package nltk.download(""stopwords"") # download the stopwords packagenltk.download(""stopwords"") Now Create a function to clean the text and return the tokens. The cleaning of the text can be done by first removing punctuation and then removing the useless words also known as stop words. def process(text): nopunc = [char for char in text if char not in string.punctuation] nopunc = ''.join(nopunc) clean = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')] return clean # to show the tokenization df['text'].head().apply(process) def process(text): nopunc = [char for char in text if char not in string.punctuation] nopunc = ''.join(nopunc)​ clean = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')] return clean# to show the tokenizationdf['text'].head().apply(process) #Output 0 [Subject, naturally, irresistible, corporate, ... 1 [Subject, stock, trading, gunslinger, fanny, m... 2 [Subject, unbelievable, new, homes, made, easy... 3 [Subject, 4, color, printing, special, request... 4 [Subject, money, get, software, cds, software,... Name: text, dtype: objec #Output0 [Subject, naturally, irresistible, corporate, ...1 [Subject, stock, trading, gunslinger, fanny, m...2 [Subject, unbelievable, new, homes, made, easy...3 [Subject, 4, color, printing, special, request...4 [Subject, money, get, software, cds, software,...Name: text, dtype: objec Now convert the text into a matrix of token counts : from sklearn.feature_extraction.text import CountVectorizer message = CountVectorizer(analyzer=process).fit_transform(df['text']) from sklearn.feature_extraction.text import CountVectorizermessage = CountVectorizer(analyzer=process).fit_transform(df['text']) Now we need to split the data into training and testing sets, and then we will use this one row of data for testing to make our prediction later on and test to see if the prediction matches with the actual value. #split the data into 80% training and 20% testing from sklearn.model_selection import train_test_split xtrain, xtest, ytrain, ytest = train_test_split(message, df['spam'], test_size=0.20, random_state=0) # To see the shape of the data print(message.shape) #split the data into 80% training and 20% testingfrom sklearn.model_selection import train_test_splitxtrain, xtest, ytrain, ytest = train_test_split(message, df['spam'], test_size=0.20, random_state=0)# To see the shape of the dataprint(message.shape) Now we need to create and train the Multinomial Naive Bayes classifier which is suitable for classification with discrete features. # create and train the Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB classifier = MultinomialNB().fit(xtrain, ytrain) # create and train the Naive Bayes Classifierfrom sklearn.naive_bayes import MultinomialNBclassifier = MultinomialNB().fit(xtrain, ytrain) To see the classifiers prediction and actual values on the data set : print(classifier.predict(xtrain)) print(ytrain.values) print(classifier.predict(xtrain))print(ytrain.values) #Output-[0 0 0 … 0 0 0][0 0 0 … 0 0 0] Now let’s see how well our model performed by evaluating the Naive Bayes classifier and the report, confusion matrix & accuracy score. # Evaluating the model on the training data set from sklearn.metrics import classification_report, confusion_matrix, accuracy_score pred = classifier.predict(xtrain) print(classification_report(ytrain, pred)) print() print(""Confusion Matrix: \n"", confusion_matrix(ytrain, pred)) print(""Accuracy: \n"", accuracy_score(ytrain, pred)) # Evaluating the model on the training data setfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_scorepred = classifier.predict(xtrain)print(classification_report(ytrain, pred))print()print(""Confusion Matrix: \n"", confusion_matrix(ytrain, pred))print(""Accuracy: \n"", accuracy_score(ytrain, pred)) #Output precision recall f1-score support 0 1.00 1.00 1.00 3457 1 0.99 1.00 0.99 1099 accuracy 1.00 4556 macro avg 0.99 1.00 1.00 4556 weighted avg 1.00 1.00 1.00 4556 Confusion Matrix: [[3445 12] [ 1 1098]] Accuracy: 0.9971466198419666 #Output precision recall f1-score support​ 0 1.00 1.00 1.00 3457 1 0.99 1.00 0.99 1099​ accuracy 1.00 4556 macro avg 0.99 1.00 1.00 4556weighted avg 1.00 1.00 1.00 4556​​Confusion Matrix: [[3445 12] [ 1 1098]]Accuracy: 0.9971466198419666 It looks like the model used is 99.71% accurate. Let’s test the model on the test data set (xtest & ytest) by printing the predicted value, and the actual value to see if the model can accurately classify the email text. #print the predictions print(classifier.predict(xtest)) #print the actual values print(ytest.values) #print the predictionsprint(classifier.predict(xtest))#print the actual valuesprint(ytest.values) #Output-[1 0 0 … 0 0 0] [1 0 0 … 0 0 0] Now let’s evaluate the model on the test data set : # Evaluating the model on the training data set from sklearn.metrics import classification_report, confusion_matrix, accuracy_score pred = classifier.predict(xtest) print(classification_report(ytest, pred)) print() print(""Confusion Matrix: \n"", confusion_matrix(ytest, pred)) print(""Accuracy: \n"", accuracy_score(ytest, pred)) # Evaluating the model on the training data setfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_scorepred = classifier.predict(xtest)print(classification_report(ytest, pred))print()print(""Confusion Matrix: \n"", confusion_matrix(ytest, pred))print(""Accuracy: \n"", accuracy_score(ytest, pred)) #Output precision recall f1-score support 0 1.00 0.99 0.99 870 1 0.97 1.00 0.98 269 accuracy 0.99 1139 macro avg 0.98 0.99 0.99 1139 weighted avg 0.99 0.99 0.99 1139 Confusion Matrix: [[862 8] [ 1 268]] Accuracy: 0.9920983318700615 #Output precision recall f1-score support​ 0 1.00 0.99 0.99 870 1 0.97 1.00 0.98 269​ accuracy 0.99 1139 macro avg 0.98 0.99 0.99 1139weighted avg 0.99 0.99 0.99 1139​​Confusion Matrix: [[862 8] [ 1 268]]Accuracy: 0.9920983318700615 The classifier accurately identified the email messages as spam or not spam with 99.2 % accuracy on the test data. Follow us on Instagram for all your Queries Instagram";Email spam Detection with Machine Learning
2020-05-20 14:20:01;Heart disease describes a range of conditions that affect your heart. Diseases under the heart disease umbrella include blood vessel diseases, such as coronary artery disease, heart rhythm problems (arrhythmia) and heart defects you’re born with (congenital heart defects), among others.Heart disease is one of the biggest causes of morbidity and mortality among the population of the world. Prediction of cardiovascular disease is regarded as one of the most important subjects in the section of clinical data science. The amount of data in the healthcare industry is huge.In this Data Science Project I will be applying Machine Learning techniques to classify whether a person is suffering from Heart Disease or not.Importing the required modules :Here we will be experimenting using KNeighborsClassifier :To get correlation of each feature in the data setIt’s always a good practice to work with a data set where the target classes are of approximately equal size. Thus, let’s check for the same :;https://thecleverprogrammer.com/2020/05/20/data-science-project-heart-disease-prediction-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Random Forest', 'Classification'];['predict', 'fit', 'model', 'machine learning', 'random forest', 'classif', 'filter', 'train', 'label'];"Heart disease describes a range of conditions that affect your heart. Diseases under the heart disease umbrella include blood vessel diseases, such as coronary artery disease, heart rhythm problems (arrhythmia) and heart defects you’re born with (congenital heart defects), among others. Heart disease is one of the biggest causes of morbidity and mortality among the population of the world. Prediction of cardiovascular disease is regarded as one of the most important subjects in the section of clinical data science. The amount of data in the healthcare industry is huge. In this Data Science Project I will be applying Machine Learning techniques to classify whether a person is suffering from Heart Disease or not. You can download the data set we need for this project from here: heartDownload Importing the required modules : import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import rcParams import seaborn as sns import warnings warnings.filterwarnings('ignore')​x import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom matplotlib import rcParamsimport seaborn as snsimport warningswarnings.filterwarnings('ignore') Here we will be experimenting using KNeighborsClassifier : from sklearn.neighbors import KNeighborsClassifier from sklearn.neighbors import KNeighborsClassifier Now let’s dive into the data df = pd.read_csv('dataset.csv') print(df.head()) df = pd.read_csv('dataset.csv')print(df.head()) print(df.info()) print(df.info()) Output <class 'pandas.core.frame.DataFrame'> RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): age 303 non-null int64 sex 303 non-null int64 cp 303 non-null int64 trestbps 303 non-null int64 chol 303 non-null int64 fbs 303 non-null int64 restecg 303 non-null int64 thalach 303 non-null int64 exang 303 non-null int64 oldpeak 303 non-null float64 slope 303 non-null int64 ca 303 non-null int64 thal 303 non-null int64 target 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.2 KB Output<class 'pandas.core.frame.DataFrame'>RangeIndex: 303 entries, 0 to 302Data columns (total 14 columns):age 303 non-null int64sex 303 non-null int64cp 303 non-null int64trestbps 303 non-null int64chol 303 non-null int64fbs 303 non-null int64restecg 303 non-null int64thalach 303 non-null int64exang 303 non-null int64oldpeak 303 non-null float64slope 303 non-null int64ca 303 non-null int64thal 303 non-null int64target 303 non-null int64dtypes: float64(1), int64(13)memory usage: 33.2 KB print(df.describe()) print(df.describe()) Feature Selection To get correlation of each feature in the data set import seaborn as sns corrmat = df.corr() top_corr_features = corrmat.index plt.figure(figsize=(16,16)) #plot heat map g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=""RdYlGn"") plt.show() import seaborn as snscorrmat = df.corr()top_corr_features = corrmat.indexplt.figure(figsize=(16,16))#plot heat mapg=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=""RdYlGn"")plt.show() It’s always a good practice to work with a data set where the target classes are of approximately equal size. Thus, let’s check for the same : sns.set_style('whitegrid') sns.countplot(x='target',data=df,palette='RdBu_r') plt.show() sns.set_style('whitegrid')sns.countplot(x='target',data=df,palette='RdBu_r')plt.show() Data Processing After exploring the data set, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. First, I’ll use the get_dummies method to create dummy columns for categorical variables. dataset = pd.get_dummies(df, columns = ['sex', 'cp', 'fbs','restecg', 'exang', 'slope', 'ca', 'thal']) from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler standardScaler = StandardScaler() columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak'] dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale]) dataset.head() dataset = pd.get_dummies(df, columns = ['sex', 'cp', 'fbs','restecg', 'exang', 'slope', 'ca', 'thal'])from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerstandardScaler = StandardScaler()columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])dataset.head() y = dataset['target'] X = dataset.drop(['target'], axis = 1) y = dataset['target']X = dataset.drop(['target'], axis = 1) from sklearn.model_selection import cross_val_score knn_scores = [] for k in range(1,21): knn_classifier = KNeighborsClassifier(n_neighbors = k) score=cross_val_score(knn_classifier,X,y,cv=10) knn_scores.append(score.mean()) from sklearn.model_selection import cross_val_scoreknn_scores = []for k in range(1,21): knn_classifier = KNeighborsClassifier(n_neighbors = k) score=cross_val_score(knn_classifier,X,y,cv=10) knn_scores.append(score.mean()) plt.plot([k for k in range(1, 21)], knn_scores, color = 'red') for i in range(1,21): plt.text(i, knn_scores[i-1], (i, knn_scores[i-1])) plt.xticks([i for i in range(1, 21)]) plt.xlabel('Number of Neighbors (K)') plt.ylabel('Scores') plt.title('K Neighbors Classifier scores for different K values') plt.show() plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')for i in range(1,21): plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))plt.xticks([i for i in range(1, 21)])plt.xlabel('Number of Neighbors (K)')plt.ylabel('Scores')plt.title('K Neighbors Classifier scores for different K values')plt.show() knn_classifier = KNeighborsClassifier(n_neighbors = 12) score=cross_val_score(knn_classifier,X,y,cv=10) score.mean() knn_classifier = KNeighborsClassifier(n_neighbors = 12)score=cross_val_score(knn_classifier,X,y,cv=10)score.mean() #Output- 0.8448387096774195 Random Forest Classifier from sklearn.ensemble import RandomForestClassifier randomforest_classifier= RandomForestClassifier(n_estimators=10) score=cross_val_score(randomforest_classifier,X,y,cv=10) score.mean() from sklearn.ensemble import RandomForestClassifierrandomforest_classifier= RandomForestClassifier(n_estimators=10)score=cross_val_score(randomforest_classifier,X,y,cv=10)score.mean() #Output- 0.8113978494623655 Follow us on Instagram for all your Queries Instagram";Heart Disease Prediction with Machine Learning
2020-05-20 21:29:01;Recommendation systems are among the most popular applications of data science. They are used to predict the Rating or Preference that a user would give to an item. Almost every major company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on auto play, and Facebook uses it to recommend pages to like and people to follow. In this Data Science project, you will see how to build a basic model of simple as well as content-based recommendation systems. While these models will be nowhere close to the industry standard in terms of complexity, quality or accuracy, it will help you to get started with building more complex models that produce even better results.Download the data sets you need to build this movie recommendation model from here:#Output-[5 rows x 20 columns]Credits: (4803, 4)Movies Dataframe: (4803, 20)Now lets make a recommendations based on the movie’s plot summaries given in the overview column. So if our user gives us a movie title, our goal is to recommend movies that share similar plot summaries.#Output<4803×10417 sparse matrix of type ”with 127220 stored elements in Compressed Sparse Row format>(4803, 10417)#Output-array([0.76163447, 0.76159416, 0.76159416, …, 0.76159416, 0.76159416, 0.76159416])#Output-1341     Obitaemyy Ostrov634      The Matrix3604     Apollo 182130     The American775       Supernova529       Tears of the Sun151       Beowulf311       The Adventures of Pluto Nash847        Semi-Pro942        The Book of LifeName:  original_title, dtype: object;https://thecleverprogrammer.com/2020/05/20/data-science-project-movie-recommendation-system/;['pattern', 'sklearn'];1.0;[];['Recommender'];['predict', 'fit', 'recommend', 'model'];"Recommendation systems are among the most popular applications of data science. They are used to predict the Rating or Preference that a user would give to an item. Almost every major company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on auto play, and Facebook uses it to recommend pages to like and people to follow. Let’s Build our own recommendation system In this Data Science project, you will see how to build a basic model of simple as well as content-based recommendation systems. While these models will be nowhere close to the industry standard in terms of complexity, quality or accuracy, it will help you to get started with building more complex models that produce even better results. Download the data sets you need to build this movie recommendation model from here: tmdb_5000_moviesDownload tmdb_5000_creditsDownload import pandas as pd import numpy as np credits = pd.read_csv(""tmdb_5000_credits.csv"") movies = pd.read_csv(""tmdb_5000_movies.csv"") credits.head()​x import pandas as pdimport numpy as npcredits = pd.read_csv(""tmdb_5000_credits.csv"")movies = pd.read_csv(""tmdb_5000_movies.csv"")credits.head() movies.head() movies.head() print(""Credits:"",credits.shape) print(""Movies Dataframe:"",movies.shape) print(""Credits:"",credits.shape)print(""Movies Dataframe:"",movies.shape) #Output-[5 rows x 20 columns]Credits: (4803, 4)Movies Dataframe: (4803, 20) credits_column_renamed = credits.rename(index=str, columns={""movie_id"": ""id""}) movies_merge = movies.merge(credits_column_renamed, on='id') print(movies_merge.head()) credits_column_renamed = credits.rename(index=str, columns={""movie_id"": ""id""})movies_merge = movies.merge(credits_column_renamed, on='id')print(movies_merge.head()) movies_cleaned = movies_merge.drop(columns=['homepage', 'title_x', 'title_y', 'status','production_countries']) print(movies_cleaned.head()) print(movies_cleaned.info()) print(movies_cleaned.head(1)['overview']) movies_cleaned = movies_merge.drop(columns=['homepage', 'title_x', 'title_y', 'status','production_countries'])print(movies_cleaned.head())print(movies_cleaned.info())print(movies_cleaned.head(1)['overview']) Content Based Recommendation System Now lets make a recommendations based on the movie’s plot summaries given in the overview column. So if our user gives us a movie title, our goal is to recommend movies that share similar plot summaries. from sklearn.feature_extraction.text import TfidfVectorizer tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}', ngram_range=(1, 3), stop_words = 'english') from sklearn.feature_extraction.text import TfidfVectorizertfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}', ngram_range=(1, 3), stop_words = 'english') # Fitting the TF-IDF on the 'overview' text tfv_matrix = tfv.fit_transform(movies_cleaned_df['overview']) print(tfv_matrix) print(tfv_matrix.shape) # Fitting the TF-IDF on the 'overview' texttfv_matrix = tfv.fit_transform(movies_cleaned_df['overview'])print(tfv_matrix)print(tfv_matrix.shape) #Output<4803×10417 sparse matrix of type ”with 127220 stored elements in Compressed Sparse Row format>(4803, 10417) from sklearn.metrics.pairwise import sigmoid_kernel # Compute the sigmoid kernel sig = sigmoid_kernel(tfv_matrix, tfv_matrix) print(sig[0]) from sklearn.metrics.pairwise import sigmoid_kernel​# Compute the sigmoid kernelsig = sigmoid_kernel(tfv_matrix, tfv_matrix)print(sig[0]) #Output-array([0.76163447, 0.76159416, 0.76159416, …, 0.76159416, 0.76159416, 0.76159416]) Reverse mapping of indices and movie titles # Reverse mapping of indices and movie titles indices = pd.Series(movies_cleaned.index, index=movies_cleaned['original_title']).drop_duplicates() print(indices) print(indices['Newlyweds']) print(sig[4799]) print(list(enumerate(sig[indices['Newlyweds']]))) print(sorted(list(enumerate(sig[indices['Newlyweds']])), key=lambda x: x[1], reverse=True)) # Reverse mapping of indices and movie titlesindices = pd.Series(movies_cleaned.index, index=movies_cleaned['original_title']).drop_duplicates()print(indices)print(indices['Newlyweds'])print(sig[4799])print(list(enumerate(sig[indices['Newlyweds']])))print(sorted(list(enumerate(sig[indices['Newlyweds']])), key=lambda x: x[1], reverse=True)) def give_recomendations(title, sig=sig): # Get the index corresponding to original_title idx = indices[title] # Get the pairwsie similarity scores sig_scores = list(enumerate(sig[idx])) # Sort the movies sig_scores = sorted(sig_scores, key=lambda x: x[1], reverse=True) # Scores of the 10 most similar movies sig_scores = sig_scores[1:11] # Movie indices movie_indices = [i[0] for i in sig_scores] # Top 10 most similar movies return movies_cleaned['original_title'].iloc[movie_indices] def give_recomendations(title, sig=sig): # Get the index corresponding to original_title idx = indices[title]​ # Get the pairwsie similarity scores sig_scores = list(enumerate(sig[idx]))​ # Sort the movies sig_scores = sorted(sig_scores, key=lambda x: x[1], reverse=True)​ # Scores of the 10 most similar movies sig_scores = sig_scores[1:11]​ # Movie indices movie_indices = [i[0] for i in sig_scores]​ # Top 10 most similar movies return movies_cleaned['original_title'].iloc[movie_indices] Testing our content-based recommendation system with the seminal film Spy Kids print(give_recomendations('Avatar')) print(give_recomendations('Avatar')) #Output- 1341 Obitaemyy Ostrov634 The Matrix3604 Apollo 182130 The American775 Supernova529 Tears of the Sun151 Beowulf311 The Adventures of Pluto Nash847 Semi-Pro942 The Book of LifeName: original_title, dtype: object Follow us on Instagram for all your Queries Instagram";Movie Recommendation System with Machine Learning
2020-05-20 14:20:01;Heart disease describes a range of conditions that affect your heart. Diseases under the heart disease umbrella include blood vessel diseases, such as coronary artery disease, heart rhythm problems (arrhythmia) and heart defects you’re born with (congenital heart defects), among others.Heart disease is one of the biggest causes of morbidity and mortality among the population of the world. Prediction of cardiovascular disease is regarded as one of the most important subjects in the section of clinical data science. The amount of data in the healthcare industry is huge.In this Data Science Project I will be applying Machine Learning techniques to classify whether a person is suffering from Heart Disease or not.Importing the required modules :Here we will be experimenting using KNeighborsClassifier :To get correlation of each feature in the data setIt’s always a good practice to work with a data set where the target classes are of approximately equal size. Thus, let’s check for the same :;https://thecleverprogrammer.com/2020/05/20/heart-disease-prediction-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Random Forest', 'Classification'];['predict', 'fit', 'model', 'machine learning', 'random forest', 'classif', 'filter', 'train', 'label'];"Heart disease describes a range of conditions that affect your heart. Diseases under the heart disease umbrella include blood vessel diseases, such as coronary artery disease, heart rhythm problems (arrhythmia) and heart defects you’re born with (congenital heart defects), among others. Heart disease is one of the biggest causes of morbidity and mortality among the population of the world. Prediction of cardiovascular disease is regarded as one of the most important subjects in the section of clinical data science. The amount of data in the healthcare industry is huge. In this Data Science Project I will be applying Machine Learning techniques to classify whether a person is suffering from Heart Disease or not. You can download the data set we need for this project from here: heartDownload Importing the required modules : import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import rcParams import seaborn as sns import warnings warnings.filterwarnings('ignore')​x import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom matplotlib import rcParamsimport seaborn as snsimport warningswarnings.filterwarnings('ignore') Here we will be experimenting using KNeighborsClassifier : from sklearn.neighbors import KNeighborsClassifier from sklearn.neighbors import KNeighborsClassifier Now let’s dive into the data df = pd.read_csv('dataset.csv') print(df.head()) df = pd.read_csv('dataset.csv')print(df.head()) print(df.info()) print(df.info()) Output <class 'pandas.core.frame.DataFrame'> RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): age 303 non-null int64 sex 303 non-null int64 cp 303 non-null int64 trestbps 303 non-null int64 chol 303 non-null int64 fbs 303 non-null int64 restecg 303 non-null int64 thalach 303 non-null int64 exang 303 non-null int64 oldpeak 303 non-null float64 slope 303 non-null int64 ca 303 non-null int64 thal 303 non-null int64 target 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.2 KB Output<class 'pandas.core.frame.DataFrame'>RangeIndex: 303 entries, 0 to 302Data columns (total 14 columns):age 303 non-null int64sex 303 non-null int64cp 303 non-null int64trestbps 303 non-null int64chol 303 non-null int64fbs 303 non-null int64restecg 303 non-null int64thalach 303 non-null int64exang 303 non-null int64oldpeak 303 non-null float64slope 303 non-null int64ca 303 non-null int64thal 303 non-null int64target 303 non-null int64dtypes: float64(1), int64(13)memory usage: 33.2 KB print(df.describe()) print(df.describe()) Feature Selection To get correlation of each feature in the data set import seaborn as sns corrmat = df.corr() top_corr_features = corrmat.index plt.figure(figsize=(16,16)) #plot heat map g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=""RdYlGn"") plt.show() import seaborn as snscorrmat = df.corr()top_corr_features = corrmat.indexplt.figure(figsize=(16,16))#plot heat mapg=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=""RdYlGn"")plt.show() It’s always a good practice to work with a data set where the target classes are of approximately equal size. Thus, let’s check for the same : sns.set_style('whitegrid') sns.countplot(x='target',data=df,palette='RdBu_r') plt.show() sns.set_style('whitegrid')sns.countplot(x='target',data=df,palette='RdBu_r')plt.show() Data Processing After exploring the data set, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. First, I’ll use the get_dummies method to create dummy columns for categorical variables. dataset = pd.get_dummies(df, columns = ['sex', 'cp', 'fbs','restecg', 'exang', 'slope', 'ca', 'thal']) from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler standardScaler = StandardScaler() columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak'] dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale]) dataset.head() dataset = pd.get_dummies(df, columns = ['sex', 'cp', 'fbs','restecg', 'exang', 'slope', 'ca', 'thal'])from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerstandardScaler = StandardScaler()columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])dataset.head() y = dataset['target'] X = dataset.drop(['target'], axis = 1) y = dataset['target']X = dataset.drop(['target'], axis = 1) from sklearn.model_selection import cross_val_score knn_scores = [] for k in range(1,21): knn_classifier = KNeighborsClassifier(n_neighbors = k) score=cross_val_score(knn_classifier,X,y,cv=10) knn_scores.append(score.mean()) from sklearn.model_selection import cross_val_scoreknn_scores = []for k in range(1,21): knn_classifier = KNeighborsClassifier(n_neighbors = k) score=cross_val_score(knn_classifier,X,y,cv=10) knn_scores.append(score.mean()) plt.plot([k for k in range(1, 21)], knn_scores, color = 'red') for i in range(1,21): plt.text(i, knn_scores[i-1], (i, knn_scores[i-1])) plt.xticks([i for i in range(1, 21)]) plt.xlabel('Number of Neighbors (K)') plt.ylabel('Scores') plt.title('K Neighbors Classifier scores for different K values') plt.show() plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')for i in range(1,21): plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))plt.xticks([i for i in range(1, 21)])plt.xlabel('Number of Neighbors (K)')plt.ylabel('Scores')plt.title('K Neighbors Classifier scores for different K values')plt.show() knn_classifier = KNeighborsClassifier(n_neighbors = 12) score=cross_val_score(knn_classifier,X,y,cv=10) score.mean() knn_classifier = KNeighborsClassifier(n_neighbors = 12)score=cross_val_score(knn_classifier,X,y,cv=10)score.mean() #Output- 0.8448387096774195 Random Forest Classifier from sklearn.ensemble import RandomForestClassifier randomforest_classifier= RandomForestClassifier(n_estimators=10) score=cross_val_score(randomforest_classifier,X,y,cv=10) score.mean() from sklearn.ensemble import RandomForestClassifierrandomforest_classifier= RandomForestClassifier(n_estimators=10)score=cross_val_score(randomforest_classifier,X,y,cv=10)score.mean() #Output- 0.8113978494623655 Follow us on Instagram for all your Queries Instagram";Heart Disease Prediction with Machine Learning
2020-05-21 18:44:27;It takes a lot of manual effort to complete the evaluation process as even one college may contain thousands of students. In this Data Science Project we will evaluate the Performance of a student using Machine Learning techniques and python.You can download the data set you need for this project from here:Let’s start with importing the libraries :To read the data set :#Output- (1000, 8)To look at the first 5 records in the data setLets check the no. of unique items present in the categorical columnlets check the percentage of missing data in each columns present in the data :To see comparison of all other attributes with respect to Math MarksComparison of all other attributes with respect to Reading Marks :Lets check the Effect of Lunch on Student’s PerformnceLets check the Effect of Test Preparation Course on Scores;https://thecleverprogrammer.com/2020/05/21/data-science-project-student-performance-analysis-with-machine-learning/;['sklearn'];1.0;[];['Regression', 'ML', 'Logistic Regression', 'Random Forest', 'Classification'];['regression', 'predict', 'fit', 'model', 'machine learning', 'logistic regression', 'random forest', 'classif', 'filter', 'training data', 'train', 'label'];"It takes a lot of manual effort to complete the evaluation process as even one college may contain thousands of students. In this Data Science Project we will evaluate the Performance of a student using Machine Learning techniques and python. You can download the data set you need for this project from here: StudentsPerformanceDownload Let’s start with importing the libraries : # for some basic operations import numpy as np import pandas as pd # for visualizations import matplotlib.pyplot as plt import seaborn as sns import plotly.express as px import dabl​x # for some basic operationsimport numpy as npimport pandas as pd​# for visualizationsimport matplotlib.pyplot as pltimport seaborn as snsimport plotly.express as pximport dabl To read the data set : data = pd.read_csv('StudentsPerformance.csv') # getting the shape of the data print(data.shape) data = pd.read_csv('StudentsPerformance.csv')​# getting the shape of the dataprint(data.shape) #Output- (1000, 8) To look at the first 5 records in the data set data.head() data.head() Descriptive Statistics data.describe() data.describe() Lets check the no. of unique items present in the categorical column data.select_dtypes('object').nunique() data.select_dtypes('object').nunique() #Output gender 2 race/ethnicity 5 parental level of education 6 lunch 2 test preparation course 2 dtype: int64 #Outputgender 2race/ethnicity 5parental level of education 6lunch 2test preparation course 2dtype: int64 lets check the percentage of missing data in each columns present in the data : no_of_columns = data.shape[0] percentage_of_missing_data = data.isnull().sum()/no_of_columns print(percentage_of_missing_data) no_of_columns = data.shape[0]percentage_of_missing_data = data.isnull().sum()/no_of_columnsprint(percentage_of_missing_data) #Output gender 0.0 race/ethnicity 0.0 parental level of education 0.0 lunch 0.0 test preparation course 0.0 math score 0.0 reading score 0.0 writing score 0.0 dtype: float64 #Outputgender 0.0race/ethnicity 0.0parental level of education 0.0lunch 0.0test preparation course 0.0math score 0.0reading score 0.0writing score 0.0dtype: float64 To see comparison of all other attributes with respect to Math Marks plt.rcParams['figure.figsize'] = (18, 6) plt.style.use('fivethirtyeight') dabl.plot(data, target_col = 'math score') plt.rcParams['figure.figsize'] = (18, 6)plt.style.use('fivethirtyeight')dabl.plot(data, target_col = 'math score') Comparison of all other attributes with respect to Reading Marks : plt.rcParams['figure.figsize'] = (18, 6) plt.style.use('fivethirtyeight') dabl.plot(data, target_col = 'reading score') plt.rcParams['figure.figsize'] = (18, 6)plt.style.use('fivethirtyeight')dabl.plot(data, target_col = 'reading score') Lets check the Effect of Lunch on Student’s Performnce data[['lunch','gender','math score','writing score', 'reading score']].groupby(['lunch','gender']).agg('median') data[['lunch','gender','math score','writing score', 'reading score']].groupby(['lunch','gender']).agg('median') Lets check the Effect of Test Preparation Course on Scores data[['test preparation course', 'gender', 'math score', 'writing score', 'reading score']].groupby(['test preparation course','gender']).agg('median') data[['test preparation course', 'gender', 'math score', 'writing score', 'reading score']].groupby(['test preparation course','gender']).agg('median') Data Visualizations Visualizing the number of male and female in the data set plt.rcParams['figure.figsize'] = (15, 5) sns.countplot(data['gender'], palette = 'bone') plt.title('Comparison of Males and Females', fontweight = 30) plt.xlabel('Gender') plt.ylabel('Count') plt.show() plt.rcParams['figure.figsize'] = (15, 5)sns.countplot(data['gender'], palette = 'bone')plt.title('Comparison of Males and Females', fontweight = 30)plt.xlabel('Gender')plt.ylabel('Count')plt.show() Visualizing the different groups in the data set plt.rcParams['figure.figsize'] = (15, 9) plt.style.use('ggplot') sns.countplot(data['race/ethnicity'], palette = 'pink') plt.title('Comparison of various groups', fontweight = 30, fontsize = 20) plt.xlabel('Groups') plt.ylabel('count') plt.show() plt.rcParams['figure.figsize'] = (15, 9)plt.style.use('ggplot')​sns.countplot(data['race/ethnicity'], palette = 'pink')plt.title('Comparison of various groups', fontweight = 30, fontsize = 20)plt.xlabel('Groups')plt.ylabel('count')plt.show() Visualizing the different parental education levels plt.rcParams['figure.figsize'] = (15, 9) plt.style.use('fivethirtyeight') sns.countplot(data['parental level of education'], palette = 'Blues') plt.title('Comparison of Parental Education', fontweight = 30, fontsize = 20) plt.xlabel('Degree') plt.ylabel('count') plt.show() plt.rcParams['figure.figsize'] = (15, 9)plt.style.use('fivethirtyeight')​sns.countplot(data['parental level of education'], palette = 'Blues')plt.title('Comparison of Parental Education', fontweight = 30, fontsize = 20)plt.xlabel('Degree')plt.ylabel('count')plt.show() Visualizing Maths score plt.rcParams['figure.figsize'] = (15, 9) plt.style.use('tableau-colorblind10') sns.countplot(data['math score'], palette = 'BuPu') plt.title('Comparison of math scores', fontweight = 30, fontsize = 20) plt.xlabel('score') plt.ylabel('count') plt.xticks(rotation = 90) plt.show() plt.rcParams['figure.figsize'] = (15, 9)plt.style.use('tableau-colorblind10')​sns.countplot(data['math score'], palette = 'BuPu')plt.title('Comparison of math scores', fontweight = 30, fontsize = 20)plt.xlabel('score')plt.ylabel('count')plt.xticks(rotation = 90)plt.show() Computing the total score for each student import warnings warnings.filterwarnings('ignore') data['total_score'] = data['math score'] + data['reading score'] + data['writing score'] sns.distplot(data['total_score'], color = 'magenta') plt.title('comparison of total score of all the students', fontweight = 30, fontsize = 20) plt.xlabel('total score scored by the students') plt.ylabel('count') plt.show() import warningswarnings.filterwarnings('ignore')​data['total_score'] = data['math score'] + data['reading score'] + data['writing score']​sns.distplot(data['total_score'], color = 'magenta')​plt.title('comparison of total score of all the students', fontweight = 30, fontsize = 20)plt.xlabel('total score scored by the students')plt.ylabel('count')plt.show() Computing percentage for each of the students # importing math library to use ceil from math import * import warnings warnings.filterwarnings('ignore') data['percentage'] = data['total_score']/3 for i in range(0, 1000): data['percentage'][i] = ceil(data['percentage'][i]) plt.rcParams['figure.figsize'] = (15, 9) sns.distplot(data['percentage'], color = 'orange') plt.title('Comparison of percentage scored by all the students', fontweight = 30, fontsize = 20) plt.xlabel('Percentage scored') plt.ylabel('Count') plt.show() # importing math library to use ceilfrom math import * import warningswarnings.filterwarnings('ignore')​data['percentage'] = data['total_score']/3​for i in range(0, 1000): data['percentage'][i] = ceil(data['percentage'][i])​plt.rcParams['figure.figsize'] = (15, 9)sns.distplot(data['percentage'], color = 'orange')​plt.title('Comparison of percentage scored by all the students', fontweight = 30, fontsize = 20)plt.xlabel('Percentage scored')plt.ylabel('Count')plt.show() Assigning grades to the grades according to the following criteria : 0 - 40 marks : grade E 41 - 60 marks : grade D 60 - 70 marks : grade C 70 - 80 marks : grade B 80 - 90 marks : grade A 90 - 100 marks : grade O Assigning grades to the grades according to the following criteria : 0 - 40 marks : grade E 41 - 60 marks : grade D 60 - 70 marks : grade C 70 - 80 marks : grade B 80 - 90 marks : grade A 90 - 100 marks : grade O def getgrade(percentage, status): if status == 'Fail': return 'E' if(percentage >= 90): return 'O' if(percentage >= 80): return 'A' if(percentage >= 70): return 'B' if(percentage >= 60): return 'C' if(percentage >= 40): return 'D' else : return 'E' data['grades'] = data.apply(lambda x: getgrade(x['percentage'], x['status']), axis = 1 ) data['grades'].value_counts() def getgrade(percentage, status): if status == 'Fail': return 'E' if(percentage >= 90): return 'O' if(percentage >= 80): return 'A' if(percentage >= 70): return 'B' if(percentage >= 60): return 'C' if(percentage >= 40): return 'D' else : return 'E'​data['grades'] = data.apply(lambda x: getgrade(x['percentage'], x['status']), axis = 1 )​data['grades'].value_counts() #Output B 260 C 252 D 223 A 156 O 58 E 51 Name: grades, dtype: int64 #OutputB 260C 252D 223A 156O 58E 51Name: grades, dtype: int64 Label Encoding from sklearn.preprocessing import LabelEncoder # creating an encoder le = LabelEncoder() # label encoding for test preparation course data['test preparation course'] = le.fit_transform(data['test preparation course']) # label encoding for lunch data['lunch'] = le.fit_transform(data['lunch']) # label encoding for race/ethnicity # we have to map values to each of the categories data['race/ethnicity'] = data['race/ethnicity'].replace('group A', 1) data['race/ethnicity'] = data['race/ethnicity'].replace('group B', 2) data['race/ethnicity'] = data['race/ethnicity'].replace('group C', 3) data['race/ethnicity'] = data['race/ethnicity'].replace('group D', 4) data['race/ethnicity'] = data['race/ethnicity'].replace('group E', 5) # label encoding for parental level of education data['parental level of education'] = le.fit_transform(data['parental level of education']) #label encoding for gender data['gender'] = le.fit_transform(data['gender']) # label encoding for pass_math data['pass_math'] = le.fit_transform(data['pass_math']) # label encoding for pass_reading data['pass_reading'] = le.fit_transform(data['pass_reading']) # label encoding for pass_writing data['pass_writing'] = le.fit_transform(data['pass_writing']) # label encoding for status data['status'] = le.fit_transform(data['status']) from sklearn.preprocessing import LabelEncoder​# creating an encoderle = LabelEncoder()​# label encoding for test preparation coursedata['test preparation course'] = le.fit_transform(data['test preparation course'])​# label encoding for lunchdata['lunch'] = le.fit_transform(data['lunch'])​# label encoding for race/ethnicity# we have to map values to each of the categoriesdata['race/ethnicity'] = data['race/ethnicity'].replace('group A', 1)data['race/ethnicity'] = data['race/ethnicity'].replace('group B', 2)data['race/ethnicity'] = data['race/ethnicity'].replace('group C', 3)data['race/ethnicity'] = data['race/ethnicity'].replace('group D', 4)data['race/ethnicity'] = data['race/ethnicity'].replace('group E', 5)​# label encoding for parental level of educationdata['parental level of education'] = le.fit_transform(data['parental level of education'])​#label encoding for genderdata['gender'] = le.fit_transform(data['gender'])​# label encoding for pass_mathdata['pass_math'] = le.fit_transform(data['pass_math'])​# label encoding for pass_readingdata['pass_reading'] = le.fit_transform(data['pass_reading'])​# label encoding for pass_writingdata['pass_writing'] = le.fit_transform(data['pass_writing'])​# label encoding for statusdata['status'] = le.fit_transform(data['status']) Data Preparation Splitting the dependent and independent variables x = data.iloc[:,:14] y = data.iloc[:,14] print(x.shape) print(y.shape) x = data.iloc[:,:14]y = data.iloc[:,14]​print(x.shape)print(y.shape) #Output-(1000, 14) (1000,) Splitting the data set into training and test sets from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 45) print(x_train.shape) print(y_train.shape) print(x_test.shape) print(y_test.shape) from sklearn.model_selection import train_test_split​x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 45)​print(x_train.shape)print(y_train.shape)print(x_test.shape)print(y_test.shape) #Output(750, 14) (750,) (250, 14) (250,) # importing the MinMaxScaler from sklearn.preprocessing import MinMaxScaler # creating a scaler mm = MinMaxScaler() # feeding the independent variable into the scaler x_train = mm.fit_transform(x_train) x_test = mm.transform(x_test) # importing the MinMaxScalerfrom sklearn.preprocessing import MinMaxScaler​# creating a scalermm = MinMaxScaler()​# feeding the independent variable into the scalerx_train = mm.fit_transform(x_train)x_test = mm.transform(x_test) Applying principal components analysis from sklearn.decomposition import PCA # creating a principal component analysis model pca = PCA(n_components = None) # feeding the independent variables to the PCA model x_train = pca.fit_transform(x_train) x_test = pca.transform(x_test) # visualising the principal components that will explain the highest share of variance explained_variance = pca.explained_variance_ratio_ print(explained_variance) # creating a principal component analysis model pca = PCA(n_components = 2) # feeding the independent variables to the PCA model x_train = pca.fit_transform(x_train) x_test = pca.transform(x_test) from sklearn.decomposition import PCA​# creating a principal component analysis modelpca = PCA(n_components = None)​# feeding the independent variables to the PCA modelx_train = pca.fit_transform(x_train)x_test = pca.transform(x_test)​# visualising the principal components that will explain the highest share of varianceexplained_variance = pca.explained_variance_ratio_print(explained_variance)​# creating a principal component analysis modelpca = PCA(n_components = 2)​# feeding the independent variables to the PCA modelx_train = pca.fit_transform(x_train)x_test = pca.transform(x_test) Modelling Logistic Regression from sklearn.linear_model import LogisticRegression # creating a model model = LogisticRegression() # feeding the training data to the model model.fit(x_train, y_train) # predicting the test set results y_pred = model.predict(x_test) # calculating the classification accuracies print(""Training Accuracy :"", model.score(x_train, y_train)) print(""Testing Accuracy :"", model.score(x_test, y_test)) from sklearn.linear_model import LogisticRegression​# creating a modelmodel = LogisticRegression()​# feeding the training data to the modelmodel.fit(x_train, y_train)​# predicting the test set resultsy_pred = model.predict(x_test)​# calculating the classification accuraciesprint(""Training Accuracy :"", model.score(x_train, y_train))print(""Testing Accuracy :"", model.score(x_test, y_test)) Output-Training Accuracy : 0.3933333333333333 Testing Accuracy : 0.424 Printing the confusion matrix from sklearn.metrics import confusion_matrix # creating a confusion matrix cm = confusion_matrix(y_test, y_pred) # printing the confusion matrix plt.rcParams['figure.figsize'] = (8, 8) sns.heatmap(cm, annot = True, cmap = 'Greens') plt.title('Confusion Matrix for Logistic Regression', fontweight = 30, fontsize = 20) plt.show() from sklearn.metrics import confusion_matrix​# creating a confusion matrixcm = confusion_matrix(y_test, y_pred)​# printing the confusion matrixplt.rcParams['figure.figsize'] = (8, 8)sns.heatmap(cm, annot = True, cmap = 'Greens')plt.title('Confusion Matrix for Logistic Regression', fontweight = 30, fontsize = 20)plt.show() Random Forest from sklearn.ensemble import RandomForestClassifier # creating a model model = RandomForestClassifier() # feeding the training data to the model model.fit(x_train, y_train) # predicting the x-test results y_pred = model.predict(x_test) # calculating the accuracies print(""Training Accuracy :"", model.score(x_train, y_train)) print(""Testing Accuracy :"", model.score(x_test, y_test)) from sklearn.ensemble import RandomForestClassifier​# creating a modelmodel = RandomForestClassifier()​# feeding the training data to the modelmodel.fit(x_train, y_train)​# predicting the x-test resultsy_pred = model.predict(x_test)​# calculating the accuraciesprint(""Training Accuracy :"", model.score(x_train, y_train))print(""Testing Accuracy :"", model.score(x_test, y_test)) Output–Training Accuracy : 0.9986666666666667 Testing Accuracy : 0.784 from sklearn.metrics import confusion_matrix # creating a confusion matrix cm = confusion_matrix(y_test, y_pred) # printing the confusion matrix plt.rcParams['figure.figsize'] = (8, 8) sns.heatmap(cm, annot = True, cmap = 'Reds') plt.title('Confusion Matrix for Random Forest', fontweight = 30, fontsize = 20) plt.show() from sklearn.metrics import confusion_matrix​# creating a confusion matrixcm = confusion_matrix(y_test, y_pred)​# printing the confusion matrixplt.rcParams['figure.figsize'] = (8, 8)sns.heatmap(cm, annot = True, cmap = 'Reds')plt.title('Confusion Matrix for Random Forest', fontweight = 30, fontsize = 20)plt.show() from pandas.plotting import radviz fig, ax = plt.subplots(figsize=(12, 12)) new_df = x.copy() new_df[""status""] = y radviz(new_df, ""status"", ax=ax, colormap=""rocket"") plt.title('Radial Visualization for Target', fontsize = 20) plt.show() from pandas.plotting import radvizfig, ax = plt.subplots(figsize=(12, 12))new_df = x.copy()new_df[""status""] = yradviz(new_df, ""status"", ax=ax, colormap=""rocket"")plt.title('Radial Visualization for Target', fontsize = 20)plt.show() It gives a clear Idea that Students getting very low grades have high correlation on Lunch and Parental Education Follow us on Instagram for all your Queries Instagram";Data Science Project – Student Performance Analysis with Machine Learning
2020-05-22 20:10:12;The problem of the fake news publication is not new and it already has been reported in ancient ages, but it has started having a huge impact especially on social media users.Such false information should be detected as soon as possible to avoid its negative influence on the readers and in some cases on their decisions, e.g., during the election.In this Data Science project we will be creating a fake news classifier using Machine Learning techniques and Python.You can download the data set you need for this task from here :Let’s start with this classifier#Output- (18285, 5)#Output–‘civilian kill singl us airstrik identifi’#Output–(18285, 5000)#Output- 0.8810273405136703;https://thecleverprogrammer.com/2020/05/22/data-science-project-fake-news-classification/;['pattern', 'sklearn', 'vocabulary', 'nltk'];1.0;[];['ML', 'Classification'];['detect', 'predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'label'];"The problem of the fake news publication is not new and it already has been reported in ancient ages, but it has started having a huge impact especially on social media users. Such false information should be detected as soon as possible to avoid its negative influence on the readers and in some cases on their decisions, e.g., during the election. In this Data Science project we will be creating a fake news classifier using Machine Learning techniques and Python. You can download the data set you need for this task from here : trainDownload Let’s start with this classifier import pandas as pd df=pd.read_csv('train.csv') print(df.head())​x import pandas as pddf=pd.read_csv('train.csv')print(df.head()) #Get the Independent Features X=df.drop('label',axis=1) print(X.head()) #Get the Independent FeaturesX=df.drop('label',axis=1)print(X.head()) # Get the Dependent features y=df['label'] y.head() # Get the Dependent featuresy=df['label']y.head() 0 1 1 0 2 1 3 1 4 1 Name: label, dtype: int64 0 11 02 13 14 1Name: label, dtype: int64 df.shape df.shape #Output- (18285, 5) from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer df=df.dropna() df.head(10) df=df.dropna()df.head(10) messages=df.copy() messages.reset_index(inplace=True) messages.head(10) messages=df.copy()messages.reset_index(inplace=True)messages.head(10) messages['text'][6] messages['text'][6] #Output 'PARIS — France chose an idealistic, traditional candidate in Sunday's primary to represent the Socialist and parties in the presidential election this spring. The candidate, Benoît Hamon, 49, who ran on the slogan that he would ""make France's heart beat,"" bested Manuel Valls, the former prime minister, whose campaign has promoted more policies and who has a strong background. Mr. Hamon appeared to have won by a wide margin, with incomplete returns showing him with an estimated 58 percent of the vote to Mr. Valls's 41 percent. ""Tonight the left holds its head up high again it is looking to the future,"" Mr. Hamon said, addressing his supporters. ""Our country needs the left, but a modern, innovative left,"" he said. Mr. Hamon's victory was the clearest sign yet that voters on the left want a break with the policies of President François Hollande, who in December announced that he would not seek . However, Mr. Hamon's strong showing is unlikely to change widespread assessments that candidates have little chance of making it into the second round of voting in the general election. The first round of the general election is set for April 23 and the runoff for May 7. The Socialist Party is deeply divided, and one measure of its lack of popular enthusiasm was the relatively low number of people voting. About two million people voted in the second round of the primary on Sunday, in contrast with about 2. 9 million in the second round of the last presidential primary on the left, in 2011. However, much of the conventional wisdom over how the elections will go has been thrown into question over the past week, because the leading candidate, François Fillon, who represents the main party, the Republicans, was accused of paying his wife large sums of money to work as his parliamentary aide. While nepotism is legal in the French political system, it is not clear that she actually did any work. Prosecutors who specialize in financial malfeasance are reviewing the case. France's electoral system allows multiple candidates to run for president in the first round of voting, but only the top two go on to a second round. Mr. Hamon is entering a race that is already crowded on the left, with candidates who include Mélenchon on the far left, and Emmanuel Macron, an independent who served as economy minister in Mr. Hollande's government and who embraces more policies. Unless he decides to withdraw, Mr. Fillon, the mainstream right candidate, will also run, as will the extreme right candidate Marine Le Pen. The two have been expected to go to the runoff. Mr. Hamon's victory can be attributed at least in part to his image as an idealist and traditional leftist candidate who appeals to union voters as well as more environmentally concerned and socially liberal young people. Unlike Mr. Valls, he also clearly distanced himself from some of Mr. Hollande's more unpopular policies, especially the economic ones. Thomas Kekenbosch, 22, a student and one of the leaders of the group the Youth With Benoît Hamon, said Mr. Hamon embodied a new hope for those on the left. ""We have a perspective we have something to do, to build,"" Mr. Kekenbosch said. Mr. Hollande had disappointed many young people because under him the party abandoned ideals, such as support for workers, that many voters believe in, according to Mr. Kekenbosch. Mr. Hollande's government, under pressure from the European Union to meet budget restraints, struggled to pass labor code reforms to make the market more attractive to foreign investors and also to encourage French businesses to expand in France. The measures ultimately passed after weeks of strikes, but they were watered down and generated little concrete progress in improving France's roughly 10 percent unemployment rate and its nearly 25 percent youth joblessness rate. Mr. Hamon strongly endorses a stimulus approach to improving the economy and has promised to phase in a universal income, which would especially help young people looking for work, but would also supplement the livelihood of French workers. The end goal would be to have everyone receive 750 euros per month (about $840). ""We have someone that trusts us,"" Mr. Kekenbosch said, ""who says: 'I give you enough to pay for your studies. You can have a scholarship which spares you from working at McDonald's on provisional contracts for 4 years. "" Mr. Hamon advocates phasing out diesel fuel and encouraging drivers to replace vehicles that use petroleum products with electrical ones. His leftist pedigree began early. His father worked at an arsenal in Brest, a city in the far west of Brittany, and his mother worked off and on as a secretary. He was an early member of the Movement of Young Socialists, and he has continued to work closely with them through his political life. He also worked for Martine Aubry, now the mayor of Lille and a former Socialist Party leader.' #Output'PARIS — France chose an idealistic, traditional candidate in Sunday's primary to represent the Socialist and parties in the presidential election this spring. The candidate, Benoît Hamon, 49, who ran on the slogan that he would ""make France's heart beat,"" bested Manuel Valls, the former prime minister, whose campaign has promoted more policies and who has a strong background. Mr. Hamon appeared to have won by a wide margin, with incomplete returns showing him with an estimated 58 percent of the vote to Mr. Valls's 41 percent. ""Tonight the left holds its head up high again it is looking to the future,"" Mr. Hamon said, addressing his supporters. ""Our country needs the left, but a modern, innovative left,"" he said. Mr. Hamon's victory was the clearest sign yet that voters on the left want a break with the policies of President François Hollande, who in December announced that he would not seek . However, Mr. Hamon's strong showing is unlikely to change widespread assessments that candidates have little chance of making it into the second round of voting in the general election. The first round of the general election is set for April 23 and the runoff for May 7. The Socialist Party is deeply divided, and one measure of its lack of popular enthusiasm was the relatively low number of people voting. About two million people voted in the second round of the primary on Sunday, in contrast with about 2. 9 million in the second round of the last presidential primary on the left, in 2011. However, much of the conventional wisdom over how the elections will go has been thrown into question over the past week, because the leading candidate, François Fillon, who represents the main party, the Republicans, was accused of paying his wife large sums of money to work as his parliamentary aide. While nepotism is legal in the French political system, it is not clear that she actually did any work. Prosecutors who specialize in financial malfeasance are reviewing the case. France's electoral system allows multiple candidates to run for president in the first round of voting, but only the top two go on to a second round. Mr. Hamon is entering a race that is already crowded on the left, with candidates who include Mélenchon on the far left, and Emmanuel Macron, an independent who served as economy minister in Mr. Hollande's government and who embraces more policies. Unless he decides to withdraw, Mr. Fillon, the mainstream right candidate, will also run, as will the extreme right candidate Marine Le Pen. The two have been expected to go to the runoff. Mr. Hamon's victory can be attributed at least in part to his image as an idealist and traditional leftist candidate who appeals to union voters as well as more environmentally concerned and socially liberal young people. Unlike Mr. Valls, he also clearly distanced himself from some of Mr. Hollande's more unpopular policies, especially the economic ones. Thomas Kekenbosch, 22, a student and one of the leaders of the group the Youth With Benoît Hamon, said Mr. Hamon embodied a new hope for those on the left. ""We have a perspective we have something to do, to build,"" Mr. Kekenbosch said. Mr. Hollande had disappointed many young people because under him the party abandoned ideals, such as support for workers, that many voters believe in, according to Mr. Kekenbosch. Mr. Hollande's government, under pressure from the European Union to meet budget restraints, struggled to pass labor code reforms to make the market more attractive to foreign investors and also to encourage French businesses to expand in France. The measures ultimately passed after weeks of strikes, but they were watered down and generated little concrete progress in improving France's roughly 10 percent unemployment rate and its nearly 25 percent youth joblessness rate. Mr. Hamon strongly endorses a stimulus approach to improving the economy and has promised to phase in a universal income, which would especially help young people looking for work, but would also supplement the livelihood of French workers. The end goal would be to have everyone receive 750 euros per month (about $840). ""We have someone that trusts us,"" Mr. Kekenbosch said, ""who says: 'I give you enough to pay for your studies. You can have a scholarship which spares you from working at McDonald's on provisional contracts for 4 years. "" Mr. Hamon advocates phasing out diesel fuel and encouraging drivers to replace vehicles that use petroleum products with electrical ones. His leftist pedigree began early. His father worked at an arsenal in Brest, a city in the far west of Brittany, and his mother worked off and on as a secretary. He was an early member of the Movement of Young Socialists, and he has continued to work closely with them through his political life. He also worked for Martine Aubry, now the mayor of Lille and a former Socialist Party leader.' from nltk.corpus import stopwords from nltk.stem.porter import PorterStemmer import re ps = PorterStemmer() corpus = [] for i in range(0, len(messages)): review = re.sub('[^a-zA-Z]', ' ', messages['text'][i]) review = review.lower() review = review.split() review = [ps.stem(word) for word in review if not word in stopwords.words('english')] review = ' '.join(review) corpus.append(review) corpus[3] from nltk.corpus import stopwordsfrom nltk.stem.porter import PorterStemmerimport reps = PorterStemmer()corpus = []for i in range(0, len(messages)): review = re.sub('[^a-zA-Z]', ' ', messages['text'][i]) review = review.lower() review = review.split() review = [ps.stem(word) for word in review if not word in stopwords.words('english')] review = ' '.join(review) corpus.append(review)corpus[3] #Output–‘civilian kill singl us airstrik identifi’ # TFidf Vectorizer from sklearn.feature_extraction.text import TfidfVectorizer tfidf_v=TfidfVectorizer(max_features=5000,ngram_range=(1,3)) X=tfidf_v.fit_transform(corpus).toarray() X.shape # TFidf Vectorizerfrom sklearn.feature_extraction.text import TfidfVectorizertfidf_v=TfidfVectorizer(max_features=5000,ngram_range=(1,3))X=tfidf_v.fit_transform(corpus).toarray()X.shape #Output–(18285, 5000) y=messages['label'] y=messages['label'] # Divide the dataset into Train and Test from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0) tfidf_v.get_feature_names()[:20] # Divide the dataset into Train and Testfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)tfidf_v.get_feature_names()[:20] #Output ['abandon', 'abc', 'abc news', 'abduct', 'abe', 'abedin', 'abl', 'abort', 'abroad', 'absolut', 'abstain', 'absurd', 'abus', 'abus new', 'abus new york', 'academi', 'accept', 'access', 'access pipelin', 'access pipelin protest'] #Output['abandon', 'abc', 'abc news', 'abduct', 'abe', 'abedin', 'abl', 'abort', 'abroad', 'absolut', 'abstain', 'absurd', 'abus', 'abus new', 'abus new york', 'academi', 'accept', 'access', 'access pipelin', 'access pipelin protest'] tfidf_v.get_params() tfidf_v.get_params() {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': numpy.int64, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': 5000, 'min_df': 1, 'ngram_range': (1, 3), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None} {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': numpy.int64, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': 5000, 'min_df': 1, 'ngram_range': (1, 3), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None} count_df = pd.DataFrame(X_train, columns=tfidf_v.get_feature_names()) count_df.head() count_df = pd.DataFrame(X_train, columns=tfidf_v.get_feature_names())count_df.head() import matplotlib.pyplot as plt def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): """""" See full source and example: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. """""" plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(""Normalized confusion matrix"") else: print('Confusion matrix, without normalization') thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=""center"", color=""white"" if cm[i, j] > thresh else ""black"") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') import matplotlib.pyplot as pltdef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): """""" See full source and example: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. """""" plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes)​ if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(""Normalized confusion matrix"") else: print('Confusion matrix, without normalization')​ thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=""center"", color=""white"" if cm[i, j] > thresh else ""black"")​ plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') MultinomialNB Algorithm from sklearn.naive_bayes import MultinomialNB classifier=MultinomialNB() from sklearn import metrics import numpy as np import itertools classifier.fit(X_train, y_train) pred = classifier.predict(X_test) score = metrics.accuracy_score(y_test, pred) print(""accuracy: %0.3f"" % score) cm = metrics.confusion_matrix(y_test, pred) plot_confusion_matrix(cm, classes=['FAKE', 'REAL']) from sklearn.naive_bayes import MultinomialNBclassifier=MultinomialNB()from sklearn import metricsimport numpy as npimport itertools​classifier.fit(X_train, y_train)pred = classifier.predict(X_test)score = metrics.accuracy_score(y_test, pred)print(""accuracy: %0.3f"" % score)cm = metrics.confusion_matrix(y_test, pred)plot_confusion_matrix(cm, classes=['FAKE', 'REAL']) classifier.fit(X_train, y_train) pred = classifier.predict(X_test) score = metrics.accuracy_score(y_test, pred) score classifier.fit(X_train, y_train)pred = classifier.predict(X_test)score = metrics.accuracy_score(y_test, pred)score #Output- 0.8810273405136703 Passive Aggressive Classifier Algorithm from sklearn.linear_model import PassiveAggressiveClassifier linear_clf = PassiveAggressiveClassifier(n_iter=50) linear_clf.fit(X_train, y_train) pred = linear_clf.predict(X_test) score = metrics.accuracy_score(y_test, pred) print(""accuracy: %0.3f"" % score) cm = metrics.confusion_matrix(y_test, pred) plot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data']) from sklearn.linear_model import PassiveAggressiveClassifierlinear_clf = PassiveAggressiveClassifier(n_iter=50)​linear_clf.fit(X_train, y_train)pred = linear_clf.predict(X_test)score = metrics.accuracy_score(y_test, pred)print(""accuracy: %0.3f"" % score)cm = metrics.confusion_matrix(y_test, pred)plot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data']) Multinomial Classifier with Hyperparameter classifier=MultinomialNB(alpha=0.1) previous_score=0 for alpha in np.arange(0,1,0.1): sub_classifier=MultinomialNB(alpha=alpha) sub_classifier.fit(X_train,y_train) y_pred=sub_classifier.predict(X_test) score = metrics.accuracy_score(y_test, y_pred) if score>previous_score: classifier=sub_classifier print(""Alpha: {}, Score : {}"".format(alpha,score)) classifier=MultinomialNB(alpha=0.1)​previous_score=0for alpha in np.arange(0,1,0.1): sub_classifier=MultinomialNB(alpha=alpha) sub_classifier.fit(X_train,y_train) y_pred=sub_classifier.predict(X_test) score = metrics.accuracy_score(y_test, y_pred) if score>previous_score: classifier=sub_classifier print(""Alpha: {}, Score : {}"".format(alpha,score)) # Get Features names feature_names = cv.get_feature_names() classifier.coef_[0] # Most real sorted(zip(classifier.coef_[0], feature_names), reverse=True)[:20] # Most fake sorted(zip(classifier.coef_[0], feature_names))[:5000] # Get Features namesfeature_names = cv.get_feature_names()classifier.coef_[0]# Most realsorted(zip(classifier.coef_[0], feature_names), reverse=True)[:20]# Most fakesorted(zip(classifier.coef_[0], feature_names))[:5000] Hashing Vectorizer hs_vectorizer=HashingVectorizer(n_features=5000,non_negative=True) X=hs_vectorizer.fit_transform(corpus).toarray() hs_vectorizer=HashingVectorizer(n_features=5000,non_negative=True)X=hs_vectorizer.fit_transform(corpus).toarray() Divide the data set into train and test from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0) from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0) from sklearn.naive_bayes import MultinomialNB classifier=MultinomialNB() classifier.fit(X_train, y_train) pred = classifier.predict(X_test) score = metrics.accuracy_score(y_test, pred) print(""accuracy: %0.3f"" % score) cm = metrics.confusion_matrix(y_test, pred) plot_confusion_matrix(cm, classes=['FAKE', 'REAL']) from sklearn.naive_bayes import MultinomialNBclassifier=MultinomialNB()classifier.fit(X_train, y_train)pred = classifier.predict(X_test)score = metrics.accuracy_score(y_test, pred)print(""accuracy: %0.3f"" % score)cm = metrics.confusion_matrix(y_test, pred)plot_confusion_matrix(cm, classes=['FAKE', 'REAL']) Follow us on Instagram for all your Queries Instagram";Fake News Classification with Machine Learning
2020-05-23 18:58:10;In this Data Science Project we will predict Bitcoin Price for the next 30 days with Machine Learning model Support Vector Machines(Regression).You can download the data set we need for this task from here:Let’s start with importing librariesRemove the date columnNow lets create a variable to predict ‘n’ days out there in the futureTo Show last 5 rows of new data set;https://thecleverprogrammer.com/2020/05/23/bitcoin-price-prediction-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Support Vector Machines', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'support vector machines', 'train'];"In this Data Science Project we will predict Bitcoin Price for the next 30 days with Machine Learning model Support Vector Machines(Regression). You can download the data set we need for this task from here: bitcoinDownload Let’s start with importing libraries import numpy as np import pandas as pd df = pd.read_csv(""bitcoin.csv"") df.head()​x import numpy as npimport pandas as pddf = pd.read_csv(""bitcoin.csv"")df.head() Remove the date column df.drop(['Date'],1,inplace=True) df.drop(['Date'],1,inplace=True) Now lets create a variable to predict ‘n’ days out there in the future predictionDays = 30 # Create another column shifted 'n' units up df['Prediction'] = df[['Price']].shift(-predictionDays) # show the first 5 rows df.head() predictionDays = 30# Create another column shifted 'n' units updf['Prediction'] = df[['Price']].shift(-predictionDays)# show the first 5 rowsdf.head() To Show last 5 rows of new data set df.tail() df.tail() # Create the independent dada set # Here we will convert the data frame into a numpy array and drp the prediction column x = np.array(df.drop(['Prediction'],1)) # Remove the last 'n' rows where 'n' is the predictionDays x = x[:len(df)-predictionDays] print(x) # Create the independent dada set# Here we will convert the data frame into a numpy array and drp the prediction columnx = np.array(df.drop(['Prediction'],1))# Remove the last 'n' rows where 'n' is the predictionDaysx = x[:len(df)-predictionDays]print(x) #Output [[ 7881.84668 ] [ 7987.371582] [ 8052.543945] [ 8673.21582 ] [ 8805.77832 ] [ 8719.961914] [ 8659.487305] [ 8319.472656] [ 8574.501953] [ 8564.016602] [ 8742.958008] [ 8208.995117] [ 7707.770996] [ 7824.231445] [ 7822.023438] [ 8043.951172] [ 7954.12793 ] [ 7688.077148] [ 8000.32959 ] [ 7927.714355] [ 8145.857422] [ 8230.923828] [ 8693.833008] [ 8838.375 ] [ 8994.488281] [ 9320.352539] [ 9081.762695] [ 9273.521484] [ 9527.160156] [10144.55664 ] [10701.69141 ] [10855.37109 ] [11011.10254 ] [11790.91699 ] [13016.23145 ] [11182.80664 ] [12407.33203 ] [11959.37109 ] [10817.15527 ] [10583.13477 ] [10801.67773 ] [11961.26953 ] [11215.4375 ] [10978.45996 ] [11208.55078 ] [11450.84668 ] [12285.95801 ] [12573.8125 ] [12156.5127 ] [11358.66211 ] [11815.98633 ] [11392.37891 ] [10256.05859 ] [10895.08984 ] [ 9477.641602] [ 9693.802734] [10666.48242 ] [10530.73242 ] [10767.13965 ] [10599.10547 ] [10343.10645 ] [ 9900.767578] [ 9811.925781] [ 9911.841797] [ 9870.303711] [ 9477.677734] [ 9552.860352] [ 9519.145508] [ 9607.423828] [10085.62793 ] [10399.66895 ] [10518.17481 ] [10821.72656 ] [10970.18457 ] [11805.65332 ] [11478.16895 ] [11941.96875 ] [11966.40723 ] [11862.93652 ] [11354.02441 ] [11523.5791 ] [11382.61621 ] [10895.83008 ] [10051.7041 ] [10311.5459 ] [10374.33887 ] [10231.74414 ] [10345.81055 ] [10916.05371 ] [10763.23242 ] [10138.04981 ] [10131.05566 ] [10407.96484 ] [10159.96094 ] [10138.51758 ] [10370.82031 ] [10185.5 ] [ 9754.422852] [ 9510.200195] [ 9598.173828] [ 9630.664063] [ 9757.970703] [10346.76074 ] [10623.54004 ] [10594.49316 ] [10575.5332 ] [10353.30273 ] [10517.25488 ] [10441.27637 ] [10334.97461 ] [10115.97559 ] [10178.37207 ] [10410.12695 ] [10360.54688 ] [10358.04883 ] [10347.71289 ] [10276.79395 ] [10241.27246 ] [10198.24805 ] [10266.41504 ] [10181.6416 ] [10019.7168 ] [10070.39258 ] [ 9729.324219] [ 8620.566406] [ 8486.993164] [ 8118.967773] [ 8251.845703] [ 8245.915039] [ 8104.185547] [ 8293.868164] [ 8343.276367] [ 8393.041992] [ 8259.992188] [ 8205.939453] [ 8151.500488] [ 7988.155762] [ 8245.623047] [ 8228.783203] [ 8595.740234] [ 8586.473633] [ 8321.756836] [ 8336.555664] [ 8321.005859] [ 8374.686523] [ 8205.369141] [ 8047.526855] [ 8103.911133] [ 7973.20752 ] [ 7988.560547] [ 8222.078125] [ 8243.720703] [ 8078.203125] [ 7514.671875] [ 7493.48877 ] [ 8660.700195] [ 9244.972656] [ 9551.714844] [ 9256.148438] [ 9427.6875 ] [ 9205.726563] [ 9199.584961] [ 9261.104492] [ 9324.717773] [ 9235.354492] [ 9412.612305] [ 9342.527344] [ 9360.879883] [ 9267.561523] [ 8804.880859] [ 8813.582031] [ 9055.526367] [ 8757.788086] [ 8815.662109] [ 8808.262695] [ 8708.094727] [ 8491.992188] [ 8550.760742] [ 8577.975586] [ 8309.286133] [ 8206.145508] [ 8027.268066] [ 7642.75 ] [ 7296.577637] [ 7397.796875] [ 7047.916992] [ 7146.133789] [ 7218.371094] [ 7531.663574] [ 7463.105957] [ 7761.243652] [ 7569.629883] [ 7424.29248 ] [ 7321.988281] [ 7320.145508] [ 7252.034668] [ 7448.307617] [ 7546.996582] [ 7556.237793] [ 7564.345215] [ 7400.899414] [ 7278.119629] [ 7217.427246] [ 7243.134277] [ 7269.68457 ] [ 7124.673828] [ 7152.301758] [ 6932.480469] [ 6640.515137] [ 7276.802734] [ 7202.844238] [ 7218.816406] [ 7191.158691] [ 7511.588867] [ 7355.628418] [ 7322.532227] [ 7275.155762] [ 7238.966797] [ 7290.088379] [ 7317.990234] [ 7422.652832] [ 7292.995117] [ 7193.599121] [ 7200.174316] [ 6985.470215] [ 7344.884277] [ 7410.656738] [ 7411.317383] [ 7769.219238] [ 8163.692383] [ 8079.862793] [ 7879.071289] [ 8166.554199] [ 8037.537598] [ 8192.494141] [ 8144.194336] [ 8827.764648] [ 8807.010742] [ 8723.786133] [ 8929.038086] [ 8942.808594] [ 8706.245117] [ 8657.642578] [ 8745.894531] [ 8680.875977] [ 8406.515625] [ 8445.43457 ] [ 8367.847656] [ 8596.830078] [ 8909.819336] [ 9358.589844] [ 9316.629883] [ 9508.993164] [ 9350.529297] [ 9392.875 ] [ 9344.365234] [ 9293.521484] [ 9180.962891] [ 9613.423828] [ 9729.801758] [ 9795.943359] [ 9865.119141] [10116.67383 ] [ 9856.611328] [10208.23633 ] [10326.05469 ] [10214.37988 ] [10312.11621 ] [ 9889.424805] [ 9934.433594] [ 9690.142578] [10141.99609 ] [ 9633.386719] [ 9608.475586] [ 9686.441406] [ 9663.181641] [ 9924.515625] [ 9650.174805] [ 9341.705078] [ 8820.522461] [ 8784.494141] [ 8672.455078] [ 8599.508789] [ 8562.454102] [ 8869.669922] [ 8787.786133] [ 8755.246094] [ 9078.762695] [ 9122.545898] [ 8909.954102] [ 8108.116211] [ 7923.644531] [ 7909.729492] [ 7911.430176] [ 4970.788086] [ 5563.707031] [ 5200.366211] [ 5392.314941] [ 5014.47998 ] [ 5225.629395] [ 5238.438477] [ 6191.192871] [ 6198.77832 ] [ 6185.066406] [ 5830.254883] [ 6416.314941] [ 6734.803711] [ 6681.062988] [ 6716.44043 ] [ 6469.79834 ] [ 6242.193848] [ 5922.042969] [ 6429.841797] [ 6438.644531] [ 6606.776367] [ 6793.624512] [ 6733.387207] [ 6867.527344] [ 6791.129395] [ 7271.78125 ] [ 7176.414551] [ 7334.098633] [ 7302.089355] [ 6865.493164] [ 6859.083008] [ 6971.091797] [ 6845.037598] [ 6842.427734] [ 6642.109863] [ 7116.804199] [ 7096.18457 ] [ 7257.665039] [ 7189.424805] [ 6881.958496] [ 6880.323242] [ 7117.20752 ] [ 7429.724609]] #Output[[ 7881.84668 ] [ 7987.371582] [ 8052.543945] [ 8673.21582 ] [ 8805.77832 ] [ 8719.961914] [ 8659.487305] [ 8319.472656] [ 8574.501953] [ 8564.016602] [ 8742.958008] [ 8208.995117] [ 7707.770996] [ 7824.231445] [ 7822.023438] [ 8043.951172] [ 7954.12793 ] [ 7688.077148] [ 8000.32959 ] [ 7927.714355] [ 8145.857422] [ 8230.923828] [ 8693.833008] [ 8838.375 ] [ 8994.488281] [ 9320.352539] [ 9081.762695] [ 9273.521484] [ 9527.160156] [10144.55664 ] [10701.69141 ] [10855.37109 ] [11011.10254 ] [11790.91699 ] [13016.23145 ] [11182.80664 ] [12407.33203 ] [11959.37109 ] [10817.15527 ] [10583.13477 ] [10801.67773 ] [11961.26953 ] [11215.4375 ] [10978.45996 ] [11208.55078 ] [11450.84668 ] [12285.95801 ] [12573.8125 ] [12156.5127 ] [11358.66211 ] [11815.98633 ] [11392.37891 ] [10256.05859 ] [10895.08984 ] [ 9477.641602] [ 9693.802734] [10666.48242 ] [10530.73242 ] [10767.13965 ] [10599.10547 ] [10343.10645 ] [ 9900.767578] [ 9811.925781] [ 9911.841797] [ 9870.303711] [ 9477.677734] [ 9552.860352] [ 9519.145508] [ 9607.423828] [10085.62793 ] [10399.66895 ] [10518.17481 ] [10821.72656 ] [10970.18457 ] [11805.65332 ] [11478.16895 ] [11941.96875 ] [11966.40723 ] [11862.93652 ] [11354.02441 ] [11523.5791 ] [11382.61621 ] [10895.83008 ] [10051.7041 ] [10311.5459 ] [10374.33887 ] [10231.74414 ] [10345.81055 ] [10916.05371 ] [10763.23242 ] [10138.04981 ] [10131.05566 ] [10407.96484 ] [10159.96094 ] [10138.51758 ] [10370.82031 ] [10185.5 ] [ 9754.422852] [ 9510.200195] [ 9598.173828] [ 9630.664063] [ 9757.970703] [10346.76074 ] [10623.54004 ] [10594.49316 ] [10575.5332 ] [10353.30273 ] [10517.25488 ] [10441.27637 ] [10334.97461 ] [10115.97559 ] [10178.37207 ] [10410.12695 ] [10360.54688 ] [10358.04883 ] [10347.71289 ] [10276.79395 ] [10241.27246 ] [10198.24805 ] [10266.41504 ] [10181.6416 ] [10019.7168 ] [10070.39258 ] [ 9729.324219] [ 8620.566406] [ 8486.993164] [ 8118.967773] [ 8251.845703] [ 8245.915039] [ 8104.185547] [ 8293.868164] [ 8343.276367] [ 8393.041992] [ 8259.992188] [ 8205.939453] [ 8151.500488] [ 7988.155762] [ 8245.623047] [ 8228.783203] [ 8595.740234] [ 8586.473633] [ 8321.756836] [ 8336.555664] [ 8321.005859] [ 8374.686523] [ 8205.369141] [ 8047.526855] [ 8103.911133] [ 7973.20752 ] [ 7988.560547] [ 8222.078125] [ 8243.720703] [ 8078.203125] [ 7514.671875] [ 7493.48877 ] [ 8660.700195] [ 9244.972656] [ 9551.714844] [ 9256.148438] [ 9427.6875 ] [ 9205.726563] [ 9199.584961] [ 9261.104492] [ 9324.717773] [ 9235.354492] [ 9412.612305] [ 9342.527344] [ 9360.879883] [ 9267.561523] [ 8804.880859] [ 8813.582031] [ 9055.526367] [ 8757.788086] [ 8815.662109] [ 8808.262695] [ 8708.094727] [ 8491.992188] [ 8550.760742] [ 8577.975586] [ 8309.286133] [ 8206.145508] [ 8027.268066] [ 7642.75 ] [ 7296.577637] [ 7397.796875] [ 7047.916992] [ 7146.133789] [ 7218.371094] [ 7531.663574] [ 7463.105957] [ 7761.243652] [ 7569.629883] [ 7424.29248 ] [ 7321.988281] [ 7320.145508] [ 7252.034668] [ 7448.307617] [ 7546.996582] [ 7556.237793] [ 7564.345215] [ 7400.899414] [ 7278.119629] [ 7217.427246] [ 7243.134277] [ 7269.68457 ] [ 7124.673828] [ 7152.301758] [ 6932.480469] [ 6640.515137] [ 7276.802734] [ 7202.844238] [ 7218.816406] [ 7191.158691] [ 7511.588867] [ 7355.628418] [ 7322.532227] [ 7275.155762] [ 7238.966797] [ 7290.088379] [ 7317.990234] [ 7422.652832] [ 7292.995117] [ 7193.599121] [ 7200.174316] [ 6985.470215] [ 7344.884277] [ 7410.656738] [ 7411.317383] [ 7769.219238] [ 8163.692383] [ 8079.862793] [ 7879.071289] [ 8166.554199] [ 8037.537598] [ 8192.494141] [ 8144.194336] [ 8827.764648] [ 8807.010742] [ 8723.786133] [ 8929.038086] [ 8942.808594] [ 8706.245117] [ 8657.642578] [ 8745.894531] [ 8680.875977] [ 8406.515625] [ 8445.43457 ] [ 8367.847656] [ 8596.830078] [ 8909.819336] [ 9358.589844] [ 9316.629883] [ 9508.993164] [ 9350.529297] [ 9392.875 ] [ 9344.365234] [ 9293.521484] [ 9180.962891] [ 9613.423828] [ 9729.801758] [ 9795.943359] [ 9865.119141] [10116.67383 ] [ 9856.611328] [10208.23633 ] [10326.05469 ] [10214.37988 ] [10312.11621 ] [ 9889.424805] [ 9934.433594] [ 9690.142578] [10141.99609 ] [ 9633.386719] [ 9608.475586] [ 9686.441406] [ 9663.181641] [ 9924.515625] [ 9650.174805] [ 9341.705078] [ 8820.522461] [ 8784.494141] [ 8672.455078] [ 8599.508789] [ 8562.454102] [ 8869.669922] [ 8787.786133] [ 8755.246094] [ 9078.762695] [ 9122.545898] [ 8909.954102] [ 8108.116211] [ 7923.644531] [ 7909.729492] [ 7911.430176] [ 4970.788086] [ 5563.707031] [ 5200.366211] [ 5392.314941] [ 5014.47998 ] [ 5225.629395] [ 5238.438477] [ 6191.192871] [ 6198.77832 ] [ 6185.066406] [ 5830.254883] [ 6416.314941] [ 6734.803711] [ 6681.062988] [ 6716.44043 ] [ 6469.79834 ] [ 6242.193848] [ 5922.042969] [ 6429.841797] [ 6438.644531] [ 6606.776367] [ 6793.624512] [ 6733.387207] [ 6867.527344] [ 6791.129395] [ 7271.78125 ] [ 7176.414551] [ 7334.098633] [ 7302.089355] [ 6865.493164] [ 6859.083008] [ 6971.091797] [ 6845.037598] [ 6842.427734] [ 6642.109863] [ 7116.804199] [ 7096.18457 ] [ 7257.665039] [ 7189.424805] [ 6881.958496] [ 6880.323242] [ 7117.20752 ] [ 7429.724609]] # Create the dependent data set # convert the data frame into a numpy array y = np.array(df['Prediction']) # Get all the values except last 'n' rows y = y[:-predictionDays] print(y) # Create the dependent data set# convert the data frame into a numpy arrayy = np.array(df['Prediction'])# Get all the values except last 'n' rowsy = y[:-predictionDays]print(y) #Output [10701.69141 10855.37109 11011.10254 11790.91699 13016.23145 11182.80664 12407.33203 11959.37109 10817.15527 10583.13477 10801.67773 11961.26953 11215.4375 10978.45996 11208.55078 11450.84668 12285.95801 12573.8125 12156.5127 11358.66211 11815.98633 11392.37891 10256.05859 10895.08984 9477.641602 9693.802734 10666.48242 10530.73242 10767.13965 10599.10547 10343.10645 9900.767578 9811.925781 9911.841797 9870.303711 9477.677734 9552.860352 9519.145508 9607.423828 10085.62793 10399.66895 10518.17481 10821.72656 10970.18457 11805.65332 11478.16895 11941.96875 11966.40723 11862.93652 11354.02441 11523.5791 11382.61621 10895.83008 10051.7041 10311.5459 10374.33887 10231.74414 10345.81055 10916.05371 10763.23242 10138.04981 10131.05566 10407.96484 10159.96094 10138.51758 10370.82031 10185.5 9754.422852 9510.200195 9598.173828 9630.664063 9757.970703 10346.76074 10623.54004 10594.49316 10575.5332 10353.30273 10517.25488 10441.27637 10334.97461 10115.97559 10178.37207 10410.12695 10360.54688 10358.04883 10347.71289 10276.79395 10241.27246 10198.24805 10266.41504 10181.6416 10019.7168 10070.39258 9729.324219 8620.566406 8486.993164 8118.967773 8251.845703 8245.915039 8104.185547 8293.868164 8343.276367 8393.041992 8259.992188 8205.939453 8151.500488 7988.155762 8245.623047 8228.783203 8595.740234 8586.473633 8321.756836 8336.555664 8321.005859 8374.686523 8205.369141 8047.526855 8103.911133 7973.20752 7988.560547 8222.078125 8243.720703 8078.203125 7514.671875 7493.48877 8660.700195 9244.972656 9551.714844 9256.148438 9427.6875 9205.726563 9199.584961 9261.104492 9324.717773 9235.354492 9412.612305 9342.527344 9360.879883 9267.561523 8804.880859 8813.582031 9055.526367 8757.788086 8815.662109 8808.262695 8708.094727 8491.992188 8550.760742 8577.975586 8309.286133 8206.145508 8027.268066 7642.75 7296.577637 7397.796875 7047.916992 7146.133789 7218.371094 7531.663574 7463.105957 7761.243652 7569.629883 7424.29248 7321.988281 7320.145508 7252.034668 7448.307617 7546.996582 7556.237793 7564.345215 7400.899414 7278.119629 7217.427246 7243.134277 7269.68457 7124.673828 7152.301758 6932.480469 6640.515137 7276.802734 7202.844238 7218.816406 7191.158691 7511.588867 7355.628418 7322.532227 7275.155762 7238.966797 7290.088379 7317.990234 7422.652832 7292.995117 7193.599121 7200.174316 6985.470215 7344.884277 7410.656738 7411.317383 7769.219238 8163.692383 8079.862793 7879.071289 8166.554199 8037.537598 8192.494141 8144.194336 8827.764648 8807.010742 8723.786133 8929.038086 8942.808594 8706.245117 8657.642578 8745.894531 8680.875977 8406.515625 8445.43457 8367.847656 8596.830078 8909.819336 9358.589844 9316.629883 9508.993164 9350.529297 9392.875 9344.365234 9293.521484 9180.962891 9613.423828 9729.801758 9795.943359 9865.119141 10116.67383 9856.611328 10208.23633 10326.05469 10214.37988 10312.11621 9889.424805 9934.433594 9690.142578 10141.99609 9633.386719 9608.475586 9686.441406 9663.181641 9924.515625 9650.174805 9341.705078 8820.522461 8784.494141 8672.455078 8599.508789 8562.454102 8869.669922 8787.786133 8755.246094 9078.762695 9122.545898 8909.954102 8108.116211 7923.644531 7909.729492 7911.430176 4970.788086 5563.707031 5200.366211 5392.314941 5014.47998 5225.629395 5238.438477 6191.192871 6198.77832 6185.066406 5830.254883 6416.314941 6734.803711 6681.062988 6716.44043 6469.79834 6242.193848 5922.042969 6429.841797 6438.644531 6606.776367 6793.624512 6733.387207 6867.527344 6791.129395 7271.78125 7176.414551 7334.098633 7302.089355 6865.493164 6859.083008 6971.091797 6845.037598 6842.427734 6642.109863 7116.804199 7096.18457 7257.665039 7189.424805 6881.958496 6880.323242 7117.20752 7429.724609 7550.900879 7569.936035 7679.867188 7795.601074 7807.058594 8801.038086 8658.553711 8864.766602 8988.59668 8897.46875 8912.654297 9003.070313 9268.761719 9951.518555 9842.666016 9593.896484 8756.430664 8601.795898 8804.477539 9269.987305 9733.72168 9328.197266 9377.013672 9670.739258 9726.575195 9729.038086 9522.981445 9081.761719 9182.577148 9180.045898] #Output[10701.69141 10855.37109 11011.10254 11790.91699 13016.23145 11182.80664 12407.33203 11959.37109 10817.15527 10583.13477 10801.67773 11961.26953 11215.4375 10978.45996 11208.55078 11450.84668 12285.95801 12573.8125 12156.5127 11358.66211 11815.98633 11392.37891 10256.05859 10895.08984 9477.641602 9693.802734 10666.48242 10530.73242 10767.13965 10599.10547 10343.10645 9900.767578 9811.925781 9911.841797 9870.303711 9477.677734 9552.860352 9519.145508 9607.423828 10085.62793 10399.66895 10518.17481 10821.72656 10970.18457 11805.65332 11478.16895 11941.96875 11966.40723 11862.93652 11354.02441 11523.5791 11382.61621 10895.83008 10051.7041 10311.5459 10374.33887 10231.74414 10345.81055 10916.05371 10763.23242 10138.04981 10131.05566 10407.96484 10159.96094 10138.51758 10370.82031 10185.5 9754.422852 9510.200195 9598.173828 9630.664063 9757.970703 10346.76074 10623.54004 10594.49316 10575.5332 10353.30273 10517.25488 10441.27637 10334.97461 10115.97559 10178.37207 10410.12695 10360.54688 10358.04883 10347.71289 10276.79395 10241.27246 10198.24805 10266.41504 10181.6416 10019.7168 10070.39258 9729.324219 8620.566406 8486.993164 8118.967773 8251.845703 8245.915039 8104.185547 8293.868164 8343.276367 8393.041992 8259.992188 8205.939453 8151.500488 7988.155762 8245.623047 8228.783203 8595.740234 8586.473633 8321.756836 8336.555664 8321.005859 8374.686523 8205.369141 8047.526855 8103.911133 7973.20752 7988.560547 8222.078125 8243.720703 8078.203125 7514.671875 7493.48877 8660.700195 9244.972656 9551.714844 9256.148438 9427.6875 9205.726563 9199.584961 9261.104492 9324.717773 9235.354492 9412.612305 9342.527344 9360.879883 9267.561523 8804.880859 8813.582031 9055.526367 8757.788086 8815.662109 8808.262695 8708.094727 8491.992188 8550.760742 8577.975586 8309.286133 8206.145508 8027.268066 7642.75 7296.577637 7397.796875 7047.916992 7146.133789 7218.371094 7531.663574 7463.105957 7761.243652 7569.629883 7424.29248 7321.988281 7320.145508 7252.034668 7448.307617 7546.996582 7556.237793 7564.345215 7400.899414 7278.119629 7217.427246 7243.134277 7269.68457 7124.673828 7152.301758 6932.480469 6640.515137 7276.802734 7202.844238 7218.816406 7191.158691 7511.588867 7355.628418 7322.532227 7275.155762 7238.966797 7290.088379 7317.990234 7422.652832 7292.995117 7193.599121 7200.174316 6985.470215 7344.884277 7410.656738 7411.317383 7769.219238 8163.692383 8079.862793 7879.071289 8166.554199 8037.537598 8192.494141 8144.194336 8827.764648 8807.010742 8723.786133 8929.038086 8942.808594 8706.245117 8657.642578 8745.894531 8680.875977 8406.515625 8445.43457 8367.847656 8596.830078 8909.819336 9358.589844 9316.629883 9508.993164 9350.529297 9392.875 9344.365234 9293.521484 9180.962891 9613.423828 9729.801758 9795.943359 9865.119141 10116.67383 9856.611328 10208.23633 10326.05469 10214.37988 10312.11621 9889.424805 9934.433594 9690.142578 10141.99609 9633.386719 9608.475586 9686.441406 9663.181641 9924.515625 9650.174805 9341.705078 8820.522461 8784.494141 8672.455078 8599.508789 8562.454102 8869.669922 8787.786133 8755.246094 9078.762695 9122.545898 8909.954102 8108.116211 7923.644531 7909.729492 7911.430176 4970.788086 5563.707031 5200.366211 5392.314941 5014.47998 5225.629395 5238.438477 6191.192871 6198.77832 6185.066406 5830.254883 6416.314941 6734.803711 6681.062988 6716.44043 6469.79834 6242.193848 5922.042969 6429.841797 6438.644531 6606.776367 6793.624512 6733.387207 6867.527344 6791.129395 7271.78125 7176.414551 7334.098633 7302.089355 6865.493164 6859.083008 6971.091797 6845.037598 6842.427734 6642.109863 7116.804199 7096.18457 7257.665039 7189.424805 6881.958496 6880.323242 7117.20752 7429.724609 7550.900879 7569.936035 7679.867188 7795.601074 7807.058594 8801.038086 8658.553711 8864.766602 8988.59668 8897.46875 8912.654297 9003.070313 9268.761719 9951.518555 9842.666016 9593.896484 8756.430664 8601.795898 8804.477539 9269.987305 9733.72168 9328.197266 9377.013672 9670.739258 9726.575195 9729.038086 9522.981445 9081.761719 9182.577148 9180.045898] # Split the data into 80% training and 20% testing from sklearn.model_selection import train_test_split xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size = 0.2) # set the predictionDays array equal to last 30 rows from the original data set predictionDays_array = np.array(df.drop(['Prediction'],1))[-predictionDays:] print(predictionDays_array) # Split the data into 80% training and 20% testingfrom sklearn.model_selection import train_test_splitxtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size = 0.2)# set the predictionDays array equal to last 30 rows from the original data setpredictionDays_array = np.array(df.drop(['Prediction'],1))[-predictionDays:]print(predictionDays_array) #Output [[7550.900879] [7569.936035] [7679.867188] [7795.601074] [7807.058594] [8801.038086] [8658.553711] [8864.766602] [8988.59668 ] [8897.46875 ] [8912.654297] [9003.070313] [9268.761719] [9951.518555] [9842.666016] [9593.896484] [8756.430664] [8601.795898] [8804.477539] [9269.987305] [9733.72168 ] [9328.197266] [9377.013672] [9670.739258] [9726.575195] [9729.038086] [9522.981445] [9081.761719] [9182.577148] [9180.045898]] #Output[[7550.900879] [7569.936035] [7679.867188] [7795.601074] [7807.058594] [8801.038086] [8658.553711] [8864.766602] [8988.59668 ] [8897.46875 ] [8912.654297] [9003.070313] [9268.761719] [9951.518555] [9842.666016] [9593.896484] [8756.430664] [8601.795898] [8804.477539] [9269.987305] [9733.72168 ] [9328.197266] [9377.013672] [9670.739258] [9726.575195] [9729.038086] [9522.981445] [9081.761719] [9182.577148] [9180.045898]] Now we will create a Machine Learning Model from sklearn.svm import SVR # Create and Train the Support Vector Machine (Regression) using radial basis function svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.00001) svr_rbf.fit(xtrain, ytrain) from sklearn.svm import SVR# Create and Train the Support Vector Machine (Regression) using radial basis functionsvr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.00001)svr_rbf.fit(xtrain, ytrain) #Output SVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=1e-05, kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) #OutputSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=1e-05, kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) Test the model svr_rbf_confidence = svr_rbf.score(xtest,ytest) print('SVR_RBF accuracy :',svr_rbf_confidence) svr_rbf_confidence = svr_rbf.score(xtest,ytest)print('SVR_RBF accuracy :',svr_rbf_confidence) #Output SVR_RBF accuracy : 0.80318203039572782 #OutputSVR_RBF accuracy : 0.80318203039572782 # print the predicted values svm_prediction = svr_rbf.predict(xtest) print(svm_prediction) print() print(ytest) # print the predicted valuessvm_prediction = svr_rbf.predict(xtest)print(svm_prediction)print()print(ytest) #Output [ 8580.88704057 8598.79383546 8939.94375214 8388.92621489 9102.56201779 8832.68229779 8329.30224101 8157.80057348 8602.29644729 8707.90682044 7643.06939601 8408.93105022 8917.45480981 8511.7652266 7834.15919638 8832.62858497 8381.02268219 7098.85213417 8805.2578118 7757.01224446 8791.58493431 8961.26396398 8218.28537299 10512.39752674 8505.95838523 8504.09557077 8416.46565526 8812.06086838 8565.94893198 8378.22399262 8585.8737782 7630.00945667 9602.30696397 8934.97064742 9812.06855777 8473.66659984 8408.82946381 10548.41305096 9362.68382564 8597.33711016 7730.30747013 7792.1701846 8840.84467855 9893.05484852 9725.48044015 8539.54247434 8566.45635477 8916.11467623 8105.74685394 9240.42186178 9606.02191396 8392.00381076 8878.46155081 8586.37388665 8307.42830793 8397.91702065 9446.57181741 8857.3956994 8599.40879784 9324.81811167 9685.85175143 8286.70385539 9344.79392774 8978.54603972 8431.46694919 9370.69251132 8513.0501515 9400.8871896 ] [ 7546.996582 9598.173828 8912.654297 10138.04981 9261.104492 7047.916992 10159.96094 5238.438477 8037.537598 7238.966797 7410.656738 8374.686523 10312.11621 7321.988281 7292.995117 6932.480469 8047.526855 6971.091797 8657.642578 7257.665039 9328.197266 8807.010742 7923.644531 9519.145508 10185.5 9630.664063 6867.527344 8804.880859 8620.566406 7531.663574 7909.729492 8745.894531 9795.943359 9269.987305 9342.527344 8192.494141 10530.73242 11862.93652 9267.561523 8245.915039 7411.317383 8079.862793 5922.042969 7334.098633 7218.816406 9729.324219 10181.6416 6793.624512 8909.954102 11959.37109 7642.75 10241.27246 11182.80664 8586.473633 9078.762695 7556.237793 9729.801758 10256.05859 8599.508789 9324.717773 11450.84668 6198.77832 8027.268066 8804.477539 10276.79395 8206.145508 8321.756836 8151.500488] #Output[ 8580.88704057 8598.79383546 8939.94375214 8388.92621489 9102.56201779 8832.68229779 8329.30224101 8157.80057348 8602.29644729 8707.90682044 7643.06939601 8408.93105022 8917.45480981 8511.7652266 7834.15919638 8832.62858497 8381.02268219 7098.85213417 8805.2578118 7757.01224446 8791.58493431 8961.26396398 8218.28537299 10512.39752674 8505.95838523 8504.09557077 8416.46565526 8812.06086838 8565.94893198 8378.22399262 8585.8737782 7630.00945667 9602.30696397 8934.97064742 9812.06855777 8473.66659984 8408.82946381 10548.41305096 9362.68382564 8597.33711016 7730.30747013 7792.1701846 8840.84467855 9893.05484852 9725.48044015 8539.54247434 8566.45635477 8916.11467623 8105.74685394 9240.42186178 9606.02191396 8392.00381076 8878.46155081 8586.37388665 8307.42830793 8397.91702065 9446.57181741 8857.3956994 8599.40879784 9324.81811167 9685.85175143 8286.70385539 9344.79392774 8978.54603972 8431.46694919 9370.69251132 8513.0501515 9400.8871896 ]​[ 7546.996582 9598.173828 8912.654297 10138.04981 9261.104492 7047.916992 10159.96094 5238.438477 8037.537598 7238.966797 7410.656738 8374.686523 10312.11621 7321.988281 7292.995117 6932.480469 8047.526855 6971.091797 8657.642578 7257.665039 9328.197266 8807.010742 7923.644531 9519.145508 10185.5 9630.664063 6867.527344 8804.880859 8620.566406 7531.663574 7909.729492 8745.894531 9795.943359 9269.987305 9342.527344 8192.494141 10530.73242 11862.93652 9267.561523 8245.915039 7411.317383 8079.862793 5922.042969 7334.098633 7218.816406 9729.324219 10181.6416 6793.624512 8909.954102 11959.37109 7642.75 10241.27246 11182.80664 8586.473633 9078.762695 7556.237793 9729.801758 10256.05859 8599.508789 9324.717773 11450.84668 6198.77832 8027.268066 8804.477539 10276.79395 8206.145508 8321.756836 8151.500488] # Print the model predictions for the next 30 days svm_prediction = svr_rbf.predict(predictionDays_array) print(svm_prediction) print() #Print the actual price for bitcoin for last 30 days print(df.tail(predictionDays)) # Print the model predictions for the next 30 dayssvm_prediction = svr_rbf.predict(predictionDays_array)print(svm_prediction)print()#Print the actual price for bitcoin for last 30 daysprint(df.tail(predictionDays)) #Output [7746.08637672 7835.76387711 8657.25433329 9554.67400231 9617.98954538 8917.61532094 8831.29326224 8885.55655284 8640.51491415 8841.78875835 8815.42825999 8602.53094625 8400.08270252 8426.55153101 8172.93870238 8395.85348921 8903.73403919 8811.70139747 8917.58878224 8402.31118948 8102.70537693 8518.83392876 8606.8071745 8195.93279966 8108.54622414 8106.38537126 8573.49097641 8410.28935674 8307.6380027 8307.33725309] Price Prediction 337 7550.900879 NaN 338 7569.936035 NaN 339 7679.867188 NaN 340 7795.601074 NaN 341 7807.058594 NaN 342 8801.038086 NaN 343 8658.553711 NaN 344 8864.766602 NaN 345 8988.596680 NaN 346 8897.468750 NaN 347 8912.654297 NaN 348 9003.070313 NaN 349 9268.761719 NaN 350 9951.518555 NaN 351 9842.666016 NaN 352 9593.896484 NaN 353 8756.430664 NaN 354 8601.795898 NaN 355 8804.477539 NaN 356 9269.987305 NaN 357 9733.721680 NaN 358 9328.197266 NaN 359 9377.013672 NaN 360 9670.739258 NaN 361 9726.575195 NaN 362 9729.038086 NaN 363 9522.981445 NaN 364 9081.761719 NaN 365 9182.577148 NaN 366 9180.045898 NaN #Output[7746.08637672 7835.76387711 8657.25433329 9554.67400231 9617.98954538 8917.61532094 8831.29326224 8885.55655284 8640.51491415 8841.78875835 8815.42825999 8602.53094625 8400.08270252 8426.55153101 8172.93870238 8395.85348921 8903.73403919 8811.70139747 8917.58878224 8402.31118948 8102.70537693 8518.83392876 8606.8071745 8195.93279966 8108.54622414 8106.38537126 8573.49097641 8410.28935674 8307.6380027 8307.33725309]​ Price Prediction337 7550.900879 NaN338 7569.936035 NaN339 7679.867188 NaN340 7795.601074 NaN341 7807.058594 NaN342 8801.038086 NaN343 8658.553711 NaN344 8864.766602 NaN345 8988.596680 NaN346 8897.468750 NaN347 8912.654297 NaN348 9003.070313 NaN349 9268.761719 NaN350 9951.518555 NaN351 9842.666016 NaN352 9593.896484 NaN353 8756.430664 NaN354 8601.795898 NaN355 8804.477539 NaN356 9269.987305 NaN357 9733.721680 NaN358 9328.197266 NaN359 9377.013672 NaN360 9670.739258 NaN361 9726.575195 NaN362 9729.038086 NaN363 9522.981445 NaN364 9081.761719 NaN365 9182.577148 NaN366 9180.045898 NaN​ Follow us on Instagram for all your Queries Instagram";Bitcoin Price Prediction with Machine Learning
2020-05-23 13:03:44;Recommendation systems are among the most popular applications of data science. They are used to predict the Rating or Preference that a user would give to an item.Almost every major company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on auto play, and Facebook uses it to recommend pages to like and people to follow.In this Data Science project, you will see how to build a Book Recommendation System model using Machine Learning Techniques.You can download the data sets we need for this task from here:Let’s start with this project#Output(1149780, 3)[‘userID’, ‘ISBN’, ‘bookRating’]#Output(271360, 8)[‘ISBN’, ‘bookTitle’, ‘bookAuthor’, ‘yearOfPublication’, ‘publisher’, ‘imageUrlS’, ‘imageUrlM’, ‘imageUrlL’]#Output(278858, 3)[‘userID’, ‘Location’, ‘Age’]To ensure statistical significance, users with less than 200 ratings, and books with less than 100 ratings are excluded.kNN is a machine learning algorithm to find clusters of similar users based on common book ratings, and make predictions using the average rating of top-k nearest neighbors. For example, we first present ratings in a matrix with the matrix having one row for each item (book) and one column for each user.Now we will group by book titles and create a new column for total rating count.Now we will combine the rating data with the total rating count data, this gives us exactly what we need to find out which books are popular and filter out lesser-known books.We convert our table to a 2D matrix, and fill the missing values with zeros (since we will calculate distances between rating vectors). We then transform the values(ratings) of the matrix dataframe into a scipy sparse matrix for more efficient calculations.;https://thecleverprogrammer.com/2020/05/23/book-recommendation-system-with-machine-learning/;['sklearn'];1.0;['NN'];['ML', 'KNN', 'Recommender', 'NN'];['predict', 'fit', 'model', 'machine learning', 'recommend', 'k-nearest neighbor', 'filter', 'label'];"Recommendation systems are among the most popular applications of data science. They are used to predict the Rating or Preference that a user would give to an item. Almost every major company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on auto play, and Facebook uses it to recommend pages to like and people to follow. Let’s Build our own recommendation system In this Data Science project, you will see how to build a Book Recommendation System model using Machine Learning Techniques. You can download the data sets we need for this task from here: BX-Book-RatingsDownload BX-BooksDownload BX-UsersDownload Let’s start with this project import pandas as pd import numpy as np import matplotlib.pyplot as plt books = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding=""latin-1"") books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL'] users = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=""latin-1"") users.columns = ['userID', 'Location', 'Age'] ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=""latin-1"") ratings.columns = ['userID', 'ISBN', 'bookRating'] print(ratings.shape) print(list(ratings.columns))​x import pandas as pdimport numpy as npimport matplotlib.pyplot as pltbooks = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL']users = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")users.columns = ['userID', 'Location', 'Age']ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")ratings.columns = ['userID', 'ISBN', 'bookRating']​print(ratings.shape)print(list(ratings.columns)) #Output(1149780, 3)[‘userID’, ‘ISBN’, ‘bookRating’] plt.rc(""font"", size=15) ratings.bookRating.value_counts(sort=False).plot(kind='bar') plt.title('Rating Distribution\n') plt.xlabel('Rating') plt.ylabel('Count') plt.show() plt.rc(""font"", size=15)ratings.bookRating.value_counts(sort=False).plot(kind='bar')plt.title('Rating Distribution\n')plt.xlabel('Rating')plt.ylabel('Count')plt.show() print(books.shape) print(list(books.columns)) print(books.shape)print(list(books.columns)) #Output(271360, 8)[‘ISBN’, ‘bookTitle’, ‘bookAuthor’, ‘yearOfPublication’, ‘publisher’, ‘imageUrlS’, ‘imageUrlM’, ‘imageUrlL’] print(users.shape) print(list(users.columns)) print(users.shape)print(list(users.columns)) #Output(278858, 3)[‘userID’, ‘Location’, ‘Age’] users.Age.hist(bins=[0, 10, 20, 30, 40, 50, 100]) plt.title('Age Distribution\n') plt.xlabel('Age') plt.ylabel('Count') plt.show() users.Age.hist(bins=[0, 10, 20, 30, 40, 50, 100])plt.title('Age Distribution\n')plt.xlabel('Age')plt.ylabel('Count')plt.show() To ensure statistical significance, users with less than 200 ratings, and books with less than 100 ratings are excluded. counts1 = ratings['userID'].value_counts() ratings = ratings[ratings['userID'].isin(counts1[counts1 >= 200].index)] counts = ratings['bookRating'].value_counts() ratings = ratings[ratings['bookRating'].isin(counts[counts >= 100].index)] counts1 = ratings['userID'].value_counts()ratings = ratings[ratings['userID'].isin(counts1[counts1 >= 200].index)]counts = ratings['bookRating'].value_counts()ratings = ratings[ratings['bookRating'].isin(counts[counts >= 100].index)] Collaborative Filtering Using k-Nearest Neighbors (kNN) kNN is a machine learning algorithm to find clusters of similar users based on common book ratings, and make predictions using the average rating of top-k nearest neighbors. For example, we first present ratings in a matrix with the matrix having one row for each item (book) and one column for each user. combine_book_rating = pd.merge(ratings, books, on='ISBN') columns = ['yearOfPublication', 'publisher', 'bookAuthor', 'imageUrlS', 'imageUrlM', 'imageUrlL'] combine_book_rating = combine_book_rating.drop(columns, axis=1) print(combine_book_rating.head()) combine_book_rating = pd.merge(ratings, books, on='ISBN')columns = ['yearOfPublication', 'publisher', 'bookAuthor', 'imageUrlS', 'imageUrlM', 'imageUrlL']combine_book_rating = combine_book_rating.drop(columns, axis=1)print(combine_book_rating.head()) #Output userID ... bookTitle 0 277427 ... Politically Correct Bedtime Stories: Modern Ta... 1 3363 ... Politically Correct Bedtime Stories: Modern Ta... 2 11676 ... Politically Correct Bedtime Stories: Modern Ta... 3 12538 ... Politically Correct Bedtime Stories: Modern Ta... 4 13552 ... Politically Correct Bedtime Stories: Modern Ta... [5 rows x 4 columns] #OutputuserID ... bookTitle0 277427 ... Politically Correct Bedtime Stories: Modern Ta...1 3363 ... Politically Correct Bedtime Stories: Modern Ta...2 11676 ... Politically Correct Bedtime Stories: Modern Ta...3 12538 ... Politically Correct Bedtime Stories: Modern Ta...4 13552 ... Politically Correct Bedtime Stories: Modern Ta...​[5 rows x 4 columns] Now we will group by book titles and create a new column for total rating count. combine_book_rating = combine_book_rating.dropna(axis = 0, subset = ['bookTitle']) book_ratingCount = (combine_book_rating. groupby(by = ['bookTitle'])['bookRating']. count(). reset_index(). rename(columns = {'bookRating': 'totalRatingCount'}) [['bookTitle', 'totalRatingCount']] ) print(book_ratingCount.head()) combine_book_rating = combine_book_rating.dropna(axis = 0, subset = ['bookTitle'])​book_ratingCount = (combine_book_rating. groupby(by = ['bookTitle'])['bookRating']. count(). reset_index(). rename(columns = {'bookRating': 'totalRatingCount'}) [['bookTitle', 'totalRatingCount']] )print(book_ratingCount.head()) #Output bookTitle totalRatingCount 0 A Light in the Storm: The Civil War Diary of ... 2 1 Always Have Popsicles 1 2 Apple Magic (The Collector's series) 1 3 Beyond IBM: Leadership Marketing and Finance ... 1 4 Clifford Visita El Hospital (Clifford El Gran... 1 #OutputbookTitle totalRatingCount0 A Light in the Storm: The Civil War Diary of ... 21 Always Have Popsicles 12 Apple Magic (The Collector's series) 13 Beyond IBM: Leadership Marketing and Finance ... 14 Clifford Visita El Hospital (Clifford El Gran... 1 Now we will combine the rating data with the total rating count data, this gives us exactly what we need to find out which books are popular and filter out lesser-known books. rating_with_totalRatingCount = combine_book_rating.merge(book_ratingCount, left_on = 'bookTitle', right_on = 'bookTitle', how = 'left') print(rating_with_totalRatingCount.head()) pd.set_option('display.float_format', lambda x: '%.3f' % x) print(book_ratingCount['totalRatingCount'].describe()) rating_with_totalRatingCount = combine_book_rating.merge(book_ratingCount, left_on = 'bookTitle', right_on = 'bookTitle', how = 'left')print(rating_with_totalRatingCount.head())​pd.set_option('display.float_format', lambda x: '%.3f' % x)print(book_ratingCount['totalRatingCount'].describe()) #Output userID ... totalRatingCount 0 277427 ... 82 1 3363 ... 82 2 11676 ... 82 3 12538 ... 82 4 13552 ... 82 [5 rows x 5 columns] #OutputuserID ... totalRatingCount0 277427 ... 821 3363 ... 822 11676 ... 823 12538 ... 824 13552 ... 82​[5 rows x 5 columns] pd.set_option('display.float_format', lambda x: '%.3f' % x) print(book_ratingCount['totalRatingCount'].describe()) pd.set_option('display.float_format', lambda x: '%.3f' % x)print(book_ratingCount['totalRatingCount'].describe()) #Output count 160576.000 mean 3.044 std 7.428 min 1.000 25% 1.000 50% 1.000 75% 2.000 max 365.000 Name: totalRatingCount, dtype: float64 #Outputcount 160576.000mean 3.044std 7.428min 1.00025% 1.00050% 1.00075% 2.000max 365.000Name: totalRatingCount, dtype: float64 print(book_ratingCount['totalRatingCount'].quantile(np.arange(.9, 1, .01))) print(book_ratingCount['totalRatingCount'].quantile(np.arange(.9, 1, .01))) #Output 0.900 5.000 0.910 6.000 0.920 7.000 0.930 7.000 0.940 8.000 0.950 10.000 0.960 11.000 0.970 14.000 0.980 19.000 0.990 31.000 Name: totalRatingCount, dtype: float64 #Output0.900 5.0000.910 6.0000.920 7.0000.930 7.0000.940 8.0000.950 10.0000.960 11.0000.970 14.0000.980 19.0000.990 31.000Name: totalRatingCount, dtype: float64 popularity_threshold = 50 rating_popular_book = rating_with_totalRatingCount.query('totalRatingCount >= @popularity_threshold') print(rating_popular_book.head()) popularity_threshold = 50rating_popular_book = rating_with_totalRatingCount.query('totalRatingCount >= @popularity_threshold')print(rating_popular_book.head()) #Output userID ... totalRatingCount 0 277427 ... 82 1 3363 ... 82 2 11676 ... 82 3 12538 ... 82 4 13552 ... 82 [5 rows x 5 columns] #OutputuserID ... totalRatingCount0 277427 ... 821 3363 ... 822 11676 ... 823 12538 ... 824 13552 ... 82​[5 rows x 5 columns] Filter to users in US and Canada only combined = rating_popular_book.merge(users, left_on = 'userID', right_on = 'userID', how = 'left') us_canada_user_rating = combined[combined['Location'].str.contains(""usa|canada"")] us_canada_user_rating=us_canada_user_rating.drop('Age', axis=1) print(us_canada_user_rating.head()) combined = rating_popular_book.merge(users, left_on = 'userID', right_on = 'userID', how = 'left')​us_canada_user_rating = combined[combined['Location'].str.contains(""usa|canada"")]us_canada_user_rating=us_canada_user_rating.drop('Age', axis=1)print(us_canada_user_rating.head()) #Output userID ISBN ... totalRatingCount Location 0 277427 002542730X ... 82 gilbert, arizona, usa 1 3363 002542730X ... 82 knoxville, tennessee, usa 3 12538 002542730X ... 82 byron, minnesota, usa 4 13552 002542730X ... 82 cordova, tennessee, usa 5 16795 002542730X ... 82 mechanicsville, maryland, usa [5 rows x 6 columns] #OutputuserID ISBN ... totalRatingCount Location0 277427 002542730X ... 82 gilbert, arizona, usa1 3363 002542730X ... 82 knoxville, tennessee, usa3 12538 002542730X ... 82 byron, minnesota, usa4 13552 002542730X ... 82 cordova, tennessee, usa5 16795 002542730X ... 82 mechanicsville, maryland, usa​[5 rows x 6 columns] Implementing kNN We convert our table to a 2D matrix, and fill the missing values with zeros (since we will calculate distances between rating vectors). We then transform the values(ratings) of the matrix dataframe into a scipy sparse matrix for more efficient calculations. from scipy.sparse import csr_matrix us_canada_user_rating = us_canada_user_rating.drop_duplicates(['userID', 'bookTitle']) us_canada_user_rating_pivot = us_canada_user_rating.pivot(index = 'bookTitle', columns = 'userID', values = 'bookRating').fillna(0) us_canada_user_rating_matrix = csr_matrix(us_canada_user_rating_pivot.values) from sklearn.neighbors import NearestNeighbors model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute') model_knn.fit(us_canada_user_rating_matrix) print(model_knn) from scipy.sparse import csr_matrixus_canada_user_rating = us_canada_user_rating.drop_duplicates(['userID', 'bookTitle'])us_canada_user_rating_pivot = us_canada_user_rating.pivot(index = 'bookTitle', columns = 'userID', values = 'bookRating').fillna(0)us_canada_user_rating_matrix = csr_matrix(us_canada_user_rating_pivot.values)​from sklearn.neighbors import NearestNeighbors​model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')model_knn.fit(us_canada_user_rating_matrix)print(model_knn) #Output NearestNeighbors(algorithm='brute', leaf_size=30, metric='cosine', metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0) #OutputNearestNeighbors(algorithm='brute', leaf_size=30, metric='cosine', metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0) query_index = np.random.choice(us_canada_user_rating_pivot.shape[0]) print(query_index) print(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1,-1)) distances, indices = model_knn.kneighbors(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 6) us_canada_user_rating_pivot.index[query_index] query_index = np.random.choice(us_canada_user_rating_pivot.shape[0])print(query_index)print(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1,-1))distances, indices = model_knn.kneighbors(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 6)us_canada_user_rating_pivot.index[query_index] #Output [[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] #Output[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] for i in range(0, len(distances.flatten())): if i == 0: print('Recommendations for {0}:\n'.format(us_canada_user_rating_pivot.index[query_index])) else: print('{0}: {1}, with distance of {2}:'.format(i, us_canada_user_rating_pivot.index[indices.flatten()[i]], distances.flatten()[i])) for i in range(0, len(distances.flatten())): if i == 0: print('Recommendations for {0}:\n'.format(us_canada_user_rating_pivot.index[query_index])) else: print('{0}: {1}, with distance of {2}:'.format(i, us_canada_user_rating_pivot.index[indices.flatten()[i]], distances.flatten()[i])) #Output Recommendations for Flesh and Blood: 1: The Murder Book, with distance of 0.596243943613731: 2: Choke, with distance of 0.6321092998573327: 3: Easy Prey, with distance of 0.704010041374638: 4: 2nd Chance, with distance of 0.7292664430521165: 5: The Empty Chair, with distance of 0.7432121818110763: #OutputRecommendations for Flesh and Blood:​1: The Murder Book, with distance of 0.596243943613731:2: Choke, with distance of 0.6321092998573327:3: Easy Prey, with distance of 0.704010041374638:4: 2nd Chance, with distance of 0.7292664430521165:5: The Empty Chair, with distance of 0.7432121818110763: Follow us on Instagram for all your Queries Instagram";Book Recommendation System with Machine Learning
2020-05-23 18:58:10;In this Data Science Project we will predict Bitcoin Price for the next 30 days with Machine Learning model Support Vector Machines(Regression).You can download the data set we need for this task from here:Let’s start with importing librariesRemove the date columnNow lets create a variable to predict ‘n’ days out there in the futureTo Show last 5 rows of new data set;https://thecleverprogrammer.com/2020/05/23/data-science-project-bitcoin-price-prediction-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Support Vector Machines', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'support vector machines', 'train'];"In this Data Science Project we will predict Bitcoin Price for the next 30 days with Machine Learning model Support Vector Machines(Regression). You can download the data set we need for this task from here: bitcoinDownload Let’s start with importing libraries import numpy as np import pandas as pd df = pd.read_csv(""bitcoin.csv"") df.head()​x import numpy as npimport pandas as pddf = pd.read_csv(""bitcoin.csv"")df.head() Remove the date column df.drop(['Date'],1,inplace=True) df.drop(['Date'],1,inplace=True) Now lets create a variable to predict ‘n’ days out there in the future predictionDays = 30 # Create another column shifted 'n' units up df['Prediction'] = df[['Price']].shift(-predictionDays) # show the first 5 rows df.head() predictionDays = 30# Create another column shifted 'n' units updf['Prediction'] = df[['Price']].shift(-predictionDays)# show the first 5 rowsdf.head() To Show last 5 rows of new data set df.tail() df.tail() # Create the independent dada set # Here we will convert the data frame into a numpy array and drp the prediction column x = np.array(df.drop(['Prediction'],1)) # Remove the last 'n' rows where 'n' is the predictionDays x = x[:len(df)-predictionDays] print(x) # Create the independent dada set# Here we will convert the data frame into a numpy array and drp the prediction columnx = np.array(df.drop(['Prediction'],1))# Remove the last 'n' rows where 'n' is the predictionDaysx = x[:len(df)-predictionDays]print(x) #Output [[ 7881.84668 ] [ 7987.371582] [ 8052.543945] [ 8673.21582 ] [ 8805.77832 ] [ 8719.961914] [ 8659.487305] [ 8319.472656] [ 8574.501953] [ 8564.016602] [ 8742.958008] [ 8208.995117] [ 7707.770996] [ 7824.231445] [ 7822.023438] [ 8043.951172] [ 7954.12793 ] [ 7688.077148] [ 8000.32959 ] [ 7927.714355] [ 8145.857422] [ 8230.923828] [ 8693.833008] [ 8838.375 ] [ 8994.488281] [ 9320.352539] [ 9081.762695] [ 9273.521484] [ 9527.160156] [10144.55664 ] [10701.69141 ] [10855.37109 ] [11011.10254 ] [11790.91699 ] [13016.23145 ] [11182.80664 ] [12407.33203 ] [11959.37109 ] [10817.15527 ] [10583.13477 ] [10801.67773 ] [11961.26953 ] [11215.4375 ] [10978.45996 ] [11208.55078 ] [11450.84668 ] [12285.95801 ] [12573.8125 ] [12156.5127 ] [11358.66211 ] [11815.98633 ] [11392.37891 ] [10256.05859 ] [10895.08984 ] [ 9477.641602] [ 9693.802734] [10666.48242 ] [10530.73242 ] [10767.13965 ] [10599.10547 ] [10343.10645 ] [ 9900.767578] [ 9811.925781] [ 9911.841797] [ 9870.303711] [ 9477.677734] [ 9552.860352] [ 9519.145508] [ 9607.423828] [10085.62793 ] [10399.66895 ] [10518.17481 ] [10821.72656 ] [10970.18457 ] [11805.65332 ] [11478.16895 ] [11941.96875 ] [11966.40723 ] [11862.93652 ] [11354.02441 ] [11523.5791 ] [11382.61621 ] [10895.83008 ] [10051.7041 ] [10311.5459 ] [10374.33887 ] [10231.74414 ] [10345.81055 ] [10916.05371 ] [10763.23242 ] [10138.04981 ] [10131.05566 ] [10407.96484 ] [10159.96094 ] [10138.51758 ] [10370.82031 ] [10185.5 ] [ 9754.422852] [ 9510.200195] [ 9598.173828] [ 9630.664063] [ 9757.970703] [10346.76074 ] [10623.54004 ] [10594.49316 ] [10575.5332 ] [10353.30273 ] [10517.25488 ] [10441.27637 ] [10334.97461 ] [10115.97559 ] [10178.37207 ] [10410.12695 ] [10360.54688 ] [10358.04883 ] [10347.71289 ] [10276.79395 ] [10241.27246 ] [10198.24805 ] [10266.41504 ] [10181.6416 ] [10019.7168 ] [10070.39258 ] [ 9729.324219] [ 8620.566406] [ 8486.993164] [ 8118.967773] [ 8251.845703] [ 8245.915039] [ 8104.185547] [ 8293.868164] [ 8343.276367] [ 8393.041992] [ 8259.992188] [ 8205.939453] [ 8151.500488] [ 7988.155762] [ 8245.623047] [ 8228.783203] [ 8595.740234] [ 8586.473633] [ 8321.756836] [ 8336.555664] [ 8321.005859] [ 8374.686523] [ 8205.369141] [ 8047.526855] [ 8103.911133] [ 7973.20752 ] [ 7988.560547] [ 8222.078125] [ 8243.720703] [ 8078.203125] [ 7514.671875] [ 7493.48877 ] [ 8660.700195] [ 9244.972656] [ 9551.714844] [ 9256.148438] [ 9427.6875 ] [ 9205.726563] [ 9199.584961] [ 9261.104492] [ 9324.717773] [ 9235.354492] [ 9412.612305] [ 9342.527344] [ 9360.879883] [ 9267.561523] [ 8804.880859] [ 8813.582031] [ 9055.526367] [ 8757.788086] [ 8815.662109] [ 8808.262695] [ 8708.094727] [ 8491.992188] [ 8550.760742] [ 8577.975586] [ 8309.286133] [ 8206.145508] [ 8027.268066] [ 7642.75 ] [ 7296.577637] [ 7397.796875] [ 7047.916992] [ 7146.133789] [ 7218.371094] [ 7531.663574] [ 7463.105957] [ 7761.243652] [ 7569.629883] [ 7424.29248 ] [ 7321.988281] [ 7320.145508] [ 7252.034668] [ 7448.307617] [ 7546.996582] [ 7556.237793] [ 7564.345215] [ 7400.899414] [ 7278.119629] [ 7217.427246] [ 7243.134277] [ 7269.68457 ] [ 7124.673828] [ 7152.301758] [ 6932.480469] [ 6640.515137] [ 7276.802734] [ 7202.844238] [ 7218.816406] [ 7191.158691] [ 7511.588867] [ 7355.628418] [ 7322.532227] [ 7275.155762] [ 7238.966797] [ 7290.088379] [ 7317.990234] [ 7422.652832] [ 7292.995117] [ 7193.599121] [ 7200.174316] [ 6985.470215] [ 7344.884277] [ 7410.656738] [ 7411.317383] [ 7769.219238] [ 8163.692383] [ 8079.862793] [ 7879.071289] [ 8166.554199] [ 8037.537598] [ 8192.494141] [ 8144.194336] [ 8827.764648] [ 8807.010742] [ 8723.786133] [ 8929.038086] [ 8942.808594] [ 8706.245117] [ 8657.642578] [ 8745.894531] [ 8680.875977] [ 8406.515625] [ 8445.43457 ] [ 8367.847656] [ 8596.830078] [ 8909.819336] [ 9358.589844] [ 9316.629883] [ 9508.993164] [ 9350.529297] [ 9392.875 ] [ 9344.365234] [ 9293.521484] [ 9180.962891] [ 9613.423828] [ 9729.801758] [ 9795.943359] [ 9865.119141] [10116.67383 ] [ 9856.611328] [10208.23633 ] [10326.05469 ] [10214.37988 ] [10312.11621 ] [ 9889.424805] [ 9934.433594] [ 9690.142578] [10141.99609 ] [ 9633.386719] [ 9608.475586] [ 9686.441406] [ 9663.181641] [ 9924.515625] [ 9650.174805] [ 9341.705078] [ 8820.522461] [ 8784.494141] [ 8672.455078] [ 8599.508789] [ 8562.454102] [ 8869.669922] [ 8787.786133] [ 8755.246094] [ 9078.762695] [ 9122.545898] [ 8909.954102] [ 8108.116211] [ 7923.644531] [ 7909.729492] [ 7911.430176] [ 4970.788086] [ 5563.707031] [ 5200.366211] [ 5392.314941] [ 5014.47998 ] [ 5225.629395] [ 5238.438477] [ 6191.192871] [ 6198.77832 ] [ 6185.066406] [ 5830.254883] [ 6416.314941] [ 6734.803711] [ 6681.062988] [ 6716.44043 ] [ 6469.79834 ] [ 6242.193848] [ 5922.042969] [ 6429.841797] [ 6438.644531] [ 6606.776367] [ 6793.624512] [ 6733.387207] [ 6867.527344] [ 6791.129395] [ 7271.78125 ] [ 7176.414551] [ 7334.098633] [ 7302.089355] [ 6865.493164] [ 6859.083008] [ 6971.091797] [ 6845.037598] [ 6842.427734] [ 6642.109863] [ 7116.804199] [ 7096.18457 ] [ 7257.665039] [ 7189.424805] [ 6881.958496] [ 6880.323242] [ 7117.20752 ] [ 7429.724609]] #Output[[ 7881.84668 ] [ 7987.371582] [ 8052.543945] [ 8673.21582 ] [ 8805.77832 ] [ 8719.961914] [ 8659.487305] [ 8319.472656] [ 8574.501953] [ 8564.016602] [ 8742.958008] [ 8208.995117] [ 7707.770996] [ 7824.231445] [ 7822.023438] [ 8043.951172] [ 7954.12793 ] [ 7688.077148] [ 8000.32959 ] [ 7927.714355] [ 8145.857422] [ 8230.923828] [ 8693.833008] [ 8838.375 ] [ 8994.488281] [ 9320.352539] [ 9081.762695] [ 9273.521484] [ 9527.160156] [10144.55664 ] [10701.69141 ] [10855.37109 ] [11011.10254 ] [11790.91699 ] [13016.23145 ] [11182.80664 ] [12407.33203 ] [11959.37109 ] [10817.15527 ] [10583.13477 ] [10801.67773 ] [11961.26953 ] [11215.4375 ] [10978.45996 ] [11208.55078 ] [11450.84668 ] [12285.95801 ] [12573.8125 ] [12156.5127 ] [11358.66211 ] [11815.98633 ] [11392.37891 ] [10256.05859 ] [10895.08984 ] [ 9477.641602] [ 9693.802734] [10666.48242 ] [10530.73242 ] [10767.13965 ] [10599.10547 ] [10343.10645 ] [ 9900.767578] [ 9811.925781] [ 9911.841797] [ 9870.303711] [ 9477.677734] [ 9552.860352] [ 9519.145508] [ 9607.423828] [10085.62793 ] [10399.66895 ] [10518.17481 ] [10821.72656 ] [10970.18457 ] [11805.65332 ] [11478.16895 ] [11941.96875 ] [11966.40723 ] [11862.93652 ] [11354.02441 ] [11523.5791 ] [11382.61621 ] [10895.83008 ] [10051.7041 ] [10311.5459 ] [10374.33887 ] [10231.74414 ] [10345.81055 ] [10916.05371 ] [10763.23242 ] [10138.04981 ] [10131.05566 ] [10407.96484 ] [10159.96094 ] [10138.51758 ] [10370.82031 ] [10185.5 ] [ 9754.422852] [ 9510.200195] [ 9598.173828] [ 9630.664063] [ 9757.970703] [10346.76074 ] [10623.54004 ] [10594.49316 ] [10575.5332 ] [10353.30273 ] [10517.25488 ] [10441.27637 ] [10334.97461 ] [10115.97559 ] [10178.37207 ] [10410.12695 ] [10360.54688 ] [10358.04883 ] [10347.71289 ] [10276.79395 ] [10241.27246 ] [10198.24805 ] [10266.41504 ] [10181.6416 ] [10019.7168 ] [10070.39258 ] [ 9729.324219] [ 8620.566406] [ 8486.993164] [ 8118.967773] [ 8251.845703] [ 8245.915039] [ 8104.185547] [ 8293.868164] [ 8343.276367] [ 8393.041992] [ 8259.992188] [ 8205.939453] [ 8151.500488] [ 7988.155762] [ 8245.623047] [ 8228.783203] [ 8595.740234] [ 8586.473633] [ 8321.756836] [ 8336.555664] [ 8321.005859] [ 8374.686523] [ 8205.369141] [ 8047.526855] [ 8103.911133] [ 7973.20752 ] [ 7988.560547] [ 8222.078125] [ 8243.720703] [ 8078.203125] [ 7514.671875] [ 7493.48877 ] [ 8660.700195] [ 9244.972656] [ 9551.714844] [ 9256.148438] [ 9427.6875 ] [ 9205.726563] [ 9199.584961] [ 9261.104492] [ 9324.717773] [ 9235.354492] [ 9412.612305] [ 9342.527344] [ 9360.879883] [ 9267.561523] [ 8804.880859] [ 8813.582031] [ 9055.526367] [ 8757.788086] [ 8815.662109] [ 8808.262695] [ 8708.094727] [ 8491.992188] [ 8550.760742] [ 8577.975586] [ 8309.286133] [ 8206.145508] [ 8027.268066] [ 7642.75 ] [ 7296.577637] [ 7397.796875] [ 7047.916992] [ 7146.133789] [ 7218.371094] [ 7531.663574] [ 7463.105957] [ 7761.243652] [ 7569.629883] [ 7424.29248 ] [ 7321.988281] [ 7320.145508] [ 7252.034668] [ 7448.307617] [ 7546.996582] [ 7556.237793] [ 7564.345215] [ 7400.899414] [ 7278.119629] [ 7217.427246] [ 7243.134277] [ 7269.68457 ] [ 7124.673828] [ 7152.301758] [ 6932.480469] [ 6640.515137] [ 7276.802734] [ 7202.844238] [ 7218.816406] [ 7191.158691] [ 7511.588867] [ 7355.628418] [ 7322.532227] [ 7275.155762] [ 7238.966797] [ 7290.088379] [ 7317.990234] [ 7422.652832] [ 7292.995117] [ 7193.599121] [ 7200.174316] [ 6985.470215] [ 7344.884277] [ 7410.656738] [ 7411.317383] [ 7769.219238] [ 8163.692383] [ 8079.862793] [ 7879.071289] [ 8166.554199] [ 8037.537598] [ 8192.494141] [ 8144.194336] [ 8827.764648] [ 8807.010742] [ 8723.786133] [ 8929.038086] [ 8942.808594] [ 8706.245117] [ 8657.642578] [ 8745.894531] [ 8680.875977] [ 8406.515625] [ 8445.43457 ] [ 8367.847656] [ 8596.830078] [ 8909.819336] [ 9358.589844] [ 9316.629883] [ 9508.993164] [ 9350.529297] [ 9392.875 ] [ 9344.365234] [ 9293.521484] [ 9180.962891] [ 9613.423828] [ 9729.801758] [ 9795.943359] [ 9865.119141] [10116.67383 ] [ 9856.611328] [10208.23633 ] [10326.05469 ] [10214.37988 ] [10312.11621 ] [ 9889.424805] [ 9934.433594] [ 9690.142578] [10141.99609 ] [ 9633.386719] [ 9608.475586] [ 9686.441406] [ 9663.181641] [ 9924.515625] [ 9650.174805] [ 9341.705078] [ 8820.522461] [ 8784.494141] [ 8672.455078] [ 8599.508789] [ 8562.454102] [ 8869.669922] [ 8787.786133] [ 8755.246094] [ 9078.762695] [ 9122.545898] [ 8909.954102] [ 8108.116211] [ 7923.644531] [ 7909.729492] [ 7911.430176] [ 4970.788086] [ 5563.707031] [ 5200.366211] [ 5392.314941] [ 5014.47998 ] [ 5225.629395] [ 5238.438477] [ 6191.192871] [ 6198.77832 ] [ 6185.066406] [ 5830.254883] [ 6416.314941] [ 6734.803711] [ 6681.062988] [ 6716.44043 ] [ 6469.79834 ] [ 6242.193848] [ 5922.042969] [ 6429.841797] [ 6438.644531] [ 6606.776367] [ 6793.624512] [ 6733.387207] [ 6867.527344] [ 6791.129395] [ 7271.78125 ] [ 7176.414551] [ 7334.098633] [ 7302.089355] [ 6865.493164] [ 6859.083008] [ 6971.091797] [ 6845.037598] [ 6842.427734] [ 6642.109863] [ 7116.804199] [ 7096.18457 ] [ 7257.665039] [ 7189.424805] [ 6881.958496] [ 6880.323242] [ 7117.20752 ] [ 7429.724609]] # Create the dependent data set # convert the data frame into a numpy array y = np.array(df['Prediction']) # Get all the values except last 'n' rows y = y[:-predictionDays] print(y) # Create the dependent data set# convert the data frame into a numpy arrayy = np.array(df['Prediction'])# Get all the values except last 'n' rowsy = y[:-predictionDays]print(y) #Output [10701.69141 10855.37109 11011.10254 11790.91699 13016.23145 11182.80664 12407.33203 11959.37109 10817.15527 10583.13477 10801.67773 11961.26953 11215.4375 10978.45996 11208.55078 11450.84668 12285.95801 12573.8125 12156.5127 11358.66211 11815.98633 11392.37891 10256.05859 10895.08984 9477.641602 9693.802734 10666.48242 10530.73242 10767.13965 10599.10547 10343.10645 9900.767578 9811.925781 9911.841797 9870.303711 9477.677734 9552.860352 9519.145508 9607.423828 10085.62793 10399.66895 10518.17481 10821.72656 10970.18457 11805.65332 11478.16895 11941.96875 11966.40723 11862.93652 11354.02441 11523.5791 11382.61621 10895.83008 10051.7041 10311.5459 10374.33887 10231.74414 10345.81055 10916.05371 10763.23242 10138.04981 10131.05566 10407.96484 10159.96094 10138.51758 10370.82031 10185.5 9754.422852 9510.200195 9598.173828 9630.664063 9757.970703 10346.76074 10623.54004 10594.49316 10575.5332 10353.30273 10517.25488 10441.27637 10334.97461 10115.97559 10178.37207 10410.12695 10360.54688 10358.04883 10347.71289 10276.79395 10241.27246 10198.24805 10266.41504 10181.6416 10019.7168 10070.39258 9729.324219 8620.566406 8486.993164 8118.967773 8251.845703 8245.915039 8104.185547 8293.868164 8343.276367 8393.041992 8259.992188 8205.939453 8151.500488 7988.155762 8245.623047 8228.783203 8595.740234 8586.473633 8321.756836 8336.555664 8321.005859 8374.686523 8205.369141 8047.526855 8103.911133 7973.20752 7988.560547 8222.078125 8243.720703 8078.203125 7514.671875 7493.48877 8660.700195 9244.972656 9551.714844 9256.148438 9427.6875 9205.726563 9199.584961 9261.104492 9324.717773 9235.354492 9412.612305 9342.527344 9360.879883 9267.561523 8804.880859 8813.582031 9055.526367 8757.788086 8815.662109 8808.262695 8708.094727 8491.992188 8550.760742 8577.975586 8309.286133 8206.145508 8027.268066 7642.75 7296.577637 7397.796875 7047.916992 7146.133789 7218.371094 7531.663574 7463.105957 7761.243652 7569.629883 7424.29248 7321.988281 7320.145508 7252.034668 7448.307617 7546.996582 7556.237793 7564.345215 7400.899414 7278.119629 7217.427246 7243.134277 7269.68457 7124.673828 7152.301758 6932.480469 6640.515137 7276.802734 7202.844238 7218.816406 7191.158691 7511.588867 7355.628418 7322.532227 7275.155762 7238.966797 7290.088379 7317.990234 7422.652832 7292.995117 7193.599121 7200.174316 6985.470215 7344.884277 7410.656738 7411.317383 7769.219238 8163.692383 8079.862793 7879.071289 8166.554199 8037.537598 8192.494141 8144.194336 8827.764648 8807.010742 8723.786133 8929.038086 8942.808594 8706.245117 8657.642578 8745.894531 8680.875977 8406.515625 8445.43457 8367.847656 8596.830078 8909.819336 9358.589844 9316.629883 9508.993164 9350.529297 9392.875 9344.365234 9293.521484 9180.962891 9613.423828 9729.801758 9795.943359 9865.119141 10116.67383 9856.611328 10208.23633 10326.05469 10214.37988 10312.11621 9889.424805 9934.433594 9690.142578 10141.99609 9633.386719 9608.475586 9686.441406 9663.181641 9924.515625 9650.174805 9341.705078 8820.522461 8784.494141 8672.455078 8599.508789 8562.454102 8869.669922 8787.786133 8755.246094 9078.762695 9122.545898 8909.954102 8108.116211 7923.644531 7909.729492 7911.430176 4970.788086 5563.707031 5200.366211 5392.314941 5014.47998 5225.629395 5238.438477 6191.192871 6198.77832 6185.066406 5830.254883 6416.314941 6734.803711 6681.062988 6716.44043 6469.79834 6242.193848 5922.042969 6429.841797 6438.644531 6606.776367 6793.624512 6733.387207 6867.527344 6791.129395 7271.78125 7176.414551 7334.098633 7302.089355 6865.493164 6859.083008 6971.091797 6845.037598 6842.427734 6642.109863 7116.804199 7096.18457 7257.665039 7189.424805 6881.958496 6880.323242 7117.20752 7429.724609 7550.900879 7569.936035 7679.867188 7795.601074 7807.058594 8801.038086 8658.553711 8864.766602 8988.59668 8897.46875 8912.654297 9003.070313 9268.761719 9951.518555 9842.666016 9593.896484 8756.430664 8601.795898 8804.477539 9269.987305 9733.72168 9328.197266 9377.013672 9670.739258 9726.575195 9729.038086 9522.981445 9081.761719 9182.577148 9180.045898] #Output[10701.69141 10855.37109 11011.10254 11790.91699 13016.23145 11182.80664 12407.33203 11959.37109 10817.15527 10583.13477 10801.67773 11961.26953 11215.4375 10978.45996 11208.55078 11450.84668 12285.95801 12573.8125 12156.5127 11358.66211 11815.98633 11392.37891 10256.05859 10895.08984 9477.641602 9693.802734 10666.48242 10530.73242 10767.13965 10599.10547 10343.10645 9900.767578 9811.925781 9911.841797 9870.303711 9477.677734 9552.860352 9519.145508 9607.423828 10085.62793 10399.66895 10518.17481 10821.72656 10970.18457 11805.65332 11478.16895 11941.96875 11966.40723 11862.93652 11354.02441 11523.5791 11382.61621 10895.83008 10051.7041 10311.5459 10374.33887 10231.74414 10345.81055 10916.05371 10763.23242 10138.04981 10131.05566 10407.96484 10159.96094 10138.51758 10370.82031 10185.5 9754.422852 9510.200195 9598.173828 9630.664063 9757.970703 10346.76074 10623.54004 10594.49316 10575.5332 10353.30273 10517.25488 10441.27637 10334.97461 10115.97559 10178.37207 10410.12695 10360.54688 10358.04883 10347.71289 10276.79395 10241.27246 10198.24805 10266.41504 10181.6416 10019.7168 10070.39258 9729.324219 8620.566406 8486.993164 8118.967773 8251.845703 8245.915039 8104.185547 8293.868164 8343.276367 8393.041992 8259.992188 8205.939453 8151.500488 7988.155762 8245.623047 8228.783203 8595.740234 8586.473633 8321.756836 8336.555664 8321.005859 8374.686523 8205.369141 8047.526855 8103.911133 7973.20752 7988.560547 8222.078125 8243.720703 8078.203125 7514.671875 7493.48877 8660.700195 9244.972656 9551.714844 9256.148438 9427.6875 9205.726563 9199.584961 9261.104492 9324.717773 9235.354492 9412.612305 9342.527344 9360.879883 9267.561523 8804.880859 8813.582031 9055.526367 8757.788086 8815.662109 8808.262695 8708.094727 8491.992188 8550.760742 8577.975586 8309.286133 8206.145508 8027.268066 7642.75 7296.577637 7397.796875 7047.916992 7146.133789 7218.371094 7531.663574 7463.105957 7761.243652 7569.629883 7424.29248 7321.988281 7320.145508 7252.034668 7448.307617 7546.996582 7556.237793 7564.345215 7400.899414 7278.119629 7217.427246 7243.134277 7269.68457 7124.673828 7152.301758 6932.480469 6640.515137 7276.802734 7202.844238 7218.816406 7191.158691 7511.588867 7355.628418 7322.532227 7275.155762 7238.966797 7290.088379 7317.990234 7422.652832 7292.995117 7193.599121 7200.174316 6985.470215 7344.884277 7410.656738 7411.317383 7769.219238 8163.692383 8079.862793 7879.071289 8166.554199 8037.537598 8192.494141 8144.194336 8827.764648 8807.010742 8723.786133 8929.038086 8942.808594 8706.245117 8657.642578 8745.894531 8680.875977 8406.515625 8445.43457 8367.847656 8596.830078 8909.819336 9358.589844 9316.629883 9508.993164 9350.529297 9392.875 9344.365234 9293.521484 9180.962891 9613.423828 9729.801758 9795.943359 9865.119141 10116.67383 9856.611328 10208.23633 10326.05469 10214.37988 10312.11621 9889.424805 9934.433594 9690.142578 10141.99609 9633.386719 9608.475586 9686.441406 9663.181641 9924.515625 9650.174805 9341.705078 8820.522461 8784.494141 8672.455078 8599.508789 8562.454102 8869.669922 8787.786133 8755.246094 9078.762695 9122.545898 8909.954102 8108.116211 7923.644531 7909.729492 7911.430176 4970.788086 5563.707031 5200.366211 5392.314941 5014.47998 5225.629395 5238.438477 6191.192871 6198.77832 6185.066406 5830.254883 6416.314941 6734.803711 6681.062988 6716.44043 6469.79834 6242.193848 5922.042969 6429.841797 6438.644531 6606.776367 6793.624512 6733.387207 6867.527344 6791.129395 7271.78125 7176.414551 7334.098633 7302.089355 6865.493164 6859.083008 6971.091797 6845.037598 6842.427734 6642.109863 7116.804199 7096.18457 7257.665039 7189.424805 6881.958496 6880.323242 7117.20752 7429.724609 7550.900879 7569.936035 7679.867188 7795.601074 7807.058594 8801.038086 8658.553711 8864.766602 8988.59668 8897.46875 8912.654297 9003.070313 9268.761719 9951.518555 9842.666016 9593.896484 8756.430664 8601.795898 8804.477539 9269.987305 9733.72168 9328.197266 9377.013672 9670.739258 9726.575195 9729.038086 9522.981445 9081.761719 9182.577148 9180.045898] # Split the data into 80% training and 20% testing from sklearn.model_selection import train_test_split xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size = 0.2) # set the predictionDays array equal to last 30 rows from the original data set predictionDays_array = np.array(df.drop(['Prediction'],1))[-predictionDays:] print(predictionDays_array) # Split the data into 80% training and 20% testingfrom sklearn.model_selection import train_test_splitxtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size = 0.2)# set the predictionDays array equal to last 30 rows from the original data setpredictionDays_array = np.array(df.drop(['Prediction'],1))[-predictionDays:]print(predictionDays_array) #Output [[7550.900879] [7569.936035] [7679.867188] [7795.601074] [7807.058594] [8801.038086] [8658.553711] [8864.766602] [8988.59668 ] [8897.46875 ] [8912.654297] [9003.070313] [9268.761719] [9951.518555] [9842.666016] [9593.896484] [8756.430664] [8601.795898] [8804.477539] [9269.987305] [9733.72168 ] [9328.197266] [9377.013672] [9670.739258] [9726.575195] [9729.038086] [9522.981445] [9081.761719] [9182.577148] [9180.045898]] #Output[[7550.900879] [7569.936035] [7679.867188] [7795.601074] [7807.058594] [8801.038086] [8658.553711] [8864.766602] [8988.59668 ] [8897.46875 ] [8912.654297] [9003.070313] [9268.761719] [9951.518555] [9842.666016] [9593.896484] [8756.430664] [8601.795898] [8804.477539] [9269.987305] [9733.72168 ] [9328.197266] [9377.013672] [9670.739258] [9726.575195] [9729.038086] [9522.981445] [9081.761719] [9182.577148] [9180.045898]] Now we will create a Machine Learning Model from sklearn.svm import SVR # Create and Train the Support Vector Machine (Regression) using radial basis function svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.00001) svr_rbf.fit(xtrain, ytrain) from sklearn.svm import SVR# Create and Train the Support Vector Machine (Regression) using radial basis functionsvr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.00001)svr_rbf.fit(xtrain, ytrain) #Output SVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=1e-05, kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) #OutputSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=1e-05, kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) Test the model svr_rbf_confidence = svr_rbf.score(xtest,ytest) print('SVR_RBF accuracy :',svr_rbf_confidence) svr_rbf_confidence = svr_rbf.score(xtest,ytest)print('SVR_RBF accuracy :',svr_rbf_confidence) #Output SVR_RBF accuracy : 0.80318203039572782 #OutputSVR_RBF accuracy : 0.80318203039572782 # print the predicted values svm_prediction = svr_rbf.predict(xtest) print(svm_prediction) print() print(ytest) # print the predicted valuessvm_prediction = svr_rbf.predict(xtest)print(svm_prediction)print()print(ytest) #Output [ 8580.88704057 8598.79383546 8939.94375214 8388.92621489 9102.56201779 8832.68229779 8329.30224101 8157.80057348 8602.29644729 8707.90682044 7643.06939601 8408.93105022 8917.45480981 8511.7652266 7834.15919638 8832.62858497 8381.02268219 7098.85213417 8805.2578118 7757.01224446 8791.58493431 8961.26396398 8218.28537299 10512.39752674 8505.95838523 8504.09557077 8416.46565526 8812.06086838 8565.94893198 8378.22399262 8585.8737782 7630.00945667 9602.30696397 8934.97064742 9812.06855777 8473.66659984 8408.82946381 10548.41305096 9362.68382564 8597.33711016 7730.30747013 7792.1701846 8840.84467855 9893.05484852 9725.48044015 8539.54247434 8566.45635477 8916.11467623 8105.74685394 9240.42186178 9606.02191396 8392.00381076 8878.46155081 8586.37388665 8307.42830793 8397.91702065 9446.57181741 8857.3956994 8599.40879784 9324.81811167 9685.85175143 8286.70385539 9344.79392774 8978.54603972 8431.46694919 9370.69251132 8513.0501515 9400.8871896 ] [ 7546.996582 9598.173828 8912.654297 10138.04981 9261.104492 7047.916992 10159.96094 5238.438477 8037.537598 7238.966797 7410.656738 8374.686523 10312.11621 7321.988281 7292.995117 6932.480469 8047.526855 6971.091797 8657.642578 7257.665039 9328.197266 8807.010742 7923.644531 9519.145508 10185.5 9630.664063 6867.527344 8804.880859 8620.566406 7531.663574 7909.729492 8745.894531 9795.943359 9269.987305 9342.527344 8192.494141 10530.73242 11862.93652 9267.561523 8245.915039 7411.317383 8079.862793 5922.042969 7334.098633 7218.816406 9729.324219 10181.6416 6793.624512 8909.954102 11959.37109 7642.75 10241.27246 11182.80664 8586.473633 9078.762695 7556.237793 9729.801758 10256.05859 8599.508789 9324.717773 11450.84668 6198.77832 8027.268066 8804.477539 10276.79395 8206.145508 8321.756836 8151.500488] #Output[ 8580.88704057 8598.79383546 8939.94375214 8388.92621489 9102.56201779 8832.68229779 8329.30224101 8157.80057348 8602.29644729 8707.90682044 7643.06939601 8408.93105022 8917.45480981 8511.7652266 7834.15919638 8832.62858497 8381.02268219 7098.85213417 8805.2578118 7757.01224446 8791.58493431 8961.26396398 8218.28537299 10512.39752674 8505.95838523 8504.09557077 8416.46565526 8812.06086838 8565.94893198 8378.22399262 8585.8737782 7630.00945667 9602.30696397 8934.97064742 9812.06855777 8473.66659984 8408.82946381 10548.41305096 9362.68382564 8597.33711016 7730.30747013 7792.1701846 8840.84467855 9893.05484852 9725.48044015 8539.54247434 8566.45635477 8916.11467623 8105.74685394 9240.42186178 9606.02191396 8392.00381076 8878.46155081 8586.37388665 8307.42830793 8397.91702065 9446.57181741 8857.3956994 8599.40879784 9324.81811167 9685.85175143 8286.70385539 9344.79392774 8978.54603972 8431.46694919 9370.69251132 8513.0501515 9400.8871896 ]​[ 7546.996582 9598.173828 8912.654297 10138.04981 9261.104492 7047.916992 10159.96094 5238.438477 8037.537598 7238.966797 7410.656738 8374.686523 10312.11621 7321.988281 7292.995117 6932.480469 8047.526855 6971.091797 8657.642578 7257.665039 9328.197266 8807.010742 7923.644531 9519.145508 10185.5 9630.664063 6867.527344 8804.880859 8620.566406 7531.663574 7909.729492 8745.894531 9795.943359 9269.987305 9342.527344 8192.494141 10530.73242 11862.93652 9267.561523 8245.915039 7411.317383 8079.862793 5922.042969 7334.098633 7218.816406 9729.324219 10181.6416 6793.624512 8909.954102 11959.37109 7642.75 10241.27246 11182.80664 8586.473633 9078.762695 7556.237793 9729.801758 10256.05859 8599.508789 9324.717773 11450.84668 6198.77832 8027.268066 8804.477539 10276.79395 8206.145508 8321.756836 8151.500488] # Print the model predictions for the next 30 days svm_prediction = svr_rbf.predict(predictionDays_array) print(svm_prediction) print() #Print the actual price for bitcoin for last 30 days print(df.tail(predictionDays)) # Print the model predictions for the next 30 dayssvm_prediction = svr_rbf.predict(predictionDays_array)print(svm_prediction)print()#Print the actual price for bitcoin for last 30 daysprint(df.tail(predictionDays)) #Output [7746.08637672 7835.76387711 8657.25433329 9554.67400231 9617.98954538 8917.61532094 8831.29326224 8885.55655284 8640.51491415 8841.78875835 8815.42825999 8602.53094625 8400.08270252 8426.55153101 8172.93870238 8395.85348921 8903.73403919 8811.70139747 8917.58878224 8402.31118948 8102.70537693 8518.83392876 8606.8071745 8195.93279966 8108.54622414 8106.38537126 8573.49097641 8410.28935674 8307.6380027 8307.33725309] Price Prediction 337 7550.900879 NaN 338 7569.936035 NaN 339 7679.867188 NaN 340 7795.601074 NaN 341 7807.058594 NaN 342 8801.038086 NaN 343 8658.553711 NaN 344 8864.766602 NaN 345 8988.596680 NaN 346 8897.468750 NaN 347 8912.654297 NaN 348 9003.070313 NaN 349 9268.761719 NaN 350 9951.518555 NaN 351 9842.666016 NaN 352 9593.896484 NaN 353 8756.430664 NaN 354 8601.795898 NaN 355 8804.477539 NaN 356 9269.987305 NaN 357 9733.721680 NaN 358 9328.197266 NaN 359 9377.013672 NaN 360 9670.739258 NaN 361 9726.575195 NaN 362 9729.038086 NaN 363 9522.981445 NaN 364 9081.761719 NaN 365 9182.577148 NaN 366 9180.045898 NaN #Output[7746.08637672 7835.76387711 8657.25433329 9554.67400231 9617.98954538 8917.61532094 8831.29326224 8885.55655284 8640.51491415 8841.78875835 8815.42825999 8602.53094625 8400.08270252 8426.55153101 8172.93870238 8395.85348921 8903.73403919 8811.70139747 8917.58878224 8402.31118948 8102.70537693 8518.83392876 8606.8071745 8195.93279966 8108.54622414 8106.38537126 8573.49097641 8410.28935674 8307.6380027 8307.33725309]​ Price Prediction337 7550.900879 NaN338 7569.936035 NaN339 7679.867188 NaN340 7795.601074 NaN341 7807.058594 NaN342 8801.038086 NaN343 8658.553711 NaN344 8864.766602 NaN345 8988.596680 NaN346 8897.468750 NaN347 8912.654297 NaN348 9003.070313 NaN349 9268.761719 NaN350 9951.518555 NaN351 9842.666016 NaN352 9593.896484 NaN353 8756.430664 NaN354 8601.795898 NaN355 8804.477539 NaN356 9269.987305 NaN357 9733.721680 NaN358 9328.197266 NaN359 9377.013672 NaN360 9670.739258 NaN361 9726.575195 NaN362 9729.038086 NaN363 9522.981445 NaN364 9081.761719 NaN365 9182.577148 NaN366 9180.045898 NaN​ Follow us on Instagram for all your Queries Instagram";Bitcoin Price Prediction with Machine Learning
2020-05-23 13:03:44;Recommendation systems are among the most popular applications of data science. They are used to predict the Rating or Preference that a user would give to an item.Almost every major company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on auto play, and Facebook uses it to recommend pages to like and people to follow.In this Data Science project, you will see how to build a Book Recommendation System model using Machine Learning Techniques.You can download the data sets we need for this task from here:Let’s start with this project#Output(1149780, 3)[‘userID’, ‘ISBN’, ‘bookRating’]#Output(271360, 8)[‘ISBN’, ‘bookTitle’, ‘bookAuthor’, ‘yearOfPublication’, ‘publisher’, ‘imageUrlS’, ‘imageUrlM’, ‘imageUrlL’]#Output(278858, 3)[‘userID’, ‘Location’, ‘Age’]To ensure statistical significance, users with less than 200 ratings, and books with less than 100 ratings are excluded.kNN is a machine learning algorithm to find clusters of similar users based on common book ratings, and make predictions using the average rating of top-k nearest neighbors. For example, we first present ratings in a matrix with the matrix having one row for each item (book) and one column for each user.Now we will group by book titles and create a new column for total rating count.Now we will combine the rating data with the total rating count data, this gives us exactly what we need to find out which books are popular and filter out lesser-known books.We convert our table to a 2D matrix, and fill the missing values with zeros (since we will calculate distances between rating vectors). We then transform the values(ratings) of the matrix dataframe into a scipy sparse matrix for more efficient calculations.;https://thecleverprogrammer.com/2020/05/23/data-science-project-book-recommendation-system-with-machine-learning/;['sklearn'];1.0;['NN'];['ML', 'KNN', 'Recommender', 'NN'];['predict', 'fit', 'model', 'machine learning', 'recommend', 'k-nearest neighbor', 'filter', 'label'];"Recommendation systems are among the most popular applications of data science. They are used to predict the Rating or Preference that a user would give to an item. Almost every major company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on auto play, and Facebook uses it to recommend pages to like and people to follow. Let’s Build our own recommendation system In this Data Science project, you will see how to build a Book Recommendation System model using Machine Learning Techniques. You can download the data sets we need for this task from here: BX-Book-RatingsDownload BX-BooksDownload BX-UsersDownload Let’s start with this project import pandas as pd import numpy as np import matplotlib.pyplot as plt books = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding=""latin-1"") books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL'] users = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=""latin-1"") users.columns = ['userID', 'Location', 'Age'] ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=""latin-1"") ratings.columns = ['userID', 'ISBN', 'bookRating'] print(ratings.shape) print(list(ratings.columns))​x import pandas as pdimport numpy as npimport matplotlib.pyplot as pltbooks = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL']users = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")users.columns = ['userID', 'Location', 'Age']ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")ratings.columns = ['userID', 'ISBN', 'bookRating']​print(ratings.shape)print(list(ratings.columns)) #Output(1149780, 3)[‘userID’, ‘ISBN’, ‘bookRating’] plt.rc(""font"", size=15) ratings.bookRating.value_counts(sort=False).plot(kind='bar') plt.title('Rating Distribution\n') plt.xlabel('Rating') plt.ylabel('Count') plt.show() plt.rc(""font"", size=15)ratings.bookRating.value_counts(sort=False).plot(kind='bar')plt.title('Rating Distribution\n')plt.xlabel('Rating')plt.ylabel('Count')plt.show() print(books.shape) print(list(books.columns)) print(books.shape)print(list(books.columns)) #Output(271360, 8)[‘ISBN’, ‘bookTitle’, ‘bookAuthor’, ‘yearOfPublication’, ‘publisher’, ‘imageUrlS’, ‘imageUrlM’, ‘imageUrlL’] print(users.shape) print(list(users.columns)) print(users.shape)print(list(users.columns)) #Output(278858, 3)[‘userID’, ‘Location’, ‘Age’] users.Age.hist(bins=[0, 10, 20, 30, 40, 50, 100]) plt.title('Age Distribution\n') plt.xlabel('Age') plt.ylabel('Count') plt.show() users.Age.hist(bins=[0, 10, 20, 30, 40, 50, 100])plt.title('Age Distribution\n')plt.xlabel('Age')plt.ylabel('Count')plt.show() To ensure statistical significance, users with less than 200 ratings, and books with less than 100 ratings are excluded. counts1 = ratings['userID'].value_counts() ratings = ratings[ratings['userID'].isin(counts1[counts1 >= 200].index)] counts = ratings['bookRating'].value_counts() ratings = ratings[ratings['bookRating'].isin(counts[counts >= 100].index)] counts1 = ratings['userID'].value_counts()ratings = ratings[ratings['userID'].isin(counts1[counts1 >= 200].index)]counts = ratings['bookRating'].value_counts()ratings = ratings[ratings['bookRating'].isin(counts[counts >= 100].index)] Collaborative Filtering Using k-Nearest Neighbors (kNN) kNN is a machine learning algorithm to find clusters of similar users based on common book ratings, and make predictions using the average rating of top-k nearest neighbors. For example, we first present ratings in a matrix with the matrix having one row for each item (book) and one column for each user. combine_book_rating = pd.merge(ratings, books, on='ISBN') columns = ['yearOfPublication', 'publisher', 'bookAuthor', 'imageUrlS', 'imageUrlM', 'imageUrlL'] combine_book_rating = combine_book_rating.drop(columns, axis=1) print(combine_book_rating.head()) combine_book_rating = pd.merge(ratings, books, on='ISBN')columns = ['yearOfPublication', 'publisher', 'bookAuthor', 'imageUrlS', 'imageUrlM', 'imageUrlL']combine_book_rating = combine_book_rating.drop(columns, axis=1)print(combine_book_rating.head()) #Output userID ... bookTitle 0 277427 ... Politically Correct Bedtime Stories: Modern Ta... 1 3363 ... Politically Correct Bedtime Stories: Modern Ta... 2 11676 ... Politically Correct Bedtime Stories: Modern Ta... 3 12538 ... Politically Correct Bedtime Stories: Modern Ta... 4 13552 ... Politically Correct Bedtime Stories: Modern Ta... [5 rows x 4 columns] #OutputuserID ... bookTitle0 277427 ... Politically Correct Bedtime Stories: Modern Ta...1 3363 ... Politically Correct Bedtime Stories: Modern Ta...2 11676 ... Politically Correct Bedtime Stories: Modern Ta...3 12538 ... Politically Correct Bedtime Stories: Modern Ta...4 13552 ... Politically Correct Bedtime Stories: Modern Ta...​[5 rows x 4 columns] Now we will group by book titles and create a new column for total rating count. combine_book_rating = combine_book_rating.dropna(axis = 0, subset = ['bookTitle']) book_ratingCount = (combine_book_rating. groupby(by = ['bookTitle'])['bookRating']. count(). reset_index(). rename(columns = {'bookRating': 'totalRatingCount'}) [['bookTitle', 'totalRatingCount']] ) print(book_ratingCount.head()) combine_book_rating = combine_book_rating.dropna(axis = 0, subset = ['bookTitle'])​book_ratingCount = (combine_book_rating. groupby(by = ['bookTitle'])['bookRating']. count(). reset_index(). rename(columns = {'bookRating': 'totalRatingCount'}) [['bookTitle', 'totalRatingCount']] )print(book_ratingCount.head()) #Output bookTitle totalRatingCount 0 A Light in the Storm: The Civil War Diary of ... 2 1 Always Have Popsicles 1 2 Apple Magic (The Collector's series) 1 3 Beyond IBM: Leadership Marketing and Finance ... 1 4 Clifford Visita El Hospital (Clifford El Gran... 1 #OutputbookTitle totalRatingCount0 A Light in the Storm: The Civil War Diary of ... 21 Always Have Popsicles 12 Apple Magic (The Collector's series) 13 Beyond IBM: Leadership Marketing and Finance ... 14 Clifford Visita El Hospital (Clifford El Gran... 1 Now we will combine the rating data with the total rating count data, this gives us exactly what we need to find out which books are popular and filter out lesser-known books. rating_with_totalRatingCount = combine_book_rating.merge(book_ratingCount, left_on = 'bookTitle', right_on = 'bookTitle', how = 'left') print(rating_with_totalRatingCount.head()) pd.set_option('display.float_format', lambda x: '%.3f' % x) print(book_ratingCount['totalRatingCount'].describe()) rating_with_totalRatingCount = combine_book_rating.merge(book_ratingCount, left_on = 'bookTitle', right_on = 'bookTitle', how = 'left')print(rating_with_totalRatingCount.head())​pd.set_option('display.float_format', lambda x: '%.3f' % x)print(book_ratingCount['totalRatingCount'].describe()) #Output userID ... totalRatingCount 0 277427 ... 82 1 3363 ... 82 2 11676 ... 82 3 12538 ... 82 4 13552 ... 82 [5 rows x 5 columns] #OutputuserID ... totalRatingCount0 277427 ... 821 3363 ... 822 11676 ... 823 12538 ... 824 13552 ... 82​[5 rows x 5 columns] pd.set_option('display.float_format', lambda x: '%.3f' % x) print(book_ratingCount['totalRatingCount'].describe()) pd.set_option('display.float_format', lambda x: '%.3f' % x)print(book_ratingCount['totalRatingCount'].describe()) #Output count 160576.000 mean 3.044 std 7.428 min 1.000 25% 1.000 50% 1.000 75% 2.000 max 365.000 Name: totalRatingCount, dtype: float64 #Outputcount 160576.000mean 3.044std 7.428min 1.00025% 1.00050% 1.00075% 2.000max 365.000Name: totalRatingCount, dtype: float64 print(book_ratingCount['totalRatingCount'].quantile(np.arange(.9, 1, .01))) print(book_ratingCount['totalRatingCount'].quantile(np.arange(.9, 1, .01))) #Output 0.900 5.000 0.910 6.000 0.920 7.000 0.930 7.000 0.940 8.000 0.950 10.000 0.960 11.000 0.970 14.000 0.980 19.000 0.990 31.000 Name: totalRatingCount, dtype: float64 #Output0.900 5.0000.910 6.0000.920 7.0000.930 7.0000.940 8.0000.950 10.0000.960 11.0000.970 14.0000.980 19.0000.990 31.000Name: totalRatingCount, dtype: float64 popularity_threshold = 50 rating_popular_book = rating_with_totalRatingCount.query('totalRatingCount >= @popularity_threshold') print(rating_popular_book.head()) popularity_threshold = 50rating_popular_book = rating_with_totalRatingCount.query('totalRatingCount >= @popularity_threshold')print(rating_popular_book.head()) #Output userID ... totalRatingCount 0 277427 ... 82 1 3363 ... 82 2 11676 ... 82 3 12538 ... 82 4 13552 ... 82 [5 rows x 5 columns] #OutputuserID ... totalRatingCount0 277427 ... 821 3363 ... 822 11676 ... 823 12538 ... 824 13552 ... 82​[5 rows x 5 columns] Filter to users in US and Canada only combined = rating_popular_book.merge(users, left_on = 'userID', right_on = 'userID', how = 'left') us_canada_user_rating = combined[combined['Location'].str.contains(""usa|canada"")] us_canada_user_rating=us_canada_user_rating.drop('Age', axis=1) print(us_canada_user_rating.head()) combined = rating_popular_book.merge(users, left_on = 'userID', right_on = 'userID', how = 'left')​us_canada_user_rating = combined[combined['Location'].str.contains(""usa|canada"")]us_canada_user_rating=us_canada_user_rating.drop('Age', axis=1)print(us_canada_user_rating.head()) #Output userID ISBN ... totalRatingCount Location 0 277427 002542730X ... 82 gilbert, arizona, usa 1 3363 002542730X ... 82 knoxville, tennessee, usa 3 12538 002542730X ... 82 byron, minnesota, usa 4 13552 002542730X ... 82 cordova, tennessee, usa 5 16795 002542730X ... 82 mechanicsville, maryland, usa [5 rows x 6 columns] #OutputuserID ISBN ... totalRatingCount Location0 277427 002542730X ... 82 gilbert, arizona, usa1 3363 002542730X ... 82 knoxville, tennessee, usa3 12538 002542730X ... 82 byron, minnesota, usa4 13552 002542730X ... 82 cordova, tennessee, usa5 16795 002542730X ... 82 mechanicsville, maryland, usa​[5 rows x 6 columns] Implementing kNN We convert our table to a 2D matrix, and fill the missing values with zeros (since we will calculate distances between rating vectors). We then transform the values(ratings) of the matrix dataframe into a scipy sparse matrix for more efficient calculations. from scipy.sparse import csr_matrix us_canada_user_rating = us_canada_user_rating.drop_duplicates(['userID', 'bookTitle']) us_canada_user_rating_pivot = us_canada_user_rating.pivot(index = 'bookTitle', columns = 'userID', values = 'bookRating').fillna(0) us_canada_user_rating_matrix = csr_matrix(us_canada_user_rating_pivot.values) from sklearn.neighbors import NearestNeighbors model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute') model_knn.fit(us_canada_user_rating_matrix) print(model_knn) from scipy.sparse import csr_matrixus_canada_user_rating = us_canada_user_rating.drop_duplicates(['userID', 'bookTitle'])us_canada_user_rating_pivot = us_canada_user_rating.pivot(index = 'bookTitle', columns = 'userID', values = 'bookRating').fillna(0)us_canada_user_rating_matrix = csr_matrix(us_canada_user_rating_pivot.values)​from sklearn.neighbors import NearestNeighbors​model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')model_knn.fit(us_canada_user_rating_matrix)print(model_knn) #Output NearestNeighbors(algorithm='brute', leaf_size=30, metric='cosine', metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0) #OutputNearestNeighbors(algorithm='brute', leaf_size=30, metric='cosine', metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0) query_index = np.random.choice(us_canada_user_rating_pivot.shape[0]) print(query_index) print(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1,-1)) distances, indices = model_knn.kneighbors(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 6) us_canada_user_rating_pivot.index[query_index] query_index = np.random.choice(us_canada_user_rating_pivot.shape[0])print(query_index)print(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1,-1))distances, indices = model_knn.kneighbors(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 6)us_canada_user_rating_pivot.index[query_index] #Output [[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] #Output[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 10. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] for i in range(0, len(distances.flatten())): if i == 0: print('Recommendations for {0}:\n'.format(us_canada_user_rating_pivot.index[query_index])) else: print('{0}: {1}, with distance of {2}:'.format(i, us_canada_user_rating_pivot.index[indices.flatten()[i]], distances.flatten()[i])) for i in range(0, len(distances.flatten())): if i == 0: print('Recommendations for {0}:\n'.format(us_canada_user_rating_pivot.index[query_index])) else: print('{0}: {1}, with distance of {2}:'.format(i, us_canada_user_rating_pivot.index[indices.flatten()[i]], distances.flatten()[i])) #Output Recommendations for Flesh and Blood: 1: The Murder Book, with distance of 0.596243943613731: 2: Choke, with distance of 0.6321092998573327: 3: Easy Prey, with distance of 0.704010041374638: 4: 2nd Chance, with distance of 0.7292664430521165: 5: The Empty Chair, with distance of 0.7432121818110763: #OutputRecommendations for Flesh and Blood:​1: The Murder Book, with distance of 0.596243943613731:2: Choke, with distance of 0.6321092998573327:3: Easy Prey, with distance of 0.704010041374638:4: 2nd Chance, with distance of 0.7292664430521165:5: The Empty Chair, with distance of 0.7432121818110763: Follow us on Instagram for all your Queries Instagram";Book Recommendation System with Machine Learning
2020-05-23 01:16:59;In this Data Science Project, I will apply a classification model with Machine Learning, that can predict a gene’s function based on the DNA sequencing of the coding sequence alone.You can download the data sets we need for this project from here:We have some data for human DNA sequence coding regions and a class label. We also have data for Chimpanzee and a more divergent species, the dog.A challenge that remains is that none of these above methods results in vectors of uniform length, and that is a requirement for feeding data to a classification or regression algorithm. So with the above methods you have to resort to things like truncating sequences or padding with “n” or “0” to get vectors of uniform length.DNA and protein sequences can be viewed metaphorically as the language of life. The language encodes instructions as well as function for the molecules that are found in all life forms. Here I am using hexamer “words” but that is arbitrary and word length can be tuned to suit the particular situation. The word length and amount of overlap need to be determined empirically for any given application.In genomics, we refer to these types of manipulations as “k-mer counting”, or counting the occurrences of each possible k-mer sequence. There are specialized tools for this, but the Python natural language processing tools make it super easy.Let’s define a function to collect all possible overlapping k-mers of a specified length from any sequence string. Now we can convert our training data sequences into short overlapping k-mers of legth 6. Lets do that for each species of data we have using our getKmers function.Now, our coding sequence data is changed to lowercase, split up into all possible k-mer words of length 6 and ready for the next step. Let’s take a look.Since we are going to use scikit-learn natural language processing tools to do the k-mer counting, we need to now convert the lists of k-mers for each gene into string sentences of words that the count vectorizer can use. We can also make a y variable to hold the class labels. Let’s do that now.#Output–array([4, 4, 3, …, 6, 6, 6])#Output-(4380, 232414) (1682, 232414) (820, 232414)If we have a look at class balance we can see we have relatively balanced data set.#Output(3504, 232414) (876, 232414)A multinomial naive Bayes classifier will be created. I previously did some parameter tuning and found the ngram size of 4 (reflected in the Countvectorizer() instance) and a model alpha of 0.1 did the best.#Output-MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)Okay, so let’s look at some model performance metrics like the confusion matrix, accuracy, precision, recall and f1 score. We are getting really good results on our unseen data, so it looks like our model did not overfit to the training data. In a real project I would go back and sample many more train test splits since we have a relatively small data set.;https://thecleverprogrammer.com/2020/05/23/data-science-project-dna-sequencing-with-machine-learning/;['sklearn'];1.0;['NLP'];['Regression', 'ML', 'NLP', 'Naive Bayes', 'Classification'];['regression', 'predict', 'fit', 'model', 'machine learning', 'classif', 'natural language processing', 'training data', 'naive bayes', 'train', 'label'];"In this Data Science Project, I will apply a classification model with Machine Learning, that can predict a gene’s function based on the DNA sequencing of the coding sequence alone. import numpy as np import pandas as pd import matplotlib.pyplot as plt​x import numpy as npimport pandas as pdimport matplotlib.pyplot as plt You can download the data sets we need for this project from here: chimp_dataDownload dog_dataDownload human_dataDownload human_data = pd.read_table('human_data.txt') human_data.head() human_data = pd.read_table('human_data.txt')human_data.head() We have some data for human DNA sequence coding regions and a class label. We also have data for Chimpanzee and a more divergent species, the dog. chimp_data = pd.read_table('chimp_data.txt') dog_data = pd.read_table('dog_data.txt') chimp_data.head() dog_data.head() chimp_data = pd.read_table('chimp_data.txt')dog_data = pd.read_table('dog_data.txt')chimp_data.head()dog_data.head() Treating DNA sequence as a “language”, otherwise known as k-mer counting A challenge that remains is that none of these above methods results in vectors of uniform length, and that is a requirement for feeding data to a classification or regression algorithm. So with the above methods you have to resort to things like truncating sequences or padding with “n” or “0” to get vectors of uniform length. DNA and protein sequences can be viewed metaphorically as the language of life. The language encodes instructions as well as function for the molecules that are found in all life forms. Here I am using hexamer “words” but that is arbitrary and word length can be tuned to suit the particular situation. The word length and amount of overlap need to be determined empirically for any given application. In genomics, we refer to these types of manipulations as “k-mer counting”, or counting the occurrences of each possible k-mer sequence. There are specialized tools for this, but the Python natural language processing tools make it super easy. Let’s define a function to collect all possible overlapping k-mers of a specified length from any sequence string. # function to convert sequence strings into k-mer words, default size = 6 (hexamer words) def getKmers(sequence, size=6): return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)] # function to convert sequence strings into k-mer words, default size = 6 (hexamer words)def getKmers(sequence, size=6): return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)] Now we can convert our training data sequences into short overlapping k-mers of legth 6. Lets do that for each species of data we have using our getKmers function. human_data['words'] = human_data.apply(lambda x: getKmers(x['sequence']), axis=1) human_data = human_data.drop('sequence', axis=1) chimp_data['words'] = chimp_data.apply(lambda x: getKmers(x['sequence']), axis=1) chimp_data = chimp_data.drop('sequence', axis=1) dog_data['words'] = dog_data.apply(lambda x: getKmers(x['sequence']), axis=1) dog_data = dog_data.drop('sequence', axis=1) human_data['words'] = human_data.apply(lambda x: getKmers(x['sequence']), axis=1)human_data = human_data.drop('sequence', axis=1)chimp_data['words'] = chimp_data.apply(lambda x: getKmers(x['sequence']), axis=1)chimp_data = chimp_data.drop('sequence', axis=1)dog_data['words'] = dog_data.apply(lambda x: getKmers(x['sequence']), axis=1)dog_data = dog_data.drop('sequence', axis=1) Now, our coding sequence data is changed to lowercase, split up into all possible k-mer words of length 6 and ready for the next step. Let’s take a look. human_data.head() human_data.head() Since we are going to use scikit-learn natural language processing tools to do the k-mer counting, we need to now convert the lists of k-mers for each gene into string sentences of words that the count vectorizer can use. We can also make a y variable to hold the class labels. Let’s do that now. human_texts = list(human_data['words']) for item in range(len(human_texts)): human_texts[item] = ' '.join(human_texts[item]) y_data = human_data.iloc[:, 0].values print(human_texts[2]) human_texts = list(human_data['words'])for item in range(len(human_texts)): human_texts[item] = ' '.join(human_texts[item])y_data = human_data.iloc[:, 0].valuesprint(human_texts[2]) #Output atgtgt tgtgtg gtgtgg tgtggc gtggca tggcat ggcatt gcattt catttg atttgg tttggg ttgggc tgggcg gggcgc ggcgct gcgctg cgctgt gctgtt ctgttt tgtttg gtttgg tttggc ttggca tggcag ggcagt gcagtg cagtga agtgat gtgatg tgatga gatgat atgatt tgattg gattgc attgcc ttgcct tgcctt gccttt cctttc ctttct tttctg ttctgt tctgtt ctgttc tgttca gttcag ttcagt tcagtg cagtgt agtgtc gtgtct tgtctg gtctga tctgag ctgagt tgagtg gagtgc agtgct gtgcta tgctat gctatg ctatga tatgaa atgaag tgaaga gaagat aagatt agattg gattgc attgca ttgcac tgcaca gcacac cacaca acacag cacaga acagag cagagg agaggt gaggtc aggtcc ggtcca gtccag tccaga ccagat cagatg agatgc gatgca atgcat tgcatt gcattc cattcc attccg ttccgt tccgtt ccgttt cgtttt gttttg ttttga tttgag ttgaga tgagaa gagaat agaatg gaatgt aatgtc atgtca tgtcaa gtcaat tcaatg caatgg aatgga atggat tggata ggatac gataca atacac tacacc acacca caccaa accaac ccaact caactg aactgc actgct ctgctg tgctgc gctgct ctgctt tgcttt gctttg ctttgg tttgga ttggat tggatt ggattt gatttc atttca tttcac ttcacc tcaccg caccgg accggt ccggtt cggttg ggttgg gttggc ttggcg tggcgg ggcggt gcggta cggtag ggtagt gtagtt tagttg agttga gttgac ttgacc tgaccc gacccg acccgc cccgct ccgctg cgctgt gctgtt ctgttt tgtttg gtttgg tttgga ttggaa tggaat ggaatg gaatgc aatgca atgcag tgcagc gcagcc cagcca agccaa gccaat ccaatt caattc aattcg attcga ttcgag tcgagt cgagtg gagtga agtgaa gtgaag tgaaga gaagaa aagaaa agaaat gaaata aaatat aatatc atatcc tatccg atccgt tccgta ccgtat cgtatt gtattt tatttg atttgt tttgtg ttgtgg tgtggc gtggct tggctc ggctct gctctg ctctgt tctgtt ctgtta tgttac gttaca ttacaa tacaat acaatg caatgg aatggt atggtg tggtga ggtgaa gtgaaa tgaaat gaaatc aaatct aatcta atctac tctaca ctacaa tacaac acaacc caacca aaccat accata ccataa cataag ataaga taagaa aagaag agaaga gaagat aagatg agatgc gatgca atgcaa tgcaac gcaaca caacag aacagc acagca cagcat agcatt gcattt catttt attttg ttttga tttgaa ttgaat tgaatt gaattt aatttg atttga tttgaa ttgaat tgaata gaatac aatacc atacca taccag accaga ccagac cagacc agacca gaccaa accaaa ccaaag caaagt aaagtg aagtgg agtgga gtggat tggatg ggatgg gatggt atggtg tggtga ggtgag gtgaga tgagat gagata agataa gataat ataatc taatcc aatcct atcctt tccttc ccttca cttcat ttcatc tcatct catctt atcttt tcttta ctttat tttatg ttatga tatgac atgaca tgacaa gacaaa acaaag caaagg aaagga aaggag aggagg ggagga gaggaa aggaat ggaatt gaattg aattga attgag ttgagc tgagca gagcaa agcaaa gcaaac caaaca aaacaa aacaat acaatt caattt aatttg atttgt tttgta ttgtat tgtatg gtatgt tatgtt atgttg tgttgg gttgga ttggat tggatg ggatgg gatggt atggtg tggtgt ggtgtg gtgtgt tgtgtt gtgttt tgtttg gtttgc tttgca ttgcat tgcatt gcattt catttg atttgt tttgtt ttgttt tgtttt gtttta ttttac tttact ttactg tactgg actgga ctggat tggata ggatac gatact atactg tactgc actgcc ctgcca tgccaa gccaat ccaata caataa aataag ataaga taagaa aagaaa agaaag gaaagt aaagtg aagtgt agtgtt gtgttc tgttcc gttcct ttcctg tcctgg cctggg ctgggt tgggta gggtag ggtaga gtagag tagaga agagat gagata agatac gataca atacat tacata acatat catatg atatgg tatgga atggag tggagt ggagtc gagtca agtcag gtcaga tcagac cagacc agacct gacctt accttt cctttg ctttgt tttgtt ttgttt tgttta gtttaa tttaaa ttaaag taaagc aaagca aagcaa agcaat gcaatg caatga aatgac atgaca tgacag gacaga acagaa cagaag agaaga gaagat aagatg agatgg gatgga atggat tggatt ggattt gatttt attttt tttttg ttttgg tttggc ttggct tggctg ggctgt gctgta ctgtat tgtatg gtatgt tatgtt atgttc tgttca gttcag ttcaga tcagaa cagaag agaagc gaagct aagcta agctaa gctaaa ctaaag taaagg aaaggt aaggtc aggtct ggtctt gtcttg tcttgt cttgtt ttgtta tgttac gttaca ttacat tacatt acattg cattga attgaa ttgaag tgaagc gaagca aagcac agcact gcactc cactcc actccg ctccgc tccgcg ccgcga cgcgac gcgact cgactc gactcc actccc ctccct tccctt cccttt cctttt cttttt ttttta ttttaa tttaaa ttaaaa taaaag aaaagt aaagtg aagtgg agtgga gtggag tggagc ggagcc gagcct agcctt gccttt cctttt cttttc ttttct tttctt ttcttc tcttcc cttcct ttcctg tcctgg cctgga ctggac tggaca ggacac gacact acacta cactat actatg ctatga tatgaa atgaag tgaagt gaagtt aagttt agtttt gttttg ttttgg tttgga ttggat tggatt ggattt gattta atttaa tttaaa ttaaag taaagc aaagcc aagcca agccaa gccaaa ccaaat caaatg aaatgg aatggc atggca tggcaa ggcaaa gcaaag caaagt aaagtt aagttg agttgc gttgca ttgcat tgcatc gcatcc catccg atccgt tccgtg ccgtgg cgtgga gtggaa tggaaa ggaaat gaaatg aaatgg aatggt atggtt tggtta ggttaa gttaaa ttaaat taaata aaatat aatatc atatca tatcat atcatc tcatca catcac atcact tcactg cactgt actgtc ctgtcg tgtcgg gtcggg tcggga cgggat gggatg ggatgt gatgta atgtac tgtacc gtaccc tacccc acccct cccctg ccctgc cctgca ctgcac tgcacg gcacgc cacgcc acgccc cgccct gccctc ccctct cctcta ctctat tctatg ctatga tatgac atgaca tgacaa gacaat acaatg caatgt aatgtg atgtgg tgtgga gtggag tggaga ggagaa gagaaa agaaac gaaact aaactc aactct actctt ctcttt tctttc ctttcc tttcca ttccag tccagg ccaggt caggtt aggttt ggtttt gttttg ttttga tttgag ttgaga tgagat gagata agatag gataga atagaa tagaaa agaaac gaaact aaactg aactgt actgtg ctgtga tgtgaa gtgaag tgaaga gaagaa aagaac agaaca gaacaa aacaac acaacc caacct aacctc acctca cctcag ctcagg tcagga caggat aggatc ggatcc gatcct atcctt tccttt cctttt cttttt ttttta ttttaa tttaat ttaata taataa aataat ataatg taatgc aatgct atgctg tgctgt gctgta ctgtaa tgtaaa gtaaag taaaga aaagaa aagaaa agaaac gaaacg aaacgt aacgtt acgttt cgtttg gtttga tttgat ttgatg tgatga gatgac atgaca tgacag gacaga acagac cagaca agacag gacaga acagaa cagaag agaagg gaagga aaggat aggatt ggattg gattgg attggc ttggct tggctg ggctgc gctgcc ctgcct tgcctt gccttt cctttt ctttta ttttat tttatc ttatca tatcag atcagg tcaggg cagggg aggggg gggggc ggggct gggctt ggcttg gcttgg cttgga ttggac tggact ggactc gactcc actcca ctccag tccagc ccagct cagctt agcttg gcttgg cttggt ttggtt tggttg ggttgc gttgct ttgctg tgctgc gctgcc ctgcca tgccac gccact ccactc cactct actctg ctctgt tctgtt ctgttg tgttga gttgaa ttgaag tgaagc gaagca aagcag agcagc gcagct cagctg agctga gctgaa ctgaaa tgaaag gaaaga aaagaa aagaag agaagc gaagcc aagccc agccca gcccaa cccaag ccaagt caagta aagtac agtaca gtacag tacagt acagta cagtat agtatc gtatcc tatcct atcctc tcctct cctctc ctctcc tctcca ctccag tccaga ccagac cagaca agacat gacatt acattt catttg atttgc tttgca ttgcaa tgcaat gcaatt caattg aattgg attggc ttggca tggcat ggcatg gcatgg catgga atggaa tggaag ggaaga gaagac aagaca agacag gacagc acagcc cagccc agcccc gccccg ccccga cccgat ccgatt cgattt gattta atttac tttact ttactg tactgg actggc ctggct tggctg ggctgc gctgct ctgcta tgctag gctaga ctagaa tagaaa agaaag gaaagg aaaggt aaggtg aggtgg ggtggc gtggca tggcag ggcaga gcagat cagatc agatca gatcat atcata tcatat catatt atattg tattgg attgga ttggaa tggaag ggaagt gaagtg aagtga agtgaa gtgaac tgaaca gaacat aacatt acatta cattat attatg ttatga tatgaa atgaag tgaagt gaagtc aagtcc agtcct gtcctt tccttt cctttt cttttt ttttta ttttaa tttaac ttaact taactc aactct actctg ctctga tctgag ctgagg tgagga gaggaa aggaag ggaagg gaaggc aaggca aggcat ggcatt gcattc cattca attcag ttcagg tcaggc caggct aggctc ggctct gctctg ctctgg tctgga ctggat tggatg ggatga gatgaa atgaag tgaagt gaagtc aagtca agtcat gtcata tcatat catatt atattt tatttt attttc ttttcc tttcct ttcctt tccttg ccttgg cttgga ttggaa tggaaa ggaaac gaaact aaactt aactta acttat cttatg ttatga tatgac atgaca tgacat gacatt acatta cattac attaca ttacaa tacaac acaaca caacag aacagt acagtt cagttc agttcg gttcgt ttcgtg tcgtgc cgtgct gtgctt tgcttc gcttca cttcag ttcagt tcagta cagtag agtagg gtaggt taggta aggtat ggtatg gtatgt tatgta atgtat tgtatt gtattt tattta atttaa tttaat ttaatt taattt aatttc atttcc tttcca ttccaa tccaag ccaagt caagta aagtat agtata gtatat tatatt atattc tattcg attcgg ttcgga tcggaa cggaag ggaaga gaagaa aagaac agaaca gaacac aacaca acacag cacaga acagat cagata agatag gatagc atagcg tagcgt agcgtg gcgtgg cgtggt gtggtg tggtga ggtgat gtgatc tgatct gatctt atcttc tcttct cttctc ttctct tctctg ctctgg tctgga ctggag tggaga ggagaa gagaag agaagg gaagga aaggat aggatc ggatca gatcag atcaga tcagat cagatg agatga gatgaa atgaac tgaact gaactt aactta acttac cttacg ttacgc tacgca acgcag cgcagg gcaggg cagggt agggtt gggtta ggttac gttaca ttacat tacata acatat catata atatat tatatt atattt tatttt attttc ttttca tttcac ttcaca tcacaa cacaag acaagg caaggc aaggct aggctc ggctcc gctcct ctcctt tccttc ccttct cttctc ttctcc tctcct ctcctg tcctga cctgaa ctgaaa tgaaaa gaaaaa aaaaag aaaagc aaagcc aagccg agccga gccgag ccgagg cgagga gaggag aggagg ggagga gaggag aggaga ggagag gagagt agagtg gagtga agtgag gtgaga tgagag gagagg agaggc gaggct aggctt ggcttc gcttct cttctg ttctga tctgag ctgagg tgaggg gaggga agggaa gggaac ggaact gaactc aactct actcta ctctat tctatt ctattt tatttg atttgt tttgtt ttgttt tgtttg gtttga tttgat ttgatg tgatgt gatgtt atgttc tgttct gttctc ttctcc tctccg ctccgc tccgcg ccgcgc cgcgca gcgcag cgcaga gcagat cagatc agatcg gatcga atcgaa tcgaac cgaact gaacta aactac actact ctactg tactgc actgct ctgctg tgctgc gctgcc ctgccc tgccca gcccat cccatg ccatgg catggt atggtc tggtct ggtctt gtcttg tcttga cttgaa ttgaac tgaact gaactg aactga actgag ctgaga tgagag gagagt agagtc gagtcc agtccc gtccca tcccat cccatt ccattt catttc atttct tttcta ttctag tctaga ctagat tagatc agatca gatcat atcatc tcatcg catcga atcgat tcgatt cgattt gatttt attttc ttttct tttctt ttcttc tcttcc cttcct ttccta tcctat cctatt ctatta tattac attact ttactt tacttg acttgt cttgtc ttgtct tgtctc gtctct tctctg ctctgc tctgcc ctgcca tgccac gccacc ccacca caccag accaga ccagaa cagaaa agaaat gaaatg aaatga aatgag atgaga tgagaa gagaat agaatt gaattc aattcc attcca ttccaa tccaaa ccaaag caaaga aaagaa aagaat agaatg gaatgg aatggg atggga tgggat gggata ggatag gataga atagaa tagaaa agaaaa gaaaaa aaaaac aaaaca aaacat aacatc acatct catctc atctcc tctcct ctcctg tcctga cctgag ctgaga tgagag gagaga agagag gagaga agagac gagacg agacgt gacgtt acgttt cgtttg gtttga tttgag ttgagg tgagga gaggat aggatt ggattc gattcc attcca ttccaa tccaat ccaatc caatct aatctg atctga tctgat ctgata tgatac gatacc ataccc taccca acccaa cccaaa ccaaag caaaga aaagag aagaga agagat gagatt agattc gattct attctc ttctct tctctg ctctgg tctggc ctggcg tggcga ggcgac gcgacc cgacca gaccaa accaaa ccaaaa caaaag aaaaga aaagaa aagaag agaagc gaagcc aagcct agcctt gccttc ccttca cttcag ttcagt tcagtg cagtga agtgat gtgatg tgatgg gatgga atggaa tggaat ggaata gaataa aataac ataact taactt aacttc acttca cttcag ttcagt tcagtt cagtta agttaa gttaag ttaaga taagaa aagaat agaatt gaattc aattcc attcct ttcctg tcctgg cctggt ctggtt tggttt ggttta gtttaa tttaag ttaaga taagat aagatt agattt gatttt atttta ttttac tttaca ttacag tacagg acagga caggaa aggaat ggaata gaatac aatacg atacgt tacgtt acgttg cgttga gttgaa ttgaac tgaaca gaacat aacatc acatca catcag atcagg tcaggt caggtt aggttg ggttga gttgat ttgatg tgatga gatgat atgatg tgatgc gatgca atgcaa tgcaat gcaatg caatga aatgat atgatg tgatgg gatggc atggca tggcaa ggcaaa gcaaat caaatg aaatgc aatgca atgcag tgcagc gcagcc cagccc agccca gcccag cccaga ccagaa cagaaa agaaat gaaatt aaattt aatttc atttcc tttccc ttccct tccctt cccttc ccttca cttcaa ttcaat tcaata caatac aatact atactc tactcc actcct ctccta tcctaa cctaaa ctaaaa taaaac aaaacc aaacca aaccaa accaaa ccaaag caaaga aaagaa aagaag agaagg gaagga aaggat aggata ggatat gatatt atatta tattac attact ttacta tactac actacc ctaccg taccgt accgtc ccgtca cgtcaa gtcaag tcaagt caagtc aagtct agtctt gtcttt tctttg ctttga tttgaa ttgaac tgaacg gaacgc aacgcc acgcca cgccat gccatt ccatta cattac attacc ttaccc taccca acccag cccagg ccaggc caggcc aggccg ggccgg gccggg ccgggc cgggct gggctg ggctga gctgac ctgact tgactg gactgg actggc ctggct tggctg ggctga gctgag ctgagc tgagcc gagcca agccat gccatt ccatta cattac attact ttactg tactgg actgga ctggat tggatg ggatgc gatgcc atgccc tgccca gcccaa cccaag ccaagt caagtg aagtgg agtgga gtggat tggatc ggatca gatcaa atcaat tcaatg caatgc aatgcc atgcca tgccac gccact ccactg cactga actgac ctgacc tgaccc gaccct accctt cccttc ccttct cttctg ttctgc tctgcc ctgccc tgcccg gcccgc cccgca ccgcac cgcacg gcacgc cacgct acgctg cgctga gctgac ctgacc tgaccc gaccca acccac cccact ccacta cactac actaca ctacaa tacaag acaagt caagtc aagtca agtcag gtcagc tcagct cagctg agctgt gctgtc ctgtca tgtcaa gtcaaa tcaaag caaagc aaagct aagctt agctta gcttag #Outputatgtgt tgtgtg gtgtgg tgtggc gtggca tggcat ggcatt gcattt catttg atttgg tttggg ttgggc tgggcg gggcgc ggcgct gcgctg cgctgt gctgtt ctgttt tgtttg gtttgg tttggc ttggca tggcag ggcagt gcagtg cagtga agtgat gtgatg tgatga gatgat atgatt tgattg gattgc attgcc ttgcct tgcctt gccttt cctttc ctttct tttctg ttctgt tctgtt ctgttc tgttca gttcag ttcagt tcagtg cagtgt agtgtc gtgtct tgtctg gtctga tctgag ctgagt tgagtg gagtgc agtgct gtgcta tgctat gctatg ctatga tatgaa atgaag tgaaga gaagat aagatt agattg gattgc attgca ttgcac tgcaca gcacac cacaca acacag cacaga acagag cagagg agaggt gaggtc aggtcc ggtcca gtccag tccaga ccagat cagatg agatgc gatgca atgcat tgcatt gcattc cattcc attccg ttccgt tccgtt ccgttt cgtttt gttttg ttttga tttgag ttgaga tgagaa gagaat agaatg gaatgt aatgtc atgtca tgtcaa gtcaat tcaatg caatgg aatgga atggat tggata ggatac gataca atacac tacacc acacca caccaa accaac ccaact caactg aactgc actgct ctgctg tgctgc gctgct ctgctt tgcttt gctttg ctttgg tttgga ttggat tggatt ggattt gatttc atttca tttcac ttcacc tcaccg caccgg accggt ccggtt cggttg ggttgg gttggc ttggcg tggcgg ggcggt gcggta cggtag ggtagt gtagtt tagttg agttga gttgac ttgacc tgaccc gacccg acccgc cccgct ccgctg cgctgt gctgtt ctgttt tgtttg gtttgg tttgga ttggaa tggaat ggaatg gaatgc aatgca atgcag tgcagc gcagcc cagcca agccaa gccaat ccaatt caattc aattcg attcga ttcgag tcgagt cgagtg gagtga agtgaa gtgaag tgaaga gaagaa aagaaa agaaat gaaata aaatat aatatc atatcc tatccg atccgt tccgta ccgtat cgtatt gtattt tatttg atttgt tttgtg ttgtgg tgtggc gtggct tggctc ggctct gctctg ctctgt tctgtt ctgtta tgttac gttaca ttacaa tacaat acaatg caatgg aatggt atggtg tggtga ggtgaa gtgaaa tgaaat gaaatc aaatct aatcta atctac tctaca ctacaa tacaac acaacc caacca aaccat accata ccataa cataag ataaga taagaa aagaag agaaga gaagat aagatg agatgc gatgca atgcaa tgcaac gcaaca caacag aacagc acagca cagcat agcatt gcattt catttt attttg ttttga tttgaa ttgaat tgaatt gaattt aatttg atttga tttgaa ttgaat tgaata gaatac aatacc atacca taccag accaga ccagac cagacc agacca gaccaa accaaa ccaaag caaagt aaagtg aagtgg agtgga gtggat tggatg ggatgg gatggt atggtg tggtga ggtgag gtgaga tgagat gagata agataa gataat ataatc taatcc aatcct atcctt tccttc ccttca cttcat ttcatc tcatct catctt atcttt tcttta ctttat tttatg ttatga tatgac atgaca tgacaa gacaaa acaaag caaagg aaagga aaggag aggagg ggagga gaggaa aggaat ggaatt gaattg aattga attgag ttgagc tgagca gagcaa agcaaa gcaaac caaaca aaacaa aacaat acaatt caattt aatttg atttgt tttgta ttgtat tgtatg gtatgt tatgtt atgttg tgttgg gttgga ttggat tggatg ggatgg gatggt atggtg tggtgt ggtgtg gtgtgt tgtgtt gtgttt tgtttg gtttgc tttgca ttgcat tgcatt gcattt catttg atttgt tttgtt ttgttt tgtttt gtttta ttttac tttact ttactg tactgg actgga ctggat tggata ggatac gatact atactg tactgc actgcc ctgcca tgccaa gccaat ccaata caataa aataag ataaga taagaa aagaaa agaaag gaaagt aaagtg aagtgt agtgtt gtgttc tgttcc gttcct ttcctg tcctgg cctggg ctgggt tgggta gggtag ggtaga gtagag tagaga agagat gagata agatac gataca atacat tacata acatat catatg atatgg tatgga atggag tggagt ggagtc gagtca agtcag gtcaga tcagac cagacc agacct gacctt accttt cctttg ctttgt tttgtt ttgttt tgttta gtttaa tttaaa ttaaag taaagc aaagca aagcaa agcaat gcaatg caatga aatgac atgaca tgacag gacaga acagaa cagaag agaaga gaagat aagatg agatgg gatgga atggat tggatt ggattt gatttt attttt tttttg ttttgg tttggc ttggct tggctg ggctgt gctgta ctgtat tgtatg gtatgt tatgtt atgttc tgttca gttcag ttcaga tcagaa cagaag agaagc gaagct aagcta agctaa gctaaa ctaaag taaagg aaaggt aaggtc aggtct ggtctt gtcttg tcttgt cttgtt ttgtta tgttac gttaca ttacat tacatt acattg cattga attgaa ttgaag tgaagc gaagca aagcac agcact gcactc cactcc actccg ctccgc tccgcg ccgcga cgcgac gcgact cgactc gactcc actccc ctccct tccctt cccttt cctttt cttttt ttttta ttttaa tttaaa ttaaaa taaaag aaaagt aaagtg aagtgg agtgga gtggag tggagc ggagcc gagcct agcctt gccttt cctttt cttttc ttttct tttctt ttcttc tcttcc cttcct ttcctg tcctgg cctgga ctggac tggaca ggacac gacact acacta cactat actatg ctatga tatgaa atgaag tgaagt gaagtt aagttt agtttt gttttg ttttgg tttgga ttggat tggatt ggattt gattta atttaa tttaaa ttaaag taaagc aaagcc aagcca agccaa gccaaa ccaaat caaatg aaatgg aatggc atggca tggcaa ggcaaa gcaaag caaagt aaagtt aagttg agttgc gttgca ttgcat tgcatc gcatcc catccg atccgt tccgtg ccgtgg cgtgga gtggaa tggaaa ggaaat gaaatg aaatgg aatggt atggtt tggtta ggttaa gttaaa ttaaat taaata aaatat aatatc atatca tatcat atcatc tcatca catcac atcact tcactg cactgt actgtc ctgtcg tgtcgg gtcggg tcggga cgggat gggatg ggatgt gatgta atgtac tgtacc gtaccc tacccc acccct cccctg ccctgc cctgca ctgcac tgcacg gcacgc cacgcc acgccc cgccct gccctc ccctct cctcta ctctat tctatg ctatga tatgac atgaca tgacaa gacaat acaatg caatgt aatgtg atgtgg tgtgga gtggag tggaga ggagaa gagaaa agaaac gaaact aaactc aactct actctt ctcttt tctttc ctttcc tttcca ttccag tccagg ccaggt caggtt aggttt ggtttt gttttg ttttga tttgag ttgaga tgagat gagata agatag gataga atagaa tagaaa agaaac gaaact aaactg aactgt actgtg ctgtga tgtgaa gtgaag tgaaga gaagaa aagaac agaaca gaacaa aacaac acaacc caacct aacctc acctca cctcag ctcagg tcagga caggat aggatc ggatcc gatcct atcctt tccttt cctttt cttttt ttttta ttttaa tttaat ttaata taataa aataat ataatg taatgc aatgct atgctg tgctgt gctgta ctgtaa tgtaaa gtaaag taaaga aaagaa aagaaa agaaac gaaacg aaacgt aacgtt acgttt cgtttg gtttga tttgat ttgatg tgatga gatgac atgaca tgacag gacaga acagac cagaca agacag gacaga acagaa cagaag agaagg gaagga aaggat aggatt ggattg gattgg attggc ttggct tggctg ggctgc gctgcc ctgcct tgcctt gccttt cctttt ctttta ttttat tttatc ttatca tatcag atcagg tcaggg cagggg aggggg gggggc ggggct gggctt ggcttg gcttgg cttgga ttggac tggact ggactc gactcc actcca ctccag tccagc ccagct cagctt agcttg gcttgg cttggt ttggtt tggttg ggttgc gttgct ttgctg tgctgc gctgcc ctgcca tgccac gccact ccactc cactct actctg ctctgt tctgtt ctgttg tgttga gttgaa ttgaag tgaagc gaagca aagcag agcagc gcagct cagctg agctga gctgaa ctgaaa tgaaag gaaaga aaagaa aagaag agaagc gaagcc aagccc agccca gcccaa cccaag ccaagt caagta aagtac agtaca gtacag tacagt acagta cagtat agtatc gtatcc tatcct atcctc tcctct cctctc ctctcc tctcca ctccag tccaga ccagac cagaca agacat gacatt acattt catttg atttgc tttgca ttgcaa tgcaat gcaatt caattg aattgg attggc ttggca tggcat ggcatg gcatgg catgga atggaa tggaag ggaaga gaagac aagaca agacag gacagc acagcc cagccc agcccc gccccg ccccga cccgat ccgatt cgattt gattta atttac tttact ttactg tactgg actggc ctggct tggctg ggctgc gctgct ctgcta tgctag gctaga ctagaa tagaaa agaaag gaaagg aaaggt aaggtg aggtgg ggtggc gtggca tggcag ggcaga gcagat cagatc agatca gatcat atcata tcatat catatt atattg tattgg attgga ttggaa tggaag ggaagt gaagtg aagtga agtgaa gtgaac tgaaca gaacat aacatt acatta cattat attatg ttatga tatgaa atgaag tgaagt gaagtc aagtcc agtcct gtcctt tccttt cctttt cttttt ttttta ttttaa tttaac ttaact taactc aactct actctg ctctga tctgag ctgagg tgagga gaggaa aggaag ggaagg gaaggc aaggca aggcat ggcatt gcattc cattca attcag ttcagg tcaggc caggct aggctc ggctct gctctg ctctgg tctgga ctggat tggatg ggatga gatgaa atgaag tgaagt gaagtc aagtca agtcat gtcata tcatat catatt atattt tatttt attttc ttttcc tttcct ttcctt tccttg ccttgg cttgga ttggaa tggaaa ggaaac gaaact aaactt aactta acttat cttatg ttatga tatgac atgaca tgacat gacatt acatta cattac attaca ttacaa tacaac acaaca caacag aacagt acagtt cagttc agttcg gttcgt ttcgtg tcgtgc cgtgct gtgctt tgcttc gcttca cttcag ttcagt tcagta cagtag agtagg gtaggt taggta aggtat ggtatg gtatgt tatgta atgtat tgtatt gtattt tattta atttaa tttaat ttaatt taattt aatttc atttcc tttcca ttccaa tccaag ccaagt caagta aagtat agtata gtatat tatatt atattc tattcg attcgg ttcgga tcggaa cggaag ggaaga gaagaa aagaac agaaca gaacac aacaca acacag cacaga acagat cagata agatag gatagc atagcg tagcgt agcgtg gcgtgg cgtggt gtggtg tggtga ggtgat gtgatc tgatct gatctt atcttc tcttct cttctc ttctct tctctg ctctgg tctgga ctggag tggaga ggagaa gagaag agaagg gaagga aaggat aggatc ggatca gatcag atcaga tcagat cagatg agatga gatgaa atgaac tgaact gaactt aactta acttac cttacg ttacgc tacgca acgcag cgcagg gcaggg cagggt agggtt gggtta ggttac gttaca ttacat tacata acatat catata atatat tatatt atattt tatttt attttc ttttca tttcac ttcaca tcacaa cacaag acaagg caaggc aaggct aggctc ggctcc gctcct ctcctt tccttc ccttct cttctc ttctcc tctcct ctcctg tcctga cctgaa ctgaaa tgaaaa gaaaaa aaaaag aaaagc aaagcc aagccg agccga gccgag ccgagg cgagga gaggag aggagg ggagga gaggag aggaga ggagag gagagt agagtg gagtga agtgag gtgaga tgagag gagagg agaggc gaggct aggctt ggcttc gcttct cttctg ttctga tctgag ctgagg tgaggg gaggga agggaa gggaac ggaact gaactc aactct actcta ctctat tctatt ctattt tatttg atttgt tttgtt ttgttt tgtttg gtttga tttgat ttgatg tgatgt gatgtt atgttc tgttct gttctc ttctcc tctccg ctccgc tccgcg ccgcgc cgcgca gcgcag cgcaga gcagat cagatc agatcg gatcga atcgaa tcgaac cgaact gaacta aactac actact ctactg tactgc actgct ctgctg tgctgc gctgcc ctgccc tgccca gcccat cccatg ccatgg catggt atggtc tggtct ggtctt gtcttg tcttga cttgaa ttgaac tgaact gaactg aactga actgag ctgaga tgagag gagagt agagtc gagtcc agtccc gtccca tcccat cccatt ccattt catttc atttct tttcta ttctag tctaga ctagat tagatc agatca gatcat atcatc tcatcg catcga atcgat tcgatt cgattt gatttt attttc ttttct tttctt ttcttc tcttcc cttcct ttccta tcctat cctatt ctatta tattac attact ttactt tacttg acttgt cttgtc ttgtct tgtctc gtctct tctctg ctctgc tctgcc ctgcca tgccac gccacc ccacca caccag accaga ccagaa cagaaa agaaat gaaatg aaatga aatgag atgaga tgagaa gagaat agaatt gaattc aattcc attcca ttccaa tccaaa ccaaag caaaga aaagaa aagaat agaatg gaatgg aatggg atggga tgggat gggata ggatag gataga atagaa tagaaa agaaaa gaaaaa aaaaac aaaaca aaacat aacatc acatct catctc atctcc tctcct ctcctg tcctga cctgag ctgaga tgagag gagaga agagag gagaga agagac gagacg agacgt gacgtt acgttt cgtttg gtttga tttgag ttgagg tgagga gaggat aggatt ggattc gattcc attcca ttccaa tccaat ccaatc caatct aatctg atctga tctgat ctgata tgatac gatacc ataccc taccca acccaa cccaaa ccaaag caaaga aaagag aagaga agagat gagatt agattc gattct attctc ttctct tctctg ctctgg tctggc ctggcg tggcga ggcgac gcgacc cgacca gaccaa accaaa ccaaaa caaaag aaaaga aaagaa aagaag agaagc gaagcc aagcct agcctt gccttc ccttca cttcag ttcagt tcagtg cagtga agtgat gtgatg tgatgg gatgga atggaa tggaat ggaata gaataa aataac ataact taactt aacttc acttca cttcag ttcagt tcagtt cagtta agttaa gttaag ttaaga taagaa aagaat agaatt gaattc aattcc attcct ttcctg tcctgg cctggt ctggtt tggttt ggttta gtttaa tttaag ttaaga taagat aagatt agattt gatttt atttta ttttac tttaca ttacag tacagg acagga caggaa aggaat ggaata gaatac aatacg atacgt tacgtt acgttg cgttga gttgaa ttgaac tgaaca gaacat aacatc acatca catcag atcagg tcaggt caggtt aggttg ggttga gttgat ttgatg tgatga gatgat atgatg tgatgc gatgca atgcaa tgcaat gcaatg caatga aatgat atgatg tgatgg gatggc atggca tggcaa ggcaaa gcaaat caaatg aaatgc aatgca atgcag tgcagc gcagcc cagccc agccca gcccag cccaga ccagaa cagaaa agaaat gaaatt aaattt aatttc atttcc tttccc ttccct tccctt cccttc ccttca cttcaa ttcaat tcaata caatac aatact atactc tactcc actcct ctccta tcctaa cctaaa ctaaaa taaaac aaaacc aaacca aaccaa accaaa ccaaag caaaga aaagaa aagaag agaagg gaagga aaggat aggata ggatat gatatt atatta tattac attact ttacta tactac actacc ctaccg taccgt accgtc ccgtca cgtcaa gtcaag tcaagt caagtc aagtct agtctt gtcttt tctttg ctttga tttgaa ttgaac tgaacg gaacgc aacgcc acgcca cgccat gccatt ccatta cattac attacc ttaccc taccca acccag cccagg ccaggc caggcc aggccg ggccgg gccggg ccgggc cgggct gggctg ggctga gctgac ctgact tgactg gactgg actggc ctggct tggctg ggctga gctgag ctgagc tgagcc gagcca agccat gccatt ccatta cattac attact ttactg tactgg actgga ctggat tggatg ggatgc gatgcc atgccc tgccca gcccaa cccaag ccaagt caagtg aagtgg agtgga gtggat tggatc ggatca gatcaa atcaat tcaatg caatgc aatgcc atgcca tgccac gccact ccactg cactga actgac ctgacc tgaccc gaccct accctt cccttc ccttct cttctg ttctgc tctgcc ctgccc tgcccg gcccgc cccgca ccgcac cgcacg gcacgc cacgct acgctg cgctga gctgac ctgacc tgaccc gaccca acccac cccact ccacta cactac actaca ctacaa tacaag acaagt caagtc aagtca agtcag gtcagc tcagct cagctg agctgt gctgtc ctgtca tgtcaa gtcaaa tcaaag caaagc aaagct aagctt agctta gcttag y_data y_data #Output–array([4, 4, 3, …, 6, 6, 6]) We will perform the same steps for chimpanzee and dog chimp_texts = list(chimp_data['words']) for item in range(len(chimp_texts)): chimp_texts[item] = ' '.join(chimp_texts[item]) y_chimp = chimp_data.iloc[:, 0].values dog_texts = list(dog_data['words']) for item in range(len(dog_texts)): dog_texts[item] = ' '.join(dog_texts[item]) y_dog = dog_data.iloc[:, 0].values chimp_texts = list(chimp_data['words'])for item in range(len(chimp_texts)): chimp_texts[item] = ' '.join(chimp_texts[item])y_chimp = chimp_data.iloc[:, 0].values ​dog_texts = list(dog_data['words'])for item in range(len(dog_texts)): dog_texts[item] = ' '.join(dog_texts[item])y_dog = dog_data.iloc[:, 0].values Now we will apply the BAG of WORDS using CountVectorizer using NLP # Creating the Bag of Words model using CountVectorizer() # This is equivalent to k-mer counting # The n-gram size of 4 was previously determined by testing from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer(ngram_range=(4,4)) X = cv.fit_transform(human_texts) X_chimp = cv.transform(chimp_texts) X_dog = cv.transform(dog_texts) print(X.shape) print(X_chimp.shape) print(X_dog.shape) # Creating the Bag of Words model using CountVectorizer()# This is equivalent to k-mer counting# The n-gram size of 4 was previously determined by testingfrom sklearn.feature_extraction.text import CountVectorizercv = CountVectorizer(ngram_range=(4,4))X = cv.fit_transform(human_texts)X_chimp = cv.transform(chimp_texts)X_dog = cv.transform(dog_texts)print(X.shape)print(X_chimp.shape)print(X_dog.shape) #Output-(4380, 232414) (1682, 232414) (820, 232414) If we have a look at class balance we can see we have relatively balanced data set. human_data['class'].value_counts().sort_index().plot.bar() human_data['class'].value_counts().sort_index().plot.bar() Splitting the human data set into the training set and test set from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y_data, test_size = 0.20, random_state=42) print(X_train.shape) print(X_test.shape) from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y_data, test_size = 0.20, random_state=42)print(X_train.shape)print(X_test.shape) #Output(3504, 232414) (876, 232414) A multinomial naive Bayes classifier will be created. I previously did some parameter tuning and found the ngram size of 4 (reflected in the Countvectorizer() instance) and a model alpha of 0.1 did the best. # The alpha parameter was determined by grid search previously from sklearn.naive_bayes import MultinomialNB classifier = MultinomialNB(alpha=0.1) classifier.fit(X_train, y_train) # The alpha parameter was determined by grid search previouslyfrom sklearn.naive_bayes import MultinomialNBclassifier = MultinomialNB(alpha=0.1)classifier.fit(X_train, y_train) #Output-MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True) y_pred = classifier.predict(X_test) y_pred = classifier.predict(X_test) Okay, so let’s look at some model performance metrics like the confusion matrix, accuracy, precision, recall and f1 score. We are getting really good results on our unseen data, so it looks like our model did not overfit to the training data. In a real project I would go back and sample many more train test splits since we have a relatively small data set. from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score print(""Confusion matrix\n"") print(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted'))) def get_metrics(y_test, y_predicted): accuracy = accuracy_score(y_test, y_predicted) precision = precision_score(y_test, y_predicted, average='weighted') recall = recall_score(y_test, y_predicted, average='weighted') f1 = f1_score(y_test, y_predicted, average='weighted') return accuracy, precision, recall, f1 accuracy, precision, recall, f1 = get_metrics(y_test, y_pred) print(""accuracy = %.3f \nprecision = %.3f \nrecall = %.3f \nf1 = %.3f"" % (accuracy, precision, recall, f1)) from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_scoreprint(""Confusion matrix\n"")print(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))def get_metrics(y_test, y_predicted): accuracy = accuracy_score(y_test, y_predicted) precision = precision_score(y_test, y_predicted, average='weighted') recall = recall_score(y_test, y_predicted, average='weighted') f1 = f1_score(y_test, y_predicted, average='weighted') return accuracy, precision, recall, f1accuracy, precision, recall, f1 = get_metrics(y_test, y_pred)print(""accuracy = %.3f \nprecision = %.3f \nrecall = %.3f \nf1 = %.3f"" % (accuracy, precision, recall, f1)) Output Confusion matrix Predicted 0 1 2 3 4 5 6 Actual 0 99 0 0 0 1 0 2 1 0 104 0 0 0 0 2 2 0 0 78 0 0 0 0 3 0 0 0 124 0 0 1 4 1 0 0 0 143 0 5 5 0 0 0 0 0 51 0 6 1 0 0 1 0 0 263 accuracy = 0.984 precision = 0.984 recall = 0.984 f1 = 0.984 OutputConfusion matrix​Predicted 0 1 2 3 4 5 6Actual 0 99 0 0 0 1 0 21 0 104 0 0 0 0 22 0 0 78 0 0 0 03 0 0 0 124 0 0 14 1 0 0 0 143 0 55 0 0 0 0 0 51 06 1 0 0 1 0 0 263accuracy = 0.984 precision = 0.984 recall = 0.984 f1 = 0.984";DNA Sequencing with Machine Learning
2020-05-24 14:17:48;Artificial neural networks are one of the main tools used in machine learning. As the “neural” part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns which are far too complex or numerous for a human programmer to extract and teach the machine to recognize.This Machine Learning Project Classifies Clothes from the Fashion MNIST Data set using Artificial Neural Networks and Python.Let’s start by importing the libraries we need for this taskTo load the data setTo print the shape of the training and testing dataTo Compile the ModelTo Train the modelTo Evaluate the ModelTo Make a PredictionTo print the maximum labelsTo Print the first 5 images;https://thecleverprogrammer.com/2020/05/24/artificial-neural-networks-with-machine-learning/;['keras', 'pattern', 'tensorflow'];1.0;[];['NN', 'ML', 'ANN', 'ReLu', 'Classification'];['hidden layer', 'artificial neural network', 'epoch', 'recogn', 'output layer', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'relu', 'train', 'label'];"Artificial neural networks are one of the main tools used in machine learning. As the “neural” part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns which are far too complex or numerous for a human programmer to extract and teach the machine to recognize. This Machine Learning Project Classifies Clothes from the Fashion MNIST Data set using Artificial Neural Networks and Python. Let’s start by importing the libraries we need for this task import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt​x import tensorflow as tffrom tensorflow import kerasimport numpy as npimport matplotlib.pyplot as plt To load the data set fashion = keras.datasets.fashion_mnist (trainImages, trainLabels), (testImages, testLabels) = fashion.load_data() fashion = keras.datasets.fashion_mnist(trainImages, trainLabels), (testImages, testLabels) = fashion.load_data() imgIndex = 0 img = trainImages[imgIndex] print(""Image Label :"",trainLabels[imgIndex]) plt.imshow(img) imgIndex = 0img = trainImages[imgIndex]print(""Image Label :"",trainLabels[imgIndex])plt.imshow(img) #Output Image Label : 9 <matplotlib.image.AxesImage at 0x7f1111a06d68> #OutputImage Label : 9<matplotlib.image.AxesImage at 0x7f1111a06d68> To print the shape of the training and testing data print(trainImages.shape) print(testImages.shape) print(trainImages.shape)print(testImages.shape) #Output (60000, 28, 28) (10000, 28, 28) #Output(60000, 28, 28)(10000, 28, 28) Now let’s create a Neural Network model = keras.Sequential([ keras.layers.Flatten(input_shape=(28,28)), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax) ]) model = keras.Sequential([ keras.layers.Flatten(input_shape=(28,28)), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax)]) To Compile the Model model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy']) model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy']) To Train the model model.fit(trainImages, trainLabels, epochs=5, batch_size=32) model.fit(trainImages, trainLabels, epochs=5, batch_size=32) #Output Epoch 1/5 1875/1875 [==============================] - 4s 2ms/step - loss: 3.6150 - accuracy: 0.6802 Epoch 2/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.7296 - accuracy: 0.7488 Epoch 3/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6374 - accuracy: 0.7725 Epoch 4/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5873 - accuracy: 0.7906 Epoch 5/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5579 - accuracy: 0.7993 <tensorflow.python.keras.callbacks.History at 0x7f1108dc3588> #OutputEpoch 1/51875/1875 [==============================] - 4s 2ms/step - loss: 3.6150 - accuracy: 0.6802Epoch 2/51875/1875 [==============================] - 4s 2ms/step - loss: 0.7296 - accuracy: 0.7488Epoch 3/51875/1875 [==============================] - 4s 2ms/step - loss: 0.6374 - accuracy: 0.7725Epoch 4/51875/1875 [==============================] - 4s 2ms/step - loss: 0.5873 - accuracy: 0.7906Epoch 5/51875/1875 [==============================] - 4s 2ms/step - loss: 0.5579 - accuracy: 0.7993<tensorflow.python.keras.callbacks.History at 0x7f1108dc3588> To Evaluate the Model model.evaluate(testImages, testLabels) model.evaluate(testImages, testLabels) #Output 313/313 [==============================] - 0s 1ms/step - loss: 0.5916 - accuracy: 0.7981 [0.5915989279747009, 0.7980999946594238] #Output313/313 [==============================] - 0s 1ms/step - loss: 0.5916 - accuracy: 0.7981[0.5915989279747009, 0.7980999946594238] To Make a Prediction predictions = model.predict(testImages[0:5]) # Print the predicted labels print(predictions) predictions = model.predict(testImages[0:5])​# Print the predicted labelsprint(predictions) #Output [[1.74235439e-07 2.69071290e-08 6.66509115e-20 3.09463957e-07 1.11526007e-20 1.34603798e-01 8.10060641e-08 7.74199590e-02 3.87958280e-05 7.87936807e-01] [2.89689321e-02 1.06601091e-02 6.28736615e-01 2.77338717e-02 1.61624148e-01 1.49910515e-02 8.56256112e-02 1.23378839e-02 2.35275514e-02 5.79410419e-03] [6.75366528e-06 9.99993205e-01 4.27281517e-12 2.68350314e-10 8.65088672e-16 1.05001736e-14 1.33745196e-12 0.00000000e+00 1.84386378e-11 0.00000000e+00] [6.56618613e-06 9.99993443e-01 1.46741508e-11 1.80866895e-08 7.95811239e-14 1.56570215e-16 5.96713607e-12 0.00000000e+00 3.94146077e-10 0.00000000e+00] [2.19924763e-01 1.00887669e-02 1.99720263e-01 6.23517819e-02 4.97664846e-02 3.40277069e-07 4.30076748e-01 7.25772731e-09 2.80708820e-02 2.27675168e-09]] #Output[[1.74235439e-07 2.69071290e-08 6.66509115e-20 3.09463957e-07 1.11526007e-20 1.34603798e-01 8.10060641e-08 7.74199590e-02 3.87958280e-05 7.87936807e-01] [2.89689321e-02 1.06601091e-02 6.28736615e-01 2.77338717e-02 1.61624148e-01 1.49910515e-02 8.56256112e-02 1.23378839e-02 2.35275514e-02 5.79410419e-03] [6.75366528e-06 9.99993205e-01 4.27281517e-12 2.68350314e-10 8.65088672e-16 1.05001736e-14 1.33745196e-12 0.00000000e+00 1.84386378e-11 0.00000000e+00] [6.56618613e-06 9.99993443e-01 1.46741508e-11 1.80866895e-08 7.95811239e-14 1.56570215e-16 5.96713607e-12 0.00000000e+00 3.94146077e-10 0.00000000e+00] [2.19924763e-01 1.00887669e-02 1.99720263e-01 6.23517819e-02 4.97664846e-02 3.40277069e-07 4.30076748e-01 7.25772731e-09 2.80708820e-02 2.27675168e-09]] To print the maximum labels print(np.argmax(predictions, axis=1)) # Print the actual label values print(testLabels[0:5]) print(np.argmax(predictions, axis=1))# Print the actual label valuesprint(testLabels[0:5]) #Output [9 2 1 1 6] [9 2 1 1 6] #Output[9 2 1 1 6][9 2 1 1 6] To Print the first 5 images for i in range(0,5): plt.imshow(testImages[i], cmap='gray') plt.show() for i in range(0,5): plt.imshow(testImages[i], cmap='gray') plt.show() Follow us on Instagram for all your Queries Instagram";Artificial Neural Networks with Machine Learning
2020-05-24 14:17:48;Artificial neural networks are one of the main tools used in machine learning. As the “neural” part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns which are far too complex or numerous for a human programmer to extract and teach the machine to recognize.This Machine Learning Project Classifies Clothes from the Fashion MNIST Data set using Artificial Neural Networks and Python.Let’s start by importing the libraries we need for this taskTo load the data setTo print the shape of the training and testing dataTo Compile the ModelTo Train the modelTo Evaluate the ModelTo Make a PredictionTo print the maximum labelsTo Print the first 5 images;https://thecleverprogrammer.com/2020/05/24/machine-learning-project-artificial-neural-networks/;['keras', 'pattern', 'tensorflow'];1.0;[];['NN', 'ML', 'ANN', 'ReLu', 'Classification'];['hidden layer', 'artificial neural network', 'epoch', 'recogn', 'output layer', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'relu', 'train', 'label'];"Artificial neural networks are one of the main tools used in machine learning. As the “neural” part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns which are far too complex or numerous for a human programmer to extract and teach the machine to recognize. This Machine Learning Project Classifies Clothes from the Fashion MNIST Data set using Artificial Neural Networks and Python. Let’s start by importing the libraries we need for this task import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt​x import tensorflow as tffrom tensorflow import kerasimport numpy as npimport matplotlib.pyplot as plt To load the data set fashion = keras.datasets.fashion_mnist (trainImages, trainLabels), (testImages, testLabels) = fashion.load_data() fashion = keras.datasets.fashion_mnist(trainImages, trainLabels), (testImages, testLabels) = fashion.load_data() imgIndex = 0 img = trainImages[imgIndex] print(""Image Label :"",trainLabels[imgIndex]) plt.imshow(img) imgIndex = 0img = trainImages[imgIndex]print(""Image Label :"",trainLabels[imgIndex])plt.imshow(img) #Output Image Label : 9 <matplotlib.image.AxesImage at 0x7f1111a06d68> #OutputImage Label : 9<matplotlib.image.AxesImage at 0x7f1111a06d68> To print the shape of the training and testing data print(trainImages.shape) print(testImages.shape) print(trainImages.shape)print(testImages.shape) #Output (60000, 28, 28) (10000, 28, 28) #Output(60000, 28, 28)(10000, 28, 28) Now let’s create a Neural Network model = keras.Sequential([ keras.layers.Flatten(input_shape=(28,28)), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax) ]) model = keras.Sequential([ keras.layers.Flatten(input_shape=(28,28)), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax)]) To Compile the Model model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy']) model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy']) To Train the model model.fit(trainImages, trainLabels, epochs=5, batch_size=32) model.fit(trainImages, trainLabels, epochs=5, batch_size=32) #Output Epoch 1/5 1875/1875 [==============================] - 4s 2ms/step - loss: 3.6150 - accuracy: 0.6802 Epoch 2/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.7296 - accuracy: 0.7488 Epoch 3/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6374 - accuracy: 0.7725 Epoch 4/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5873 - accuracy: 0.7906 Epoch 5/5 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5579 - accuracy: 0.7993 <tensorflow.python.keras.callbacks.History at 0x7f1108dc3588> #OutputEpoch 1/51875/1875 [==============================] - 4s 2ms/step - loss: 3.6150 - accuracy: 0.6802Epoch 2/51875/1875 [==============================] - 4s 2ms/step - loss: 0.7296 - accuracy: 0.7488Epoch 3/51875/1875 [==============================] - 4s 2ms/step - loss: 0.6374 - accuracy: 0.7725Epoch 4/51875/1875 [==============================] - 4s 2ms/step - loss: 0.5873 - accuracy: 0.7906Epoch 5/51875/1875 [==============================] - 4s 2ms/step - loss: 0.5579 - accuracy: 0.7993<tensorflow.python.keras.callbacks.History at 0x7f1108dc3588> To Evaluate the Model model.evaluate(testImages, testLabels) model.evaluate(testImages, testLabels) #Output 313/313 [==============================] - 0s 1ms/step - loss: 0.5916 - accuracy: 0.7981 [0.5915989279747009, 0.7980999946594238] #Output313/313 [==============================] - 0s 1ms/step - loss: 0.5916 - accuracy: 0.7981[0.5915989279747009, 0.7980999946594238] To Make a Prediction predictions = model.predict(testImages[0:5]) # Print the predicted labels print(predictions) predictions = model.predict(testImages[0:5])​# Print the predicted labelsprint(predictions) #Output [[1.74235439e-07 2.69071290e-08 6.66509115e-20 3.09463957e-07 1.11526007e-20 1.34603798e-01 8.10060641e-08 7.74199590e-02 3.87958280e-05 7.87936807e-01] [2.89689321e-02 1.06601091e-02 6.28736615e-01 2.77338717e-02 1.61624148e-01 1.49910515e-02 8.56256112e-02 1.23378839e-02 2.35275514e-02 5.79410419e-03] [6.75366528e-06 9.99993205e-01 4.27281517e-12 2.68350314e-10 8.65088672e-16 1.05001736e-14 1.33745196e-12 0.00000000e+00 1.84386378e-11 0.00000000e+00] [6.56618613e-06 9.99993443e-01 1.46741508e-11 1.80866895e-08 7.95811239e-14 1.56570215e-16 5.96713607e-12 0.00000000e+00 3.94146077e-10 0.00000000e+00] [2.19924763e-01 1.00887669e-02 1.99720263e-01 6.23517819e-02 4.97664846e-02 3.40277069e-07 4.30076748e-01 7.25772731e-09 2.80708820e-02 2.27675168e-09]] #Output[[1.74235439e-07 2.69071290e-08 6.66509115e-20 3.09463957e-07 1.11526007e-20 1.34603798e-01 8.10060641e-08 7.74199590e-02 3.87958280e-05 7.87936807e-01] [2.89689321e-02 1.06601091e-02 6.28736615e-01 2.77338717e-02 1.61624148e-01 1.49910515e-02 8.56256112e-02 1.23378839e-02 2.35275514e-02 5.79410419e-03] [6.75366528e-06 9.99993205e-01 4.27281517e-12 2.68350314e-10 8.65088672e-16 1.05001736e-14 1.33745196e-12 0.00000000e+00 1.84386378e-11 0.00000000e+00] [6.56618613e-06 9.99993443e-01 1.46741508e-11 1.80866895e-08 7.95811239e-14 1.56570215e-16 5.96713607e-12 0.00000000e+00 3.94146077e-10 0.00000000e+00] [2.19924763e-01 1.00887669e-02 1.99720263e-01 6.23517819e-02 4.97664846e-02 3.40277069e-07 4.30076748e-01 7.25772731e-09 2.80708820e-02 2.27675168e-09]] To print the maximum labels print(np.argmax(predictions, axis=1)) # Print the actual label values print(testLabels[0:5]) print(np.argmax(predictions, axis=1))# Print the actual label valuesprint(testLabels[0:5]) #Output [9 2 1 1 6] [9 2 1 1 6] #Output[9 2 1 1 6][9 2 1 1 6] To Print the first 5 images for i in range(0,5): plt.imshow(testImages[i], cmap='gray') plt.show() for i in range(0,5): plt.imshow(testImages[i], cmap='gray') plt.show() Follow us on Instagram for all your Queries Instagram";Artificial Neural Networks with Machine Learning
2020-05-25 20:53:24;In this Machine Learning Project, we’ll build  binary classification that puts movie reviews texts into one of two categories — negative or positive sentiment. We’re going to have a brief look at the Bayes theorem and relax its requirements using the Naive assumption.Let’s start by importing the LibrariesYou can download the data set you need for this task from here:No null values, Label encode sentiment to 1(positive) and 0(negative);https://thecleverprogrammer.com/2020/05/25/movie-reviews-sentiment-analysis-binary-classification-with-machine-learning/;['sklearn', 'vocabulary', 'nltk'];1.0;['ML'];['ML', 'Classification'];['predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'label'];"In this Machine Learning Project, we’ll build binary classification that puts movie reviews texts into one of two categories — negative or positive sentiment. We’re going to have a brief look at the Bayes theorem and relax its requirements using the Naive assumption. Let’s start by importing the Libraries import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import re # for regex from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from nltk.stem import SnowballStemmer from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB from sklearn.metrics import accuracy_score import pickle​x import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)import re # for regexfrom nltk.corpus import stopwordsfrom nltk.tokenize import word_tokenizefrom nltk.stem import SnowballStemmerfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNBfrom sklearn.metrics import accuracy_scoreimport pickle You can download the data set you need for this task from here: IMDB-DatasetDownload DataSEt data = pd.read_csv('IMDB Dataset.csv') print(data.shape) data.head() data = pd.read_csv('IMDB Dataset.csv')print(data.shape)data.head() data.info() data.info() #Output <class 'pandas.core.frame.DataFrame'> RangeIndex: 50000 entries, 0 to 49999 Data columns (total 2 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 review 50000 non-null object 1 sentiment 50000 non-null object dtypes: object(2) memory usage: 781.4+ KB #Output<class 'pandas.core.frame.DataFrame'>RangeIndex: 50000 entries, 0 to 49999Data columns (total 2 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 review 50000 non-null object 1 sentiment 50000 non-null objectdtypes: object(2)memory usage: 781.4+ KB No null values, Label encode sentiment to 1(positive) and 0(negative) data.sentiment.value_counts() data.sentiment.value_counts() #Output positive 25000 negative 25000 Name: sentiment, dtype: int64 #Outputpositive 25000negative 25000Name: sentiment, dtype: int64 data.sentiment.replace('positive',1,inplace=True) data.sentiment.replace('negative',0,inplace=True) data.head(10) data.sentiment.replace('positive',1,inplace=True)data.sentiment.replace('negative',0,inplace=True)data.head(10) data.review[0] data.review[0] #Output One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side. #OutputOne of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side. STEPS TO CLEAN THE REVIEWS : Remove HTML tagsRemove special charactersConvert everything to lowercaseRemove stopwordsStemming 1. Remove HTML tags Regex rule : ‘<.*?>’ def clean(text): cleaned = re.compile(r'<.*?>') return re.sub(cleaned,'',text) data.review = data.review.apply(clean) data.review[0] def clean(text): cleaned = re.compile(r'<.*?>') return re.sub(cleaned,'',text)​data.review = data.review.apply(clean)data.review[0] #Output ""One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side."" #Output""One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side."" 2. Remove special characters def is_special(text): rem = '' for i in text: if i.isalnum(): rem = rem + i else: rem = rem + ' ' return rem data.review = data.review.apply(is_special) data.review[0] def is_special(text): rem = '' for i in text: if i.isalnum(): rem = rem + i else: rem = rem + ' ' return rem​data.review = data.review.apply(is_special)data.review[0] #Output 'One of the other reviewers has mentioned that after watching just 1 Oz episode you ll be hooked They are right as this is exactly what happened with me The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the word It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to many Aryans Muslims gangstas Latinos Christians Italians Irish and more so scuffles death stares dodgy dealings and shady agreements are never far away I would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare Forget pretty pictures painted for mainstream audiences forget charm forget romance OZ doesn t mess around The first episode I ever saw struck me as so nasty it was surreal I couldn t say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards who ll be sold out for a nickel inmates who ll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side ' #Output'One of the other reviewers has mentioned that after watching just 1 Oz episode you ll be hooked They are right as this is exactly what happened with me The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the word It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to many Aryans Muslims gangstas Latinos Christians Italians Irish and more so scuffles death stares dodgy dealings and shady agreements are never far away I would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare Forget pretty pictures painted for mainstream audiences forget charm forget romance OZ doesn t mess around The first episode I ever saw struck me as so nasty it was surreal I couldn t say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards who ll be sold out for a nickel inmates who ll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side ' 3. Convert everything to lowercase def to_lower(text): return text.lower() data.review = data.review.apply(to_lower) data.review[0] def to_lower(text): return text.lower()​data.review = data.review.apply(to_lower)data.review[0] #Output 'one of the other reviewers has mentioned that after watching just 1 oz episode you ll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the word it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to many aryans muslims gangstas latinos christians italians irish and more so scuffles death stares dodgy dealings and shady agreements are never far away i would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare forget pretty pictures painted for mainstream audiences forget charm forget romance oz doesn t mess around the first episode i ever saw struck me as so nasty it was surreal i couldn t say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards who ll be sold out for a nickel inmates who ll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side ' #Output'one of the other reviewers has mentioned that after watching just 1 oz episode you ll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the word it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to many aryans muslims gangstas latinos christians italians irish and more so scuffles death stares dodgy dealings and shady agreements are never far away i would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare forget pretty pictures painted for mainstream audiences forget charm forget romance oz doesn t mess around the first episode i ever saw struck me as so nasty it was surreal i couldn t say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards who ll be sold out for a nickel inmates who ll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side ' 4. Remove stopwords def rem_stopwords(text): stop_words = set(stopwords.words('english')) words = word_tokenize(text) return [w for w in words if w not in stop_words] data.review = data.review.apply(rem_stopwords) data.review[0] def rem_stopwords(text): stop_words = set(stopwords.words('english')) words = word_tokenize(text) return [w for w in words if w not in stop_words]​data.review = data.review.apply(rem_stopwords)data.review[0] 5. Stem the words def stem_txt(text): ss = SnowballStemmer('english') return "" "".join([ss.stem(w) for w in text]) data.review = data.review.apply(stem_txt) data.review[0] def stem_txt(text): ss = SnowballStemmer('english') return "" "".join([ss.stem(w) for w in text])​data.review = data.review.apply(stem_txt)data.review[0] data.head() data.head() CREATING THE MODEL 1. Creating Bag Of Words (BOW) X = np.array(data.iloc[:,0].values) y = np.array(data.sentiment.values) cv = CountVectorizer(max_features = 1000) X = cv.fit_transform(data.review).toarray() print(""X.shape = "",X.shape) print(""y.shape = "",y.shape) X = np.array(data.iloc[:,0].values)y = np.array(data.sentiment.values)cv = CountVectorizer(max_features = 1000)X = cv.fit_transform(data.review).toarray()print(""X.shape = "",X.shape)print(""y.shape = "",y.shape) #Output X.shape = (50000, 1000) y.shape = (50000,) #OutputX.shape = (50000, 1000)y.shape = (50000,) print(X) print(X) #Output array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 1, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 1, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]]) #Outputarray([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 1, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 1, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]]) 2. Train test split trainx,testx,trainy,testy = train_test_split(X,y,test_size=0.2,random_state=9) print(""Train shapes : X = {}, y = {}"".format(trainx.shape,trainy.shape)) print(""Test shapes : X = {}, y = {}"".format(testx.shape,testy.shape)) trainx,testx,trainy,testy = train_test_split(X,y,test_size=0.2,random_state=9)print(""Train shapes : X = {}, y = {}"".format(trainx.shape,trainy.shape))print(""Test shapes : X = {}, y = {}"".format(testx.shape,testy.shape)) #Output Train shapes : X = (40000, 1000), y = (40000,) Test shapes : X = (10000, 1000), y = (10000,) #OutputTrain shapes : X = (40000, 1000), y = (40000,)Test shapes : X = (10000, 1000), y = (10000,) 3. Defining the models and Training them gnb,mnb,bnb = GaussianNB(),MultinomialNB(alpha=1.0,fit_prior=True),BernoulliNB(alpha=1.0,fit_prior=True) gnb.fit(trainx,trainy) mnb.fit(trainx,trainy) bnb.fit(trainx,trainy) gnb,mnb,bnb = GaussianNB(),MultinomialNB(alpha=1.0,fit_prior=True),BernoulliNB(alpha=1.0,fit_prior=True)gnb.fit(trainx,trainy)mnb.fit(trainx,trainy)bnb.fit(trainx,trainy) #Output BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) #OutputBernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) 4. Prediction and accuracy metrics to choose best model ypg = gnb.predict(testx) ypm = mnb.predict(testx) ypb = bnb.predict(testx) print(""Gaussian = "",accuracy_score(testy,ypg)) print(""Multinomial = "",accuracy_score(testy,ypm)) print(""Bernoulli = "",accuracy_score(testy,ypb)) ypg = gnb.predict(testx)ypm = mnb.predict(testx)ypb = bnb.predict(testx)​print(""Gaussian = "",accuracy_score(testy,ypg))print(""Multinomial = "",accuracy_score(testy,ypm))print(""Bernoulli = "",accuracy_score(testy,ypb)) #Output Gaussian = 0.7843 Multinomial = 0.831 Bernoulli = 0.8386 #OutputGaussian = 0.7843Multinomial = 0.831Bernoulli = 0.8386 pickle.dump(bnb,open('model1.pkl','wb')) pickle.dump(bnb,open('model1.pkl','wb')) rev = """"""Terrible. Complete trash. Brainless tripe. Insulting to anyone who isn't an 8 year old fan boy. Im actually pretty disgusted that this movie is making the money it is - what does it say about the people who brainlessly hand over the hard earned cash to be 'entertained' in this fashion and then come here to leave a positive 8.8 review?? Oh yes, they are morons. Its the only sensible conclusion to draw. How anyone can rate this movie amongst the pantheon of great titles is beyond me. So trying to find something constructive to say about this title is hard...I enjoyed Iron Man? Tony Stark is an inspirational character in his own movies but here he is a pale shadow of that...About the only 'hook' this movie had into me was wondering when and if Iron Man would knock Captain America out...Oh how I wished he had :( What were these other characters anyways? Useless, bickering idiots who really couldn't organise happy times in a brewery. The film was a chaotic mish mash of action elements and failed 'set pieces'... I found the villain to be quite amusing. And now I give up. This movie is not robbing any more of my time but I felt I ought to contribute to restoring the obvious fake rating and reviews this movie has been getting on IMDb."""""" f1 = clean(rev) f2 = is_special(f1) f3 = to_lower(f2) f4 = rem_stopwords(f3) f5 = stem_txt(f4) bow,words = [],word_tokenize(f5) for word in words: bow.append(words.count(word)) #np.array(bow).reshape(1,3000) #bow.shape word_dict = cv.vocabulary_ pickle.dump(word_dict,open('bow.pkl','wb')) rev = """"""Terrible. Complete trash. Brainless tripe. Insulting to anyone who isn't an 8 year old fan boy. Im actually pretty disgusted that this movie is making the money it is - what does it say about the people who brainlessly hand over the hard earned cash to be 'entertained' in this fashion and then come here to leave a positive 8.8 review?? Oh yes, they are morons. Its the only sensible conclusion to draw. How anyone can rate this movie amongst the pantheon of great titles is beyond me.​So trying to find something constructive to say about this title is hard...I enjoyed Iron Man? Tony Stark is an inspirational character in his own movies but here he is a pale shadow of that...About the only 'hook' this movie had into me was wondering when and if Iron Man would knock Captain America out...Oh how I wished he had :( What were these other characters anyways? Useless, bickering idiots who really couldn't organise happy times in a brewery. The film was a chaotic mish mash of action elements and failed 'set pieces'...​I found the villain to be quite amusing.​And now I give up. This movie is not robbing any more of my time but I felt I ought to contribute to restoring the obvious fake rating and reviews this movie has been getting on IMDb.""""""f1 = clean(rev)f2 = is_special(f1)f3 = to_lower(f2)f4 = rem_stopwords(f3)f5 = stem_txt(f4)​bow,words = [],word_tokenize(f5)for word in words: bow.append(words.count(word))#np.array(bow).reshape(1,3000)#bow.shapeword_dict = cv.vocabulary_pickle.dump(word_dict,open('bow.pkl','wb')) inp = [] for i in word_dict: inp.append(f5.count(i[0])) y_pred = bnb.predict(np.array(inp).reshape(1,1000)) inp = []for i in word_dict: inp.append(f5.count(i[0]))y_pred = bnb.predict(np.array(inp).reshape(1,1000)) [0] 0 mean negative. I hope it will help you. Follow us on Instagram for all your Queries Instagram";Movie Reviews Sentiment Analysis -Binary Classification with Machine Learning
2020-05-26 13:06:23;In this Data Science Project, I am investigating the dataset “Countries of the World”. I will be focusing on the factors affecting a country’s GDP per capita and try to make a model using the data of 227 countries from the dataset. I will also briefly discuss the total GDP.Let’s start by importing the required Python libraries You can download the data set we need for this task from here:Let’s look at the dataWe noticed that there are some missing data in the table. For simplicity, I will just fill the missing data using the median of the region that a country belongs, as countries that are close geologically are often similar in many ways. For example, lets check the region median of ‘GDP ($ per capita)’, ‘Literacy (%)’ and ‘Agriculture’. Note that for ‘climate’ we use the mode instead of median as it seems that ‘climate’ is a categorical feature here.;https://thecleverprogrammer.com/2020/05/26/data-science-project-gdp-analysis/;['sklearn'];1.0;[];['Random Forest', 'Linear Regression', 'Regression'];['regression', 'linear regression', 'predict', 'fit', 'model', 'random forest', 'ground truth', 'train', 'label', 'rank'];"In this Data Science Project, I am investigating the dataset “Countries of the World”. I will be focusing on the factors affecting a country’s GDP per capita and try to make a model using the data of 227 countries from the dataset. I will also briefly discuss the total GDP. Let’s start by importing the required Python libraries import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import seaborn as sns from matplotlib import pyplot as plt from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error, mean_squared_log_error import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)import seaborn as snsfrom matplotlib import pyplot as plt​from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.ensemble import RandomForestRegressorfrom sklearn.metrics import mean_squared_error, mean_squared_log_error You can download the data set we need for this task from here: worldDownload Let’s look at the data data = pd.read_csv('world.csv',decimal=',') print('number of missing data:') print(data.isnull().sum()) data.describe(include='all') data = pd.read_csv('world.csv',decimal=',')print('number of missing data:')print(data.isnull().sum())data.describe(include='all') #Output number of missing data: Country 0 Region 0 Population 0 Area (sq. mi.) 0 Pop. Density (per sq. mi.) 0 Coastline (coast/area ratio) 0 Net migration 3 Infant mortality (per 1000 births) 3 GDP ($ per capita) 1 Literacy (%) 18 Phones (per 1000) 4 Arable (%) 2 Crops (%) 2 Other (%) 2 Climate 22 Birthrate 3 Deathrate 4 Agriculture 15 Industry 16 Service 15 dtype: int64 #Outputnumber of missing data:Country 0Region 0Population 0Area (sq. mi.) 0Pop. Density (per sq. mi.) 0Coastline (coast/area ratio) 0Net migration 3Infant mortality (per 1000 births) 3GDP ($ per capita) 1Literacy (%) 18Phones (per 1000) 4Arable (%) 2Crops (%) 2Other (%) 2Climate 22Birthrate 3Deathrate 4Agriculture 15Industry 16Service 15dtype: int64 Data Preparation – fill in missing values We noticed that there are some missing data in the table. For simplicity, I will just fill the missing data using the median of the region that a country belongs, as countries that are close geologically are often similar in many ways. For example, lets check the region median of ‘GDP ($ per capita)’, ‘Literacy (%)’ and ‘Agriculture’. Note that for ‘climate’ we use the mode instead of median as it seems that ‘climate’ is a categorical feature here. data.groupby('Region')[['GDP ($ per capita)','Literacy (%)','Agriculture']].median() data.groupby('Region')[['GDP ($ per capita)','Literacy (%)','Agriculture']].median() for col in data.columns.values: if data[col].isnull().sum() == 0: continue if col == 'Climate': guess_values = data.groupby('Region')['Climate'].apply(lambda x: x.mode().max()) else: guess_values = data.groupby('Region')[col].median() for region in data['Region'].unique(): data[col].loc[(data[col].isnull())&(data['Region']==region)] = guess_values[region] for col in data.columns.values: if data[col].isnull().sum() == 0: continue if col == 'Climate': guess_values = data.groupby('Region')['Climate'].apply(lambda x: x.mode().max()) else: guess_values = data.groupby('Region')[col].median() for region in data['Region'].unique(): data[col].loc[(data[col].isnull())&(data['Region']==region)] = guess_values[region] Data Exploration Top Countries with highest GDP per capita Look at the top 20 countries with highest GDP per capita. Luxembourg is quite ahead, the next 19 countries are close. German, the 20th has about 2.5 times GDP per capita of the world average. fig, ax = plt.subplots(figsize=(16,6)) #ax = fig.add_subplot(111) top_gdp_countries = data.sort_values('GDP ($ per capita)',ascending=False).head(20) mean = pd.DataFrame({'Country':['World mean'], 'GDP ($ per capita)':[data['GDP ($ per capita)'].mean()]}) gdps = pd.concat([top_gdp_countries[['Country','GDP ($ per capita)']],mean],ignore_index=True) sns.barplot(x='Country',y='GDP ($ per capita)',data=gdps, palette='Set3') ax.set_xlabel(ax.get_xlabel(),labelpad=15) ax.set_ylabel(ax.get_ylabel(),labelpad=30) ax.xaxis.label.set_fontsize(16) ax.yaxis.label.set_fontsize(16) plt.xticks(rotation=90) plt.show() fig, ax = plt.subplots(figsize=(16,6))#ax = fig.add_subplot(111)top_gdp_countries = data.sort_values('GDP ($ per capita)',ascending=False).head(20)mean = pd.DataFrame({'Country':['World mean'], 'GDP ($ per capita)':[data['GDP ($ per capita)'].mean()]})gdps = pd.concat([top_gdp_countries[['Country','GDP ($ per capita)']],mean],ignore_index=True)​sns.barplot(x='Country',y='GDP ($ per capita)',data=gdps, palette='Set3')ax.set_xlabel(ax.get_xlabel(),labelpad=15)ax.set_ylabel(ax.get_ylabel(),labelpad=30)ax.xaxis.label.set_fontsize(16)ax.yaxis.label.set_fontsize(16)plt.xticks(rotation=90)plt.show() Correlation between Variables The heatmap shows the correlation between all numerical columns. plt.figure(figsize=(16,12)) sns.heatmap(data=data.iloc[:,2:].corr(),annot=True,fmt='.2f',cmap='coolwarm') plt.show() plt.figure(figsize=(16,12))sns.heatmap(data=data.iloc[:,2:].corr(),annot=True,fmt='.2f',cmap='coolwarm')plt.show() Top Factors affecting GDP per capita We pick the six columns that mostly correlated to GDP per capita and make scatter plots. The results agree with our common sense. Also we notice there are many countries with low average GDP and few with high average GDP —- a pyramid structure. fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20,12)) plt.subplots_adjust(hspace=0.4) corr_to_gdp = pd.Series() for col in data.columns.values[2:]: if ((col!='GDP ($ per capita)')&(col!='Climate')): corr_to_gdp[col] = data['GDP ($ per capita)'].corr(data[col]) abs_corr_to_gdp = corr_to_gdp.abs().sort_values(ascending=False) corr_to_gdp = corr_to_gdp.loc[abs_corr_to_gdp.index] for i in range(2): for j in range(3): sns.regplot(x=corr_to_gdp.index.values[i*3+j], y='GDP ($ per capita)', data=data, ax=axes[i,j], fit_reg=False, marker='.') title = 'correlation='+str(corr_to_gdp[i*3+j]) axes[i,j].set_title(title) axes[1,2].set_xlim(0,102) plt.show() fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20,12))plt.subplots_adjust(hspace=0.4)​corr_to_gdp = pd.Series()for col in data.columns.values[2:]: if ((col!='GDP ($ per capita)')&(col!='Climate')): corr_to_gdp[col] = data['GDP ($ per capita)'].corr(data[col])abs_corr_to_gdp = corr_to_gdp.abs().sort_values(ascending=False)corr_to_gdp = corr_to_gdp.loc[abs_corr_to_gdp.index]​for i in range(2): for j in range(3): sns.regplot(x=corr_to_gdp.index.values[i*3+j], y='GDP ($ per capita)', data=data, ax=axes[i,j], fit_reg=False, marker='.') title = 'correlation='+str(corr_to_gdp[i*3+j]) axes[i,j].set_title(title)axes[1,2].set_xlim(0,102)plt.show() Countries with low Birthrate and low GDP per capita Some features, like phones, are related to the average GDP more linearly, while others are not. For example, High birthrate usually means low GDP per capita, but average GDP in low birthrate countries can vary a lot. Let’s look at the countries with low birthrate (<14%) and low GDP per capita (<10000 $). They also have hight literacy, like other high average GDP countires. But we hope their other features can help distiguish them from those with low birthrate but high average GDPs, like service are not quite an importent portion in their economy, not a lot phone procession, some have negative net migration, and many of them are from eastern Europe or C.W. of IND. STATES, so the ‘region’ feature may also be useful. data.loc[(data['Birthrate']<14)&(data['GDP ($ per capita)']<10000)] data.loc[(data['Birthrate']<14)&(data['GDP ($ per capita)']<10000)] Modeling Training and Testing First label encode the categorical features ‘Region’ and ‘Climate’, and I will just use all features given in the data set without further feature engineering. LE = LabelEncoder() data['Region_label'] = LE.fit_transform(data['Region']) data['Climate_label'] = LE.fit_transform(data['Climate']) data.head() LE = LabelEncoder()data['Region_label'] = LE.fit_transform(data['Region'])data['Climate_label'] = LE.fit_transform(data['Climate'])data.head() train, test = train_test_split(data, test_size=0.3, shuffle=True) training_features = ['Population', 'Area (sq. mi.)', 'Pop. Density (per sq. mi.)', 'Coastline (coast/area ratio)', 'Net migration', 'Infant mortality (per 1000 births)', 'Literacy (%)', 'Phones (per 1000)', 'Arable (%)', 'Crops (%)', 'Other (%)', 'Birthrate', 'Deathrate', 'Agriculture', 'Industry', 'Service', 'Region_label', 'Climate_label','Service'] target = 'GDP ($ per capita)' train_X = train[training_features] train_Y = train[target] test_X = test[training_features] test_Y = test[target] train, test = train_test_split(data, test_size=0.3, shuffle=True)training_features = ['Population', 'Area (sq. mi.)', 'Pop. Density (per sq. mi.)', 'Coastline (coast/area ratio)', 'Net migration', 'Infant mortality (per 1000 births)', 'Literacy (%)', 'Phones (per 1000)', 'Arable (%)', 'Crops (%)', 'Other (%)', 'Birthrate', 'Deathrate', 'Agriculture', 'Industry', 'Service', 'Region_label', 'Climate_label','Service']target = 'GDP ($ per capita)'train_X = train[training_features]train_Y = train[target]test_X = test[training_features]test_Y = test[target] First let’s try the linear regression model. As for metric, I will check both root mean squared error and mean squared log error. model = LinearRegression() model.fit(train_X, train_Y) train_pred_Y = model.predict(train_X) test_pred_Y = model.predict(test_X) train_pred_Y = pd.Series(train_pred_Y.clip(0, train_pred_Y.max()), index=train_Y.index) test_pred_Y = pd.Series(test_pred_Y.clip(0, test_pred_Y.max()), index=test_Y.index) rmse_train = np.sqrt(mean_squared_error(train_pred_Y, train_Y)) msle_train = mean_squared_log_error(train_pred_Y, train_Y) rmse_test = np.sqrt(mean_squared_error(test_pred_Y, test_Y)) msle_test = mean_squared_log_error(test_pred_Y, test_Y) print('rmse_train:',rmse_train,'msle_train:',msle_train) print('rmse_test:',rmse_test,'msle_test:',msle_test) model = LinearRegression()model.fit(train_X, train_Y)train_pred_Y = model.predict(train_X)test_pred_Y = model.predict(test_X)train_pred_Y = pd.Series(train_pred_Y.clip(0, train_pred_Y.max()), index=train_Y.index)test_pred_Y = pd.Series(test_pred_Y.clip(0, test_pred_Y.max()), index=test_Y.index)​rmse_train = np.sqrt(mean_squared_error(train_pred_Y, train_Y))msle_train = mean_squared_log_error(train_pred_Y, train_Y)rmse_test = np.sqrt(mean_squared_error(test_pred_Y, test_Y))msle_test = mean_squared_log_error(test_pred_Y, test_Y)​print('rmse_train:',rmse_train,'msle_train:',msle_train)print('rmse_test:',rmse_test,'msle_test:',msle_test) #Output rmse_train: 4758.157709814054 msle_train: 5.3698006522946615 rmse_test: 5094.846170365958 msle_test: 4.385275374909623 #Outputrmse_train: 4758.157709814054 msle_train: 5.3698006522946615rmse_test: 5094.846170365958 msle_test: 4.385275374909623 As we know the target not linear with many features, it is worth trying some nonlinear models. For example, the random forest model: model = RandomForestRegressor(n_estimators = 50, max_depth = 6, min_weight_fraction_leaf = 0.05, max_features = 0.8, random_state = 42) model.fit(train_X, train_Y) train_pred_Y = model.predict(train_X) test_pred_Y = model.predict(test_X) train_pred_Y = pd.Series(train_pred_Y.clip(0, train_pred_Y.max()), index=train_Y.index) test_pred_Y = pd.Series(test_pred_Y.clip(0, test_pred_Y.max()), index=test_Y.index) rmse_train = np.sqrt(mean_squared_error(train_pred_Y, train_Y)) msle_train = mean_squared_log_error(train_pred_Y, train_Y) rmse_test = np.sqrt(mean_squared_error(test_pred_Y, test_Y)) msle_test = mean_squared_log_error(test_pred_Y, test_Y) print('rmse_train:',rmse_train,'msle_train:',msle_train) print('rmse_test:',rmse_test,'msle_test:',msle_test) model = RandomForestRegressor(n_estimators = 50, max_depth = 6, min_weight_fraction_leaf = 0.05, max_features = 0.8, random_state = 42)model.fit(train_X, train_Y)train_pred_Y = model.predict(train_X)test_pred_Y = model.predict(test_X)train_pred_Y = pd.Series(train_pred_Y.clip(0, train_pred_Y.max()), index=train_Y.index)test_pred_Y = pd.Series(test_pred_Y.clip(0, test_pred_Y.max()), index=test_Y.index)​rmse_train = np.sqrt(mean_squared_error(train_pred_Y, train_Y))msle_train = mean_squared_log_error(train_pred_Y, train_Y)rmse_test = np.sqrt(mean_squared_error(test_pred_Y, test_Y))msle_test = mean_squared_log_error(test_pred_Y, test_Y)​print('rmse_train:',rmse_train,'msle_train:',msle_train)print('rmse_test:',rmse_test,'msle_test:',msle_test) #Output rmse_train: 3150.4941453460574 msle_train: 0.17932710000306362 rmse_test: 3772.4650915245516 msle_test: 0.2156354026094192 #Outputrmse_train: 3150.4941453460574 msle_train: 0.17932710000306362rmse_test: 3772.4650915245516 msle_test: 0.2156354026094192 Visualization of Results To see how the model is doing, we can make scatter plot of prediction against ground truth. The model gives a reasonable prediction, as the data points are gathering around the line y=x. plt.figure(figsize=(18,12)) train_test_Y = train_Y.append(test_Y) train_test_pred_Y = train_pred_Y.append(test_pred_Y) data_shuffled = data.loc[train_test_Y.index] label = data_shuffled['Country'] colors = {'ASIA (EX. NEAR EAST) ':'red', 'EASTERN EUROPE ':'orange', 'NORTHERN AFRICA ':'gold', 'OCEANIA ':'green', 'WESTERN EUROPE ':'blue', 'SUB-SAHARAN AFRICA ':'purple', 'LATIN AMER. & CARIB ':'olive', 'C.W. OF IND. STATES ':'cyan', 'NEAR EAST ':'hotpink', 'NORTHERN AMERICA ':'lightseagreen', 'BALTICS ':'rosybrown'} for region, color in colors.items(): X = train_test_Y.loc[data_shuffled['Region']==region] Y = train_test_pred_Y.loc[data_shuffled['Region']==region] ax = sns.regplot(x=X, y=Y, marker='.', fit_reg=False, color=color, scatter_kws={'s':200, 'linewidths':0}, label=region) plt.legend(loc=4,prop={'size': 12}) ax.set_xlabel('GDP ($ per capita) ground truth',labelpad=40) ax.set_ylabel('GDP ($ per capita) predicted',labelpad=40) ax.xaxis.label.set_fontsize(24) ax.yaxis.label.set_fontsize(24) ax.tick_params(labelsize=12) x = np.linspace(-1000,50000,100) # 100 linearly spaced numbers y = x plt.plot(x,y,c='gray') plt.xlim(-1000,60000) plt.ylim(-1000,40000) for i in range(0,train_test_Y.shape[0]): if((data_shuffled['Area (sq. mi.)'].iloc[i]>8e5) | (data_shuffled['Population'].iloc[i]>1e8) | (data_shuffled['GDP ($ per capita)'].iloc[i]>10000)): plt.text(train_test_Y.iloc[i]+200, train_test_pred_Y.iloc[i]-200, label.iloc[i], size='small') plt.figure(figsize=(18,12))​train_test_Y = train_Y.append(test_Y)train_test_pred_Y = train_pred_Y.append(test_pred_Y)​data_shuffled = data.loc[train_test_Y.index]label = data_shuffled['Country']​colors = {'ASIA (EX. NEAR EAST) ':'red', 'EASTERN EUROPE ':'orange', 'NORTHERN AFRICA ':'gold', 'OCEANIA ':'green', 'WESTERN EUROPE ':'blue', 'SUB-SAHARAN AFRICA ':'purple', 'LATIN AMER. & CARIB ':'olive', 'C.W. OF IND. STATES ':'cyan', 'NEAR EAST ':'hotpink', 'NORTHERN AMERICA ':'lightseagreen', 'BALTICS ':'rosybrown'}​for region, color in colors.items(): X = train_test_Y.loc[data_shuffled['Region']==region] Y = train_test_pred_Y.loc[data_shuffled['Region']==region] ax = sns.regplot(x=X, y=Y, marker='.', fit_reg=False, color=color, scatter_kws={'s':200, 'linewidths':0}, label=region) plt.legend(loc=4,prop={'size': 12}) ​ax.set_xlabel('GDP ($ per capita) ground truth',labelpad=40)ax.set_ylabel('GDP ($ per capita) predicted',labelpad=40)ax.xaxis.label.set_fontsize(24)ax.yaxis.label.set_fontsize(24)ax.tick_params(labelsize=12)​x = np.linspace(-1000,50000,100) # 100 linearly spaced numbersy = xplt.plot(x,y,c='gray')​plt.xlim(-1000,60000)plt.ylim(-1000,40000)​for i in range(0,train_test_Y.shape[0]): if((data_shuffled['Area (sq. mi.)'].iloc[i]>8e5) | (data_shuffled['Population'].iloc[i]>1e8) | (data_shuffled['GDP ($ per capita)'].iloc[i]>10000)): plt.text(train_test_Y.iloc[i]+200, train_test_pred_Y.iloc[i]-200, label.iloc[i], size='small') Total GDP Top Countries It is also interesting to look at the total GDPs, which I take as ‘GDP ($ per capita)’ × ‘Population’. Here are the top 10 countries with highest total GDPs, their GDP make up to about 2/3 of the global GDP. data['Total_GDP ($)'] = data['GDP ($ per capita)'] * data['Population'] #plt.figure(figsize=(16,6)) top_gdp_countries = data.sort_values('Total_GDP ($)',ascending=False).head(10) other = pd.DataFrame({'Country':['Other'], 'Total_GDP ($)':[data['Total_GDP ($)'].sum() - top_gdp_countries['Total_GDP ($)'].sum()]}) gdps = pd.concat([top_gdp_countries[['Country','Total_GDP ($)']],other],ignore_index=True) fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7),gridspec_kw = {'width_ratios':[2,1]}) sns.barplot(x='Country',y='Total_GDP ($)',data=gdps,ax=axes[0],palette='Set3') axes[0].set_xlabel('Country',labelpad=30,fontsize=16) axes[0].set_ylabel('Total_GDP',labelpad=30,fontsize=16) colors = sns.color_palette(""Set3"", gdps.shape[0]).as_hex() axes[1].pie(gdps['Total_GDP ($)'], labels=gdps['Country'],colors=colors,autopct='%1.1f%%',shadow=True) axes[1].axis('equal') plt.show() data['Total_GDP ($)'] = data['GDP ($ per capita)'] * data['Population']#plt.figure(figsize=(16,6))top_gdp_countries = data.sort_values('Total_GDP ($)',ascending=False).head(10)other = pd.DataFrame({'Country':['Other'], 'Total_GDP ($)':[data['Total_GDP ($)'].sum() - top_gdp_countries['Total_GDP ($)'].sum()]})gdps = pd.concat([top_gdp_countries[['Country','Total_GDP ($)']],other],ignore_index=True)​fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7),gridspec_kw = {'width_ratios':[2,1]})sns.barplot(x='Country',y='Total_GDP ($)',data=gdps,ax=axes[0],palette='Set3')axes[0].set_xlabel('Country',labelpad=30,fontsize=16)axes[0].set_ylabel('Total_GDP',labelpad=30,fontsize=16)​colors = sns.color_palette(""Set3"", gdps.shape[0]).as_hex()axes[1].pie(gdps['Total_GDP ($)'], labels=gdps['Country'],colors=colors,autopct='%1.1f%%',shadow=True)axes[1].axis('equal')plt.show() Let’s compare the above ten countries’ rank in total GDP and GDP per capita. Rank1 = data[['Country','Total_GDP ($)']].sort_values('Total_GDP ($)', ascending=False).reset_index() Rank2 = data[['Country','GDP ($ per capita)']].sort_values('GDP ($ per capita)', ascending=False).reset_index() Rank1 = pd.Series(Rank1.index.values+1, index=Rank1.Country) Rank2 = pd.Series(Rank2.index.values+1, index=Rank2.Country) Rank_change = (Rank2-Rank1).sort_values(ascending=False) print('rank of total GDP - rank of GDP per capita:') Rank_change.loc[top_gdp_countries.Country] Rank1 = data[['Country','Total_GDP ($)']].sort_values('Total_GDP ($)', ascending=False).reset_index()Rank2 = data[['Country','GDP ($ per capita)']].sort_values('GDP ($ per capita)', ascending=False).reset_index()Rank1 = pd.Series(Rank1.index.values+1, index=Rank1.Country)Rank2 = pd.Series(Rank2.index.values+1, index=Rank2.Country)Rank_change = (Rank2-Rank1).sort_values(ascending=False)print('rank of total GDP - rank of GDP per capita:')Rank_change.loc[top_gdp_countries.Country] #Output rank of total GDP - rank of GDP per capita: Country United States 1 China 118 Japan 14 India 146 Germany 15 France 15 United Kingdom 12 Italy 17 Brazil 84 Russia 75 dtype: int64 #Outputrank of total GDP - rank of GDP per capita:CountryUnited States 1China 118Japan 14India 146Germany 15France 15United Kingdom 12Italy 17Brazil 84Russia 75dtype: int64 We see the countries with high total GDPs are quite different from those with high average GDPs. China and India jump above a lot when it comes to the total GDP. The only country that is with in top 10 (in fact top 2) for both total and average GDPs is the United States. Factors affecting Total GDP We can also check the correlation between total GDP and the other columns. The top two factors are population and area, following many factors that have also been found mostly correlated to GDP per capita. corr_to_gdp = pd.Series() for col in data.columns.values[2:]: if ((col!='Total_GDP ($)')&(col!='Climate')&(col!='GDP ($ per capita)')): corr_to_gdp[col] = data['Total_GDP ($)'].corr(data[col]) abs_corr_to_gdp = corr_to_gdp.abs().sort_values(ascending=False) corr_to_gdp = corr_to_gdp.loc[abs_corr_to_gdp.index] print(corr_to_gdp) corr_to_gdp = pd.Series()for col in data.columns.values[2:]: if ((col!='Total_GDP ($)')&(col!='Climate')&(col!='GDP ($ per capita)')): corr_to_gdp[col] = data['Total_GDP ($)'].corr(data[col])abs_corr_to_gdp = corr_to_gdp.abs().sort_values(ascending=False)corr_to_gdp = corr_to_gdp.loc[abs_corr_to_gdp.index]print(corr_to_gdp) #Output Population 0.639528 Area (sq. mi.) 0.556396 Phones (per 1000) 0.233484 Birthrate -0.166889 Agriculture -0.139516 Arable (%) 0.129928 Climate_label 0.125791 Infant mortality (per 1000 births) -0.122076 Literacy (%) 0.099417 Service 0.085096 Region_label -0.079745 Crops (%) -0.077078 Coastline (coast/area ratio) -0.065211 Other (%) -0.064882 Net migration 0.054632 Industry 0.050399 Deathrate -0.035820 Pop. Density (per sq. mi.) -0.028487 dtype: float64 #OutputPopulation 0.639528Area (sq. mi.) 0.556396Phones (per 1000) 0.233484Birthrate -0.166889Agriculture -0.139516Arable (%) 0.129928Climate_label 0.125791Infant mortality (per 1000 births) -0.122076Literacy (%) 0.099417Service 0.085096Region_label -0.079745Crops (%) -0.077078Coastline (coast/area ratio) -0.065211Other (%) -0.064882Net migration 0.054632Industry 0.050399Deathrate -0.035820Pop. Density (per sq. mi.) -0.028487dtype: float64 Comparison of the Top 10 Finally, let us do a comparison of the economy structure for the ten countries with highest total GDP. plot_data = top_gdp_countries.head(10)[['Country','Agriculture', 'Industry', 'Service']] plot_data = plot_data.set_index('Country') ax = plot_data.plot.bar(stacked=True,figsize=(10,6)) ax.legend(bbox_to_anchor=(1, 1)) plt.show() plot_data = top_gdp_countries.head(10)[['Country','Agriculture', 'Industry', 'Service']]plot_data = plot_data.set_index('Country')ax = plot_data.plot.bar(stacked=True,figsize=(10,6))ax.legend(bbox_to_anchor=(1, 1))plt.show() As well as their land usage: plot_data = top_gdp_countries[['Country','Arable (%)', 'Crops (%)', 'Other (%)']] plot_data = plot_data.set_index('Country') ax = plot_data.plot.bar(stacked=True,figsize=(10,6)) ax.legend(bbox_to_anchor=(1, 1)) plt.show() plot_data = top_gdp_countries[['Country','Arable (%)', 'Crops (%)', 'Other (%)']]plot_data = plot_data.set_index('Country')ax = plot_data.plot.bar(stacked=True,figsize=(10,6))ax.legend(bbox_to_anchor=(1, 1))plt.show() Follow us on Instagram for all your Queries Instagram";GDP Analysis with Data Science
2020-05-26 17:29:11;In this project we will be building a model that Predicts customer churn with Machine Learning. We do this by implementing a predictive model with the help of python.Prediction of Customer Churn means our beloved customers with the intention of leaving us in the future.Let’s Start by Importing the required LibrariesLet’s read and look at the dataTo show the number of rows and columns#Output(7043, 21)To see all column namesTo check for NA or missing valuesTo show some statisticsTo get Customer Churn countTo see the percentage of customers that are leavingTo remove unnecessary columnsTo show the data typesTo show first 5 rows of the new data;https://thecleverprogrammer.com/2020/05/26/data-science-project-predict-customer-churn-with-python-and-machine-learning/;['sklearn'];1.0;[];['ML', 'Classification', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'label'];"In this project we will be building a model that Predicts customer churn with Machine Learning. We do this by implementing a predictive model with the help of python. Prediction of Customer Churn means our beloved customers with the intention of leaving us in the future. Let’s Start by Importing the required Libraries import numpy as np import pandas as pd import sklearn import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split​x import numpy as npimport pandas as pdimport sklearnimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.preprocessing import LabelEncoderfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import classification_reportfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_split Download the data set churnDownload Let’s read and look at the data df = pd.read_csv(""churn.csv"") df df = pd.read_csv(""churn.csv"")df To show the number of rows and columns df.shape df.shape #Output(7043, 21) To see all column names df.columns.values df.columns.values #Output array(['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn'], dtype=object) #Outputarray(['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn'], dtype=object) To check for NA or missing values df.isna().sum() df.isna().sum() #Output customerID 0 gender 0 SeniorCitizen 0 Partner 0 Dependents 0 tenure 0 PhoneService 0 MultipleLines 0 InternetService 0 OnlineSecurity 0 OnlineBackup 0 DeviceProtection 0 TechSupport 0 StreamingTV 0 StreamingMovies 0 Contract 0 PaperlessBilling 0 PaymentMethod 0 MonthlyCharges 0 TotalCharges 0 Churn 0 dtype: int64 #OutputcustomerID 0gender 0SeniorCitizen 0Partner 0Dependents 0tenure 0PhoneService 0MultipleLines 0InternetService 0OnlineSecurity 0OnlineBackup 0DeviceProtection 0TechSupport 0StreamingTV 0StreamingMovies 0Contract 0PaperlessBilling 0PaymentMethod 0MonthlyCharges 0TotalCharges 0Churn 0dtype: int64 To show some statistics df.describe() df.describe() To get Customer Churn count df['Churn'].value_counts() df['Churn'].value_counts() #Output No 5174 Yes 1869 Name: Churn, dtype: int64 #OutputNo 5174Yes 1869Name: Churn, dtype: int64 Visualize the count of customer churn sns.countplot(df['Churn']) sns.countplot(df['Churn']) To see the percentage of customers that are leaving numRetained = df[df.Churn == 'No'].shape[0] numChurned = df[df.Churn == 'Yes'].shape[0] # print the percentage of customers that stayed print(numRetained/(numRetained + numChurned) * 100,'% of customers stayed in the company') # peint the percentage of customers that left print(numChurned/(numRetained + numChurned) * 100, '% of customers left with the company') numRetained = df[df.Churn == 'No'].shape[0]numChurned = df[df.Churn == 'Yes'].shape[0]​# print the percentage of customers that stayedprint(numRetained/(numRetained + numChurned) * 100,'% of customers stayed in the company')# peint the percentage of customers that leftprint(numChurned/(numRetained + numChurned) * 100, '% of customers left with the company') #Output 73.4630129206304 % of customers stayed in the company 26.536987079369588 % of customers left with the company #Output73.4630129206304 % of customers stayed in the company26.536987079369588 % of customers left with the company Visualize the churn count for both males and females sns.countplot(x ='gender', hue='Churn', data=df) sns.countplot(x ='gender', hue='Churn', data=df) Visualize the churn count for the internet service sns.countplot(x='InternetService', hue='Churn', data=df) sns.countplot(x='InternetService', hue='Churn', data=df) To Visualize Numeric data numericFeatures = ['tenure', 'MonthlyCharges'] fig, ax = plt.subplots(1,2, figsize=(28, 8)) df[df.Churn == ""No""][numericFeatures].hist(bins=20, color='blue', alpha=0.5, ax=ax) df[df.Churn == ""Yes""][numericFeatures].hist(bins=20, color='orange', alpha=0.5, ax=ax) numericFeatures = ['tenure', 'MonthlyCharges']fig, ax = plt.subplots(1,2, figsize=(28, 8))df[df.Churn == ""No""][numericFeatures].hist(bins=20, color='blue', alpha=0.5, ax=ax)df[df.Churn == ""Yes""][numericFeatures].hist(bins=20, color='orange', alpha=0.5, ax=ax) To remove unnecessary columns cleanDF = df.drop('customerID', axis=1) cleanDF = df.drop('customerID', axis=1) Convert all the non-numeric columns to numeric Convert all the non-numeric columns to numeric for column in cleanDF.columns: if cleanDF[column].dtype == np.number: continue cleanDF[column] = LabelEncoder().fit_transform(cleanDF[column]) Convert all the non-numeric columns to numericfor column in cleanDF.columns: if cleanDF[column].dtype == np.number: continue cleanDF[column] = LabelEncoder().fit_transform(cleanDF[column]) To show the data types cleanDF.dtypes cleanDF.dtypes #Output gender int64 SeniorCitizen int64 Partner int64 Dependents int64 tenure int64 PhoneService int64 MultipleLines int64 InternetService int64 OnlineSecurity int64 OnlineBackup int64 DeviceProtection int64 TechSupport int64 StreamingTV int64 StreamingMovies int64 Contract int64 PaperlessBilling int64 PaymentMethod int64 MonthlyCharges float64 TotalCharges int64 Churn int64 dtype: object #Outputgender int64SeniorCitizen int64Partner int64Dependents int64tenure int64PhoneService int64MultipleLines int64InternetService int64OnlineSecurity int64OnlineBackup int64DeviceProtection int64TechSupport int64StreamingTV int64StreamingMovies int64Contract int64PaperlessBilling int64PaymentMethod int64MonthlyCharges float64TotalCharges int64Churn int64dtype: object To show first 5 rows of the new data Scale the data Scaled the data x = cleanDF.drop('Churn', axis=1) y = cleanDF['Churn'] x = StandardScaler().fit_transform(x) Scaled the datax = cleanDF.drop('Churn', axis=1)y = cleanDF['Churn']x = StandardScaler().fit_transform(x) Split the data into 80% training and 20% testing xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=42) xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=42) Create and Train the model model = LogisticRegression() # Train the model model.fit(xtrain, ytrain) model = LogisticRegression()# Train the modelmodel.fit(xtrain, ytrain) #Output LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) #OutputLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) Create the predictions on the test data predictions = model.predict(xtest) # print the predictions print(predictions) predictions = model.predict(xtest)​# print the predictionsprint(predictions) #Output [1 0 0 ... 0 0 0] #Output[1 0 0 ... 0 0 0] And Finally check the precision, recall and f1-score print(classification_report(ytest, predictions)) print(classification_report(ytest, predictions)) #Output precision recall f1-score support 0 0.85 0.91 0.88 1036 1 0.69 0.56 0.62 373 accuracy 0.82 1409 macro avg 0.77 0.74 0.75 1409 weighted avg 0.81 0.82 0.81 1409 #Output precision recall f1-score support​ 0 0.85 0.91 0.88 1036 1 0.69 0.56 0.62 373​ accuracy 0.82 1409 macro avg 0.77 0.74 0.75 1409weighted avg 0.81 0.82 0.81 1409";Predict Customer Churn with Python and Machine Learning
2020-05-26 17:29:11;In this project we will be building a model that Predicts customer churn with Machine Learning. We do this by implementing a predictive model with the help of python.Prediction of Customer Churn means our beloved customers with the intention of leaving us in the future.Let’s Start by Importing the required LibrariesLet’s read and look at the dataTo show the number of rows and columns#Output(7043, 21)To see all column namesTo check for NA or missing valuesTo show some statisticsTo get Customer Churn countTo see the percentage of customers that are leavingTo remove unnecessary columnsTo show the data typesTo show first 5 rows of the new data;https://thecleverprogrammer.com/2020/05/26/predict-customer-churn-with-python-and-machine-learning/;['sklearn'];1.0;[];['ML', 'Classification', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'label'];"In this project we will be building a model that Predicts customer churn with Machine Learning. We do this by implementing a predictive model with the help of python. Prediction of Customer Churn means our beloved customers with the intention of leaving us in the future. Let’s Start by Importing the required Libraries import numpy as np import pandas as pd import sklearn import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split​x import numpy as npimport pandas as pdimport sklearnimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.preprocessing import LabelEncoderfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import classification_reportfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_split Download the data set churnDownload Let’s read and look at the data df = pd.read_csv(""churn.csv"") df df = pd.read_csv(""churn.csv"")df To show the number of rows and columns df.shape df.shape #Output(7043, 21) To see all column names df.columns.values df.columns.values #Output array(['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn'], dtype=object) #Outputarray(['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn'], dtype=object) To check for NA or missing values df.isna().sum() df.isna().sum() #Output customerID 0 gender 0 SeniorCitizen 0 Partner 0 Dependents 0 tenure 0 PhoneService 0 MultipleLines 0 InternetService 0 OnlineSecurity 0 OnlineBackup 0 DeviceProtection 0 TechSupport 0 StreamingTV 0 StreamingMovies 0 Contract 0 PaperlessBilling 0 PaymentMethod 0 MonthlyCharges 0 TotalCharges 0 Churn 0 dtype: int64 #OutputcustomerID 0gender 0SeniorCitizen 0Partner 0Dependents 0tenure 0PhoneService 0MultipleLines 0InternetService 0OnlineSecurity 0OnlineBackup 0DeviceProtection 0TechSupport 0StreamingTV 0StreamingMovies 0Contract 0PaperlessBilling 0PaymentMethod 0MonthlyCharges 0TotalCharges 0Churn 0dtype: int64 To show some statistics df.describe() df.describe() To get Customer Churn count df['Churn'].value_counts() df['Churn'].value_counts() #Output No 5174 Yes 1869 Name: Churn, dtype: int64 #OutputNo 5174Yes 1869Name: Churn, dtype: int64 Visualize the count of customer churn sns.countplot(df['Churn']) sns.countplot(df['Churn']) To see the percentage of customers that are leaving numRetained = df[df.Churn == 'No'].shape[0] numChurned = df[df.Churn == 'Yes'].shape[0] # print the percentage of customers that stayed print(numRetained/(numRetained + numChurned) * 100,'% of customers stayed in the company') # peint the percentage of customers that left print(numChurned/(numRetained + numChurned) * 100, '% of customers left with the company') numRetained = df[df.Churn == 'No'].shape[0]numChurned = df[df.Churn == 'Yes'].shape[0]​# print the percentage of customers that stayedprint(numRetained/(numRetained + numChurned) * 100,'% of customers stayed in the company')# peint the percentage of customers that leftprint(numChurned/(numRetained + numChurned) * 100, '% of customers left with the company') #Output 73.4630129206304 % of customers stayed in the company 26.536987079369588 % of customers left with the company #Output73.4630129206304 % of customers stayed in the company26.536987079369588 % of customers left with the company Visualize the churn count for both males and females sns.countplot(x ='gender', hue='Churn', data=df) sns.countplot(x ='gender', hue='Churn', data=df) Visualize the churn count for the internet service sns.countplot(x='InternetService', hue='Churn', data=df) sns.countplot(x='InternetService', hue='Churn', data=df) To Visualize Numeric data numericFeatures = ['tenure', 'MonthlyCharges'] fig, ax = plt.subplots(1,2, figsize=(28, 8)) df[df.Churn == ""No""][numericFeatures].hist(bins=20, color='blue', alpha=0.5, ax=ax) df[df.Churn == ""Yes""][numericFeatures].hist(bins=20, color='orange', alpha=0.5, ax=ax) numericFeatures = ['tenure', 'MonthlyCharges']fig, ax = plt.subplots(1,2, figsize=(28, 8))df[df.Churn == ""No""][numericFeatures].hist(bins=20, color='blue', alpha=0.5, ax=ax)df[df.Churn == ""Yes""][numericFeatures].hist(bins=20, color='orange', alpha=0.5, ax=ax) To remove unnecessary columns cleanDF = df.drop('customerID', axis=1) cleanDF = df.drop('customerID', axis=1) Convert all the non-numeric columns to numeric Convert all the non-numeric columns to numeric for column in cleanDF.columns: if cleanDF[column].dtype == np.number: continue cleanDF[column] = LabelEncoder().fit_transform(cleanDF[column]) Convert all the non-numeric columns to numericfor column in cleanDF.columns: if cleanDF[column].dtype == np.number: continue cleanDF[column] = LabelEncoder().fit_transform(cleanDF[column]) To show the data types cleanDF.dtypes cleanDF.dtypes #Output gender int64 SeniorCitizen int64 Partner int64 Dependents int64 tenure int64 PhoneService int64 MultipleLines int64 InternetService int64 OnlineSecurity int64 OnlineBackup int64 DeviceProtection int64 TechSupport int64 StreamingTV int64 StreamingMovies int64 Contract int64 PaperlessBilling int64 PaymentMethod int64 MonthlyCharges float64 TotalCharges int64 Churn int64 dtype: object #Outputgender int64SeniorCitizen int64Partner int64Dependents int64tenure int64PhoneService int64MultipleLines int64InternetService int64OnlineSecurity int64OnlineBackup int64DeviceProtection int64TechSupport int64StreamingTV int64StreamingMovies int64Contract int64PaperlessBilling int64PaymentMethod int64MonthlyCharges float64TotalCharges int64Churn int64dtype: object To show first 5 rows of the new data Scale the data Scaled the data x = cleanDF.drop('Churn', axis=1) y = cleanDF['Churn'] x = StandardScaler().fit_transform(x) Scaled the datax = cleanDF.drop('Churn', axis=1)y = cleanDF['Churn']x = StandardScaler().fit_transform(x) Split the data into 80% training and 20% testing xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=42) xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=42) Create and Train the model model = LogisticRegression() # Train the model model.fit(xtrain, ytrain) model = LogisticRegression()# Train the modelmodel.fit(xtrain, ytrain) #Output LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) #OutputLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) Create the predictions on the test data predictions = model.predict(xtest) # print the predictions print(predictions) predictions = model.predict(xtest)​# print the predictionsprint(predictions) #Output [1 0 0 ... 0 0 0] #Output[1 0 0 ... 0 0 0] And Finally check the precision, recall and f1-score print(classification_report(ytest, predictions)) print(classification_report(ytest, predictions)) #Output precision recall f1-score support 0 0.85 0.91 0.88 1036 1 0.69 0.56 0.62 373 accuracy 0.82 1409 macro avg 0.77 0.74 0.75 1409 weighted avg 0.81 0.82 0.81 1409 #Output precision recall f1-score support​ 0 0.85 0.91 0.88 1036 1 0.69 0.56 0.62 373​ accuracy 0.82 1409 macro avg 0.77 0.74 0.75 1409weighted avg 0.81 0.82 0.81 1409";Predict Customer Churn with Python and Machine Learning
2020-05-27 02:02:12;Human activity recognition is the problem of classifying sequences of data recorded by specialized harnesses or smart phones into known well-defined Human activities.It is a challenging problem as the large number of observations are produced each second, the temporal nature of the observations, and the lack of a clear way to relate data to known movements increase the challenges.In this Machine Learning Project, we will create a model for recognition of  human activity using  the smartphone data.Let’s start with Importing necessary librariesTo Combine both the  data frames#Output((7352, 564), (2947, 564))Test options and evaluation metric;https://thecleverprogrammer.com/2020/05/27/human-activity-recognition-using-smartphone-data-with-machine-learning/;['sklearn'];1.0;['AI'];['ML', 'Classification', 'AI'];['recogn', 'predict', 'fit', 'model', 'machine learning', 'classif', 'filter', 'train', 'label'];"Human activity recognition is the problem of classifying sequences of data recorded by specialized harnesses or smart phones into known well-defined Human activities. It is a challenging problem as the large number of observations are produced each second, the temporal nature of the observations, and the lack of a clear way to relate data to known movements increase the challenges. In this Machine Learning Project, we will create a model for recognition of human activity using the smartphone data. Let’s start with Importing necessary libraries import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(""ignore"")​x import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as plt%matplotlib inlineimport warningswarnings.filterwarnings(""ignore"") Download the data sets testDownload train-1Download Reading the data train = pd.read_csv(""train.csv"") test = pd.read_csv(""test.csv"") train = pd.read_csv(""train.csv"")test = pd.read_csv(""test.csv"") To Combine both the data frames train['Data'] = 'Train' test['Data'] = 'Test' both = pd.concat([train, test], axis=0).reset_index(drop=True) both['subject'] = '#' + both['subject'].astype(str) train['Data'] = 'Train'test['Data'] = 'Test'both = pd.concat([train, test], axis=0).reset_index(drop=True)both['subject'] = '#' + both['subject'].astype(str) train.shape, test.shape train.shape, test.shape #Output((7352, 564), (2947, 564)) both.head() both.head() both.dtypes.value_counts() both.dtypes.value_counts() #Output float64 561 object 3 dtype: int64 #Outputfloat64 561object 3dtype: int64 def basic_details(df): b = pd.DataFrame() b['Missing value'] = df.isnull().sum() b['N unique value'] = df.nunique() b['dtype'] = df.dtypes return b basic_details(both) def basic_details(df): b = pd.DataFrame() b['Missing value'] = df.isnull().sum() b['N unique value'] = df.nunique() b['dtype'] = df.dtypes return bbasic_details(both) activity = both['Activity'] label_counts = activity.value_counts() plt.figure(figsize= (12, 8)) plt.bar(label_counts.index, label_counts) activity = both['Activity']label_counts = activity.value_counts()​plt.figure(figsize= (12, 8))plt.bar(label_counts.index, label_counts) Data = both['Data'] Subject = both['subject'] train = both.copy() train = train.drop(['Data','subject','Activity'], axis =1) Data = both['Data']Subject = both['subject']train = both.copy()train = train.drop(['Data','subject','Activity'], axis =1) To Scale the data # Standard Scaler from sklearn.preprocessing import StandardScaler slc = StandardScaler() train = slc.fit_transform(train) # dimensionality reduction from sklearn.decomposition import PCA pca = PCA(n_components=0.9, random_state=0) train = pca.fit_transform(train) # Standard Scalerfrom sklearn.preprocessing import StandardScalerslc = StandardScaler()train = slc.fit_transform(train)​# dimensionality reductionfrom sklearn.decomposition import PCApca = PCA(n_components=0.9, random_state=0)train = pca.fit_transform(train) Splitting the data into training and testing from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(train, activity, test_size = 0.2, random_state = 0) from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(train, activity, test_size = 0.2, random_state = 0) Test options and evaluation metric num_folds = 10 seed = 0 scoring = 'accuracy' results = {} accuracy = {} num_folds = 10seed = 0scoring = 'accuracy'results = {}accuracy = {} Activity Recognition Algorithm # Finalizing the model and comparing the test, predict results from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import confusion_matrix, accuracy_score, classification_report from sklearn.model_selection import KFold, cross_val_score model = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 8, p= 1, weights= 'distance') _ = cross_val_score(model, X_train, y_train, cv=10, scoring=scoring) results[""GScv""] = (_.mean(), _.std()) model.fit(X_train, y_train) y_predict = model.predict(X_test) accuracy[""GScv""] = accuracy_score(y_test, y_predict) print(classification_report(y_test, y_predict)) cm= confusion_matrix(y_test, y_predict) sns.heatmap(cm, annot=True) # Finalizing the model and comparing the test, predict resultsfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_reportfrom sklearn.model_selection import KFold, cross_val_scoremodel = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 8, p= 1, weights= 'distance')​_ = cross_val_score(model, X_train, y_train, cv=10, scoring=scoring)results[""GScv""] = (_.mean(), _.std())​model.fit(X_train, y_train) y_predict = model.predict(X_test)​accuracy[""GScv""] = accuracy_score(y_test, y_predict)​print(classification_report(y_test, y_predict))​cm= confusion_matrix(y_test, y_predict)sns.heatmap(cm, annot=True) #Output precision recall f1-score support LAYING 1.00 1.00 1.00 377 SITTING 0.92 0.87 0.90 364 STANDING 0.89 0.93 0.91 390 WALKING 0.96 0.99 0.97 335 WALKING_DOWNSTAIRS 0.99 0.95 0.97 278 WALKING_UPSTAIRS 0.98 0.98 0.98 316 accuracy 0.95 2060 macro avg 0.96 0.95 0.95 2060 weighted avg 0.95 0.95 0.95 2060 #Output precision recall f1-score support​ LAYING 1.00 1.00 1.00 377 SITTING 0.92 0.87 0.90 364 STANDING 0.89 0.93 0.91 390 WALKING 0.96 0.99 0.97 335WALKING_DOWNSTAIRS 0.99 0.95 0.97 278 WALKING_UPSTAIRS 0.98 0.98 0.98 316​ accuracy 0.95 2060 macro avg 0.96 0.95 0.95 2060 weighted avg 0.95 0.95 0.95 2060​ Follow us on Instagram for all your Queries Instagram";Human Activity Recognition using Smartphone Data with Machine Learning
2020-05-27 02:02:12;Human activity recognition is the problem of classifying sequences of data recorded by specialized harnesses or smart phones into known well-defined Human activities.It is a challenging problem as the large number of observations are produced each second, the temporal nature of the observations, and the lack of a clear way to relate data to known movements increase the challenges.In this Machine Learning Project, we will create a model for recognition of  human activity using  the smartphone data.Let’s start with Importing necessary librariesTo Combine both the  data frames#Output((7352, 564), (2947, 564))Test options and evaluation metric;https://thecleverprogrammer.com/2020/05/27/machine-learning-project-human-activity-recognition-using-smartphone-data/;['sklearn'];1.0;['AI'];['ML', 'Classification', 'AI'];['recogn', 'predict', 'fit', 'model', 'machine learning', 'classif', 'filter', 'train', 'label'];"Human activity recognition is the problem of classifying sequences of data recorded by specialized harnesses or smart phones into known well-defined Human activities. It is a challenging problem as the large number of observations are produced each second, the temporal nature of the observations, and the lack of a clear way to relate data to known movements increase the challenges. In this Machine Learning Project, we will create a model for recognition of human activity using the smartphone data. Let’s start with Importing necessary libraries import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(""ignore"")​x import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as plt%matplotlib inlineimport warningswarnings.filterwarnings(""ignore"") Download the data sets testDownload train-1Download Reading the data train = pd.read_csv(""train.csv"") test = pd.read_csv(""test.csv"") train = pd.read_csv(""train.csv"")test = pd.read_csv(""test.csv"") To Combine both the data frames train['Data'] = 'Train' test['Data'] = 'Test' both = pd.concat([train, test], axis=0).reset_index(drop=True) both['subject'] = '#' + both['subject'].astype(str) train['Data'] = 'Train'test['Data'] = 'Test'both = pd.concat([train, test], axis=0).reset_index(drop=True)both['subject'] = '#' + both['subject'].astype(str) train.shape, test.shape train.shape, test.shape #Output((7352, 564), (2947, 564)) both.head() both.head() both.dtypes.value_counts() both.dtypes.value_counts() #Output float64 561 object 3 dtype: int64 #Outputfloat64 561object 3dtype: int64 def basic_details(df): b = pd.DataFrame() b['Missing value'] = df.isnull().sum() b['N unique value'] = df.nunique() b['dtype'] = df.dtypes return b basic_details(both) def basic_details(df): b = pd.DataFrame() b['Missing value'] = df.isnull().sum() b['N unique value'] = df.nunique() b['dtype'] = df.dtypes return bbasic_details(both) activity = both['Activity'] label_counts = activity.value_counts() plt.figure(figsize= (12, 8)) plt.bar(label_counts.index, label_counts) activity = both['Activity']label_counts = activity.value_counts()​plt.figure(figsize= (12, 8))plt.bar(label_counts.index, label_counts) Data = both['Data'] Subject = both['subject'] train = both.copy() train = train.drop(['Data','subject','Activity'], axis =1) Data = both['Data']Subject = both['subject']train = both.copy()train = train.drop(['Data','subject','Activity'], axis =1) To Scale the data # Standard Scaler from sklearn.preprocessing import StandardScaler slc = StandardScaler() train = slc.fit_transform(train) # dimensionality reduction from sklearn.decomposition import PCA pca = PCA(n_components=0.9, random_state=0) train = pca.fit_transform(train) # Standard Scalerfrom sklearn.preprocessing import StandardScalerslc = StandardScaler()train = slc.fit_transform(train)​# dimensionality reductionfrom sklearn.decomposition import PCApca = PCA(n_components=0.9, random_state=0)train = pca.fit_transform(train) Splitting the data into training and testing from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(train, activity, test_size = 0.2, random_state = 0) from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(train, activity, test_size = 0.2, random_state = 0) Test options and evaluation metric num_folds = 10 seed = 0 scoring = 'accuracy' results = {} accuracy = {} num_folds = 10seed = 0scoring = 'accuracy'results = {}accuracy = {} Activity Recognition Algorithm # Finalizing the model and comparing the test, predict results from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import confusion_matrix, accuracy_score, classification_report from sklearn.model_selection import KFold, cross_val_score model = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 8, p= 1, weights= 'distance') _ = cross_val_score(model, X_train, y_train, cv=10, scoring=scoring) results[""GScv""] = (_.mean(), _.std()) model.fit(X_train, y_train) y_predict = model.predict(X_test) accuracy[""GScv""] = accuracy_score(y_test, y_predict) print(classification_report(y_test, y_predict)) cm= confusion_matrix(y_test, y_predict) sns.heatmap(cm, annot=True) # Finalizing the model and comparing the test, predict resultsfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_reportfrom sklearn.model_selection import KFold, cross_val_scoremodel = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 8, p= 1, weights= 'distance')​_ = cross_val_score(model, X_train, y_train, cv=10, scoring=scoring)results[""GScv""] = (_.mean(), _.std())​model.fit(X_train, y_train) y_predict = model.predict(X_test)​accuracy[""GScv""] = accuracy_score(y_test, y_predict)​print(classification_report(y_test, y_predict))​cm= confusion_matrix(y_test, y_predict)sns.heatmap(cm, annot=True) #Output precision recall f1-score support LAYING 1.00 1.00 1.00 377 SITTING 0.92 0.87 0.90 364 STANDING 0.89 0.93 0.91 390 WALKING 0.96 0.99 0.97 335 WALKING_DOWNSTAIRS 0.99 0.95 0.97 278 WALKING_UPSTAIRS 0.98 0.98 0.98 316 accuracy 0.95 2060 macro avg 0.96 0.95 0.95 2060 weighted avg 0.95 0.95 0.95 2060 #Output precision recall f1-score support​ LAYING 1.00 1.00 1.00 377 SITTING 0.92 0.87 0.90 364 STANDING 0.89 0.93 0.91 390 WALKING 0.96 0.99 0.97 335WALKING_DOWNSTAIRS 0.99 0.95 0.97 278 WALKING_UPSTAIRS 0.98 0.98 0.98 316​ accuracy 0.95 2060 macro avg 0.96 0.95 0.95 2060 weighted avg 0.95 0.95 0.95 2060​ Follow us on Instagram for all your Queries Instagram";Human Activity Recognition using Smartphone Data with Machine Learning
2020-06-05 23:37:55;This article is based on Statistics tutorial to learn essential concepts of Statistics, that we need in Data Science. I will try to present the concepts in a fun and interactive way and I encourage you to play with the code to get a better grasp of the concepts.Lets start with importing the libraries and reading the data set;https://thecleverprogrammer.com/2020/06/05/statistics-tutorial-for-data-science/;['pattern', 'sklearn'];1.0;['ML'];['ML', 'Linear Regression', 'Regression'];['epoch', 'regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'train', 'label'];"This article is based on Statistics tutorial to learn essential concepts of Statistics, that we need in Data Science. I will try to present the concepts in a fun and interactive way and I encourage you to play with the code to get a better grasp of the concepts. Download the data set toy_datasetDownload Lets start with importing the libraries and reading the data set # Dependencies # Standard Dependencies import os import numpy as np import pandas as pd from math import sqrt # Visualization from pylab import * import matplotlib.mlab as mlab import matplotlib.pyplot as plt import seaborn as sns # Statistics from statistics import median from scipy import signal from scipy.special import factorial import scipy.stats as stats from scipy.stats import sem, binom, lognorm, poisson, bernoulli, spearmanr from scipy.fftpack import fft, fftshift # Scikit-learn for Machine Learning models from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split # Seed for reproducability seed = 12345 np.random.seed(seed) from google.colab import files uploaded = files.upload() df = pd.read_csv(""toy_dataset.csv"")​x # Dependencies​# Standard Dependenciesimport osimport numpy as npimport pandas as pdfrom math import sqrt​# Visualizationfrom pylab import *import matplotlib.mlab as mlabimport matplotlib.pyplot as pltimport seaborn as sns​# Statisticsfrom statistics import medianfrom scipy import signalfrom scipy.special import factorialimport scipy.stats as statsfrom scipy.stats import sem, binom, lognorm, poisson, bernoulli, spearmanrfrom scipy.fftpack import fft, fftshift​# Scikit-learn for Machine Learning modelsfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_split​# Seed for reproducabilityseed = 12345np.random.seed(seed)​from google.colab import filesuploaded = files.upload()df = pd.read_csv(""toy_dataset.csv"") Discrete and Continuous Variables A discrete variable is a variable that can only take on a countable number of values. If you can count a set of items, then it’s a discrete variable. An example of a discrete variable is the outcome of a dice. It can only have 1 of 6 different possible outcomes and is therefore discrete. A discrete random variable can have an infinite number of values. For example, the whole set of natural numbers (1,2,3,etc.) is countable and therefore discrete.A continuous variable takes on an uncountable number of values. An example of a continuous variable is length. Length can be measured to an arbitrary degree and is therefore continuous.In statistics we represent a distribution of discrete variables with PMF’s (Probability Mass Functions) and CDF’s (Cumulative Distribution Functions). We represent distributions of continuous variables with PDF’s (Probability Density Functions) and CDF’s.The PMF defines the probability of all possible values x of the random variable. A PDF is the same but for continuous values. The CDF represents the probability that the random variable X will have an outcome less or equal to the value x. The name CDF is used for both discrete and continuous distributions.The functions that describe PMF’s, PDF’s and CDF’s can be quite daunting at first, but their visual counterpart often looks quite intuitive. PMF (Probability Mass Function) Here we visualize a PMF of a binomial distribution. You can see that the possible values are all integers. For example, no values are between 50 and 51. The PMF of a binomial distribution in function form: # PMF Visualization n = 100 p = 0.5 fig, ax = plt.subplots(1, 1, figsize=(17,5)) x = np.arange(binom.ppf(0.01, n, p), binom.ppf(0.99, n, p)) ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='Binomial PMF') ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5) rv = binom(n, p) #ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1, label='frozen PMF') ax.legend(loc='best', frameon=False, fontsize='xx-large') plt.title('PMF of a binomial distribution (n=100, p=0.5)', fontsize='xx-large') plt.show() # PMF Visualizationn = 100p = 0.5​fig, ax = plt.subplots(1, 1, figsize=(17,5))x = np.arange(binom.ppf(0.01, n, p), binom.ppf(0.99, n, p))ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='Binomial PMF')ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)rv = binom(n, p)#ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1, label='frozen PMF')ax.legend(loc='best', frameon=False, fontsize='xx-large')plt.title('PMF of a binomial distribution (n=100, p=0.5)', fontsize='xx-large')plt.show() PDF (Probability Density Functions) The PDF is the same as a PMF, but continuous. It can be said that the distribution has an infinite number of possible values. Here we visualize a simple normal distribution with a mean of 0 and standard deviation of 1. PDF of a normal distribution in formula form: # Plot normal distribution mu = 0 variance = 1 sigma = sqrt(variance) x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100) plt.figure(figsize=(16,5)) plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Normal Distribution') plt.title('Normal Distribution with mean = 0 and std = 1') plt.legend(fontsize='xx-large') plt.show() # Plot normal distributionmu = 0variance = 1sigma = sqrt(variance)x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)plt.figure(figsize=(16,5))plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Normal Distribution')plt.title('Normal Distribution with mean = 0 and std = 1')plt.legend(fontsize='xx-large')plt.show() CDF (Cumulative Distribution Function) The CDF maps the probability that a random variable X will take a value of less than or equal to a value x (P(X ≤ x)). CDF’s can be discrete or continuous. In this section we visualize the continuous case. You can see in the plot that the CDF accumulates all probabilities and is therefore bounded between 0 ≤ x ≤ 1. The CDF of a normal distribution as a formula: # Data X = np.arange(-2, 2, 0.01) Y = exp(-X ** 2) # Normalize data Y = Y / (0.01 * Y).sum() # Plot the PDF and CDF plt.figure(figsize=(15,5)) plt.title('Continuous Normal Distributions', fontsize='xx-large') plot(X, Y, label='Probability Density Function (PDF)') plot(X, np.cumsum(Y * 0.01), 'r', label='Cumulative Distribution Function (CDF)') plt.legend(fontsize='xx-large') plt.show() # DataX = np.arange(-2, 2, 0.01)Y = exp(-X ** 2)​# Normalize dataY = Y / (0.01 * Y).sum()​# Plot the PDF and CDFplt.figure(figsize=(15,5))plt.title('Continuous Normal Distributions', fontsize='xx-large')plot(X, Y, label='Probability Density Function (PDF)')plot(X, np.cumsum(Y * 0.01), 'r', label='Cumulative Distribution Function (CDF)')plt.legend(fontsize='xx-large')plt.show() Distributions A Probability distribution tells us something about the likelihood of each value of the random variable.A random variable X is a function that maps events to real numbers.The visualizations in this section are of discrete distributions. Many of these distributions can however also be continuous. Uniform Distribution A Uniform distribution is pretty straightforward. Every value has an equal change of occuring. Therefore, the distribution consists of random values with no patterns in them. In this example we generate random floating numbers between 0 and 1. The PDF of a Uniform Distribution: CDF: # Uniform distribution (between 0 and 1) uniform_dist = np.random.random(1000) uniform_df = pd.DataFrame({'value' : uniform_dist}) uniform_dist = pd.Series(uniform_dist) plt.figure(figsize=(18,5)) sns.scatterplot(data=uniform_df) plt.legend(fontsize='xx-large') plt.title('Scatterplot of a Random/Uniform Distribution', fontsize='xx-large') # Uniform distribution (between 0 and 1)uniform_dist = np.random.random(1000)uniform_df = pd.DataFrame({'value' : uniform_dist})uniform_dist = pd.Series(uniform_dist)plt.figure(figsize=(18,5))sns.scatterplot(data=uniform_df)plt.legend(fontsize='xx-large')plt.title('Scatterplot of a Random/Uniform Distribution', fontsize='xx-large') plt.figure(figsize=(18,5)) sns.distplot(uniform_df) plt.title('Random/Uniform distribution', fontsize='xx-large') plt.figure(figsize=(18,5))sns.distplot(uniform_df)plt.title('Random/Uniform distribution', fontsize='xx-large') Normal Distribution A normal distribution (also called Gaussian or Bell Curve) is very common and convenient. This is mainly because of the Central Limit Theorem (CLT), which states that as the amount independent random samples (like multiple coin flips) goes to infinity the distribution of the sample mean tends towards a normal distribution. PDF of a normal distribution: CDF: # Generate Normal Distribution normal_dist = np.random.randn(10000) normal_df = pd.DataFrame({'value' : normal_dist}) # Create a Pandas Series for easy sample function normal_dist = pd.Series(normal_dist) normal_dist2 = np.random.randn(10000) normal_df2 = pd.DataFrame({'value' : normal_dist2}) # Create a Pandas Series for easy sample function normal_dist2 = pd.Series(normal_dist) normal_df_total = pd.DataFrame({'value1' : normal_dist, 'value2' : normal_dist2}) # Generate Normal Distributionnormal_dist = np.random.randn(10000)normal_df = pd.DataFrame({'value' : normal_dist})# Create a Pandas Series for easy sample functionnormal_dist = pd.Series(normal_dist)​normal_dist2 = np.random.randn(10000)normal_df2 = pd.DataFrame({'value' : normal_dist2})# Create a Pandas Series for easy sample functionnormal_dist2 = pd.Series(normal_dist)​normal_df_total = pd.DataFrame({'value1' : normal_dist, 'value2' : normal_dist2}) # Scatterplot plt.figure(figsize=(18,5)) sns.scatterplot(data=normal_df) plt.legend(fontsize='xx-large') plt.title('Scatterplot of a Normal Distribution', fontsize='xx-large') # Scatterplotplt.figure(figsize=(18,5))sns.scatterplot(data=normal_df)plt.legend(fontsize='xx-large')plt.title('Scatterplot of a Normal Distribution', fontsize='xx-large') # Normal Distribution as a Bell Curve plt.figure(figsize=(18,5)) sns.distplot(normal_df) plt.title('Normal distribution (n=1000)', fontsize='xx-large') # Normal Distribution as a Bell Curveplt.figure(figsize=(18,5))sns.distplot(normal_df)plt.title('Normal distribution (n=1000)', fontsize='xx-large') Binomial Distribution A Binomial Distribution has a countable number of outcomes and is therefore discrete. Binomial distributions must meet the following three criteria: The number of observations or trials is fixed. In other words, you can only figure out the probability of something happening if you do it a certain number of times.Each observation or trial is independent. In other words, none of your trials have an effect on the probability of the next trial.The probability of success is exactly the same from one trial to another. An intuitive explanation of a binomial distribution is flipping a coin 10 times. If we have a fair coin our chance of getting heads (p) is 0.50. Now we throw the coin 10 times and count how many times it comes up heads. In most situations we will get heads 5 times, but there is also a change that we get heads 9 times. The PMF of a binomial distribution will give these probabilities if we say N = 10 and p = 0.5. We say that the x for heads is 1 and 0 for tails. PMF: CDF: A Bernoulli Distribution is a special case of a Binomial Distribution. All values in a Bernoulli Distribution are either 0 or 1. For example, if we take an unfair coin which falls on heads 60 % of the time, we can describe the Bernoulli distribution as follows: p (change of heads) = 0.6 1 – p (change of tails) = 0.4 heads = 1 tails = 0 Formally, we can describe a Bernoulli distribution with the following PMF (Probability Mass Function): # Change of heads (outcome 1) p = 0.6 # Create Bernoulli samples bern_dist = bernoulli.rvs(p, size=1000) bern_df = pd.DataFrame({'value' : bern_dist}) bern_values = bern_df['value'].value_counts() # Plot Distribution plt.figure(figsize=(18,4)) bern_values.plot(kind='bar', rot=0) plt.annotate(xy=(0.85,300), s='Samples that came up Tails\nn = {}'.format(bern_values[0]), fontsize='large', color='white') plt.annotate(xy=(-0.2,300), s='Samples that came up Heads\nn = {}'.format(bern_values[1]), fontsize='large', color='white') plt.title('Bernoulli Distribution: p = 0.6, n = 1000') # Change of heads (outcome 1)p = 0.6​# Create Bernoulli samplesbern_dist = bernoulli.rvs(p, size=1000)bern_df = pd.DataFrame({'value' : bern_dist})bern_values = bern_df['value'].value_counts()​# Plot Distributionplt.figure(figsize=(18,4))bern_values.plot(kind='bar', rot=0)plt.annotate(xy=(0.85,300), s='Samples that came up Tails\nn = {}'.format(bern_values[0]), fontsize='large', color='white')plt.annotate(xy=(-0.2,300), s='Samples that came up Heads\nn = {}'.format(bern_values[1]), fontsize='large', color='white')plt.title('Bernoulli Distribution: p = 0.6, n = 1000') Poisson Distribution The Poisson distribution is a discrete distribution and is popular for modelling the number of times an event occurs in an interval of time or space. It takes a value lambda, which is equal to the mean of the distribution. PMF: CDF: x = np.arange(0, 20, 0.1) y = np.exp(-5)*np.power(5, x)/factorial(x) plt.figure(figsize=(15,8)) plt.title('Poisson distribution with lambda=5', fontsize='xx-large') plt.plot(x, y, 'bs') plt.show() x = np.arange(0, 20, 0.1)y = np.exp(-5)*np.power(5, x)/factorial(x)​plt.figure(figsize=(15,8))plt.title('Poisson distribution with lambda=5', fontsize='xx-large')plt.plot(x, y, 'bs')plt.show() Log-Normal Distribution A log-normal distribution is continuous. The main characteristic of a log-normal distribution is that it’s logarithm is normally distributed. It is also referred to as Galton’s distribution. PDF: CDF: Where Phi is the CDF of the standard normal distribution. # Specify standard deviation and mean std = 1 mean = 5 # Create log-normal distribution dist=lognorm(std,loc=mean) x=np.linspace(0,15,200) # Visualize log-normal distribution plt.figure(figsize=(15,6)) plt.xlim(5, 10) plt.plot(x,dist.pdf(x), label='log-normal PDF') plt.plot(x,dist.cdf(x), label='log-normal CDF') plt.legend(fontsize='xx-large') plt.title('Visualization of log-normal PDF and CDF', fontsize='xx-large') plt.show() # Specify standard deviation and meanstd = 1mean = 5​# Create log-normal distributiondist=lognorm(std,loc=mean)x=np.linspace(0,15,200)​# Visualize log-normal distributionplt.figure(figsize=(15,6))plt.xlim(5, 10)plt.plot(x,dist.pdf(x), label='log-normal PDF')plt.plot(x,dist.cdf(x), label='log-normal CDF')plt.legend(fontsize='xx-large')plt.title('Visualization of log-normal PDF and CDF', fontsize='xx-large')plt.show() Summary Statistics and Moments Mean, Median and Mode Note: The mean is also called the first moment. Moments A moment is a quantitative measure that says something about the shape of a distribution. There are central moments and non-central moments. This section is focused on the central moments. The 0th central moment is the total probability and is always equal to 1. The 1st moment is the mean (expected value). The 2nd central moment is the variance. Variance = The average of the squared distance of the mean. Variance is interesting in a mathematical sense, but the standard deviation is often a much better measure of how spread out the distribution is. Standard Deviation = The square root of the variance The 3rd central moment is the skewness. Skewness = A measure that describes the contrast of one tail versus the other tail. For example, if there are more high values in your distribution than low values then your distribution is ‘skewed’ towards the high values. The 4th central moment is the kurtosis. Kurtosis = A measure of how ‘fat’ the tails in the distribution are. The higher the moment, the harder it is to estimate with samples. Larger samples are required in order to obtain good estimates. # Summary print('Summary Statistics for a normal distribution: ') # Median medi = median(normal_dist) print('Median: ', medi) display(normal_df.describe()) # Standard Deviation std = sqrt(np.var(normal_dist)) print('The first four calculated moments of a normal distribution: ') # Mean mean = normal_dist.mean() print('Mean: ', mean) # Variance var = np.var(normal_dist) print('Variance: ', var) # Return unbiased skew normalized by N-1 skew = normal_df['value'].skew() print('Skewness: ', skew) # Return unbiased kurtosis over requested axis using Fisher's definition of kurtosis # (kurtosis of normal == 0.0) normalized by N-1 kurt = normal_df['value'].kurtosis() print('Kurtosis: ', kurt) # Summaryprint('Summary Statistics for a normal distribution: ')# Medianmedi = median(normal_dist)print('Median: ', medi)display(normal_df.describe())​# Standard Deviationstd = sqrt(np.var(normal_dist))​print('The first four calculated moments of a normal distribution: ')# Meanmean = normal_dist.mean()print('Mean: ', mean)​# Variancevar = np.var(normal_dist)print('Variance: ', var)​# Return unbiased skew normalized by N-1skew = normal_df['value'].skew()print('Skewness: ', skew)​# Return unbiased kurtosis over requested axis using Fisher's definition of kurtosis # (kurtosis of normal == 0.0) normalized by N-1kurt = normal_df['value'].kurtosis()print('Kurtosis: ', kurt) Bias, MSE and SE Bias is a measure of how far the sample mean deviates from the population mean. The sample mean is also called Expected value. Formula for Bias: The formula for expected value (EV) makes it apparent that the bias can also be formulated as the expected value minus the population mean: # Take sample normal_df_sample = normal_df.sample(100) # Calculate Expected Value (EV), population mean and bias ev = normal_df_sample.mean()[0] pop_mean = normal_df.mean()[0] bias = ev - pop_mean print('Sample mean (Expected Value): ', ev) print('Population mean: ', pop_mean) print('Bias: ', bias) # Take samplenormal_df_sample = normal_df.sample(100)​# Calculate Expected Value (EV), population mean and biasev = normal_df_sample.mean()[0]pop_mean = normal_df.mean()[0]bias = ev - pop_mean​print('Sample mean (Expected Value): ', ev)print('Population mean: ', pop_mean)print('Bias: ', bias) #Output Sample mean (Expected Value): -0.09989709931399528 Population mean: -0.012836443467010622 Bias: -0.08706065584698465 #OutputSample mean (Expected Value): -0.09989709931399528Population mean: -0.012836443467010622Bias: -0.08706065584698465 MSE (Mean Squared Error) is a formula to measure how much estimators deviate from the true distribution. This can be very useful with for example, evaluating regression models. RMSE (Root Mean Squared Error) is just the root of the MSE. from math import sqrt Y = 100 # Actual Value YH = 94 # Predicted Value # MSE Formula def MSE(Y, YH): return np.square(YH - Y).mean() # RMSE formula def RMSE(Y, YH): return sqrt(np.square(YH - Y).mean()) print('MSE: ', MSE(Y, YH)) print('RMSE: ', RMSE(Y, YH)) from math import sqrt​Y = 100 # Actual ValueYH = 94 # Predicted Value​# MSE Formula def MSE(Y, YH): return np.square(YH - Y).mean()​# RMSE formuladef RMSE(Y, YH): return sqrt(np.square(YH - Y).mean())​​print('MSE: ', MSE(Y, YH))​print('RMSE: ', RMSE(Y, YH)) #Output MSE: 36.0 RMSE: 6.0 #OutputMSE: 36.0RMSE: 6.0 The Standard Error (SE) measures how spread the distribution is from the sample mean. The formula can also be defined as the standard deviation divided by the square root of the number of samples. # Standard Error (SE) uni_sample = uniform_dist.sample(100) norm_sample = normal_dist.sample(100) print('Standard Error of uniform sample: ', sem(uni_sample)) print('Standard Error of normal sample: ', sem(norm_sample)) # The random samples from the normal distribution should have a higher standard error # Standard Error (SE)uni_sample = uniform_dist.sample(100)norm_sample = normal_dist.sample(100)​print('Standard Error of uniform sample: ', sem(uni_sample))print('Standard Error of normal sample: ', sem(norm_sample))​# The random samples from the normal distribution should have a higher standard error #Output Standard Error of uniform sample: 0.028951451192463368 Standard Error of normal sample: 0.10887812225934831 #OutputStandard Error of uniform sample: 0.028951451192463368Standard Error of normal sample: 0.10887812225934831 Sampling methods Non-Representative Sampling: Convenience Sampling = Pick samples that are most convenient, like the top of a shelf or people that can be easily approached.Haphazard Sampling = Pick samples without thinking about it. This often gives the illusion take you are picking out samples at random.Purposive Sampling = Pick samples for a specific purpose. An example is to focus on extreme cases. This can be useful but is limited because it doesn’t allow you to make statements about the whole population. Representative Sampling: Simple Random Sampling = Pick samples (psuedo)randomly.Systematic Sampling = Pick samples with a fixed interval. For example every 10th sample (0, 10, 20, etc.).Stratified Sampling = Pick the same amount of samples from different groups (strata) in the population.Cluster Sampling = Divide the population into groups (clusters) and pick samples from those groups. # Note that we take very small samples just to illustrate the different sampling methods print('---Non-Representative samples:---\n') # Convenience samples con_samples = normal_dist[0:5] print('Convenience samples:\n\n{}\n'.format(con_samples)) # Haphazard samples (Picking out some numbers) hap_samples = [normal_dist[12], normal_dist[55], normal_dist[582], normal_dist[821], normal_dist[999]] print('Haphazard samples:\n\n{}\n'.format(hap_samples)) # Purposive samples (Pick samples for a specific purpose) # In this example we pick the 5 highest values in our distribution purp_samples = normal_dist.nlargest(n=5) print('Purposive samples:\n\n{}\n'.format(purp_samples)) print('---Representative samples:---\n') # Simple (pseudo)random sample rand_samples = normal_dist.sample(5) print('Random samples:\n\n{}\n'.format(rand_samples)) # Systematic sample (Every 2000th value) sys_samples = normal_dist[normal_dist.index % 2000 == 0] print('Systematic samples:\n\n{}\n'.format(sys_samples)) # Stratified Sampling # We will get 1 person from every city in the dataset # We have 8 cities so that makes a total of 8 samples df = pd.read_csv('toy_dataset.csv') strat_samples = [] for city in df['City'].unique(): samp = df[df['City'] == city].sample(1) strat_samples.append(samp['Income'].item()) print('Stratified samples:\n\n{}\n'.format(strat_samples)) # Cluster Sampling # Make random clusters of ten people (Here with replacement) c1 = normal_dist.sample(10) c2 = normal_dist.sample(10) c3 = normal_dist.sample(10) c4 = normal_dist.sample(10) c5 = normal_dist.sample(10) # Take sample from every cluster (with replacement) clusters = [c1,c2,c3,c4,c5] cluster_samples = [] for c in clusters: clus_samp = c.sample(1) cluster_samples.extend(clus_samp) print('Cluster samples:\n\n{}'.format(cluster_samples)) # Note that we take very small samples just to illustrate the different sampling methods​print('---Non-Representative samples:---\n')# Convenience samplescon_samples = normal_dist[0:5]print('Convenience samples:\n\n{}\n'.format(con_samples))​# Haphazard samples (Picking out some numbers)hap_samples = [normal_dist[12], normal_dist[55], normal_dist[582], normal_dist[821], normal_dist[999]]print('Haphazard samples:\n\n{}\n'.format(hap_samples))​# Purposive samples (Pick samples for a specific purpose)# In this example we pick the 5 highest values in our distributionpurp_samples = normal_dist.nlargest(n=5)print('Purposive samples:\n\n{}\n'.format(purp_samples))​print('---Representative samples:---\n')​# Simple (pseudo)random samplerand_samples = normal_dist.sample(5)print('Random samples:\n\n{}\n'.format(rand_samples))​# Systematic sample (Every 2000th value)sys_samples = normal_dist[normal_dist.index % 2000 == 0]print('Systematic samples:\n\n{}\n'.format(sys_samples))​# Stratified Sampling# We will get 1 person from every city in the dataset# We have 8 cities so that makes a total of 8 samplesdf = pd.read_csv('toy_dataset.csv')​strat_samples = []​for city in df['City'].unique(): samp = df[df['City'] == city].sample(1) strat_samples.append(samp['Income'].item()) print('Stratified samples:\n\n{}\n'.format(strat_samples))​# Cluster Sampling# Make random clusters of ten people (Here with replacement)c1 = normal_dist.sample(10)c2 = normal_dist.sample(10)c3 = normal_dist.sample(10)c4 = normal_dist.sample(10)c5 = normal_dist.sample(10)​# Take sample from every cluster (with replacement)clusters = [c1,c2,c3,c4,c5]cluster_samples = []for c in clusters: clus_samp = c.sample(1) cluster_samples.extend(clus_samp)print('Cluster samples:\n\n{}'.format(cluster_samples)) #Output ---Non-Representative samples:--- Convenience samples: 0 0.652346 1 0.871600 2 0.268216 3 0.947681 4 0.147268 dtype: float64 Haphazard samples: [-0.5010027615175823, -0.5066784173896016, 0.6814697928519314, 0.8359965545029714, -0.5374734651177141] Purposive samples: 3378 3.525865 2792 3.366626 490 3.260383 7169 3.189940 7057 3.082067 dtype: float64 ---Representative samples:--- Random samples: 3442 0.236063 9169 -0.042718 8511 -1.529128 6500 -0.787297 2141 1.134073 dtype: float64 Systematic samples: 0 0.652346 2000 -0.914345 4000 -0.198577 6000 -0.030696 8000 -1.084444 dtype: float64 Stratified samples: [29892.0, 110450.0, 85635.0, 122364.0, 83214.0, 74469.0, 113187.0, 92854.0] Cluster samples: [-1.1500083079696, 1.2888783369868333, 1.2540511669530716, -0.6914086007291129, 2.259205774351393] #Output---Non-Representative samples:---​Convenience samples:​0 0.6523461 0.8716002 0.2682163 0.9476814 0.147268dtype: float64​Haphazard samples:​[-0.5010027615175823, -0.5066784173896016, 0.6814697928519314, 0.8359965545029714, -0.5374734651177141]​Purposive samples:​3378 3.5258652792 3.366626490 3.2603837169 3.1899407057 3.082067dtype: float64​---Representative samples:---​Random samples:​3442 0.2360639169 -0.0427188511 -1.5291286500 -0.7872972141 1.134073dtype: float64​Systematic samples:​0 0.6523462000 -0.9143454000 -0.1985776000 -0.0306968000 -1.084444dtype: float64​Stratified samples:​[29892.0, 110450.0, 85635.0, 122364.0, 83214.0, 74469.0, 113187.0, 92854.0]​Cluster samples:​[-1.1500083079696, 1.2888783369868333, 1.2540511669530716, -0.6914086007291129, 2.259205774351393] Covariance Covariance is a measure of how much two random variables vary together. Variance is similar to covariance in that variance shows you how much one variable varies.If two variables are independent, their covariance is 0. However, a covariance of 0 does not imply that the variables are independent. # Covariance between Age and Income print('Covariance between Age and Income: ') df[['Age', 'Income']].cov() # Covariance between Age and Incomeprint('Covariance between Age and Income: ')​df[['Age', 'Income']].cov() #Output Covariance between Age and Income: Age Income Age 133.922426	-3.811863e+02 Income	-381.186341	6.244752e+08 #OutputCovariance between Age and Income: Age IncomeAge 133.922426 -3.811863e+02Income -381.186341 6.244752e+08 Correlation Correlation is a standardized version of covariance. Here it becomes more clear that Age and Income do not have a strong correlation in our dataset. Formula for Pearson’s correlation coefficient: # Correlation between two normal distributions # Using Pearson's correlation print('Pearson: ') df[['Age', 'Income']].corr(method='pearson') # Correlation between two normal distributions# Using Pearson's correlationprint('Pearson: ')df[['Age', 'Income']].corr(method='pearson') #Output Pearson: Age Income Age 1.000000 -0.001318 Income	-0.001318	1.000000 #OutputPearson: Age IncomeAge 1.000000 -0.001318Income -0.001318 1.000000 Another method for calculating a correlation coefficient is ‘Spearman’s Rho’. The formula looks different but it will give similar results as Pearson’s method. In this example we see almost no difference, but this is partly because it is obvious that the Age and Income columns in our dataset have no correlation.Formula for Spearmans Rho: # Using Spearman's rho correlation print('Spearman: ') df[['Age', 'Income']].corr(method='spearman') # Using Spearman's rho correlationprint('Spearman: ')df[['Age', 'Income']].corr(method='spearman') #Output Spearman: Age Income Age 1.000000	-0.001452 Income	-0.001452	1.000000 #OutputSpearman: Age IncomeAge 1.000000 -0.001452Income -0.001452 1.000000 Linear regression Linear Regression can be performed through Ordinary Least Squares (OLS) or Maximum Likelihood Estimation (MLE). Most Python libraries use OLS to fit linear models. # Generate data x = np.random.uniform(low=20, high=260, size=100) y = 50000 + 2000*x - 4.5 * x**2 + np.random.normal(size=100, loc=0, scale=10000) # Plot data with Linear Regression plt.figure(figsize=(16,5)) plt.title('Well fitted but not well fitting: Linear regression plot on quadratic data', fontsize='xx-large') sns.regplot(x, y) # Generate datax = np.random.uniform(low=20, high=260, size=100)y = 50000 + 2000*x - 4.5 * x**2 + np.random.normal(size=100, loc=0, scale=10000)​# Plot data with Linear Regressionplt.figure(figsize=(16,5))plt.title('Well fitted but not well fitting: Linear regression plot on quadratic data', fontsize='xx-large')sns.regplot(x, y) # Linear regression from scratch import random # Create data from regression xs = np.array(range(1,20)) ys = [0,8,10,8,15,20,26,29,38,35,40,60,50,61,70,75,80,88,96] # Put data in dictionary data = dict() for i in list(xs): data.update({xs[i-1] : ys[i-1]}) # Slope m = 0 # y intercept b = 0 # Learning rate lr = 0.0001 # Number of epochs epochs = 100000 # Formula for linear line def lin(x): return m * x + b # Linear regression algorithm for i in range(epochs): # Pick a random point and calculate vertical distance and horizontal distance rand_point = random.choice(list(data.items())) vert_dist = abs((m * rand_point[0] + b) - rand_point[1]) hor_dist = rand_point[0] if (m * rand_point[0] + b) - rand_point[1] < 0: # Adjust line upwards m += lr * vert_dist * hor_dist b += lr * vert_dist else: # Adjust line downwards m -= lr * vert_dist * hor_dist b -= lr * vert_dist # Plot data points and regression line plt.figure(figsize=(15,6)) plt.scatter(data.keys(), data.values()) plt.plot(xs, lin(xs)) plt.title('Linear Regression result') print('Slope: {}\nIntercept: {}'.format(m, b)) # Linear regression from scratchimport random# Create data from regressionxs = np.array(range(1,20))ys = [0,8,10,8,15,20,26,29,38,35,40,60,50,61,70,75,80,88,96]​# Put data in dictionarydata = dict()for i in list(xs): data.update({xs[i-1] : ys[i-1]})​# Slopem = 0# y interceptb = 0# Learning ratelr = 0.0001# Number of epochsepochs = 100000​# Formula for linear linedef lin(x): return m * x + b​# Linear regression algorithmfor i in range(epochs): # Pick a random point and calculate vertical distance and horizontal distance rand_point = random.choice(list(data.items())) vert_dist = abs((m * rand_point[0] + b) - rand_point[1]) hor_dist = rand_point[0]​ if (m * rand_point[0] + b) - rand_point[1] < 0: # Adjust line upwards m += lr * vert_dist * hor_dist b += lr * vert_dist else: # Adjust line downwards m -= lr * vert_dist * hor_dist b -= lr * vert_dist # Plot data points and regression lineplt.figure(figsize=(15,6))plt.scatter(data.keys(), data.values())plt.plot(xs, lin(xs))plt.title('Linear Regression result') print('Slope: {}\nIntercept: {}'.format(m, b))";Statistics Tutorial for Data Science
2020-06-10 23:13:12;In this article, I will create a model for credit card fraud detection using machine learning predictive model Autoencoder and python.Lets start with importing librariesThe data set I am going to use contains data about credit card transactions that occurred during a period of two days, with 492 frauds out of 284,807 transactions. All variables in the data set are numerical. The data has been transformed using PCA transformation(s) due to privacy reasons. The two features that haven’t been changed are Time and Amount. Time contains the seconds elapsed between each transaction and the first transaction in the data set.You can download this data set from here – Credit cardChecking the shape of dataChecking for null valuesThere are no null values in the data.Checking number of records of each kind of transaction class (Fraud and Non-Fraud)The data set is highly imbalanced. Looking at each of the fraud(1) and non-fraud(0) transactions.Since only 3 of the features (time, amount and Class) are non-anomyzed, let’s explore them.The time does not seem to be a crucial feature in distinguishing normal vs fraud cases. Hence, I will drop it.The numerical amount in fraud and normal cases differ highly, hence we scale them.We will be using autoencoders for the fraud detection model. Using autoencoders, we train the database only to learn the representation of the non-fraudulent transactions. The reason behind applying this method is to let the model learn the best representation of non-fraudulent cases so that it automatically distinguishes the other case from it.Spiting the data into 80% training and 20% testing;https://thecleverprogrammer.com/2020/06/10/credit-card-fraud-detection-with-machine-learning/;['keras', 'sklearn', 'tensorflow'];1.0;[];['ML', 'ReLu', 'Classification', 'Regression'];['detect', 'epoch', 'regression', 'output layer', 'predict', 'fit', 'autoencoder', 'loss', 'machine learning', 'model', 'classif', 'layer', 'relu', 'train', 'label'];"In this article, I will create a model for credit card fraud detection using machine learning predictive model Autoencoder and python. Lets start with importing libraries import pandas as pd import numpy as np import pickle import matplotlib.pyplot as plt from scipy import stats import tensorflow as tf import seaborn as sns from pylab import rcParams from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LogisticRegression from sklearn.manifold import TSNE from sklearn.metrics import classification_report, accuracy_score from keras.models import Model, load_model from keras.layers import Input, Dense from keras.callbacks import ModelCheckpoint, TensorBoard from keras import regularizers, Sequential %matplotlib inline sns.set(style='whitegrid', palette='muted', font_scale=1.5) rcParams['figure.figsize'] = 14, 8 RANDOM_SEED = 42 LABELS = [""Normal"", ""Fraud""]​x import pandas as pdimport numpy as npimport pickleimport matplotlib.pyplot as pltfrom scipy import statsimport tensorflow as tfimport seaborn as snsfrom pylab import rcParamsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.linear_model import LogisticRegressionfrom sklearn.manifold import TSNEfrom sklearn.metrics import classification_report, accuracy_scorefrom keras.models import Model, load_modelfrom keras.layers import Input, Densefrom keras.callbacks import ModelCheckpoint, TensorBoardfrom keras import regularizers, Sequential%matplotlib inlinesns.set(style='whitegrid', palette='muted', font_scale=1.5)rcParams['figure.figsize'] = 14, 8RANDOM_SEED = 42LABELS = [""Normal"", ""Fraud""] The data set I am going to use contains data about credit card transactions that occurred during a period of two days, with 492 frauds out of 284,807 transactions. All variables in the data set are numerical. The data has been transformed using PCA transformation(s) due to privacy reasons. The two features that haven’t been changed are Time and Amount. Time contains the seconds elapsed between each transaction and the first transaction in the data set. Download the data set You can download this data set from here – Credit card df = pd.read_csv(""creditcard.csv"") df.head() df = pd.read_csv(""creditcard.csv"")df.head() Checking the shape of data df.shape df.shape (284807, 31) (284807, 31) Checking for null values df.isnull().values.any() df.isnull().values.any() False False There are no null values in the data. Checking number of records of each kind of transaction class (Fraud and Non-Fraud) count_classes = pd.value_counts(df['Class'], sort = True) count_classes.plot(kind = 'bar', rot=0) plt.title(""Transaction class distribution"") plt.xticks(range(2), LABELS) plt.xlabel(""Class"") plt.ylabel(""Frequency"") count_classes = pd.value_counts(df['Class'], sort = True)count_classes.plot(kind = 'bar', rot=0)plt.title(""Transaction class distribution"")plt.xticks(range(2), LABELS)plt.xlabel(""Class"")plt.ylabel(""Frequency"") The data set is highly imbalanced. Looking at each of the fraud(1) and non-fraud(0) transactions. frauds = df[df.Class == 1] normal = df[df.Class == 0] frauds.shape frauds = df[df.Class == 1]normal = df[df.Class == 0]frauds.shape (492, 31) (492, 31) normal.shape normal.shape (284315, 31) (284315, 31) Since only 3 of the features (time, amount and Class) are non-anomyzed, let’s explore them. Checking the amount of money involved in each kind of transaction # Fraud transactions frauds.Amount.describe() # Fraud transactionsfrauds.Amount.describe() count 492.000000 mean 122.211321 std 256.683288 min 0.000000 25% 1.000000 50% 9.250000 75% 105.890000 max 2125.870000 Name: Amount, dtype: float64 count 492.000000mean 122.211321std 256.683288min 0.00000025% 1.00000050% 9.25000075% 105.890000max 2125.870000Name: Amount, dtype: float64 # Non-fraud transactions normal.Amount.describe() # Non-fraud transactionsnormal.Amount.describe() count 284315.000000 mean 88.291022 std 250.105092 min 0.000000 25% 5.650000 50% 22.000000 75% 77.050000 max 25691.160000 Name: Amount, dtype: float64 count 284315.000000mean 88.291022std 250.105092min 0.00000025% 5.65000050% 22.00000075% 77.050000max 25691.160000Name: Amount, dtype: float64 Graphical representation of Amount f, (ax1, ax2) = plt.subplots(2, 1, sharex=True) f.suptitle('Amount per transaction by class') bins = 50 ax1.hist(frauds.Amount, bins = bins) ax1.set_title('Fraud') ax2.hist(normal.Amount, bins = bins) ax2.set_title('Normal') plt.xlabel('Amount ($)') plt.ylabel('Number of Transactions') plt.xlim((0, 20000)) plt.yscale('log') plt.show() f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)f.suptitle('Amount per transaction by class')​bins = 50​ax1.hist(frauds.Amount, bins = bins)ax1.set_title('Fraud')​ax2.hist(normal.Amount, bins = bins)ax2.set_title('Normal')​plt.xlabel('Amount ($)')plt.ylabel('Number of Transactions')plt.xlim((0, 20000))plt.yscale('log')plt.show() Plotting time of transaction to check for correlations f, (ax1, ax2) = plt.subplots(2, 1, sharex=True) f.suptitle('Time of transaction vs Amount by class') ax1.scatter(frauds.Time, frauds.Amount) ax1.set_title('Fraud') ax2.scatter(normal.Time, normal.Amount) ax2.set_title('Normal') plt.xlabel('Time (in Seconds)') plt.ylabel('Amount') plt.show() f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)f.suptitle('Time of transaction vs Amount by class')​ax1.scatter(frauds.Time, frauds.Amount)ax1.set_title('Fraud')​ax2.scatter(normal.Time, normal.Amount)ax2.set_title('Normal')​plt.xlabel('Time (in Seconds)')plt.ylabel('Amount')plt.show() The time does not seem to be a crucial feature in distinguishing normal vs fraud cases. Hence, I will drop it. data = df.drop(['Time'], axis=1) data = df.drop(['Time'], axis=1) The numerical amount in fraud and normal cases differ highly, hence we scale them. Scaling the Amount using StandardScaler from sklearn.preprocessing import StandardScaler data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1)) from sklearn.preprocessing import StandardScaler​data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1)) Building the model We will be using autoencoders for the fraud detection model. Using autoencoders, we train the database only to learn the representation of the non-fraudulent transactions. The reason behind applying this method is to let the model learn the best representation of non-fraudulent cases so that it automatically distinguishes the other case from it. non_fraud = data[data['Class'] == 0] #.sample(1000) fraud = data[data['Class'] == 1] df = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True) X = df.drop(['Class'], axis = 1).values Y = df[""Class""].values non_fraud = data[data['Class'] == 0] #.sample(1000)fraud = data[data['Class'] == 1]​df = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)X = df.drop(['Class'], axis = 1).valuesY = df[""Class""].values Spiting the data into 80% training and 20% testing X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED) X_train_fraud = X_train[X_train.Class == 1] X_train = X_train[X_train.Class == 0] X_train = X_train.drop(['Class'], axis=1) y_test = X_test['Class'] X_test = X_test.drop(['Class'], axis=1) X_train = X_train.values X_test = X_test.values X_train.shape X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)X_train_fraud = X_train[X_train.Class == 1]X_train = X_train[X_train.Class == 0]X_train = X_train.drop(['Class'], axis=1)y_test = X_test['Class']X_test = X_test.drop(['Class'], axis=1)X_train = X_train.valuesX_test = X_test.valuesX_train.shape Autoencoder model input_layer = Input(shape=(X.shape[1],)) ## encoding part encoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer) encoded = Dense(50, activation='relu')(encoded) ## decoding part decoded = Dense(50, activation='tanh')(encoded) decoded = Dense(100, activation='tanh')(decoded) ## output layer output_layer = Dense(X.shape[1], activation='relu')(decoded) input_layer = Input(shape=(X.shape[1],))​## encoding partencoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)encoded = Dense(50, activation='relu')(encoded)​## decoding partdecoded = Dense(50, activation='tanh')(encoded)decoded = Dense(100, activation='tanh')(decoded)​## output layeroutput_layer = Dense(X.shape[1], activation='relu')(decoded) Training the credit card fraud detection model autoencoder = Model(input_layer, output_layer) autoencoder.compile(optimizer=""adadelta"", loss=""mse"") autoencoder = Model(input_layer, output_layer)autoencoder.compile(optimizer=""adadelta"", loss=""mse"") Scaling the values x = data.drop([""Class""], axis=1) y = data[""Class""].values x_scale = MinMaxScaler().fit_transform(x.values) x_norm, x_fraud = x_scale[y == 0], x_scale[y == 1] autoencoder.fit(x_norm[0:2000], x_norm[0:2000], batch_size = 256, epochs = 10, shuffle = True, validation_split = 0.20); x = data.drop([""Class""], axis=1)y = data[""Class""].values​x_scale = MinMaxScaler().fit_transform(x.values)x_norm, x_fraud = x_scale[y == 0], x_scale[y == 1]​autoencoder.fit(x_norm[0:2000], x_norm[0:2000], batch_size = 256, epochs = 10, shuffle = True, validation_split = 0.20); Train on 1600 samples, validate on 400 samples Epoch 1/10 1600/1600 [==============================] - 0s 222us/step - loss: 0.6763 - val_loss: 0.4293 Epoch 2/10 1600/1600 [==============================] - 0s 15us/step - loss: 0.4016 - val_loss: 0.2752 Epoch 3/10 1600/1600 [==============================] - 0s 14us/step - loss: 0.2627 - val_loss: 0.1992 Epoch 4/10 1600/1600 [==============================] - 0s 14us/step - loss: 0.1949 - val_loss: 0.1585 Epoch 5/10 1600/1600 [==============================] - 0s 15us/step - loss: 0.1719 - val_loss: 0.1544 Epoch 6/10 1600/1600 [==============================] - 0s 14us/step - loss: 0.1702 - val_loss: 0.1567 Epoch 7/10 1600/1600 [==============================] - 0s 14us/step - loss: 0.1678 - val_loss: 0.1405 Epoch 8/10 1600/1600 [==============================] - 0s 14us/step - loss: 0.1493 - val_loss: 0.1440 Epoch 9/10 1600/1600 [==============================] - 0s 14us/step - loss: 0.1609 - val_loss: 0.1335 Epoch 10/10 1600/1600 [==============================] - 0s 13us/step - loss: 0.1408 - val_loss: 0.1326 Train on 1600 samples, validate on 400 samplesEpoch 1/101600/1600 [==============================] - 0s 222us/step - loss: 0.6763 - val_loss: 0.4293Epoch 2/101600/1600 [==============================] - 0s 15us/step - loss: 0.4016 - val_loss: 0.2752Epoch 3/101600/1600 [==============================] - 0s 14us/step - loss: 0.2627 - val_loss: 0.1992Epoch 4/101600/1600 [==============================] - 0s 14us/step - loss: 0.1949 - val_loss: 0.1585Epoch 5/101600/1600 [==============================] - 0s 15us/step - loss: 0.1719 - val_loss: 0.1544Epoch 6/101600/1600 [==============================] - 0s 14us/step - loss: 0.1702 - val_loss: 0.1567Epoch 7/101600/1600 [==============================] - 0s 14us/step - loss: 0.1678 - val_loss: 0.1405Epoch 8/101600/1600 [==============================] - 0s 14us/step - loss: 0.1493 - val_loss: 0.1440Epoch 9/101600/1600 [==============================] - 0s 14us/step - loss: 0.1609 - val_loss: 0.1335Epoch 10/101600/1600 [==============================] - 0s 13us/step - loss: 0.1408 - val_loss: 0.1326 Obtain the Hidden Representation hidden_representation = Sequential() hidden_representation.add(autoencoder.layers[0]) hidden_representation.add(autoencoder.layers[1]) hidden_representation.add(autoencoder.layers[2]) hidden_representation = Sequential()hidden_representation.add(autoencoder.layers[0])hidden_representation.add(autoencoder.layers[1])hidden_representation.add(autoencoder.layers[2]) Model Prediction norm_hid_rep = hidden_representation.predict(x_norm[:3000]) fraud_hid_rep = hidden_representation.predict(x_fraud) norm_hid_rep = hidden_representation.predict(x_norm[:3000])fraud_hid_rep = hidden_representation.predict(x_fraud) Getting the representation data rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0) y_n = np.zeros(norm_hid_rep.shape[0]) y_f = np.ones(fraud_hid_rep.shape[0]) rep_y = np.append(y_n, y_f) rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)y_n = np.zeros(norm_hid_rep.shape[0])y_f = np.ones(fraud_hid_rep.shape[0])rep_y = np.append(y_n, y_f) Train, test, split train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25) train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25) Credit Card Fraud Detection Prediction model clf = LogisticRegression(solver=""lbfgs"").fit(train_x, train_y) pred_y = clf.predict(val_x) print ("""") print (""Classification Report: "") print (classification_report(val_y, pred_y)) print ("""") print (""Accuracy Score: "", accuracy_score(val_y, pred_y)) clf = LogisticRegression(solver=""lbfgs"").fit(train_x, train_y)pred_y = clf.predict(val_x)​print ("""")print (""Classification Report: "")print (classification_report(val_y, pred_y))​print ("""")print (""Accuracy Score: "", accuracy_score(val_y, pred_y)) Classification Report: precision recall f1-score support 0.0 0.96 1.00 0.98 748 1.0 1.00 0.76 0.86 125 accuracy 0.97 873 macro avg 0.98 0.88 0.92 873 weighted avg 0.97 0.97 0.96 873 Accuracy Score: 0.9656357388316151 Classification Report: precision recall f1-score support​ 0.0 0.96 1.00 0.98 748 1.0 1.00 0.76 0.86 125​ accuracy 0.97 873 macro avg 0.98 0.88 0.92 873weighted avg 0.97 0.97 0.96 873​​Accuracy Score: 0.9656357388316151";Credit Card Fraud Detection with Machine Learning
2020-06-11 23:29:55;In this article, I will show how we can do Weather Forecasting with Machine Learning algorithm and compare some frameworks for further classification.Lets start this task by importing the librariesDownload and read the data setTo look at first 5 rows of the dataWe have got an unexpected column named Unnamed: 0. Well, this is a very common problem. We face this when our csv file has an index column which has no name. here is how we can get rid of it.Now, we’ll make an attribute that would contain date (month, year). So that we could get temperature values with the timeline.On a closer look, by clicking on One Year View, we can see that the graph seems distorted because this is how the values really are. The temperature varies every year with months.We also will stick to these seasons for our analysis.A cluster size of 3 seems a good choice hereThere is a cluster from 26.2-27.5 and mean temperature for most months during history has been between 26.8-26.9Let’s see if we can get some insights from yearly mean temperature data. I am going to treat this as a time series as well.We can see clear positive trend lines. Let’s see if we could find any trend in seasonal mean temperatures.;https://thecleverprogrammer.com/2020/06/11/weather-forecasting-with-machine-learning/;['pattern', 'sklearn'];1.0;[];['ML', 'Classification', 'Decision Tree'];['predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'label', 'decision tree'];"In this article, I will show how we can do Weather Forecasting with Machine Learning algorithm and compare some frameworks for further classification. Lets start this task by importing the libraries import numpy as np # For Linear Algebra import pandas as pd # To Work With Data # for visualizations import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots from datetime import datetime # Time Series analysis.​x import numpy as np # For Linear Algebraimport pandas as pd # To Work With Data# for visualizationsimport plotly.express as px import plotly.graph_objects as gofrom plotly.subplots import make_subplotsfrom datetime import datetime # Time Series analysis. Download and read the data set WeatherDownload df = pd.read_csv(""Weather.csv"") df = pd.read_csv(""Weather.csv"") To look at first 5 rows of the data df.head() # This will show us top 5 rows of the dataset by default. df.head() # This will show us top 5 rows of the dataset by default. We have got an unexpected column named Unnamed: 0. Well, this is a very common problem. We face this when our csv file has an index column which has no name. here is how we can get rid of it. df = pd.read_csv(""Weather.csv"", index_col=0) df = pd.read_csv(""Weather.csv"", index_col=0) Now, we’ll make an attribute that would contain date (month, year). So that we could get temperature values with the timeline. df1 = pd.melt(df, id_vars='YEAR', value_vars=df.columns[1:]) ## This will melt the data df1.head() df1 = pd.melt(df, id_vars='YEAR', value_vars=df.columns[1:]) ## This will melt the datadf1.head() df1['Date'] = df1['variable'] + ' ' + df1['YEAR'].astype(str) df1.loc[:,'Date'] = df1['Date'].apply(lambda x : datetime.strptime(x, '%b %Y')) ## Converting String to datetime object df1.head() df1['Date'] = df1['variable'] + ' ' + df1['YEAR'].astype(str) df1.loc[:,'Date'] = df1['Date'].apply(lambda x : datetime.strptime(x, '%b %Y')) ## Converting String to datetime objectdf1.head() Temperature through time df1.columns=['Year', 'Month', 'Temprature', 'Date'] df1.sort_values(by='Date', inplace=True) ## To get the time series right. fig = go.Figure(layout = go.Layout(yaxis=dict(range=[0, df1['Temprature'].max()+1]))) fig.add_trace(go.Scatter(x=df1['Date'], y=df1['Temprature']), ) fig.update_layout(title='Temprature Throught Timeline:', xaxis_title='Time', yaxis_title='Temprature in Degrees') fig.update_layout(xaxis=go.layout.XAxis( rangeselector=dict( buttons=list([dict(label=""Whole View"", step=""all""), dict(count=1,label=""One Year View"",step=""year"",stepmode=""todate"") ])), rangeslider=dict(visible=True),type=""date"") ) fig.show() df1.columns=['Year', 'Month', 'Temprature', 'Date']df1.sort_values(by='Date', inplace=True) ## To get the time series right.fig = go.Figure(layout = go.Layout(yaxis=dict(range=[0, df1['Temprature'].max()+1])))fig.add_trace(go.Scatter(x=df1['Date'], y=df1['Temprature']), )fig.update_layout(title='Temprature Throught Timeline:', xaxis_title='Time', yaxis_title='Temprature in Degrees')fig.update_layout(xaxis=go.layout.XAxis( rangeselector=dict( buttons=list([dict(label=""Whole View"", step=""all""), dict(count=1,label=""One Year View"",step=""year"",stepmode=""todate"") ])), rangeslider=dict(visible=True),type=""date""))fig.show() On a closer look, by clicking on One Year View, we can see that the graph seems distorted because this is how the values really are. The temperature varies every year with months. Insights: May 1921 has been the hottest month in india in the history. What could be the reason ?Dec, Jan and Feb are the coldest months. One could group them together as “Winter”.Apr, May, Jun, July and Aug are the hottest months. One could group them together as “Summer”. But, since this is not how seasons work. We have four main seasons in India and this is how they are grouped: Winter : December, January and February.Summer(Also called, “Pre Monsoon Season”) : March, April and May.Monsoon : June, July, August and September.Autumn(Also called “Post Monsoon Season) : October and November. We also will stick to these seasons for our analysis. Warmest /Coldest/Average : fig = px.box(df1, 'Month', 'Temprature') fig.update_layout(title='Warmest, Coldest and Median Monthly Tempratue.') fig.show() fig = px.box(df1, 'Month', 'Temprature')fig.update_layout(title='Warmest, Coldest and Median Monthly Tempratue.')fig.show() Insights: January has the coldest Days in an Year.May has the hottest days in an Year.July is the month with least Standard Deviation which means, temperature in July vary least. We can expect any day in July to be a warm day. from sklearn.cluster import KMeans sse = [] target = df1['Temprature'].to_numpy().reshape(-1,1) num_clusters = list(range(1, 10)) for k in num_clusters: km = KMeans(n_clusters=k) km.fit(target) sse.append(km.inertia_) fig = go.Figure(data=[ go.Scatter(x = num_clusters, y=sse, mode='lines'), go.Scatter(x = num_clusters, y=sse, mode='markers') ]) fig.update_layout(title=""Evaluation on number of clusters:"", xaxis_title = ""Number of Clusters:"", yaxis_title = ""Sum of Squared Distance"", showlegend=False) fig.show() from sklearn.cluster import KMeanssse = []target = df1['Temprature'].to_numpy().reshape(-1,1)num_clusters = list(range(1, 10))​for k in num_clusters: km = KMeans(n_clusters=k) km.fit(target) sse.append(km.inertia_)​fig = go.Figure(data=[ go.Scatter(x = num_clusters, y=sse, mode='lines'), go.Scatter(x = num_clusters, y=sse, mode='markers')])​fig.update_layout(title=""Evaluation on number of clusters:"", xaxis_title = ""Number of Clusters:"", yaxis_title = ""Sum of Squared Distance"", showlegend=False)fig.show() A cluster size of 3 seems a good choice here km = KMeans(3) km.fit(df1['Temprature'].to_numpy().reshape(-1,1)) df1.loc[:,'Temp Labels'] = km.labels_ fig = px.scatter(df1, 'Date', 'Temprature', color='Temp Labels') fig.update_layout(title = ""Temprature clusters."", xaxis_title=""Date"", yaxis_title=""Temprature"") fig.show() km = KMeans(3)km.fit(df1['Temprature'].to_numpy().reshape(-1,1))df1.loc[:,'Temp Labels'] = km.labels_fig = px.scatter(df1, 'Date', 'Temprature', color='Temp Labels')fig.update_layout(title = ""Temprature clusters."", xaxis_title=""Date"", yaxis_title=""Temprature"")fig.show() Insights: Despite having 4 seasons we can see 3 main clusters based on temperatures.Jan, Feb and Dec are the coldest months.Apr, May, Jun, Jul, Aug and Sep; all have hotter temperatures.Mar, Oct and Nov are the months that have temperatures neither too hot nor too cold. fig = px.histogram(x=df1['Temprature'], nbins=200, histnorm='density') fig.update_layout(title='Frequency chart of temprature readings:', xaxis_title='Temprature', yaxis_title='Count') fig = px.histogram(x=df1['Temprature'], nbins=200, histnorm='density')fig.update_layout(title='Frequency chart of temprature readings:', xaxis_title='Temprature', yaxis_title='Count') There is a cluster from 26.2-27.5 and mean temperature for most months during history has been between 26.8-26.9 Let’s see if we can get some insights from yearly mean temperature data. I am going to treat this as a time series as well. Yearly average temperature df['Yearly Mean'] = df.iloc[:,1:].mean(axis=1) ## Axis 1 for row wise and axis 0 for columns. fig = go.Figure(data=[ go.Scatter(name='Yearly Tempratures' , x=df['YEAR'], y=df['Yearly Mean'], mode='lines'), go.Scatter(name='Yearly Tempratures' , x=df['YEAR'], y=df['Yearly Mean'], mode='markers') ]) fig.update_layout(title='Yearly Mean Temprature :', xaxis_title='Time', yaxis_title='Temprature in Degrees') fig.show() df['Yearly Mean'] = df.iloc[:,1:].mean(axis=1) ## Axis 1 for row wise and axis 0 for columns.fig = go.Figure(data=[ go.Scatter(name='Yearly Tempratures' , x=df['YEAR'], y=df['Yearly Mean'], mode='lines'), go.Scatter(name='Yearly Tempratures' , x=df['YEAR'], y=df['Yearly Mean'], mode='markers')])fig.update_layout(title='Yearly Mean Temprature :', xaxis_title='Time', yaxis_title='Temprature in Degrees')fig.show() We can see that the issue of global warning is true. The yearly mean temperature was not increasing till 1980. It was only after 1979 that we can see the gradual increase in yearly mean temperature.After 2015, yearly temperature has increased drastically.But, There are some problems in this figure.We are seeing a monthly like up-down pattern in yearly temperatures as well.This is not understandable. Because with months, we have a phenomena of seasons and the earth the revolving around sun in a elliptic path. But this pattern is not expected in yearly temperature. Monthly temperatures through history fig = px.line(df1, 'Year', 'Temprature', facet_col='Month', facet_col_wrap=4) fig.update_layout(title='Monthly temprature throught history:') fig.show() fig = px.line(df1, 'Year', 'Temprature', facet_col='Month', facet_col_wrap=4)fig.update_layout(title='Monthly temprature throught history:')fig.show() We can see clear positive trend lines. Let’s see if we could find any trend in seasonal mean temperatures. Seasonal Weather Analysis df['Winter'] = df[['DEC', 'JAN', 'FEB']].mean(axis=1) df['Summer'] = df[['MAR', 'APR', 'MAY']].mean(axis=1) df['Monsoon'] = df[['JUN', 'JUL', 'AUG', 'SEP']].mean(axis=1) df['Autumn'] = df[['OCT', 'NOV']].mean(axis=1) seasonal_df = df[['YEAR', 'Winter', 'Summer', 'Monsoon', 'Autumn']] seasonal_df = pd.melt(seasonal_df, id_vars='YEAR', value_vars=seasonal_df.columns[1:]) seasonal_df.columns=['Year', 'Season', 'Temprature'] fig = px.scatter(seasonal_df, 'Year', 'Temprature', facet_col='Season', facet_col_wrap=2, trendline='ols') fig.update_layout(title='Seasonal mean tempratures throught years:') fig.show() df['Winter'] = df[['DEC', 'JAN', 'FEB']].mean(axis=1)df['Summer'] = df[['MAR', 'APR', 'MAY']].mean(axis=1)df['Monsoon'] = df[['JUN', 'JUL', 'AUG', 'SEP']].mean(axis=1)df['Autumn'] = df[['OCT', 'NOV']].mean(axis=1)seasonal_df = df[['YEAR', 'Winter', 'Summer', 'Monsoon', 'Autumn']]seasonal_df = pd.melt(seasonal_df, id_vars='YEAR', value_vars=seasonal_df.columns[1:])seasonal_df.columns=['Year', 'Season', 'Temprature']​fig = px.scatter(seasonal_df, 'Year', 'Temprature', facet_col='Season', facet_col_wrap=2, trendline='ols')fig.update_layout(title='Seasonal mean tempratures throught years:')fig.show() We can again see a positive trend line between temperature and time. The trend line does not have a very high positive correlation with years but still it is not negligible. Let’s try to find out if we can get something out of an animation px.scatter(df1, 'Month', 'Temprature', size='Temprature', animation_frame='Year') px.scatter(df1, 'Month', 'Temprature', size='Temprature', animation_frame='Year') On first look, we can see some fluctuations but that doesn’t give much of insights for us. However, if we again see by arranging bar below to early years and late years we can notice the change. But this is certainly not the best way to visualize it. Let’s find some better way. Weather Forecasting with Machine Learning Let’s try to forecast monthly mean temperature for year 2018. # I am using decision tree regressor for prediction as the data does not actually have a linear trend. from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score df2 = df1[['Year', 'Month', 'Temprature']].copy() df2 = pd.get_dummies(df2) y = df2[['Temprature']] x = df2.drop(columns='Temprature') dtr = DecisionTreeRegressor() train_x, test_x, train_y, test_y = train_test_split(x,y,test_size=0.3) dtr.fit(train_x, train_y) pred = dtr.predict(test_x) r2_score(test_y, pred) # I am using decision tree regressor for prediction as the data does not actually have a linear trend.from sklearn.tree import DecisionTreeRegressorfrom sklearn.model_selection import train_test_split from sklearn.metrics import r2_score ​df2 = df1[['Year', 'Month', 'Temprature']].copy()df2 = pd.get_dummies(df2)y = df2[['Temprature']]x = df2.drop(columns='Temprature')​dtr = DecisionTreeRegressor()train_x, test_x, train_y, test_y = train_test_split(x,y,test_size=0.3)dtr.fit(train_x, train_y)pred = dtr.predict(test_x)r2_score(test_y, pred) #Output0.9678939568898212 A high r2 value means that our predictive model is working good. Now, Let’s see the foretasted data for 2018. next_Year = df1[df1['Year']==2017][['Year', 'Month']] next_Year.Year.replace(2017,2018, inplace=True) next_Year= pd.get_dummies(next_Year) temp_2018 = dtr.predict(next_Year) temp_2018 = {'Month':df1['Month'].unique(), 'Temprature':temp_2018} temp_2018=pd.DataFrame(temp_2018) temp_2018['Year'] = 2018 temp_2018 next_Year = df1[df1['Year']==2017][['Year', 'Month']]next_Year.Year.replace(2017,2018, inplace=True)next_Year= pd.get_dummies(next_Year)temp_2018 = dtr.predict(next_Year)​temp_2018 = {'Month':df1['Month'].unique(), 'Temprature':temp_2018}temp_2018=pd.DataFrame(temp_2018)temp_2018['Year'] = 2018temp_2018 forecasted_temp = pd.concat([df1,temp_2018], sort=False).groupby(by='Year')['Temprature'].mean().reset_index() fig = go.Figure(data=[ go.Scatter(name='Yearly Mean Temprature', x=forecasted_temp['Year'], y=forecasted_temp['Temprature'], mode='lines'), go.Scatter(name='Yearly Mean Temprature', x=forecasted_temp ['Year'], y=forecasted_temp['Temprature'], mode='markers') ]) fig.update_layout(title='Forecasted Temprature:', xaxis_title='Time', yaxis_title='Temprature in Degrees') fig.show() forecasted_temp = pd.concat([df1,temp_2018], sort=False).groupby(by='Year')['Temprature'].mean().reset_index()fig = go.Figure(data=[ go.Scatter(name='Yearly Mean Temprature', x=forecasted_temp['Year'], y=forecasted_temp['Temprature'], mode='lines'), go.Scatter(name='Yearly Mean Temprature', x=forecasted_temp ['Year'], y=forecasted_temp['Temprature'], mode='markers')])fig.update_layout(title='Forecasted Temprature:', xaxis_title='Time', yaxis_title='Temprature in Degrees')fig.show()";Weather Forecasting with Machine Learning
2020-06-12 15:36:15;Yolo is a deep learning algorithm that uses convolutional neural networks for object detection. So what’s great about object detection? In comparison to recognition algorithms, a detection algorithm does not only predict class labels, but detects locations of objects as well.;https://thecleverprogrammer.com/2020/06/12/object-detection-with-deep-learning-using-yolo-and-tensorflow/;['Pillow', 'tensorflow'];1.0;[];['ResNet', 'NN', 'Regression', 'DL', 'Object Detection', 'CNN', 'ReLu', 'Clustering', 'K-Means'];['detect', 'regression', 'relu', 'filter', 'convolutional neural network', 'train', 'label', 'propagation', 'model', 'activation function', 'clustering', 'recogn', 'k-means', 'predict', 'neural network', 'layer', 'deep learning', 'resnet', 'object detection', 'neuron'];"Yolo v3 Object Detection in Tensorflow full tutorial What is Yolo? Yolo is a deep learning algorithm that uses convolutional neural networks for object detection.So what’s great about object detection? In comparison to recognition algorithms, a detection algorithm does not only predict class labels, but detects locations of objects as well. Dependencies To build Yolo we’re going to need Tensorflow (deep learning), NumPy (numerical computation) and Pillow (image processing) libraries. Also I am going to use seaborn’s color palette for bounding boxes colors. Finally, let’s import IPython function display() to display images in the notebook. Download required files Lets start with importing necessary libraries import tensorflow as tf import numpy as np from PIL import Image, ImageDraw, ImageFont from IPython.display import display from seaborn import color_palette import cv2 import tensorflow as tfimport numpy as npfrom PIL import Image, ImageDraw, ImageFontfrom IPython.display import displayfrom seaborn import color_paletteimport cv2 Model hyperparameters Next, I will define some configurations for Yolo. _BATCH_NORM_DECAY = 0.9 _BATCH_NORM_EPSILON = 1e-05 _LEAKY_RELU = 0.1 _ANCHORS = [(10, 13), (16, 30), (33, 23), (30, 61), (62, 45), (59, 119), (116, 90), (156, 198), (373, 326)] _MODEL_SIZE = (416, 416) _BATCH_NORM_DECAY = 0.9_BATCH_NORM_EPSILON = 1e-05_LEAKY_RELU = 0.1_ANCHORS = [(10, 13), (16, 30), (33, 23), (30, 61), (62, 45), (59, 119), (116, 90), (156, 198), (373, 326)]_MODEL_SIZE = (416, 416) _MODEL_SIZE refers to the input size of the model. Let’s look at all parameters step-by-step. Batch normalization Almost every convolutional layer in Yolo has batch normalization after it. It helps the model train faster and reduces variance between units (and total variance as well). Batch normalization is defined as follows. BATCH_NORM_EPSILON refers to epsilon in this formula, whereas _BATCH_NORM_DECAY refers to momentum, which is used for computing moving average and variance. We use them in forward propagation during inference (after training). moving_average = momentum * moving_average + (1 - momentum) * current_average Leaky ReLU Leaky ReLU is a slight modification of ReLU activation function. The idea behind Leaky ReLU is to prevent so-called “neuron dying” when a large number of activations become 0. _LEAKY_RELU refers to alpha. Anchors Anchors are sort of bounding box priors, that were calculated on the COCO dataset using k-means clustering. We are going to predict the width and height of the box as offsets from cluster centroids. The center coordinates of the box relative to the location of filter application are predicted using a sigmoid function.bx=σ(tx)+cxby=σ(ty)+cybw=pwetwbh=pheth Where bx and by are the center coordinates of the box, bw and bh are the width and height of the box, cx and cy are the location of filter application and ti are predicted during regression. Model definition I refered to the official ResNet implementation in Tensorflow in terms of how to arange the code. Batch norm and fixed padding It’s useful to define batch_norm function since the model uses batch norms with shared parameters heavily. Also, same as ResNet, Yolo uses convolution with fixed padding, which means that padding is defined only by the size of the kernel. def batch_norm(inputs, training, data_format): """"""Performs a batch normalization using a standard set of parameters."""""" return tf.layers.batch_normalization( inputs=inputs, axis=1 if data_format == 'channels_first' else 3, momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, scale=True, training=training) def fixed_padding(inputs, kernel_size, data_format): """"""ResNet implementation of fixed padding. Pads the input along the spatial dimensions independently of input size. Args: inputs: Tensor input to be padded. kernel_size: The kernel to be used in the conv2d or max_pool2d. data_format: The input format. Returns: A tensor with the same format as the input. """""" pad_total = kernel_size - 1 pad_beg = pad_total // 2 pad_end = pad_total - pad_beg if data_format == 'channels_first': padded_inputs = tf.pad(inputs, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]]) else: padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]]) return padded_inputs def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1): """"""Strided 2-D convolution with explicit padding."""""" if strides > 1: inputs = fixed_padding(inputs, kernel_size, data_format) return tf.layers.conv2d( inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, padding=('SAME' if strides == 1 else 'VALID'), use_bias=False, data_format=data_format) def batch_norm(inputs, training, data_format): """"""Performs a batch normalization using a standard set of parameters."""""" return tf.layers.batch_normalization( inputs=inputs, axis=1 if data_format == 'channels_first' else 3, momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, scale=True, training=training)​​def fixed_padding(inputs, kernel_size, data_format): """"""ResNet implementation of fixed padding.​ Pads the input along the spatial dimensions independently of input size.​ Args: inputs: Tensor input to be padded. kernel_size: The kernel to be used in the conv2d or max_pool2d. data_format: The input format. Returns: A tensor with the same format as the input. """""" pad_total = kernel_size - 1 pad_beg = pad_total // 2 pad_end = pad_total - pad_beg​ if data_format == 'channels_first': padded_inputs = tf.pad(inputs, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]]) else: padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]]) return padded_inputs​​def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1): """"""Strided 2-D convolution with explicit padding."""""" if strides > 1: inputs = fixed_padding(inputs, kernel_size, data_format)​ return tf.layers.conv2d( inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, padding=('SAME' if strides == 1 else 'VALID'), use_bias=False, data_format=data_format) Feature extraction: Darknet-53 For feature extraction Yolo uses Darknet-53 neural net pretrained on ImageNet. Same as ResNet, Darknet-53 has shortcut (residual) connections, which help information from earlier layers flow further. We omit the last 3 layers (Avgpool, Connected and Softmax) since we only need the features. def darknet53_residual_block(inputs, filters, training, data_format, strides=1): """"""Creates a residual block for Darknet."""""" shortcut = inputs inputs = conv2d_fixed_padding( inputs, filters=filters, kernel_size=1, strides=strides, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) inputs = conv2d_fixed_padding( inputs, filters=2 * filters, kernel_size=3, strides=strides, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) inputs += shortcut return inputs def darknet53(inputs, training, data_format): """"""Creates Darknet53 model for feature extraction."""""" inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) inputs = darknet53_residual_block(inputs, filters=32, training=training, data_format=data_format) inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) for _ in range(2): inputs = darknet53_residual_block(inputs, filters=64, training=training, data_format=data_format) inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) for _ in range(8): inputs = darknet53_residual_block(inputs, filters=128, training=training, data_format=data_format) route1 = inputs inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) for _ in range(8): inputs = darknet53_residual_block(inputs, filters=256, training=training, data_format=data_format) route2 = inputs inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) for _ in range(4): inputs = darknet53_residual_block(inputs, filters=512, training=training, data_format=data_format) return route1, route2, inputs def darknet53_residual_block(inputs, filters, training, data_format, strides=1): """"""Creates a residual block for Darknet."""""" shortcut = inputs​ inputs = conv2d_fixed_padding( inputs, filters=filters, kernel_size=1, strides=strides, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ inputs = conv2d_fixed_padding( inputs, filters=2 * filters, kernel_size=3, strides=strides, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ inputs += shortcut​ return inputs​​def darknet53(inputs, training, data_format): """"""Creates Darknet53 model for feature extraction."""""" inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ inputs = darknet53_residual_block(inputs, filters=32, training=training, data_format=data_format)​ inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ for _ in range(2): inputs = darknet53_residual_block(inputs, filters=64, training=training, data_format=data_format)​ inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ for _ in range(8): inputs = darknet53_residual_block(inputs, filters=128, training=training, data_format=data_format)​ route1 = inputs​ inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ for _ in range(8): inputs = darknet53_residual_block(inputs, filters=256, training=training, data_format=data_format)​ route2 = inputs​ inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3, strides=2, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ for _ in range(4): inputs = darknet53_residual_block(inputs, filters=512, training=training, data_format=data_format)​ return route1, route2, inputs Convolution layers Yolo has a large number of convolutional layers. It’s useful to group them in blocks. def yolo_convolution_block(inputs, filters, training, data_format): """"""Creates convolution operations layer used after Darknet."""""" inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) route = inputs inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) return route, inputs def yolo_convolution_block(inputs, filters, training, data_format): """"""Creates convolution operations layer used after Darknet."""""" inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ route = inputs​ inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3, data_format=data_format) inputs = batch_norm(inputs, training=training, data_format=data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)​ return route, inputs Detection layers Yolo has 3 detection layers, that detect on 3 different scales using respective anchors. For each cell in the feature map the detection layer predicts n_anchors * (5 + n_classes) values using 1×1 convolution. For each scale we have n_anchors = 3. 5 + n_classes means that respectively to each of 3 anchors we are going to predict 4 coordinates of the box, its confidence score (the probability of containing an object) and class probabilities. def yolo_layer(inputs, n_classes, anchors, img_size, data_format): """"""Creates Yolo final detection layer. Detects boxes with respect to anchors. Args: inputs: Tensor input. n_classes: Number of labels. anchors: A list of anchor sizes. img_size: The input size of the model. data_format: The input format. Returns: Tensor output. """""" n_anchors = len(anchors) inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes), kernel_size=1, strides=1, use_bias=True, data_format=data_format) shape = inputs.get_shape().as_list() grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3] if data_format == 'channels_first': inputs = tf.transpose(inputs, [0, 2, 3, 1]) inputs = tf.reshape(inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1], 5 + n_classes]) strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1]) box_centers, box_shapes, confidence, classes = tf.split(inputs, [2, 2, 1, n_classes], axis=-1) x = tf.range(grid_shape[0], dtype=tf.float32) y = tf.range(grid_shape[1], dtype=tf.float32) x_offset, y_offset = tf.meshgrid(x, y) x_offset = tf.reshape(x_offset, (-1, 1)) y_offset = tf.reshape(y_offset, (-1, 1)) x_y_offset = tf.concat([x_offset, y_offset], axis=-1) x_y_offset = tf.tile(x_y_offset, [1, n_anchors]) x_y_offset = tf.reshape(x_y_offset, [1, -1, 2]) box_centers = tf.nn.sigmoid(box_centers) box_centers = (box_centers + x_y_offset) * strides anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1], 1]) box_shapes = tf.exp(box_shapes) * tf.to_float(anchors) confidence = tf.nn.sigmoid(confidence) classes = tf.nn.sigmoid(classes) inputs = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1) return inputs def yolo_layer(inputs, n_classes, anchors, img_size, data_format): """"""Creates Yolo final detection layer.​ Detects boxes with respect to anchors.​ Args: inputs: Tensor input. n_classes: Number of labels. anchors: A list of anchor sizes. img_size: The input size of the model. data_format: The input format.​ Returns: Tensor output. """""" n_anchors = len(anchors)​ inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes), kernel_size=1, strides=1, use_bias=True, data_format=data_format)​ shape = inputs.get_shape().as_list() grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3] if data_format == 'channels_first': inputs = tf.transpose(inputs, [0, 2, 3, 1]) inputs = tf.reshape(inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1], 5 + n_classes])​ strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])​ box_centers, box_shapes, confidence, classes = tf.split(inputs, [2, 2, 1, n_classes], axis=-1)​ x = tf.range(grid_shape[0], dtype=tf.float32) y = tf.range(grid_shape[1], dtype=tf.float32) x_offset, y_offset = tf.meshgrid(x, y) x_offset = tf.reshape(x_offset, (-1, 1)) y_offset = tf.reshape(y_offset, (-1, 1)) x_y_offset = tf.concat([x_offset, y_offset], axis=-1) x_y_offset = tf.tile(x_y_offset, [1, n_anchors]) x_y_offset = tf.reshape(x_y_offset, [1, -1, 2]) box_centers = tf.nn.sigmoid(box_centers) box_centers = (box_centers + x_y_offset) * strides​ anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1], 1]) box_shapes = tf.exp(box_shapes) * tf.to_float(anchors)​ confidence = tf.nn.sigmoid(confidence)​ classes = tf.nn.sigmoid(classes)​ inputs = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)​ return inputs Upsample layer In order to concatenate with shortcut outputs from Darknet-53 before applying detection on a different scale, we are going to upsample the feature map using nearest neighbor interpolation. def upsample(inputs, out_shape, data_format): """"""Upsamples to `out_shape` using nearest neighbor interpolation."""""" if data_format == 'channels_first': inputs = tf.transpose(inputs, [0, 2, 3, 1]) new_height = out_shape[3] new_width = out_shape[2] else: new_height = out_shape[2] new_width = out_shape[1] inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width)) if data_format == 'channels_first': inputs = tf.transpose(inputs, [0, 3, 1, 2]) return inputs def upsample(inputs, out_shape, data_format): """"""Upsamples to `out_shape` using nearest neighbor interpolation."""""" if data_format == 'channels_first': inputs = tf.transpose(inputs, [0, 2, 3, 1]) new_height = out_shape[3] new_width = out_shape[2] else: new_height = out_shape[2] new_width = out_shape[1]​ inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width))​ if data_format == 'channels_first': inputs = tf.transpose(inputs, [0, 3, 1, 2])​ return inputs Non-max suppression The model is going to produce a lot of boxes, so we need a way to discard the boxes with low confidence scores. Also, to avoid having multiple boxes for one object, we will discard the boxes with high overlap as well using non-max suppresion for each class. def build_boxes(inputs): """"""Computes top left and bottom right points of the boxes."""""" center_x, center_y, width, height, confidence, classes = tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1) top_left_x = center_x - width / 2 top_left_y = center_y - height / 2 bottom_right_x = center_x + width / 2 bottom_right_y = center_y + height / 2 boxes = tf.concat([top_left_x, top_left_y, bottom_right_x, bottom_right_y, confidence, classes], axis=-1) return boxes def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold, confidence_threshold): """"""Performs non-max suppression separately for each class. Args: inputs: Tensor input. n_classes: Number of classes. max_output_size: Max number of boxes to be selected for each class. iou_threshold: Threshold for the IOU. confidence_threshold: Threshold for the confidence score. Returns: A list containing class-to-boxes dictionaries for each sample in the batch. """""" batch = tf.unstack(inputs) boxes_dicts = [] for boxes in batch: boxes = tf.boolean_mask(boxes, boxes[:, 4] > confidence_threshold) classes = tf.argmax(boxes[:, 5:], axis=-1) classes = tf.expand_dims(tf.to_float(classes), axis=-1) boxes = tf.concat([boxes[:, :5], classes], axis=-1) boxes_dict = dict() for cls in range(n_classes): mask = tf.equal(boxes[:, 5], cls) mask_shape = mask.get_shape() if mask_shape.ndims != 0: class_boxes = tf.boolean_mask(boxes, mask) boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes, [4, 1, -1], axis=-1) boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1]) indices = tf.image.non_max_suppression(boxes_coords, boxes_conf_scores, max_output_size, iou_threshold) class_boxes = tf.gather(class_boxes, indices) boxes_dict[cls] = class_boxes[:, :5] boxes_dicts.append(boxes_dict) return boxes_dicts def build_boxes(inputs): """"""Computes top left and bottom right points of the boxes."""""" center_x, center_y, width, height, confidence, classes = tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)​ top_left_x = center_x - width / 2 top_left_y = center_y - height / 2 bottom_right_x = center_x + width / 2 bottom_right_y = center_y + height / 2​ boxes = tf.concat([top_left_x, top_left_y, bottom_right_x, bottom_right_y, confidence, classes], axis=-1)​ return boxes​​def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold, confidence_threshold): """"""Performs non-max suppression separately for each class.​ Args: inputs: Tensor input. n_classes: Number of classes. max_output_size: Max number of boxes to be selected for each class. iou_threshold: Threshold for the IOU. confidence_threshold: Threshold for the confidence score. Returns: A list containing class-to-boxes dictionaries for each sample in the batch. """""" batch = tf.unstack(inputs) boxes_dicts = [] for boxes in batch: boxes = tf.boolean_mask(boxes, boxes[:, 4] > confidence_threshold) classes = tf.argmax(boxes[:, 5:], axis=-1) classes = tf.expand_dims(tf.to_float(classes), axis=-1) boxes = tf.concat([boxes[:, :5], classes], axis=-1)​ boxes_dict = dict() for cls in range(n_classes): mask = tf.equal(boxes[:, 5], cls) mask_shape = mask.get_shape() if mask_shape.ndims != 0: class_boxes = tf.boolean_mask(boxes, mask) boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes, [4, 1, -1], axis=-1) boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1]) indices = tf.image.non_max_suppression(boxes_coords, boxes_conf_scores, max_output_size, iou_threshold) class_boxes = tf.gather(class_boxes, indices) boxes_dict[cls] = class_boxes[:, :5]​ boxes_dicts.append(boxes_dict)​ return boxes_dicts Final model class Finally, let’s define the model class using all of the layers described previously. class Yolo_v3: """"""Yolo v3 model class."""""" def __init__(self, n_classes, model_size, max_output_size, iou_threshold, confidence_threshold, data_format=None): """"""Creates the model. Args: n_classes: Number of class labels. model_size: The input size of the model. max_output_size: Max number of boxes to be selected for each class. iou_threshold: Threshold for the IOU. confidence_threshold: Threshold for the confidence score. data_format: The input format. Returns: None. """""" if not data_format: if tf.test.is_built_with_cuda(): data_format = 'channels_first' else: data_format = 'channels_last' self.n_classes = n_classes self.model_size = model_size self.max_output_size = max_output_size self.iou_threshold = iou_threshold self.confidence_threshold = confidence_threshold self.data_format = data_format def __call__(self, inputs, training): """"""Add operations to detect boxes for a batch of input images. Args: inputs: A Tensor representing a batch of input images. training: A boolean, whether to use in training or inference mode. Returns: A list containing class-to-boxes dictionaries for each sample in the batch. """""" with tf.variable_scope('yolo_v3_model'): if self.data_format == 'channels_first': inputs = tf.transpose(inputs, [0, 3, 1, 2]) inputs = inputs / 255 route1, route2, inputs = darknet53(inputs, training=training, data_format=self.data_format) route, inputs = yolo_convolution_block( inputs, filters=512, training=training, data_format=self.data_format) detect1 = yolo_layer(inputs, n_classes=self.n_classes, anchors=_ANCHORS[6:9], img_size=self.model_size, data_format=self.data_format) inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1, data_format=self.data_format) inputs = batch_norm(inputs, training=training, data_format=self.data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) upsample_size = route2.get_shape().as_list() inputs = upsample(inputs, out_shape=upsample_size, data_format=self.data_format) axis = 1 if self.data_format == 'channels_first' else 3 inputs = tf.concat([inputs, route2], axis=axis) route, inputs = yolo_convolution_block( inputs, filters=256, training=training, data_format=self.data_format) detect2 = yolo_layer(inputs, n_classes=self.n_classes, anchors=_ANCHORS[3:6], img_size=self.model_size, data_format=self.data_format) inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1, data_format=self.data_format) inputs = batch_norm(inputs, training=training, data_format=self.data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) upsample_size = route1.get_shape().as_list() inputs = upsample(inputs, out_shape=upsample_size, data_format=self.data_format) inputs = tf.concat([inputs, route1], axis=axis) route, inputs = yolo_convolution_block( inputs, filters=128, training=training, data_format=self.data_format) detect3 = yolo_layer(inputs, n_classes=self.n_classes, anchors=_ANCHORS[0:3], img_size=self.model_size, data_format=self.data_format) inputs = tf.concat([detect1, detect2, detect3], axis=1) inputs = build_boxes(inputs) boxes_dicts = non_max_suppression( inputs, n_classes=self.n_classes, max_output_size=self.max_output_size, iou_threshold=self.iou_threshold, confidence_threshold=self.confidence_threshold) return boxes_dicts class Yolo_v3: """"""Yolo v3 model class.""""""​ def __init__(self, n_classes, model_size, max_output_size, iou_threshold, confidence_threshold, data_format=None): """"""Creates the model.​ Args: n_classes: Number of class labels. model_size: The input size of the model. max_output_size: Max number of boxes to be selected for each class. iou_threshold: Threshold for the IOU. confidence_threshold: Threshold for the confidence score. data_format: The input format.​ Returns: None. """""" if not data_format: if tf.test.is_built_with_cuda(): data_format = 'channels_first' else: data_format = 'channels_last'​ self.n_classes = n_classes self.model_size = model_size self.max_output_size = max_output_size self.iou_threshold = iou_threshold self.confidence_threshold = confidence_threshold self.data_format = data_format​ def __call__(self, inputs, training): """"""Add operations to detect boxes for a batch of input images.​ Args: inputs: A Tensor representing a batch of input images. training: A boolean, whether to use in training or inference mode.​ Returns: A list containing class-to-boxes dictionaries for each sample in the batch. """""" with tf.variable_scope('yolo_v3_model'): if self.data_format == 'channels_first': inputs = tf.transpose(inputs, [0, 3, 1, 2])​ inputs = inputs / 255​ route1, route2, inputs = darknet53(inputs, training=training, data_format=self.data_format)​ route, inputs = yolo_convolution_block( inputs, filters=512, training=training, data_format=self.data_format) detect1 = yolo_layer(inputs, n_classes=self.n_classes, anchors=_ANCHORS[6:9], img_size=self.model_size, data_format=self.data_format)​ inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1, data_format=self.data_format) inputs = batch_norm(inputs, training=training, data_format=self.data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) upsample_size = route2.get_shape().as_list() inputs = upsample(inputs, out_shape=upsample_size, data_format=self.data_format) axis = 1 if self.data_format == 'channels_first' else 3 inputs = tf.concat([inputs, route2], axis=axis) route, inputs = yolo_convolution_block( inputs, filters=256, training=training, data_format=self.data_format) detect2 = yolo_layer(inputs, n_classes=self.n_classes, anchors=_ANCHORS[3:6], img_size=self.model_size, data_format=self.data_format)​ inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1, data_format=self.data_format) inputs = batch_norm(inputs, training=training, data_format=self.data_format) inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU) upsample_size = route1.get_shape().as_list() inputs = upsample(inputs, out_shape=upsample_size, data_format=self.data_format) inputs = tf.concat([inputs, route1], axis=axis) route, inputs = yolo_convolution_block( inputs, filters=128, training=training, data_format=self.data_format) detect3 = yolo_layer(inputs, n_classes=self.n_classes, anchors=_ANCHORS[0:3], img_size=self.model_size, data_format=self.data_format)​ inputs = tf.concat([detect1, detect2, detect3], axis=1)​ inputs = build_boxes(inputs)​ boxes_dicts = non_max_suppression( inputs, n_classes=self.n_classes, max_output_size=self.max_output_size, iou_threshold=self.iou_threshold, confidence_threshold=self.confidence_threshold)​ return boxes_dicts Utility functions Here are some utility functions that will help us load images as NumPy arrays, load class names from the official file and draw the predicted boxes. def load_images(img_names, model_size): """"""Loads images in a 4D array. Args: img_names: A list of images names. model_size: The input size of the model. data_format: A format for the array returned ('channels_first' or 'channels_last'). Returns: A 4D NumPy array. """""" imgs = [] for img_name in img_names: img = Image.open(img_name) img = img.resize(size=model_size) img = np.array(img, dtype=np.float32) img = np.expand_dims(img, axis=0) imgs.append(img) imgs = np.concatenate(imgs) return imgs def load_class_names(file_name): """"""Returns a list of class names read from `file_name`."""""" with open(file_name, 'r') as f: class_names = f.read().splitlines() return class_names def draw_boxes(img_names, boxes_dicts, class_names, model_size): """"""Draws detected boxes. Args: img_names: A list of input images names. boxes_dict: A class-to-boxes dictionary. class_names: A class names list. model_size: The input size of the model. Returns: None. """""" colors = ((np.array(color_palette(""hls"", 80)) * 255)).astype(np.uint8) for num, img_name, boxes_dict in zip(range(len(img_names)), img_names, boxes_dicts): img = Image.open(img_name) draw = ImageDraw.Draw(img) font = ImageFont.truetype(font='futur.ttf', size=(img.size[0] + img.size[1]) // 100) resize_factor = (img.size[0] / model_size[0], img.size[1] / model_size[1]) for cls in range(len(class_names)): boxes = boxes_dict[cls] if np.size(boxes) != 0: color = colors[cls] for box in boxes: xy, confidence = box[:4], box[4] xy = [xy[i] * resize_factor[i % 2] for i in range(4)] x0, y0 = xy[0], xy[1] thickness = (img.size[0] + img.size[1]) // 200 for t in np.linspace(0, 1, thickness): xy[0], xy[1] = xy[0] + t, xy[1] + t xy[2], xy[3] = xy[2] - t, xy[3] - t draw.rectangle(xy, outline=tuple(color)) text = '{} {:.1f}%'.format(class_names[cls], confidence * 100) text_size = draw.textsize(text, font=font) draw.rectangle( [x0, y0 - text_size[1], x0 + text_size[0], y0], fill=tuple(color)) draw.text((x0, y0 - text_size[1]), text, fill='black', font=font) display(img) def load_images(img_names, model_size): """"""Loads images in a 4D array.​ Args: img_names: A list of images names. model_size: The input size of the model. data_format: A format for the array returned ('channels_first' or 'channels_last').​ Returns: A 4D NumPy array. """""" imgs = []​ for img_name in img_names: img = Image.open(img_name) img = img.resize(size=model_size) img = np.array(img, dtype=np.float32) img = np.expand_dims(img, axis=0) imgs.append(img)​ imgs = np.concatenate(imgs)​ return imgs​​def load_class_names(file_name): """"""Returns a list of class names read from `file_name`."""""" with open(file_name, 'r') as f: class_names = f.read().splitlines() return class_names​​def draw_boxes(img_names, boxes_dicts, class_names, model_size): """"""Draws detected boxes.​ Args: img_names: A list of input images names. boxes_dict: A class-to-boxes dictionary. class_names: A class names list. model_size: The input size of the model.​ Returns: None. """""" colors = ((np.array(color_palette(""hls"", 80)) * 255)).astype(np.uint8) for num, img_name, boxes_dict in zip(range(len(img_names)), img_names, boxes_dicts): img = Image.open(img_name) draw = ImageDraw.Draw(img) font = ImageFont.truetype(font='futur.ttf', size=(img.size[0] + img.size[1]) // 100) resize_factor = (img.size[0] / model_size[0], img.size[1] / model_size[1]) for cls in range(len(class_names)): boxes = boxes_dict[cls] if np.size(boxes) != 0: color = colors[cls] for box in boxes: xy, confidence = box[:4], box[4] xy = [xy[i] * resize_factor[i % 2] for i in range(4)] x0, y0 = xy[0], xy[1] thickness = (img.size[0] + img.size[1]) // 200 for t in np.linspace(0, 1, thickness): xy[0], xy[1] = xy[0] + t, xy[1] + t xy[2], xy[3] = xy[2] - t, xy[3] - t draw.rectangle(xy, outline=tuple(color)) text = '{} {:.1f}%'.format(class_names[cls], confidence * 100) text_size = draw.textsize(text, font=font) draw.rectangle( [x0, y0 - text_size[1], x0 + text_size[0], y0], fill=tuple(color)) draw.text((x0, y0 - text_size[1]), text, fill='black', font=font)​ display(img) Converting weights to Tensorflow format Now it’s time to load the official weights. We are going to iterate through the file and gradually create tf.assign operations. def load_weights(variables, file_name): """"""Reshapes and loads official pretrained Yolo weights. Args: variables: A list of tf.Variable to be assigned. file_name: A name of a file containing weights. Returns: A list of assign operations. """""" with open(file_name, ""rb"") as f: # Skip first 5 values containing irrelevant info np.fromfile(f, dtype=np.int32, count=5) weights = np.fromfile(f, dtype=np.float32) assign_ops = [] ptr = 0 # Load weights for Darknet part. # Each convolution layer has batch normalization. for i in range(52): conv_var = variables[5 * i] gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5] batch_norm_vars = [beta, gamma, mean, variance] for var in batch_norm_vars: shape = var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape(shape) ptr += num_params assign_ops.append(tf.assign(var, var_weights)) shape = conv_var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape( (shape[3], shape[2], shape[0], shape[1])) var_weights = np.transpose(var_weights, (2, 3, 1, 0)) ptr += num_params assign_ops.append(tf.assign(conv_var, var_weights)) # Loading weights for Yolo part. # 7th, 15th and 23rd convolution layer has biases and no batch norm. ranges = [range(0, 6), range(6, 13), range(13, 20)] unnormalized = [6, 13, 20] for j in range(3): for i in ranges[j]: current = 52 * 5 + 5 * i + j * 2 conv_var = variables[current] gamma, beta, mean, variance = variables[current + 1:current + 5] batch_norm_vars = [beta, gamma, mean, variance] for var in batch_norm_vars: shape = var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape(shape) ptr += num_params assign_ops.append(tf.assign(var, var_weights)) shape = conv_var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape( (shape[3], shape[2], shape[0], shape[1])) var_weights = np.transpose(var_weights, (2, 3, 1, 0)) ptr += num_params assign_ops.append(tf.assign(conv_var, var_weights)) bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1] shape = bias.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape(shape) ptr += num_params assign_ops.append(tf.assign(bias, var_weights)) conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2] shape = conv_var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape( (shape[3], shape[2], shape[0], shape[1])) var_weights = np.transpose(var_weights, (2, 3, 1, 0)) ptr += num_params assign_ops.append(tf.assign(conv_var, var_weights)) return assign_ops def load_weights(variables, file_name): """"""Reshapes and loads official pretrained Yolo weights.​ Args: variables: A list of tf.Variable to be assigned. file_name: A name of a file containing weights.​ Returns: A list of assign operations. """""" with open(file_name, ""rb"") as f: # Skip first 5 values containing irrelevant info np.fromfile(f, dtype=np.int32, count=5) weights = np.fromfile(f, dtype=np.float32)​ assign_ops = [] ptr = 0​ # Load weights for Darknet part. # Each convolution layer has batch normalization. for i in range(52): conv_var = variables[5 * i] gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5] batch_norm_vars = [beta, gamma, mean, variance]​ for var in batch_norm_vars: shape = var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape(shape) ptr += num_params assign_ops.append(tf.assign(var, var_weights))​ shape = conv_var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape( (shape[3], shape[2], shape[0], shape[1])) var_weights = np.transpose(var_weights, (2, 3, 1, 0)) ptr += num_params assign_ops.append(tf.assign(conv_var, var_weights))​ # Loading weights for Yolo part. # 7th, 15th and 23rd convolution layer has biases and no batch norm. ranges = [range(0, 6), range(6, 13), range(13, 20)] unnormalized = [6, 13, 20] for j in range(3): for i in ranges[j]: current = 52 * 5 + 5 * i + j * 2 conv_var = variables[current] gamma, beta, mean, variance = variables[current + 1:current + 5] batch_norm_vars = [beta, gamma, mean, variance]​ for var in batch_norm_vars: shape = var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape(shape) ptr += num_params assign_ops.append(tf.assign(var, var_weights))​ shape = conv_var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape( (shape[3], shape[2], shape[0], shape[1])) var_weights = np.transpose(var_weights, (2, 3, 1, 0)) ptr += num_params assign_ops.append(tf.assign(conv_var, var_weights))​ bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1] shape = bias.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape(shape) ptr += num_params assign_ops.append(tf.assign(bias, var_weights))​ conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2] shape = conv_var.shape.as_list() num_params = np.prod(shape) var_weights = weights[ptr:ptr + num_params].reshape( (shape[3], shape[2], shape[0], shape[1])) var_weights = np.transpose(var_weights, (2, 3, 1, 0)) ptr += num_params assign_ops.append(tf.assign(conv_var, var_weights))​ return assign_ops Running the model Now we can run the model using some sample images. Sample images from google.colab import files uploaded = files.upload() img_names = ['dog.jpg', 'office.jpg'] for img in img_names: display(Image.open(img)) from google.colab import filesuploaded = files.upload()img_names = ['dog.jpg', 'office.jpg']for img in img_names: display(Image.open(img)) Detections Testing the model with IoU (Interception over Union ratio used in non-max suppression) threshold and confidence threshold both set to 0.5. from google.colab import files uploaded = files.upload() batch_size = len(img_names) batch = load_images(img_names, model_size=_MODEL_SIZE) class_names = load_class_names('coco.names') n_classes = len(class_names) max_output_size = 10 iou_threshold = 0.5 confidence_threshold = 0.5 model = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE, max_output_size=max_output_size, iou_threshold=iou_threshold, confidence_threshold=confidence_threshold) inputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3]) detections = model(inputs, training=False) model_vars = tf.global_variables(scope='yolo_v3_model') assign_ops = load_weights(model_vars, 'yolov3.weights') with tf.Session() as sess: sess.run(assign_ops) detection_result = sess.run(detections, feed_dict={inputs: batch}) draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE) from google.colab import filesuploaded = files.upload()batch_size = len(img_names)batch = load_images(img_names, model_size=_MODEL_SIZE)class_names = load_class_names('coco.names')n_classes = len(class_names)max_output_size = 10iou_threshold = 0.5confidence_threshold = 0.5​model = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE, max_output_size=max_output_size, iou_threshold=iou_threshold, confidence_threshold=confidence_threshold)​inputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3])​detections = model(inputs, training=False)​model_vars = tf.global_variables(scope='yolo_v3_model')assign_ops = load_weights(model_vars, 'yolov3.weights')​with tf.Session() as sess: sess.run(assign_ops) detection_result = sess.run(detections, feed_dict={inputs: batch}) draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE)";Object Detection with Deep Learning using Yolo and Tensorflow
2020-06-12 23:15:05;This Article is based on SMS Spam detection classification with Machine Learning. I will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input. On the other hand Gaussian Naive Bayes is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution.Lets Start by importing the librariesDownload and read the data setDropping the unwanted columns Unnamed:2, Unnamed: 3 and Unnamed:4Number of observations in each label spam and hamWhat we have here in our data set is a large collection of text data (5,572 rows of data). Most Machine Learning algorithms rely on numerical data to be fed into them as input, and email/sms messages are usually text heavy. We need a way to represent text data for machine learning algorithm and the bag-of-words model helps us to achieve that task. It is a way of extracting features from the text for use in machine learning algorithms. In this approach, we use the tokenized words for each observation and find out the frequency of each token. Using a process which we will go through now, we can convert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrence of each word or token in that document.For example:Lets say we have 4 documents as follows:[‘Hello, how are you!’, ‘Win money, win from home.’, ‘Call me now’, ‘Hello, Call you tomorrow?’]Our objective here is to convert this set of text to a frequency distribution matrix, as follows: Here as we can see, the documents are numbered in the rows, and each word is a column name, with the corresponding value being the frequency of that word in the document.Lets break this down and see how we can do this conversion using a small set of documents.To handle this, we will be using sklearns count vectorizer method which does the following:Step 1: Convert all strings to their lower case form.Step 2: Removing all punctuationsStep 3: TokenizationStep 4: Count frequenciesHere we will look to create a frequency matrix on a smaller document set to make sure we understand how the document-term matrix generation happens. We have created a sample document set ‘documents’.documents = [‘Hello, how are you!’, ‘Win money, win from home.’, ‘Call me now.’, ‘Hello, Call hello you tomorrow?’]In above step, we implemented a version of the CountVectorizer() method from scratch that entailed cleaning our data first. This cleaning involved converting all of our data to lower case and removing all punctuation marks. CountVectorizer() has certain parameters which take care of these steps for us. They are:lowercase = TrueThe lowercase parameter has a default value of True which converts all of our text to its lower case form.token_pattern = (?u)\b\w\w+\bThe token_pattern parameter has a default regular expression value of (?u)\b\w\w+\b which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words.stop_wordsThe stop_words parameter, if set to english will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. Considering the size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not be setting this parameter value.I will use sklearns sklearn.naive_bayes method to make predictions on our dataset for SMS Spam Detection.Specifically, we will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features. It takes in integer word counts as its input. On the other hand Gaussian Naive Bayes is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution.Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. There are various mechanisms for doing so, but first let’s do quick recap of them.Accuracy measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).Precision tells us what proportion of messages we classified as spam, actually were spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of[True Positives/(True Positives + False Positives)]Recall(sensitivity) tells us what proportion of messages that actually were spam were classified by us as spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of[True Positives/(True Positives + False Negatives)]For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren’t, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score.We will be using all 4 metrics to make sure our model does well. For all 4 metrics whose values can range from 0 to 1, having a score as close to 1 as possible is a good indicator of how well our model is doing.;https://thecleverprogrammer.com/2020/06/12/sms-spam-detection-with-machine-learning/;['pattern', 'sklearn', 'nltk'];1.0;[];['ML', 'Text Classification', 'Naive Bayes', 'Classification'];['detect', 'text classification', 'predict', 'fit', 'model', 'machine learning', 'classif', 'training data', 'naive bayes', 'train', 'label', 'test data'];"This Article is based on SMS Spam detection classification with Machine Learning. I will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input. On the other hand Gaussian Naive Bayes is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution. SMS Spam Detection Lets Start by importing the libraries import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import nltk​x import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)import nltk spamDownload Download and read the data set import pandas df_sms = pd.read_csv('spam.csv',encoding='latin-1') df_sms.head() import pandasdf_sms = pd.read_csv('spam.csv',encoding='latin-1')df_sms.head() Dropping the unwanted columns Unnamed:2, Unnamed: 3 and Unnamed:4 df_sms = df_sms.drop([""Unnamed: 2"", ""Unnamed: 3"", ""Unnamed: 4""], axis=1) df_sms = df_sms.rename(columns={""v1"":""label"", ""v2"":""sms""}) df_sms.head() df_sms = df_sms.drop([""Unnamed: 2"", ""Unnamed: 3"", ""Unnamed: 4""], axis=1)df_sms = df_sms.rename(columns={""v1"":""label"", ""v2"":""sms""})df_sms.head() Checking the maximum length of SMS print(len(df_sms)) print(len(df_sms)) Number of observations in each label spam and ham df_sms.label.value_counts() df_sms.label.value_counts() ham 4825 spam 747 Name: label, dtype: int64 ham 4825spam 747Name: label, dtype: int64 df_sms.describe() df_sms.describe() label	sms count	5572	5572 unique 2	5169 top ham Sorry, I'll call later freq	4825	30 label smscount 5572 5572unique 2 5169top ham Sorry, I'll call laterfreq 4825 30 df_sms['length'] = df_sms['sms'].apply(len) df_sms.head() df_sms['length'] = df_sms['sms'].apply(len)df_sms.head() label	sms length 0	ham Go until jurong point, crazy.. Available only ...	111 1	ham Ok lar... Joking wif u oni... 29 2	spam	Free entry in 2 a wkly comp to win FA Cup fina...	155 3	ham U dun say so early hor... U c already then say...	49 4	ham Nah I don't think he goes to usf, he lives aro...	61 label sms length0 ham Go until jurong point, crazy.. Available only ... 1111 ham Ok lar... Joking wif u oni... 292 spam Free entry in 2 a wkly comp to win FA Cup fina... 1553 ham U dun say so early hor... U c already then say... 494 ham Nah I don't think he goes to usf, he lives aro... 61 import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline df_sms['length'].plot(bins=50, kind='hist') import matplotlib.pyplot as pltimport seaborn as sns%matplotlib inlinedf_sms['length'].plot(bins=50, kind='hist') df_sms.hist(column='length', by='label', bins=50,figsize=(10,4)) df_sms.hist(column='length', by='label', bins=50,figsize=(10,4)) df_sms.loc[:,'label'] = df_sms.label.map({'ham':0, 'spam':1}) print(df_sms.shape) df_sms.head() df_sms.loc[:,'label'] = df_sms.label.map({'ham':0, 'spam':1})print(df_sms.shape)df_sms.head() (5572, 3) label	sms length 0	0	Go until jurong point, crazy.. Available only ...	111 1	0	Ok lar... Joking wif u oni... 29 2	1	Free entry in 2 a wkly comp to win FA Cup fina...	155 3	0	U dun say so early hor... U c already then say...	49 4	0	Nah I don't think he goes to usf, he lives aro...	61 (5572, 3) label sms length0 0 Go until jurong point, crazy.. Available only ... 1111 0 Ok lar... Joking wif u oni... 292 1 Free entry in 2 a wkly comp to win FA Cup fina... 1553 0 U dun say so early hor... U c already then say... 494 0 Nah I don't think he goes to usf, he lives aro... 61 Bag of Words Approach What we have here in our data set is a large collection of text data (5,572 rows of data). Most Machine Learning algorithms rely on numerical data to be fed into them as input, and email/sms messages are usually text heavy. We need a way to represent text data for machine learning algorithm and the bag-of-words model helps us to achieve that task. It is a way of extracting features from the text for use in machine learning algorithms. In this approach, we use the tokenized words for each observation and find out the frequency of each token. Using a process which we will go through now, we can convert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrence of each word or token in that document. For example: Lets say we have 4 documents as follows: [‘Hello, how are you!’, ‘Win money, win from home.’, ‘Call me now’, ‘Hello, Call you tomorrow?’] Our objective here is to convert this set of text to a frequency distribution matrix, as follows: Here as we can see, the documents are numbered in the rows, and each word is a column name, with the corresponding value being the frequency of that word in the document. Lets break this down and see how we can do this conversion using a small set of documents. To handle this, we will be using sklearns count vectorizer method which does the following: It tokenizes the string(separates the string into individual words) and gives an integer ID to each token.It counts the occurrence of each of those tokens. Implementation of Bag of Words Approach Step 1: Convert all strings to their lower case form. documents = ['Hello, how are you!', 'Win money, win from home.', 'Call me now.', 'Hello, Call hello you tomorrow?'] lower_case_documents = [] lower_case_documents = [d.lower() for d in documents] print(lower_case_documents) documents = ['Hello, how are you!', 'Win money, win from home.', 'Call me now.', 'Hello, Call hello you tomorrow?']​lower_case_documents = []lower_case_documents = [d.lower() for d in documents]print(lower_case_documents) ['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?'] ['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?'] Step 2: Removing all punctuations sans_punctuation_documents = [] import string for i in lower_case_documents: sans_punctuation_documents.append(i.translate(str.maketrans("""","""", string.punctuation))) sans_punctuation_documents sans_punctuation_documents = []import string​for i in lower_case_documents: sans_punctuation_documents.append(i.translate(str.maketrans("""","""", string.punctuation))) sans_punctuation_documents ['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow'] ['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow'] Step 3: Tokenization preprocessed_documents = [[w for w in d.split()] for d in sans_punctuation_documents] preprocessed_documents preprocessed_documents = [[w for w in d.split()] for d in sans_punctuation_documents]preprocessed_documents [['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']] [['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']] Step 4: Count frequencies frequency_list = [] import pprint from collections import Counter frequency_list = [Counter(d) for d in preprocessed_documents] pprint.pprint(frequency_list) frequency_list = []import pprintfrom collections import Counter​frequency_list = [Counter(d) for d in preprocessed_documents]pprint.pprint(frequency_list) [Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}), Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}), Counter({'call': 1, 'me': 1, 'now': 1}), Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})] [Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}), Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}), Counter({'call': 1, 'me': 1, 'now': 1}), Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})] Implementing Bag of Words in scikit-learn Here we will look to create a frequency matrix on a smaller document set to make sure we understand how the document-term matrix generation happens. We have created a sample document set ‘documents’. documents = [‘Hello, how are you!’, ‘Win money, win from home.’, ‘Call me now.’, ‘Hello, Call hello you tomorrow?’] from sklearn.feature_extraction.text import CountVectorizer count_vector = CountVectorizer() from sklearn.feature_extraction.text import CountVectorizercount_vector = CountVectorizer() Data preprocessing with CountVectorizer() In above step, we implemented a version of the CountVectorizer() method from scratch that entailed cleaning our data first. This cleaning involved converting all of our data to lower case and removing all punctuation marks. CountVectorizer() has certain parameters which take care of these steps for us. They are: lowercase = True The lowercase parameter has a default value of True which converts all of our text to its lower case form. token_pattern = (?u)\b\w\w+\b The token_pattern parameter has a default regular expression value of (?u)\b\w\w+\b which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words. stop_words The stop_words parameter, if set to english will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. Considering the size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not be setting this parameter value. count_vector.fit(documents) count_vector.get_feature_names() count_vector.fit(documents)count_vector.get_feature_names() ['are', 'call', 'from', 'hello', 'home', 'how', 'me', 'money', 'now', 'tomorrow', 'win', 'you'] ['are', 'call', 'from', 'hello', 'home', 'how', 'me', 'money', 'now', 'tomorrow', 'win', 'you'] doc_array = count_vector.transform(documents).toarray() doc_array doc_array = count_vector.transform(documents).toarray()doc_array array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0], [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]]) array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0], [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]]) frequency_matrix = pd.DataFrame(doc_array, columns = count_vector.get_feature_names()) frequency_matrix frequency_matrix = pd.DataFrame(doc_array, columns = count_vector.get_feature_names())frequency_matrix from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(df_sms['sms'], df_sms['label'],test_size=0.20, random_state=1) from sklearn.model_selection import train_test_split​X_train, X_test, y_train, y_test = train_test_split(df_sms['sms'], df_sms['label'],test_size=0.20, random_state=1) # Instantiate the CountVectorizer method count_vector = CountVectorizer() # Fit the training data and then return the matrix training_data = count_vector.fit_transform(X_train) # Transform testing data and return the matrix. testing_data = count_vector.transform(X_test) # Instantiate the CountVectorizer methodcount_vector = CountVectorizer()​# Fit the training data and then return the matrixtraining_data = count_vector.fit_transform(X_train)​# Transform testing data and return the matrix. testing_data = count_vector.transform(X_test) Implementation of Naive Bayes Machine Learning Algorithm I will use sklearns sklearn.naive_bayes method to make predictions on our dataset for SMS Spam Detection. Specifically, we will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features. It takes in integer word counts as its input. On the other hand Gaussian Naive Bayes is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution. from sklearn.naive_bayes import MultinomialNB naive_bayes = MultinomialNB() naive_bayes.fit(training_data,y_train) from sklearn.naive_bayes import MultinomialNBnaive_bayes = MultinomialNB()naive_bayes.fit(training_data,y_train) MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) predictions = naive_bayes.predict(testing_data) predictions = naive_bayes.predict(testing_data) Evaluating our SMS Spam Detection Model Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. There are various mechanisms for doing so, but first let’s do quick recap of them. Accuracy measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points). Precision tells us what proportion of messages we classified as spam, actually were spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of [True Positives/(True Positives + False Positives)] Recall(sensitivity) tells us what proportion of messages that actually were spam were classified by us as spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of [True Positives/(True Positives + False Negatives)] For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren’t, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score. We will be using all 4 metrics to make sure our model does well. For all 4 metrics whose values can range from 0 to 1, having a score as close to 1 as possible is a good indicator of how well our model is doing. from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score print('Accuracy score: {}'.format(accuracy_score(y_test, predictions))) print('Precision score: {}'.format(precision_score(y_test, predictions))) print('Recall score: {}'.format(recall_score(y_test, predictions))) print('F1 score: {}'.format(f1_score(y_test, predictions))) from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_scoreprint('Accuracy score: {}'.format(accuracy_score(y_test, predictions)))print('Precision score: {}'.format(precision_score(y_test, predictions)))print('Recall score: {}'.format(recall_score(y_test, predictions)))print('F1 score: {}'.format(f1_score(y_test, predictions))) Accuracy score: 0.9847533632286996 Precision score: 0.9420289855072463 Recall score: 0.935251798561151 F1 score: 0.9386281588447652 Accuracy score: 0.9847533632286996Precision score: 0.9420289855072463Recall score: 0.935251798561151F1 score: 0.9386281588447652";SMS Spam Detection with Machine Learning
2020-06-13 13:03:58;Twitter Sentiment Analysis is the process of computationally identifying and categorizing tweets expressed in a piece of text, especially in order to determine whether the writer’s attitude towards a particular topic, product, etc. is positive, negative, or neutral.In this Article I will do twitter sentiment analysis with Natural Language Processing using the nltk library with python.Lets start with importing the librariesFirst of all, splitting the data set into a training and a testing set. The test set is the 10% of the original data set. For this particular analysis I dropped the neutral tweets, as my goal was to only differentiate positive and negative tweets.As a next step I separated the Positive and Negative tweets of the training set in order to easily visualize their contained words. After that I cleaned the text from hashtags, mentions and links. Now they were ready for a WordCloud visualization which shows only the most emphatic words of the Positive and Negative tweets.Interesting to notice the following words and expressions in the positive word set: truth, strong, legitimate, together, love, jobIn my interpretation, people tend to believe that their ideal candidate is truthful, legitimate, above good and bad.At the same time, negative tweets contains words like: influence, news, elevatormusic, disappointing, softball, makeup, cherry picking, tryingIn my understanding people missed the decisively acting and considered the scolded candidates too soft and cherry picking.After the vizualization, I removed the hashtags, mentions, links and stopwords from the training set.Stop Word: Stop Words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return vast amount of unnecessary information. ( the, for, this etc. )As a next step I extracted the so called features with nltk lib, first by measuring a frequent distribution and by selecting the resulting keys.Hereby I plotted the most frequently distributed words. The most words are centered around debate nights.Using the nltk NaiveBayes Classifier I classified the extracted tweet word features.Finally, with not-so-intelligent metrics, I tried to measure how the classifier algorithm scored.;https://thecleverprogrammer.com/2020/06/13/twitter-sentiment-analysis/;['sklearn', 'nltk'];1.0;[];['Classification', 'NLP', 'Naive Bayes', 'Sentiment Analysis'];['sentiment analysis', 'model', 'classif', 'filter', 'natural language processing', 'naive bayes', 'train'];"Twitter Sentiment Analysis is the process of computationally identifying and categorizing tweets expressed in a piece of text, especially in order to determine whether the writer’s attitude towards a particular topic, product, etc. is positive, negative, or neutral. In this Article I will do twitter sentiment analysis with Natural Language Processing using the nltk library with python. Twitter Sentiment Analysis Lets start with importing the libraries import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from sklearn.model_selection import train_test_split # function for splitting data to train and test sets import nltk from nltk.corpus import stopwords from nltk.classify import SklearnClassifier from wordcloud import WordCloud,STOPWORDS import matplotlib.pyplot as plt​x import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)from sklearn.model_selection import train_test_split # function for splitting data to train and test sets​import nltkfrom nltk.corpus import stopwordsfrom nltk.classify import SklearnClassifier​from wordcloud import WordCloud,STOPWORDSimport matplotlib.pyplot as plt Download the data set SentimentDownload data = pd.read_csv('Sentiment.csv') # Keeping only the neccessary columns data = data[['text','sentiment']] data = pd.read_csv('Sentiment.csv')# Keeping only the neccessary columnsdata = data[['text','sentiment']] First of all, splitting the data set into a training and a testing set. The test set is the 10% of the original data set. For this particular analysis I dropped the neutral tweets, as my goal was to only differentiate positive and negative tweets. # Splitting the dataset into train and test set train, test = train_test_split(data,test_size = 0.1) # Removing neutral sentiments train = train[train.sentiment != ""Neutral""] # Splitting the dataset into train and test settrain, test = train_test_split(data,test_size = 0.1)# Removing neutral sentimentstrain = train[train.sentiment != ""Neutral""] As a next step I separated the Positive and Negative tweets of the training set in order to easily visualize their contained words. After that I cleaned the text from hashtags, mentions and links. Now they were ready for a WordCloud visualization which shows only the most emphatic words of the Positive and Negative tweets. train_pos = train[ train['sentiment'] == 'Positive'] train_pos = train_pos['text'] train_neg = train[ train['sentiment'] == 'Negative'] train_neg = train_neg['text'] def wordcloud_draw(data, color = 'black'): words = ' '.join(data) cleaned_word = "" "".join([word for word in words.split() if 'http' not in word and not word.startswith('@') and not word.startswith('#') and word != 'RT' ]) wordcloud = WordCloud(stopwords=STOPWORDS, background_color=color, width=2500, height=2000 ).generate(cleaned_word) plt.figure(1,figsize=(13, 13)) plt.imshow(wordcloud) plt.axis('off') plt.show() print(""Positive words"") wordcloud_draw(train_pos,'white') print(""Negative words"") wordcloud_draw(train_neg) train_pos = train[ train['sentiment'] == 'Positive']train_pos = train_pos['text']train_neg = train[ train['sentiment'] == 'Negative']train_neg = train_neg['text']​def wordcloud_draw(data, color = 'black'): words = ' '.join(data) cleaned_word = "" "".join([word for word in words.split() if 'http' not in word and not word.startswith('@') and not word.startswith('#') and word != 'RT' ]) wordcloud = WordCloud(stopwords=STOPWORDS, background_color=color, width=2500, height=2000 ).generate(cleaned_word) plt.figure(1,figsize=(13, 13)) plt.imshow(wordcloud) plt.axis('off') plt.show() print(""Positive words"")wordcloud_draw(train_pos,'white')print(""Negative words"")wordcloud_draw(train_neg) Interesting to notice the following words and expressions in the positive word set: truth, strong, legitimate, together, love, job In my interpretation, people tend to believe that their ideal candidate is truthful, legitimate, above good and bad. At the same time, negative tweets contains words like: influence, news, elevatormusic, disappointing, softball, makeup, cherry picking, trying In my understanding people missed the decisively acting and considered the scolded candidates too soft and cherry picking. After the vizualization, I removed the hashtags, mentions, links and stopwords from the training set. Stop Word: Stop Words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return vast amount of unnecessary information. ( the, for, this etc. ) tweets = [] stopwords_set = set(stopwords.words(""english"")) for index, row in train.iterrows(): words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3] words_cleaned = [word for word in words_filtered if 'http' not in word and not word.startswith('@') and not word.startswith('#') and word != 'RT'] words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set] tweets.append((words_without_stopwords, row.sentiment)) test_pos = test[ test['sentiment'] == 'Positive'] test_pos = test_pos['text'] test_neg = test[ test['sentiment'] == 'Negative'] test_neg = test_neg['text'] tweets = []stopwords_set = set(stopwords.words(""english""))​for index, row in train.iterrows(): words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3] words_cleaned = [word for word in words_filtered if 'http' not in word and not word.startswith('@') and not word.startswith('#') and word != 'RT'] words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set] tweets.append((words_without_stopwords, row.sentiment))​test_pos = test[ test['sentiment'] == 'Positive']test_pos = test_pos['text']test_neg = test[ test['sentiment'] == 'Negative']test_neg = test_neg['text'] As a next step I extracted the so called features with nltk lib, first by measuring a frequent distribution and by selecting the resulting keys. # Extracting word features def get_words_in_tweets(tweets): all = [] for (words, sentiment) in tweets: all.extend(words) return all def get_word_features(wordlist): wordlist = nltk.FreqDist(wordlist) features = wordlist.keys() return features w_features = get_word_features(get_words_in_tweets(tweets)) def extract_features(document): document_words = set(document) features = {} for word in w_features: features['contains(%s)' % word] = (word in document_words) return features # Extracting word featuresdef get_words_in_tweets(tweets): all = [] for (words, sentiment) in tweets: all.extend(words) return all​def get_word_features(wordlist): wordlist = nltk.FreqDist(wordlist) features = wordlist.keys() return featuresw_features = get_word_features(get_words_in_tweets(tweets))​def extract_features(document): document_words = set(document) features = {} for word in w_features: features['contains(%s)' % word] = (word in document_words) return features Hereby I plotted the most frequently distributed words. The most words are centered around debate nights. wordcloud_draw(w_features) wordcloud_draw(w_features) Using the nltk NaiveBayes Classifier I classified the extracted tweet word features. # Training the Naive Bayes classifier training_set = nltk.classify.apply_features(extract_features,tweets) classifier = nltk.NaiveBayesClassifier.train(training_set) # Training the Naive Bayes classifiertraining_set = nltk.classify.apply_features(extract_features,tweets)classifier = nltk.NaiveBayesClassifier.train(training_set) Finally, with not-so-intelligent metrics, I tried to measure how the classifier algorithm scored. neg_cnt = 0 pos_cnt = 0 for obj in test_neg: res = classifier.classify(extract_features(obj.split())) if(res == 'Negative'): neg_cnt = neg_cnt + 1 for obj in test_pos: res = classifier.classify(extract_features(obj.split())) if(res == 'Positive'): pos_cnt = pos_cnt + 1 print('[Negative]: %s/%s ' % (len(test_neg),neg_cnt)) print('[Positive]: %s/%s ' % (len(test_pos),pos_cnt)) neg_cnt = 0pos_cnt = 0for obj in test_neg: res = classifier.classify(extract_features(obj.split())) if(res == 'Negative'): neg_cnt = neg_cnt + 1for obj in test_pos: res = classifier.classify(extract_features(obj.split())) if(res == 'Positive'): pos_cnt = pos_cnt + 1 print('[Negative]: %s/%s ' % (len(test_neg),neg_cnt)) print('[Positive]: %s/%s ' % (len(test_pos),pos_cnt)) [Negative]: 842/795 [Positive]: 220/74 [Negative]: 842/795 [Positive]: 220/74";Twitter Sentiment Analysis
2020-06-16 21:39:43;Convolutional neural networks (CNN) are primarily used to classify images or identify pattern similarities between them. So a convolutional network receives a normal color image as a rectangular box whose width and height are measured by the number of pixels along those dimensions, and whose depth is three layers deep, one for each letter in RGB.As images move through a convolutional network, different patterns are recognised just like a normal neural network. But here rather than focussing on one pixel at a time, a convolutional net takes in square patches of pixels and passes them through a filter.That filter is also a square matrix smaller than the image itself, and equal in size to the patch. It is also called a kernel.We need to train a model first so we will check training data In the below code we are iterating through all images in train folder and then we will split image name with deliminiter “.” We have names like dog.0, dog.1, cat.2 etc.. Hence after splitting we are gonna get results like “dog’, “cat” as category value of the image. To make this example more easy we will consider dog as “1” and cat as “0”.Now every image is actually a set of pixels so how to get our computer know that. Its simple convert all those pixels into an array. So we are going to use here a cv2 library to read our image into an array and also it will read as a gray scale image.Okay so the above code was more for understanding purpose. Nowe we will get to the real part of coding here.Declare your training array X and your target array y. Here X will be the array of pixels and y will be value 0 or 1 indicating its a dog or cat Write convert function to map category “dog” or “cat” into 1 and 0Create a function create_test_data which takes all training images into a loop. Converts into image array. Resize image into 80 X80. Append image into X array. Append category value into y array.Now call the function, but also later convert X and y into numpy array We also have to reshape X with the below codeIf you see the values of X you can see a variety of values between 0- 255 . Its because every pixel has different density of black and white. But with the wide range of values it becomes difficult for a training model to learn ( sometimes memorize ).How to resolve this And you guessed it right . You can normalize the data. We can use Keras normalize here also . But well we already know all values are having range between 0-255 so we can just divide it by 255 and get all values scaled between 0 -1That’s what we have done below. You can skip this step to see the difference between accuracy. Don’t believe everything I say. Experiment and see for yourself;https://thecleverprogrammer.com/2020/06/16/dog-and-cat-classification-using-convolutional-neural-networks-cnn/;['keras', 'pattern', 'tensorflow'];1.0;['NN', 'CNN'];['ReLu', 'Classification', 'NN', 'CNN'];['recogn', 'epoch', 'predict', 'fit', 'model', 'loss', 'neural network', 'relu', 'classif', 'layer', 'filter', 'training data', 'convolutional neural network', 'train', 'label', 'test data'];"Introduction to CNN Convolutional neural networks (CNN) are primarily used to classify images or identify pattern similarities between them. So a convolutional network receives a normal color image as a rectangular box whose width and height are measured by the number of pixels along those dimensions, and whose depth is three layers deep, one for each letter in RGB. As images move through a convolutional network, different patterns are recognised just like a normal neural network. But here rather than focussing on one pixel at a time, a convolutional net takes in square patches of pixels and passes them through a filter. That filter is also a square matrix smaller than the image itself, and equal in size to the patch. It is also called a kernel. Now lets start with importing the libraries import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import cv2 import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D​x import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)import cv2import matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D We need to train a model first so we will check training data In the below code we are iterating through all images in train folder and then we will split image name with deliminiter “.” We have names like dog.0, dog.1, cat.2 etc.. Hence after splitting we are gonna get results like “dog’, “cat” as category value of the image. To make this example more easy we will consider dog as “1” and cat as “0”. Now every image is actually a set of pixels so how to get our computer know that. Its simple convert all those pixels into an array. So we are going to use here a cv2 library to read our image into an array and also it will read as a gray scale image. train_dir = # your path to train dataset path = os.path.join(main_dir,train_dir) for p in os.listdir(path): category = p.split(""."")[0] img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE) new_img_array = cv2.resize(img_array, dsize=(80, 80)) plt.imshow(new_img_array,cmap=""gray"") break train_dir = # your path to train datasetpath = os.path.join(main_dir,train_dir)​for p in os.listdir(path): category = p.split(""."")[0] img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE) new_img_array = cv2.resize(img_array, dsize=(80, 80)) plt.imshow(new_img_array,cmap=""gray"") break Okay so the above code was more for understanding purpose. Nowe we will get to the real part of coding here. Declare your training array X and your target array y. Here X will be the array of pixels and y will be value 0 or 1 indicating its a dog or cat Write convert function to map category “dog” or “cat” into 1 and 0 Create a function create_test_data which takes all training images into a loop. Converts into image array. Resize image into 80 X80. Append image into X array. Append category value into y array. X = [] y = [] convert = lambda category : int(category == 'dog') def create_test_data(path): for p in os.listdir(path): category = p.split(""."")[0] category = convert(category) img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE) new_img_array = cv2.resize(img_array, dsize=(80, 80)) X.append(new_img_array) y.append(category) X = []y = []convert = lambda category : int(category == 'dog')def create_test_data(path): for p in os.listdir(path): category = p.split(""."")[0] category = convert(category) img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE) new_img_array = cv2.resize(img_array, dsize=(80, 80)) X.append(new_img_array) y.append(category) Now call the function, but also later convert X and y into numpy array We also have to reshape X with the below code create_test_data(path) X = np.array(X).reshape(-1, 80,80,1) y = np.array(y) create_test_data(path)X = np.array(X).reshape(-1, 80,80,1)y = np.array(y) If you see the values of X you can see a variety of values between 0- 255 . Its because every pixel has different density of black and white. But with the wide range of values it becomes difficult for a training model to learn ( sometimes memorize ). How to resolve this And you guessed it right . You can normalize the data. We can use Keras normalize here also . But well we already know all values are having range between 0-255 so we can just divide it by 255 and get all values scaled between 0 -1 That’s what we have done below. You can skip this step to see the difference between accuracy. Don’t believe everything I say. Experiment and see for yourself #Normalize data X = X/255.0 #Normalize dataX = X/255.0 Now Lets train our model model = Sequential() # Adds a densely-connected layer with 64 units to the model: model.add(Conv2D(64,(3,3), activation = 'relu', input_shape = X.shape[1:])) model.add(MaxPooling2D(pool_size = (2,2))) # Add another: model.add(Conv2D(64,(3,3), activation = 'relu')) model.add(MaxPooling2D(pool_size = (2,2))) model.add(Flatten()) model.add(Dense(64, activation='relu')) # Add a softmax layer with 10 output units: model.add(Dense(1, activation='sigmoid')) model.compile(optimizer=""adam"", loss='binary_crossentropy', metrics=['accuracy']) model = Sequential()# Adds a densely-connected layer with 64 units to the model:model.add(Conv2D(64,(3,3), activation = 'relu', input_shape = X.shape[1:]))model.add(MaxPooling2D(pool_size = (2,2)))# Add another:model.add(Conv2D(64,(3,3), activation = 'relu'))model.add(MaxPooling2D(pool_size = (2,2)))​model.add(Flatten())model.add(Dense(64, activation='relu'))# Add a softmax layer with 10 output units:model.add(Dense(1, activation='sigmoid'))​model.compile(optimizer=""adam"", loss='binary_crossentropy', metrics=['accuracy']) Now we will fit our model with training data. Epochs :- How many times our model will go through data Batch size :- How much amount of data at once you wanna pass through the model validation_split :- How much amount of data (in this case its 20 %) you will need to check cross validation error model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2) model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2) Train on 20000 samples, validate on 5000 samples Epoch 1/10 20000/20000 [==============================] - 16s 790us/step - loss: 0.6109 - acc: 0.6558 - val_loss: 0.5383 - val_acc: 0.7308 Epoch 2/10 20000/20000 [==============================] - 14s 679us/step - loss: 0.4989 - acc: 0.7557 - val_loss: 0.4989 - val_acc: 0.7564 Epoch 3/10 20000/20000 [==============================] - 14s 679us/step - loss: 0.4502 - acc: 0.7916 - val_loss: 0.4728 - val_acc: 0.7796 Epoch 4/10 20000/20000 [==============================] - 14s 680us/step - loss: 0.4059 - acc: 0.8143 - val_loss: 0.5290 - val_acc: 0.7644 Epoch 5/10 20000/20000 [==============================] - 14s 679us/step - loss: 0.3675 - acc: 0.8334 - val_loss: 0.4572 - val_acc: 0.7938 Epoch 6/10 20000/20000 [==============================] - 14s 679us/step - loss: 0.3181 - acc: 0.8610 - val_loss: 0.4744 - val_acc: 0.7958 Epoch 7/10 20000/20000 [==============================] - 14s 680us/step - loss: 0.2704 - acc: 0.8841 - val_loss: 0.4575 - val_acc: 0.7976 Epoch 8/10 20000/20000 [==============================] - 14s 681us/step - loss: 0.2155 - acc: 0.9104 - val_loss: 0.5198 - val_acc: 0.7878 Epoch 9/10 20000/20000 [==============================] - 14s 679us/step - loss: 0.1646 - acc: 0.9357 - val_loss: 0.6021 - val_acc: 0.7928 Epoch 10/10 20000/20000 [==============================] - 14s 680us/step - loss: 0.1227 - acc: 0.9532 - val_loss: 0.6653 - val_acc: 0.7874 Train on 20000 samples, validate on 5000 samplesEpoch 1/1020000/20000 [==============================] - 16s 790us/step - loss: 0.6109 - acc: 0.6558 - val_loss: 0.5383 - val_acc: 0.7308Epoch 2/1020000/20000 [==============================] - 14s 679us/step - loss: 0.4989 - acc: 0.7557 - val_loss: 0.4989 - val_acc: 0.7564Epoch 3/1020000/20000 [==============================] - 14s 679us/step - loss: 0.4502 - acc: 0.7916 - val_loss: 0.4728 - val_acc: 0.7796Epoch 4/1020000/20000 [==============================] - 14s 680us/step - loss: 0.4059 - acc: 0.8143 - val_loss: 0.5290 - val_acc: 0.7644Epoch 5/1020000/20000 [==============================] - 14s 679us/step - loss: 0.3675 - acc: 0.8334 - val_loss: 0.4572 - val_acc: 0.7938Epoch 6/1020000/20000 [==============================] - 14s 679us/step - loss: 0.3181 - acc: 0.8610 - val_loss: 0.4744 - val_acc: 0.7958Epoch 7/1020000/20000 [==============================] - 14s 680us/step - loss: 0.2704 - acc: 0.8841 - val_loss: 0.4575 - val_acc: 0.7976Epoch 8/1020000/20000 [==============================] - 14s 681us/step - loss: 0.2155 - acc: 0.9104 - val_loss: 0.5198 - val_acc: 0.7878Epoch 9/1020000/20000 [==============================] - 14s 679us/step - loss: 0.1646 - acc: 0.9357 - val_loss: 0.6021 - val_acc: 0.7928Epoch 10/1020000/20000 [==============================] - 14s 680us/step - loss: 0.1227 - acc: 0.9532 - val_loss: 0.6653 - val_acc: 0.7874 Now the time has come to finally PREDICT, so feed your CNN model with test data to predict. predictions = model.predict(X_test) predictions = model.predict(X_test) We are rounding the result here as we used sigmoid function and we got the probability values in our predicted data set predicted_val = [int(round(p[0])) for p in predictions] predicted_val = [int(round(p[0])) for p in predictions] Now you have to make submission data frame to submit your result set. submission_df = pd.DataFrame({'id':id_line, 'label':predicted_val}) submission_df = pd.DataFrame({'id':id_line, 'label':predicted_val}) Write your data frame to a csv file submission_df.to_csv(""submission.csv"", index=False) submission_df.to_csv(""submission.csv"", index=False) I hope this CNN model will help you, mention in comments on what topic you want the next article.";Dog and Cat Classification using CNN
2020-06-25 00:00:26;Using the HOG features of Machine Learning, we can build up a simple facial detection algorithm with any Image processing estimator,  here we will use a linear support vector machine, and it’s steps are as follows:Let’s go through these steps and try it out: Let’s start by finding some positive training samples for Image processing, that show a variety of faces. We have one easy set of data to work with, the Labeled Faces in the Wild dataset, which can be downloaded by Scikit-Learn:#Output- (3185, 62, 47)This gives us a sample of more 13,000 face images to use for training.Next we need a set of similarly sized thumbnails that do not have a face in them. One way to do this is to take any corpus of input images, and extract thumbnails from them at a variety of scales. Here we can use some of the images shipped with Scikit-Image, along with Scikit-Learn’s PatchExtractor:#Output- (30000, 62, 47)We now have 30,000 suitable image patches that do not contain faces. Let’s take a look at a few of them to get an idea of what they look like:My hope is that these would sufficiently cover the space of “nonfaces” that our algorithm is likely to see.Now that we have these positive samples and negative samples, we can combine them and compute HOG features. This step takes a little while, because the HOG features involve a nontrivial computation for each image:#Output- (33185, 1215)We are left with 33,185 training samples in 1,215 dimensions, and we now have our data in a form that we can feed into Scikit-Learn.Next we use the tools to create a classifier of thumbnail patches. For such a high-dimensional binary classification task, a linear support vector machine is a good choice. We will use Scikit-Learn’s Linear SVC, because in comparison to SVC it often has better scaling for large number of samples.First, though, let’s use a simple Gaussian naive Bayes to get a quick baseline:#Output- array([0.96112702, 0.986741 , 0.98900105, 0.99261715, 0.98885038])We see that on our training data, even a simple naive Bayes algorithm gets us upward of 90% accuracy. Let’s try the support vector machine, with a grid search over a few choices of the C parameter:#Output- 0.9934910351062227#Output- {‘C’: 1.0}Let’s take the best estimator and retrain it on the full dataset:Now that we have this model in place, let’s grab a new image and see how the model does. We will use one portion of the astronaut image for simplicity and run a sliding window over it and evaluate each patch.Next, let’s create a window that iterates over patches of this image, and compute HOG features for each patch:#Output- (1911, 1215)Finally, we can take these HOG-featured patches and use our model to evaluate whether each patch contains a face:#output- 36.0We see that out of nearly 2,000 patches, we have found 36 detections.  Let’s use the information we have about these patches to show where they lie on our test image, drawing them as rectangles:All of the detected patches overlap and found the face in the image! Not bad for a few lines of Python. I hope you liked this article on Image Processing. Feel free to ask your valuable questions in the comments section below.;https://thecleverprogrammer.com/2020/06/25/image-processing-with-machine-learning-and-python/;['sklearn', 'skimage'];1.0;['CV'];['ML', 'Naive Bayes', 'Classification', 'CV'];['detect', 'predict', 'fit', 'model', 'machine learning', 'classif', 'training data', 'naive bayes', 'train', 'label'];Using the HOG features of Machine Learning, we can build up a simple facial detection algorithm with any Image processing estimator, here we will use a linear support vector machine, and it’s steps are as follows: Obtain a set of image thumbnails of faces to constitute “positive” training samples.Obtain a set of image thumbnails of nonfaces to constitute “negative” training samples.Extract HOG features from these training samples.Train a linear SVM classifier on these samples.For an “unknown” image, pass a sliding window across the image, using the model to evaluate whether that window contains a face or not.If detections overlap, combine them into a single window. Let’s go through these steps and try it out: Obtain a set of positive training samples: Let’s start by finding some positive training samples for Image processing, that show a variety of faces. We have one easy set of data to work with, the Labeled Faces in the Wild dataset, which can be downloaded by Scikit-Learn: import numpy as np from sklearn.datasets import fetch_lfw_people from skimage import data, color, transform, feature faces = fetch_lfw_people() positive_patches = faces.images print(positive_patches.shape)Code language: Python (python) #Output- (3185, 62, 47) This gives us a sample of more 13,000 face images to use for training. Obtain a set of negative training samples: Next we need a set of similarly sized thumbnails that do not have a face in them. One way to do this is to take any corpus of input images, and extract thumbnails from them at a variety of scales. Here we can use some of the images shipped with Scikit-Image, along with Scikit-Learn’s PatchExtractor: from skimage import data, transform imgs_to_use = ['camera', 'text', 'coins', 'moon', 'page', 'clock', 'immunohistochemistry', 'chelsea', 'coffee', 'hubble_deep_field'] images = [color.rgb2gray(getattr(data, name)()) for name in imgs_to_use]Code language: Python (python) from sklearn.feature_extraction.image import PatchExtractor def extract_patches(img, N, scale=1.0, patch_size=positive_patches[0].shape): extracted_patch_size = \ tuple((scale * np.array(patch_size)).astype(int)) extractor = PatchExtractor(patch_size=extracted_patch_size, max_patches=N, random_state=0) patches = extractor.transform(img[np.newaxis]) if scale != 1: patches = np.array([transform.resize(patch, patch_size) for patch in patches]) return patches negative_patches = np.vstack([extract_patches(im, 1000, scale) for im in images for scale in [0.5, 1.0, 2.0]]) print(negative_patches.shape)Code language: Python (python) #Output- (30000, 62, 47) We now have 30,000 suitable image patches that do not contain faces. Let’s take a look at a few of them to get an idea of what they look like: import matplotlib.pyplot as plt fig, ax = plt.subplots(6, 10) for i, axi in enumerate(ax.flat): axi.imshow(negative_patches[500 * i], cmap='gray') axi.axis('off') plt.show()Code language: Python (python) My hope is that these would sufficiently cover the space of “nonfaces” that our algorithm is likely to see. Combine sets and extract HOG features for Image Processing: Now that we have these positive samples and negative samples, we can combine them and compute HOG features. This step takes a little while, because the HOG features involve a nontrivial computation for each image: from itertools import chain X_train = np.array([feature.hog(im) for im in chain(positive_patches, negative_patches)]) y_train = np.zeros(X_train.shape[0]) y_train[:positive_patches.shape[0]] = 1 print(X_train.shape)Code language: Python (python) #Output- (33185, 1215) We are left with 33,185 training samples in 1,215 dimensions, and we now have our data in a form that we can feed into Scikit-Learn. Train a support vector machine for Image Processing : Next we use the tools to create a classifier of thumbnail patches. For such a high-dimensional binary classification task, a linear support vector machine is a good choice. We will use Scikit-Learn’s Linear SVC, because in comparison to SVC it often has better scaling for large number of samples. First, though, let’s use a simple Gaussian naive Bayes to get a quick baseline: from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import cross_val_score cross_val_score(GaussianNB(), X_train, y_train)Code language: Python (python) #Output- array([0.96112702, 0.986741 , 0.98900105, 0.99261715, 0.98885038]) We see that on our training data, even a simple naive Bayes algorithm gets us upward of 90% accuracy. Let’s try the support vector machine, with a grid search over a few choices of the C parameter: from sklearn.svm import LinearSVC from sklearn.model_selection import GridSearchCV grid = GridSearchCV(LinearSVC(), {'C': [1.0, 2.0, 4.0, 8.0]}) grid.fit(X_train, y_train) grid.best_score_Code language: Python (python) #Output- 0.9934910351062227 grid.best_params_Code language: Python (python) #Output- {‘C’: 1.0} Let’s take the best estimator and retrain it on the full dataset: model = grid.best_estimator_ model.fit(X_train, y_train) #OutputCode language: Python (python) Find faces in a new image: Now that we have this model in place, let’s grab a new image and see how the model does. We will use one portion of the astronaut image for simplicity and run a sliding window over it and evaluate each patch. import skimage test_image = skimage.data.astronaut() test_image = skimage.color.rgb2gray(test_image) test_image = skimage.transform.rescale(test_image, 0.5) test_image = test_image[:160, 40:180] plt.imshow(test_image, cmap='gray') plt.axis('off') plt.show()Code language: Python (python) Next, let’s create a window that iterates over patches of this image, and compute HOG features for each patch: def sliding_window(img, patch_size=positive_patches[0].shape, istep=2, jstep=2, scale=1.0): Ni, Nj = (int(scale * s) for s in patch_size) for i in range(0, img.shape[0] - Ni, istep): for j in range(0, img.shape[1] - Ni, jstep): patch = img[i:i + Ni, j:j + Nj] if scale != 1: patch = transform.resize(patch, patch_size) yield (i, j), patch indices, patches = zip(*sliding_window(test_image)) patches_hog = np.array([feature.hog(patch) for patch in patches]) patches_hog.shapeCode language: Python (python) #Output- (1911, 1215) Finally, we can take these HOG-featured patches and use our model to evaluate whether each patch contains a face: labels = model.predict(patches_hog) labels.sum()Code language: Python (python) #output- 36.0 We see that out of nearly 2,000 patches, we have found 36 detections. Let’s use the information we have about these patches to show where they lie on our test image, drawing them as rectangles: fig, ax = plt.subplots() ax.imshow(test_image, cmap='gray') ax.axis('off') Ni, Nj = positive_patches[0].shape indices = np.array(indices) for i, j in indices[labels == 1]: ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='yellow', alpha=0.4, lw=3, facecolor='none')) plt.show()Code language: Python (python) All of the detected patches overlap and found the face in the image! Not bad for a few lines of Python. I hope you liked this article on Image Processing. Feel free to ask your valuable questions in the comments section below.;Image Processing with Machine Learning and Python
2020-06-29 22:31:15;Skin Cancer is one of the most common types of disease in the United States. Up to 4 Million cases have been reported dead due to skin cancer in the United States over the year. In this article, I will create a model for skin cancer classification with Machine Learning. This is a huge number, really 4 million, people died just from Skin cancer in a country. As all those people have been dying, but half of those cases or maybe even more, didn’t have went to the doctor at the early stages of the disease when it might have been prevented.If people are getting the symptoms of skin cancer, they still don’t reach out for a doctor, which is not a good signal because skin cancer can be cured at the early stages.So this is where a machine learning algorithm works in the classification of Skin Cancer. As I mentioned earlier Skin Cancer can be easily cured in the early stages of the disease, but it’s the people who don’t want to visit the doctor. So here is a simple machine learning algorithm, which can help those people to identify if they are having skin cancer or not while sitting at their homes. This Machine Learning algorithm is based on Convolutional Neural Networks (CNN).Lets start with importing the librariesNow I will simply upload images to train our machine learning model using the skimage library in python. You can download these images from belowThese images are sample images of  Benign mole and Malign mole which are a type of skin problems.Let’s visualize these imagesNow lets train our model for further classificationTo see the summary of our trained model we will execute the code belowNow lets visualize the output of our trained modelNow lets zoom and visualize some of the filtersNow let’s visualize and test the actual ability of our trained model;https://thecleverprogrammer.com/2020/06/29/skin-cancer-classification-with-machine-learning/;['keras', 'skimage'];1.0;['NN', 'CNN'];['NN', 'ML', 'CNN', 'VGG', 'Classification'];['detect', 'model', 'machine learning', 'neural network', 'classif', 'layer', 'filter', 'convolutional neural network', 'train', 'vgg'];Skin Cancer is one of the most common types of disease in the United States. Up to 4 Million cases have been reported dead due to skin cancer in the United States over the year. In this article, I will create a model for skin cancer classification with Machine Learning. This is a huge number, really 4 million, people died just from Skin cancer in a country. As all those people have been dying, but half of those cases or maybe even more, didn’t have went to the doctor at the early stages of the disease when it might have been prevented. If people are getting the symptoms of skin cancer, they still don’t reach out for a doctor, which is not a good signal because skin cancer can be cured at the early stages. Skin Cancer Classification with Machine Learning So this is where a machine learning algorithm works in the classification of Skin Cancer. As I mentioned earlier Skin Cancer can be easily cured in the early stages of the disease, but it’s the people who don’t want to visit the doctor. So here is a simple machine learning algorithm, which can help those people to identify if they are having skin cancer or not while sitting at their homes. This Machine Learning algorithm is based on Convolutional Neural Networks (CNN). CNN Layers Classification for Skin Cancer Detection Lets start with importing the libraries import numpy as np from skimage import io import matplotlib.pyplot as pltCode language: Python (python) Now I will simply upload images to train our machine learning model using the skimage library in python. imgb = io.imread('bimg-1049.png') imgm = io.imread('mimg-178.png') imgb = io.imread('bimg-721.png') imgm = io.imread('mimg-57.png')Code language: Python (python) You can download these images from below Download Images These images are sample images of Benign mole and Malign mole which are a type of skin problems. Let’s visualize these images plt.figure(figsize=(10,20)) plt.subplot(121) plt.imshow(imgb) plt.axis('off') plt.subplot(122) plt.imshow(imgm) plt.axis('off')Code language: Python (python) Now lets train our model for further classification from keras.models import load_model model = load_model('BM_VA_VGG_FULL_DA.hdf5') from keras import backend as K def activ_viewer(model, layer_name, im_put): layer_dict = dict([(layer.name, layer) for layer in model.layers]) layer = layer_dict[layer_name] activ1 = K.function([model.layers[0].input, K.learning_phase()], [layer.output,]) activations = activ1((im_put, False)) return activations def normalize(x): # utility function to normalize a tensor by its L2 norm return x / (K.sqrt(K.mean(K.square(x))) + 1e-5) def deprocess_image(x): # normalize tensor: center on 0., ensure std is 0.1 x -= x.mean() x /= (x.std() + 1e-5) x *= 0.1 # clip to [0, 1] x += 0.5 x = np.clip(x, 0, 1) # convert to RGB array x *= 255 if K.image_data_format() == 'channels_first': x = x.transpose((1, 2, 0)) x = np.clip(x, 0, 255).astype('uint8') return x def plot_filters(filters): newimage = np.zeros((16*filters.shape[0],8*filters.shape[1])) for i in range(filters.shape[2]): y = i%8 x = i//8 newimage[x*filters.shape[0]:x*filters.shape[0]+filters.shape[0], y*filters.shape[1]:y*filters.shape[1]+filters.shape[1]] = filters[:,:,i] plt.figure(figsize = (10,20)) plt.imshow(newimage) plt.axis('off')Code language: Python (python) layer_dict = dict([(layer.name, layer) for layer in model.layers])Code language: Python (python) Skin Cancer Classification Model Summary To see the summary of our trained model we will execute the code below model.summary()Code language: Python (python) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 128, 128, 3) 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 128, 128, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 128, 128, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 64, 64, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 64, 64, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 64, 64, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 32, 32, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 32, 32, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 32, 32, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 32, 32, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 16, 16, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 16, 16, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 16, 16, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 16, 16, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 8, 8, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 8, 8, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 8, 8, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 8, 8, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 4, 4, 512) 0 _________________________________________________________________ sequential_3 (Sequential) (None, 1) 2097665 ================================================================= Total params: 16,812,353 Trainable params: 16,812,353 Non-trainable params: 0 Now lets visualize the output of our trained model activ_benign = activ_viewer(model,'block2_conv1',imgb.reshape(1,128,128,3)) img_benign = deprocess_image(activ_benign[0]) plot_filters(img_benign[0]) plt.figure(figsize=(20,20)) for f in range(128): plt.subplot(8,16,f+1) plt.imshow(img_benign[0,:,:,f]) plt.axis('off')Code language: Python (python) activ_malign = activ_viewer(model,'block2_conv1',imgm.reshape(1,128,128,3)) img_malign = deprocess_image(activ_malign[0]) plot_filters(img_malign[0]) plt.figure(figsize=(20,20)) for f in range(128): plt.subplot(8,16,f+1) plt.imshow(img_malign[0,:,:,f]) plt.axis('off')Code language: Python (python) Now lets zoom and visualize some of the filters plt.figure(figsize=(10,10)) plt.subplot(121) plt.imshow(img_benign[0,:,:,49]) plt.axis('off') plt.subplot(122) plt.imshow(img_malign[0,:,:,49]) plt.axis('off')Code language: Python (python) plt.figure(figsize=(10,10)) plt.subplot(121) plt.imshow(img_benign[0,:,:,94]) plt.axis('off') plt.subplot(122) plt.imshow(img_malign[0,:,:,94]) plt.axis('off')Code language: Python (python) Testing the Model Now let’s visualize and test the actual ability of our trained model def plot_filters32(filters): newimage = np.zeros((16*filters.shape[0],16*filters.shape[1])) for i in range(filters.shape[2]): y = i%16 x = i//16 newimage[x*filters.shape[0]:x*filters.shape[0]+filters.shape[0], y*filters.shape[1]:y*filters.shape[1]+filters.shape[1]] = filters[:,:,i] plt.figure(figsize = (15,25)) plt.imshow(newimage) activ_benign = activ_viewer(model,'block3_conv3',imgb.reshape(1,128,128,3)) img_benign = deprocess_image(activ_benign[0]) plot_filters32(img_benign[0])Code language: Python (python) activ_malign = activ_viewer(model,'block3_conv3',imgm.reshape(1,128,128,3)) img_malign = deprocess_image(activ_malign[0]) plot_filters32(img_malign[0])Code language: Python (python);Skin Cancer Classification with Machine Learning
2020-07-01 21:08:52;"Time Series Analysis carries methods to research time-series statistics to extract statistical features from the data. Time Series Forecasting is used in training a Machine learning model to predict future values with the usage of historical importance.Time Series Analysis is broadly speaking used in training machine learning models for the Economy, Weather forecasting, stock price prediction, and additionally in Sales forecasting. It can be said that Time Series Analysis is widely used in facts based on non-stationary features.In this article, I will use different methods for sales forecasting using the time series analysis with python. You can download the dataset that I have used in this article below.Let’s start with this tutorial on Time Series Forecasting using Python by importing the libraries.There are different categories in the dataset, lets start from time series analysis and sales forecasting of furniture.Timestamp(‘2014–01–06 00:00:00’), Timestamp(‘2017–12–30 00:00:00’)Data Preprocessing includes removing columns that we don’t need, looking for missing values, etc.The current DateTime looks a little challenging to work within the dataset, so I will use the price of each day sales on average of the month for maintaining it simple. I will use the start of each month as a timestamp. Some patterns can be drawn from the above figure, the time series is patterned seasonally like sales are low at the beginning of every year, and sales increases at the end of the year.Now let’s visualize this data using the time series decomposition method which will allow our time series to decompose into three components:The above figure shows that the sales of furniture is not stable because of the seasons.ARIMA is one of the most used methods in time series forecasting. ARIMA stands for Autoregressive Integrated Moving Average. Now I will use the ARIMA method in the further process of time series forecasting.This step is the process of selection of parameters in our Time Series Forecasting model for furniture sales.Now I will run Model diagnosis; running a model diagnosis is essential in Time Series Forecasting to investigate any unusual behavior in the model.To understand the accuracy of our time series forecasting model, I will compare predicted sales with actual sales, and I will set the forecasts to start at 2017-01-01 to the end of the dataset.The above figure is showing the observed values in comparison with the forecast predictions. The picture is aligned with the actual sales, really well, which is showing an upward shift in the beginning and captures the seasonality at the end of the year.The Mean Squared Error of our forecasts is 22993.58The Root Mean Squared Error of our forecasts is 151.64In statistics, the Mean Squared Error (MSE) of an estimator measures the average of the squares of the error that is, the common squared distinction among the anticipated values and what is estimated. The MSE is a measure of the fine of an estimator, its miles continually non-negative, and the smaller the MSE, the nearer we are to locating the road of an excellent fit.Root Mean Square Error (RMSE) tells us that our version was capable of forecast the average daily furniture income in the test set within 151.64 of the actual income. Our furniture day by day income range from around 400 to over 1200. In my opinion, that is a pretty good version so far.Our Time Series Forecasting model, without a doubt, captured furniture profits seasonality. As we forecast further out into the future, it’s very natural for us to become very much less assured in our values. This gets reflected by way of the self-belief intervals generated via our model, which grows more significant as we move similarly out into the future.";https://thecleverprogrammer.com/2020/07/01/time-series-analysis-and-forecasting-with-python/;['statsmodels', 'pattern'];1.0;['AI'];['ML', 'AI'];['predict', 'fit', 'model', 'machine learning', 'filter', 'train', 'label'];"Time Series Analysis carries methods to research time-series statistics to extract statistical features from the data. Time Series Forecasting is used in training a Machine learning model to predict future values with the usage of historical importance. Time Series Analysis is broadly speaking used in training machine learning models for the Economy, Weather forecasting, stock price prediction, and additionally in Sales forecasting. It can be said that Time Series Analysis is widely used in facts based on non-stationary features. Time Series Analysis and Forecasting with Python In this article, I will use different methods for sales forecasting using the time series analysis with python. You can download the dataset that I have used in this article below. Download Dataset Let’s start with this tutorial on Time Series Forecasting using Python by importing the libraries. import warnings import itertools import numpy as np import matplotlib.pyplot as plt warnings.filterwarnings(""ignore"") plt.style.use('fivethirtyeight') import pandas as pd import statsmodels.api as sm import matplotlib matplotlib.rcParams['axes.labelsize'] = 14 matplotlib.rcParams['xtick.labelsize'] = 12 matplotlib.rcParams['ytick.labelsize'] = 12 matplotlib.rcParams['text.color'] = 'k'Code language: Python (python) There are different categories in the dataset, lets start from time series analysis and sales forecasting of furniture. df = pd.read_excel(""Superstore.xls"") furniture = df.loc[df['Category'] == 'Furniture'] furniture['Order Date'].min(), furniture['Order Date'].max()Code language: Python (python) Timestamp(‘2014–01–06 00:00:00’), Timestamp(‘2017–12–30 00:00:00’) Data Preprocessing Data Preprocessing includes removing columns that we don’t need, looking for missing values, etc. cols = ['Row ID', 'Order ID', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Quantity', 'Discount', 'Profit'] furniture.drop(cols, axis=1, inplace=True) furniture = furniture.sort_values('Order Date') furniture.isnull().sum()Code language: Python (python) furniture = furniture.groupby('Order Date')['Sales'].sum().reset_index()Code language: Python (python) Indexing Time Series Data furniture = furniture.set_index('Order Date') furniture.index​x furniture = furniture.set_index('Order Date')furniture.index The current DateTime looks a little challenging to work within the dataset, so I will use the price of each day sales on average of the month for maintaining it simple. I will use the start of each month as a timestamp. y = furniture['Sales'].resample('MS').mean() y = furniture['Sales'].resample('MS').mean() Visualizing The Furniture Sales Data y.plot(figsize=(15, 6)) plt.show()Code language: Python (python) Some patterns can be drawn from the above figure, the time series is patterned seasonally like sales are low at the beginning of every year, and sales increases at the end of the year. Now let’s visualize this data using the time series decomposition method which will allow our time series to decompose into three components: TrendSeasonNoise from pylab import rcParams rcParams['figure.figsize'] = 18, 8 decomposition = sm.tsa.seasonal_decompose(y, model='additive') fig = decomposition.plot() plt.show()Code language: Python (python) The above figure shows that the sales of furniture is not stable because of the seasons. Time Series Forecasting with ARIMA ARIMA is one of the most used methods in time series forecasting. ARIMA stands for Autoregressive Integrated Moving Average. Now I will use the ARIMA method in the further process of time series forecasting. p = d = q = range(0, 2) pdq = list(itertools.product(p, d, q)) seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))] print('Examples of parameter combinations for Seasonal ARIMA...') print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1])) print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2])) print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3])) print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))Code language: Python (python) This step is the process of selection of parameters in our Time Series Forecasting model for furniture sales. for param in pdq: for param_seasonal in seasonal_pdq: try: mod = sm.tsa.statespace.SARIMAX(y, order=param, seasonal_order=param_seasonal, enforce_stationarity=False, enforce_invertibility=False) results = mod.fit() print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic)) except: continueCode language: Python (python) Fitting ARIMA Model mod = sm.tsa.statespace.SARIMAX(y, order=(1, 1, 1), seasonal_order=(1, 1, 0, 12), enforce_stationarity=False, enforce_invertibility=False) results = mod.fit() print(results.summary().tables[1])Code language: Python (python) Now I will run Model diagnosis; running a model diagnosis is essential in Time Series Forecasting to investigate any unusual behavior in the model. results.plot_diagnostics(figsize=(16, 8)) plt.show()Code language: Python (python) Validating Time Series Forecasts To understand the accuracy of our time series forecasting model, I will compare predicted sales with actual sales, and I will set the forecasts to start at 2017-01-01 to the end of the dataset. pred = results.get_prediction(start=pd.to_datetime('2017-01-01'), dynamic=False) pred_ci = pred.conf_int() ax = y['2014':].plot(label='observed') pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7)) ax.fill_between(pred_ci.index, pred_ci.iloc[:, 0], pred_ci.iloc[:, 1], color='k', alpha=.2) ax.set_xlabel('Date') ax.set_ylabel('Furniture Sales') plt.legend() plt.show()Code language: Python (python) The above figure is showing the observed values in comparison with the forecast predictions. The picture is aligned with the actual sales, really well, which is showing an upward shift in the beginning and captures the seasonality at the end of the year. y_forecasted = pred.predicted_mean y_truth = y['2017-01-01':] mse = ((y_forecasted - y_truth) ** 2).mean() print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))Code language: Python (python) The Mean Squared Error of our forecasts is 22993.58 print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))Code language: Python (python) The Root Mean Squared Error of our forecasts is 151.64 In statistics, the Mean Squared Error (MSE) of an estimator measures the average of the squares of the error that is, the common squared distinction among the anticipated values and what is estimated. The MSE is a measure of the fine of an estimator, its miles continually non-negative, and the smaller the MSE, the nearer we are to locating the road of an excellent fit. Root Mean Square Error (RMSE) tells us that our version was capable of forecast the average daily furniture income in the test set within 151.64 of the actual income. Our furniture day by day income range from around 400 to over 1200. In my opinion, that is a pretty good version so far. Producing and visualizing forecasts pred_uc = results.get_forecast(steps=100) pred_ci = pred_uc.conf_int() ax = y.plot(label='observed', figsize=(14, 7)) pred_uc.predicted_mean.plot(ax=ax, label='Forecast') ax.fill_between(pred_ci.index, pred_ci.iloc[:, 0], pred_ci.iloc[:, 1], color='k', alpha=.25) ax.set_xlabel('Date') ax.set_ylabel('Furniture Sales') plt.legend() plt.show()Code language: Python (python) Our Time Series Forecasting model, without a doubt, captured furniture profits seasonality. As we forecast further out into the future, it’s very natural for us to become very much less assured in our values. This gets reflected by way of the self-belief intervals generated via our model, which grows more significant as we move similarly out into the future. Follow Us: Facebook Instagram";Time Series Analysis and Forecasting with Python
2020-07-02 13:39:41;One of the best things about the scikit-learn library in python is that it provides four steps modeling patterns that make it easy for the programmer to train a machine learning classifier. In this article, I will use Logistic Regression with python, to classify the digits which are based on images. After preparing our machine learning model with this logistic regression, we can use it to predict an image labeled with the numbers.The scikit-learn library comes with a preloaded digits dataset. That means we need to load the digits dataset, and we are not required to download any dataset for this classification. Now let’s load our dataset.Now let’s look at some insights from the dataset.Now let’s see what our data contains, I will visualize the images and labels present in the dataset, to know what I need to work with.Now I will split the data into 75 percent training and 25 percent testing sets. The need to break the data into training and testing sets is to ensure that our classification model can fit properly in the new data.Step one is the import the model that we want to use, As this article is based on the logistic regression so, I will import the logistic regression model from the scikit-learn library in python.Step two is to create an instance of the model, which means that we need to store the Logistic Regression model into a variable.Step three will be to train the model. For this, we need the fit the data into our Logistic Regression model.Step four is to predict the labels for the new data,In this step, we need to use the information that we learned while training the model.I will measure the Accuracy of our trained Logistic Regressing Model, where Accuracy is defined as the fraction of correct predictions, which is correct predictions/total number of data points.So our Accuracy gives the output as 95.3 percent, which is generally appreciated.Confusion Matrix is the table used in describing the performance of a Classifier that we have trained using the dataset. Here I will use Matplotlib and Seaborn in python to describe the performance of our trained model.Now let’s visualize our performance using the confusion matrix. First, I will visualize the confusion matrix using the Seaborn library in python.Now let’s visualize our Logistic Regression model’s performance with the confusion matrix using the matplotlib library in python.The Logistic Regression model that you saw above was you give you an idea of how this classifier works with python to train a machine learning model. Now let’s prepare a Logistic Regression model for a real-world example using more significant data to fit our model.Now after loading the MNIST dataset, let’s see some insights into the data.In the output, you will see 70000 images and 70000 labels in this dataset, which sounds very challenging for a real-world problem.Now let’s split the data into training and testing sets. Here I will break the dataset into 60000 images as a training set and 10000 images as a testing set.As I told you earlier, that we need to look at the data before moving forward to see what we need to work with. Here I will visualize the data using the matplotlib library in python.Now let’s follow the scikit-learn’s modeling pattern as I did earlier in the above example.So, this is how you can efficiently train a machine learning model. If you prepare a model in python with Scikit-learn, you will never find it difficult. I hope this article helps you. Feel free to ask questions on Logistic Regression in Machine Learning with Python or any other topic, in the comments section.;https://thecleverprogrammer.com/2020/07/02/logistic-regression-in-machine-learning-with-python/;['pattern', 'sklearn'];1.0;[];['ML', 'Logistic Regression', 'Classification', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'logistic regression', 'classif', 'train', 'label'];"One of the best things about the scikit-learn library in python is that it provides four steps modeling patterns that make it easy for the programmer to train a machine learning classifier. In this article, I will use Logistic Regression with python, to classify the digits which are based on images. After preparing our machine learning model with this logistic regression, we can use it to predict an image labeled with the numbers. Logistic Regression on Digits with Python The scikit-learn library comes with a preloaded digits dataset. That means we need to load the digits dataset, and we are not required to download any dataset for this classification. Now let’s load our dataset. from sklearn.datasets import load_digits digits = load_digits()Code language: Python (python) Now let’s look at some insights from the dataset. # Print to show there are 1797 images (8 by 8 images for a dimensionality of 64) print(""Image Data Shape"" , digits.data.shape) # Print to show there are 1797 labels (integers from 0–9) print(""Label Data Shape"", digits.target.shape)Code language: Python (python) To Show the Images and Labels in Digits Dataset Now let’s see what our data contains, I will visualize the images and labels present in the dataset, to know what I need to work with. import numpy as np import matplotlib.pyplot as plt plt.figure(figsize=(20,4)) for index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])): plt.subplot(1, 5, index + 1) plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray) plt.title('Training: %in' % label, fontsize = 20)Code language: Python (python) Split the Data into Training and Test Set Now I will split the data into 75 percent training and 25 percent testing sets. The need to break the data into training and testing sets is to ensure that our classification model can fit properly in the new data. Scikit-learn 4 Steps Modelling Pattern(Logistic Regression) Step one is the import the model that we want to use, As this article is based on the logistic regression so, I will import the logistic regression model from the scikit-learn library in python. from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)Code language: Python (python) from sklearn.linear_model import LogisticRegressionCode language: Python (python) Step two is to create an instance of the model, which means that we need to store the Logistic Regression model into a variable. logisticRegr = LogisticRegression()Code language: Python (python) Step three will be to train the model. For this, we need the fit the data into our Logistic Regression model. logisticRegr.fit(x_train, y_train)Code language: Python (python) Step four is to predict the labels for the new data,In this step, we need to use the information that we learned while training the model. # Returns a NumPy Array # Predict for One Observation (image) logisticRegr.predict(x_test[0].reshape(1,-1)) logisticRegr.predict(x_test[0:10]) predictions = logisticRegr.predict(x_test)Code language: Python (python) Measure the Accuracy of our Logistic Regression Model I will measure the Accuracy of our trained Logistic Regressing Model, where Accuracy is defined as the fraction of correct predictions, which is correct predictions/total number of data points. # Use score method to get accuracy of model score = logisticRegr.score(x_test, y_test) print(score)Code language: Python (python) So our Accuracy gives the output as 95.3 percent, which is generally appreciated. Confusion Matrix Confusion Matrix is the table used in describing the performance of a Classifier that we have trained using the dataset. Here I will use Matplotlib and Seaborn in python to describe the performance of our trained model. import matplotlib.pyplot as plt import seaborn as sns from sklearn import metrics cm = metrics.confusion_matrix(y_test, predictions) print(cm)Code language: Python (python) Now let’s visualize our performance using the confusion matrix. First, I will visualize the confusion matrix using the Seaborn library in python. plt.figure(figsize=(9,9)) sns.heatmap(cm, annot=True, fmt="".3f"", linewidths=.5, square = True, cmap = 'Blues_r'); plt.ylabel('Actual label'); plt.xlabel('Predicted label'); all_sample_title = 'Accuracy Score: {0}'.format(score) plt.title(all_sample_title, size = 15) plt.show()Code language: Python (python) Now let’s visualize our Logistic Regression model’s performance with the confusion matrix using the matplotlib library in python. plt.figure(figsize=(9,9)) plt.imshow(cm, interpolation='nearest', cmap='Pastel1') plt.title('Confusion matrix', size = 15) plt.colorbar() tick_marks = np.arange(10) plt.xticks(tick_marks, [""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""], rotation=45, size = 10) plt.yticks(tick_marks, [""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""], size = 10) plt.tight_layout() plt.ylabel('Actual label', size = 15) plt.xlabel('Predicted label', size = 15) width, height = cm.shape for x in xrange(width): for y in xrange(height): plt.annotate(str(cm[x][y]), xy=(y, x), horizontalalignment='center', verticalalignment='center')Code language: Python (python) Logistic Regression (MNIST) The Logistic Regression model that you saw above was you give you an idea of how this classifier works with python to train a machine learning model. Now let’s prepare a Logistic Regression model for a real-world example using more significant data to fit our model. Load the MNIST Dataset from sklearn.datasets import fetch_mldata mnist = fetch_mldata('MNIST original')Code language: Python (python) Now after loading the MNIST dataset, let’s see some insights into the data. # These are the images # There are 70,000 images (28 by 28 images for a dimensionality of 784) print(mnist.data.shape) # These are the labels print(mnist.target.shape)Code language: Python (python) In the output, you will see 70000 images and 70000 labels in this dataset, which sounds very challenging for a real-world problem. Split the Data into Training and Testing Now let’s split the data into training and testing sets. Here I will break the dataset into 60000 images as a training set and 10000 images as a testing set. Visualize the Data As I told you earlier, that we need to look at the data before moving forward to see what we need to work with. Here I will visualize the data using the matplotlib library in python. from sklearn.model_selection import train_test_split train_img, test_img, train_lbl, test_lbl = train_test_split( mnist.data, mnist.target, test_size=1/7.0, random_state=0)Code language: Python (python) import numpy as np import matplotlib.pyplot as plt plt.figure(figsize=(20,4)) for index, (image, label) in enumerate(zip(train_img[0:5], train_lbl[0:5])): plt.subplot(1, 5, index + 1) plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray) plt.title('Training: %in' % label, fontsize = 20) plt.show()Code language: Python (python) Scikit-Learn Modelling Pattern Now let’s follow the scikit-learn’s modeling pattern as I did earlier in the above example. from sklearn.linear_model import LogisticRegression # all parameters not specified are set to their defaults # default solver is incredibly slow thats why we change it logisticRegr = LogisticRegression(solver = 'lbfgs') logisticRegr.fit(train_img, train_lbl) # Returns a NumPy Array # Predict for One Observation (image) logisticRegr.predict(test_img[0].reshape(1,-1)) logisticRegr.predict(test_img[0:10]) predictions = logisticRegr.predict(test_img)Code language: Python (python) So, this is how you can efficiently train a machine learning model. If you prepare a model in python with Scikit-learn, you will never find it difficult. I hope this article helps you. Feel free to ask questions on Logistic Regression in Machine Learning with Python or any other topic, in the comments section. Follow Us: Facebook Instagram";Logistic Regression in Machine Learning with Python
2020-07-05 20:45:14;In Machine Learning Naive Bayes models are a group of high-speed and simple classification algorithms that are often suitable for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem. This article will focus on an intuitive explanation of how naive Bayes classification work, followed by a couple of examples of them in action on some datasets.;https://thecleverprogrammer.com/2020/07/05/naive-bayes-classification-in-machine-learning/;['sklearn'];1.0;[];['ML', 'Bayesian', 'Text Classification', 'Naive Bayes', 'Classification'];['text classification', 'predict', 'fit', 'model', 'bayesian', 'machine learning', 'classif', 'training data', 'naive bayes', 'train', 'label', 'test data'];"In Machine Learning Naive Bayes models are a group of high-speed and simple classification algorithms that are often suitable for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem. This article will focus on an intuitive explanation of how naive Bayes classification work, followed by a couple of examples of them in action on some datasets. Bayesian Classification Naive Bayes classifiers are built on Bayesian classification methods. These rely on Bayes’s theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities. In Bayesian classification, we’re interested in finding the probability of a label given some observed features, which we can write as P(L | features)P(L | features). Bayes’s theorem tells us how to express this in terms of quantities we can compute more directly: If we are trying to decide between two labels—let’s call them L1 and L2—then one way to make this decision is to compute the ratio of the posterior probabilities for each label: All we need now is some model by which we can compute P(features | Li)P(features | Li) for each label. Such a model is called a generative model because it specifies the hypothetical random process that generates the data. Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier. The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model. This is where the “naive” in “naive Bayes” comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification. Different types of naive Bayes classifiers rest on different naive assumptions about the data, and we will examine a few of these in the following sections. I will begin with the standard imports: %matplotlib inline import numpy as np import matplotlib.pyplot as plt import seaborn as sns; sns.set()Code language: Python (python) Gaussian Naive Bayes Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes. In this classifier, the assumption is that data from each label is drawn from a simple Gaussian distribution. Imagine that you have the following data: from sklearn.datasets import make_blobs X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')Code language: Python (python) One extremely fast way to create a simple model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions. This model can be fit by simply finding the mean and standard deviation of the points within each label, which is all you need to define such a distribution. The result of this naive Gaussian assumption is shown in the following figure: The ellipses here represent the Gaussian generative model for each label, with larger probability toward the center of the ellipses. With this generative model in place for each class, we have a simple recipe to compute the likelihood P(features | L1)P(features | L1) for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point. This procedure is implemented in Scikit-Learn’s sklearn.naive_bayes.GaussianNB estimator: from sklearn.naive_bayes import GaussianNB model = GaussianNB() model.fit(X, y)Code language: Python (python) Now let’s generate some new data and predict the label: rng = np.random.RandomState(0) Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2) ynew = model.predict(Xnew)Code language: Python (python) Now we can plot this new data to get an idea of where the decision boundary is: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu') lim = plt.axis() plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1) plt.axis(lim)Code language: Python (python) We see a slightly curved boundary in the classifications—in general, the boundary in Gaussian naive Bayes is quadratic. A nice piece of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the predict_proba method: yprob = model.predict_proba(Xnew) yprob[-8:].round(2)Code language: Python (python) array([[ 0.89, 0.11], [ 1. , 0. ], [ 1. , 0. ], [ 1. , 0. ], [ 1. , 0. ], [ 1. , 0. ], [ 0. , 1. ], [ 0.15, 0.85]]) The columns give the posterior probabilities of the first and second label, respectively. If you are looking for estimates of uncertainty in your classification, Bayesian approaches like this can be a useful approach. Of course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce perfect results. Still, in many cases—especially as the number of features becomes large—this assumption is not detrimental enough to prevent Gaussian naive Bayes from being a useful method. Multinomial Naive Bayes The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label. Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution. The multinomial distribution describes the probability of observing counts among several categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates. The idea is precisely the same as before, except that instead of modelling the data distribution with the best-fit Gaussian, we model the data distribution with a best-fit multinomial distribution. Example : Classifying Text One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified. We discussed the extraction of such features from text in Feature Engineering; here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories. Let’s download the data and take a look at the target names: from sklearn.datasets import fetch_20newsgroups data = fetch_20newsgroups() data.target_namesCode language: Python (python) ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'] For simplicity here, I will select just a few of these categories, and download the training and testing set: categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics'] train = fetch_20newsgroups(subset='train', categories=categories) test = fetch_20newsgroups(subset='test', categories=categories)Code language: Python (python) Here is a representative entry from the data: print(train.data[5])Code language: Python (python) From: dmcgee@uluhe.soest.hawaii.edu (Don McGee) Subject: Federal Hearing Originator: dmcgee@uluhe Organization: School of Ocean and Earth Science and Technology Distribution: usa Lines: 10 Fact or rumor....? Madalyn Murray O'Hare an atheist who eliminated the use of the bible reading and prayer in public schools 15 years ago is now going to appear before the FCC with a petition to stop the reading of the Gospel on the airways of America. And she is also campaigning to remove Christmas programs, songs, etc from the public schools. If it is true then mail to Federal Communications Commission 1919 H Street Washington DC 20054 expressing your opposition to her request. Reference Petition number 2493. In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers. For this we will use the TF-IDF vectorizer (discussed in Feature Engineering), and create a pipeline that attaches it to a multinomial naive Bayes classifier: from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import make_pipeline model = make_pipeline(TfidfVectorizer(), MultinomialNB())Code language: Python (python) With this pipeline, we can apply the model to the training data, and predict labels for the test data: model.fit(train.data, train.target) labels = model.predict(test.data)Code language: Python (python) Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator. For example, here is the confusion matrix between the true and predicted labels for the test data: from sklearn.metrics import confusion_matrix mat = confusion_matrix(test.target, labels) sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=train.target_names, yticklabels=train.target_names) plt.xlabel('true label') plt.ylabel('predicted label')Code language: Python (python) Evidently, even this very simple classifier can successfully separate space talk from computer talk, but it gets confused between talk about religion and talk about Christianity. This is perhaps an expected area of confusion. The very cool thing here is that we now have the tools to determine the category for any string, using the predict() method of this pipeline. Here’s a quick utility function that will return the prediction for a single string: def predict_category(s, train=train, model=model): pred = model.predict([s]) return train.target_names[pred[0]]Code language: Python (python) Now Let’s Test our Naive Bayes Classification Model predict_category('sending a payload to the ISS')Code language: Python (python) 'sci.space' predict_category('discussing islam vs atheism')Code language: Python (python) 'soc.religion.christian' predict_category('determining the screen resolution')Code language: Python (python) Remember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking. Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective. When to Use Naive Bayes Because naive Bayes classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model. That said, they have several advantages: They are extremely fast for both training and predictionThey provide straightforward probabilistic predictionThey are often very easily interpretableThey have very few (if any) tunable parameters These advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification. If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem. If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform. Naive Bayes classifiers tend to perform especially well in one of the following situations: When the naive assumptions actually match the data (very rare in practice).For very well-separated categories, when model complexity is less important.For very high-dimensional data, when model complexity is less important. I hope you liked this article on Naive Bayes Classification in Machine Learning. Feel free to ask your questions on naive Bayes classification model or any other topic in the comments section below. Follow Us: Facebook Instagram";Naive Bayes Classification in Machine Learning
2020-07-06 15:49:55;"Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this article, I will develop the intuition behind support vector machines and their use in classification problems.I will begin with the standard imports:Let’s consider the simple case of a classification task, in which the two classes of points are well separated:A linear discriminative classifier would attempt to draw a straight line separating the two sets of data and thereby create a model for classification. For two dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can correctly discriminate between the two classes.I will draw them as follows:These are three very different separators, which, nevertheless, correctly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the “X” in this plot) will be assigned a different label! Our simple intuition of “drawing a line between classes” is not enough, and we need to think a bit deeper.Support vector machines offer one way to improve on this. The intuition is this: rather than merely drawing a zero-width line between the classes, we can bring around each edge a margin of some width, up to the nearest point. Here is an example of how this might look:In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a maximum margin estimator.Let’s see the result of an exact fit for this data: we will use Scikit-Learn’s support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the C parameter to a very large number:To better visualize what’s happening here, let’s create a quick convenience function that will plot SVM decision boundaries for us:This is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin: the black circles in this figure indicate them. These points are the pivotal elements of this fit, and are known as the support vectors, and give the algorithm its name. In Scikit-Learn, the identity of these points are stored in the support_vectors_ the attribute of the classifier:A key to this classifier’s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the right side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:As an example of support vector machines in action, let’s take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn:Let’s plot a few of these faces to see what we’re working with:Each image contains [62×47] or nearly 3,000 pixels. We could proceed by merely using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here, we will use a principal component analysis to remove 150 fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:For the sake of testing our classifier output, we will split the data into a training and testing set:Finally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model:The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen:Let’s take a look at a few of the test images along with their predicted values:Out of this small sample, our optimal estimator mislabeled only a single face (Bush’s face in the bottom row was mislabeled as Blair). We can get a better sense of our estimator’s performance using the classification report, which lists recovery statistics label by label:We might also display the confusion matrix between these classes:With those traits in mind, I generally only turn to Support Vector Machines once another simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs. Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results. I hope you liked this article, feel free to ask questions on SVMs, or any other topic, in the comments section below.";https://thecleverprogrammer.com/2020/07/06/support-vector-machines-svm-in-machine-learning/;['sklearn'];1.0;['CV'];['CV', 'Classification', 'Regression', 'Support Vector Machines'];['recogn', 'regression', 'predict', 'fit', 'model', 'loss', 'classif', 'support vector machines', 'train', 'label', 'test data'];"Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this article, I will develop the intuition behind support vector machines and their use in classification problems. I will begin with the standard imports: %matplotlib inline import numpy as np import matplotlib.pyplot as plt from scipy import stats # use seaborn plotting defaults import seaborn as sns; sns.set()Code language: Python (python) Motivating Support Vector Machines Let’s consider the simple case of a classification task, in which the two classes of points are well separated: from sklearn.datasets.samples_generator import make_blobs X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')Code language: Python (python) A linear discriminative classifier would attempt to draw a straight line separating the two sets of data and thereby create a model for classification. For two dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can correctly discriminate between the two classes. I will draw them as follows: xfit = np.linspace(-1, 3.5) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn') plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10) for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]: plt.plot(xfit, m * xfit + b, '-k') plt.xlim(-1, 3.5)Code language: Python (python) These are three very different separators, which, nevertheless, correctly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the “X” in this plot) will be assigned a different label! Our simple intuition of “drawing a line between classes” is not enough, and we need to think a bit deeper. Support Vector Machines: Maximizing the Margin Support vector machines offer one way to improve on this. The intuition is this: rather than merely drawing a zero-width line between the classes, we can bring around each edge a margin of some width, up to the nearest point. Here is an example of how this might look: xfit = np.linspace(-1, 3.5) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn') for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]: yfit = m * xfit + b plt.plot(xfit, yfit, '-k') plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4) plt.xlim(-1, 3.5)Code language: Python (python) In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a maximum margin estimator. Fitting a Support Vector Machine Let’s see the result of an exact fit for this data: we will use Scikit-Learn’s support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the C parameter to a very large number: from sklearn.svm import SVC # ""Support vector classifier"" model = SVC(kernel='linear', C=1E10) model.fit(X, y)Code language: Python (python) SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) To better visualize what’s happening here, let’s create a quick convenience function that will plot SVM decision boundaries for us: def plot_svc_decision_function(model, ax=None, plot_support=True): """"""Plot the decision function for a 2D SVC"""""" if ax is None: ax = plt.gca() xlim = ax.get_xlim() ylim = ax.get_ylim() # create grid to evaluate model x = np.linspace(xlim[0], xlim[1], 30) y = np.linspace(ylim[0], ylim[1], 30) Y, X = np.meshgrid(y, x) xy = np.vstack([X.ravel(), Y.ravel()]).T P = model.decision_function(xy).reshape(X.shape) # plot decision boundary and margins ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--']) # plot support vectors if plot_support: ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors='none'); ax.set_xlim(xlim) ax.set_ylim(ylim) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn') plot_svc_decision_function(model)Code language: Python (python) This is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin: the black circles in this figure indicate them. These points are the pivotal elements of this fit, and are known as the support vectors, and give the algorithm its name. In Scikit-Learn, the identity of these points are stored in the support_vectors_ the attribute of the classifier: model.support_vectors_Code language: Python (python) array([[ 0.44359863, 3.11530945], [ 2.33812285, 3.43116792], [ 2.06156753, 1.96918596]]) A key to this classifier’s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the right side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin. We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset: def plot_svm(N=10, ax=None): X, y = make_blobs(n_samples=200, centers=2, random_state=0, cluster_std=0.60) X = X[:N] y = y[:N] model = SVC(kernel='linear', C=1E10) model.fit(X, y) ax = ax or plt.gca() ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn') ax.set_xlim(-1, 4) ax.set_ylim(-1, 6) plot_svc_decision_function(model, ax) fig, ax = plt.subplots(1, 2, figsize=(16, 6)) fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1) for axi, N in zip(ax, [60, 120]): plot_svm(N, axi) axi.set_title('N = {0}'.format(N))Code language: Python (python) Example: Face Recognition As an example of support vector machines in action, let’s take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn: from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people(min_faces_per_person=60) print(faces.target_names) print(faces.images.shape)Code language: Python (python) ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair'] (1348, 62, 47) Let’s plot a few of these faces to see what we’re working with: fig, ax = plt.subplots(3, 5) for i, axi in enumerate(ax.flat): axi.imshow(faces.images[i], cmap='bone') axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]])Code language: Python (python) Each image contains [62×47] or nearly 3,000 pixels. We could proceed by merely using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here, we will use a principal component analysis to remove 150 fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline: from sklearn.svm import SVC from sklearn.decomposition import RandomizedPCA from sklearn.pipeline import make_pipeline pca = RandomizedPCA(n_components=150, whiten=True, random_state=42) svc = SVC(kernel='rbf', class_weight='balanced') model = make_pipeline(pca, svc)Code language: Python (python) For the sake of testing our classifier output, we will split the data into a training and testing set: from sklearn.cross_validation import train_test_split Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)Code language: Python (python) Finally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model: from sklearn.grid_search import GridSearchCV param_grid = {'svc__C': [1, 5, 10, 50], 'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]} grid = GridSearchCV(model, param_grid) %time grid.fit(Xtrain, ytrain) print(grid.best_params_)Code language: Python (python) CPU times: user 47.8 s, sys: 4.08 s, total: 51.8 s Wall time: 26 s {'svc__gamma': 0.001, 'svc__C': 10} The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum. Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen: model = grid.best_estimator_ yfit = model.predict(Xtest)Code language: Python (python) Let’s take a look at a few of the test images along with their predicted values: fig, ax = plt.subplots(4, 6) for i, axi in enumerate(ax.flat): axi.imshow(Xtest[i].reshape(62, 47), cmap='bone') axi.set(xticks=[], yticks=[]) axi.set_ylabel(faces.target_names[yfit[i]].split()[-1], color='black' if yfit[i] == ytest[i] else 'red') fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)Code language: Python (python) Out of this small sample, our optimal estimator mislabeled only a single face (Bush’s face in the bottom row was mislabeled as Blair). We can get a better sense of our estimator’s performance using the classification report, which lists recovery statistics label by label: from sklearn.metrics import classification_report print(classification_report(ytest, yfit, target_names=faces.target_names))Code language: Python (python) precision recall f1-score support Ariel Sharon 0.65 0.73 0.69 15 Colin Powell 0.81 0.87 0.84 68 Donald Rumsfeld 0.75 0.87 0.81 31 George W Bush 0.93 0.83 0.88 126 Gerhard Schroeder 0.86 0.78 0.82 23 Hugo Chavez 0.93 0.70 0.80 20 Junichiro Koizumi 0.80 1.00 0.89 12 Tony Blair 0.83 0.93 0.88 42 avg / total 0.85 0.85 0.85 337 We might also display the confusion matrix between these classes: from sklearn.metrics import confusion_matrix mat = confusion_matrix(ytest, yfit) sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=faces.target_names, yticklabels=faces.target_names) plt.xlabel('true label') plt.ylabel('predicted label')Code language: Python (python) With those traits in mind, I generally only turn to Support Vector Machines once another simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs. Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results. I hope you liked this article, feel free to ask questions on SVMs, or any other topic, in the comments section below. Follow Us: Facebook Instagram";Support Vector Machines (SVM) in Machine Learning
2020-07-08 17:06:40;Rotating, re-orienting, or stretching the piece of paper in three-dimensional space doesn’t change the flat geometry of the article: such operations are akin to linear embeddings. If you bend, curl, or crumple the paper, it is still a two-dimensional manifold, but the embedding into the three-dimensional space is no longer linear. Manifold learning algorithms would seek to learn about the fundamental two-dimensional nature of the paper, even as it is contorted to fill the three-dimensional space.Here I will demonstrate several different methods, going most deeply into a couple of techniques: multidimensional scaling (MDS), locally linear embedding (LLE), and isometric mapping (IsoMap).I will begin with the standard imports:To make these concepts more clear, let’s start by generating some two-dimensional data that we can use to define a manifold. Here is a function that will create data in the shape of the word “HELLO”:Let’s call the function and visualize the resulting data:The output is two dimensional, and consists of points drawn in the shape of the word, “HELLO”. This data form will help us to see visually what these algorithms are doing.Looking at data like this, we can see that the particular choice of x and y values of the dataset are not the most fundamental description of the data: we can scale, shrink, or rotate the data, and the “HELLO” will still be apparent. For example, if we use a rotation matrix to switch the data, the x and y values change, but the data is still fundamentally the same:This tells us that the x and y values are not necessarily fundamental to the relationships in the data. What is fundamental, in this case, is the distance between each point and the other points in the dataset. A common way to represent this is to use a distance matrix: for $N$ points, we construct an N \times N array such that entry (i, j) contains the distance between point i and point j. Let’s use Scikit-Learn’s efficient pairwise_distances function to do this for our original data:As promised, for our N=1,000 points, we obtain a 1000×1000 matrix, which can be visualised as shown here:This distance matrix gives us a representation of our data that is invariant to rotations and translations, but the visualisation of the matrix above is not entirely intuitive. In the image shown in this figure, we have lost any visible sign of the impressive structure in the data: the “HELLO” that we saw before.Further, while computing this distance matrix from the (x, y) coordinates is straightforward, transforming the distances back into x and y coordinates is somewhat tricky. This is precisely what the multidimensional scaling algorithm aims to do: given a distance matrix between points, it recovers a $D$-dimensional coordinate representation of the data. Let’s see how it works for our distance matrix, using the precomputed dissimilarity to specify that we are passing a distance matrix:The MDS algorithm recovers one of the possible two-dimensional coordinate representations of our data, using only the N\times N distance matrix describing the relationship between the data points.The usefulness of this becomes more apparent when we consider the fact that distance matrices can be computed from data in any dimension. So, for example, instead of simply rotating the data in the two-dimensional plane, we can project it into three dimensions using the following function:Let’s visualize these points to see what we’re working with:We can now ask the MDS estimator to input this three-dimensional data, compute the distance matrix and then determine the optimal two-dimensional embedding for this distance matrix. The result recovers a representation of the original data:This is essentially the goal of a manifold learning estimator: given high-dimensional embedded data, it seeks a low-dimensional representation of the data that preserves individual relationships within the data. In the case of MDS, the quantity protected is the distance between every pair of points.One place manifold learning is often used is in understanding the relationship between high-dimensional data points. A typical case of high-dimensional data is images: for example, a set of pictures with 1,000 pixels each can be thought of as a collection of points in 1,000 dimensions – the brightness of each pixel in each image defines the coordinate in that dimension.Here let’s apply Isomap on some faces data. I will use the Labeled Faces in the Wild dataset provided by scikit-learn.We have 2,370 images, each with 2,914 pixels. In other words, the images can be thought of as data points in a 2,914-dimensional space.Let’s quickly visualize several of these images to see what we’re working with:I would like to plot a low-dimensional embedding of the 2,914-dimensional data to learn the fundamental relationships between the images. One useful way to start is to compute a PCA, and examine the explained variance ratio, which will give us an idea of how many linear features are required to describe the data:We see that for this data, nearly 100 components are required to preserve 90% of the variance: this tells us that the data is intrinsically very high dimensional—it can’t be described linearly with just a few components.When this is the case, nonlinear manifold embeddings like LLE and Isomap can be helpful. We can compute an Isomap embedding on these faces using the same pattern shown before:The output is a two-dimensional projection of all the input images. To get a better idea of what the forecast tells us, let’s define a function that will output image thumbnails at the locations of the projections:Calling this function now, we see the result:The result is impressive: the first two Isomap dimensions seem to describe global image features: the overall darkness or lightness of the image from left to right, and the general orientation of the face from bottom to top. This gives us an excellent visual indication of some of the fundamental features in our data.I hope you liked this article on manifold learning in machine learning. Feel free to ask questions in the comments section below on any topic you want.;https://thecleverprogrammer.com/2020/07/08/manifold-learning/;['pattern', 'sklearn'];1.0;[];['ML'];['fit', 'label', 'model', 'machine learning'];"Rotating, re-orienting, or stretching the piece of paper in three-dimensional space doesn’t change the flat geometry of the article: such operations are akin to linear embeddings. If you bend, curl, or crumple the paper, it is still a two-dimensional manifold, but the embedding into the three-dimensional space is no longer linear. Manifold learning algorithms would seek to learn about the fundamental two-dimensional nature of the paper, even as it is contorted to fill the three-dimensional space. Here I will demonstrate several different methods, going most deeply into a couple of techniques: multidimensional scaling (MDS), locally linear embedding (LLE), and isometric mapping (IsoMap). I will begin with the standard imports: import matplotlib.pyplot as plt import seaborn as sns; sns.set() import numpy as npCode language: Python (python) Manifold Learning: “HELLO” To make these concepts more clear, let’s start by generating some two-dimensional data that we can use to define a manifold. Here is a function that will create data in the shape of the word “HELLO”: def make_hello(N=1000, rseed=42): # Make a plot with ""HELLO"" text; save as PNG fig, ax = plt.subplots(figsize=(4, 1)) fig.subplots_adjust(left=0, right=1, bottom=0, top=1) ax.axis('off') ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold', size=85) fig.savefig('hello.png') plt.close(fig) # Open this PNG and draw random points from it from matplotlib.image import imread data = imread('hello.png')[::-1, :, 0].T rng = np.random.RandomState(rseed) X = rng.rand(4 * N, 2) i, j = (X * data.shape).astype(int).T mask = (data[i, j] &lt; 1) X = X[mask] X[:, 0] *= (data.shape[0] / data.shape[1]) X = X[:N] return X[np.argsort(X[:, 0])]Code language: Python (python) Let’s call the function and visualize the resulting data: X = make_hello(1000) colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5)) plt.scatter(X[:, 0], X[:, 1], **colorize) plt.axis('equal')Code language: Python (python) The output is two dimensional, and consists of points drawn in the shape of the word, “HELLO”. This data form will help us to see visually what these algorithms are doing. Multidimensional Scaling (MDS) Looking at data like this, we can see that the particular choice of x and y values of the dataset are not the most fundamental description of the data: we can scale, shrink, or rotate the data, and the “HELLO” will still be apparent. For example, if we use a rotation matrix to switch the data, the x and y values change, but the data is still fundamentally the same: def rotate(X, angle): theta = np.deg2rad(angle) R = [[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]] return np.dot(X, R) X2 = rotate(X, 20) + 5 plt.scatter(X2[:, 0], X2[:, 1], **colorize) plt.axis('equal')Code language: Python (python) This tells us that the x and y values are not necessarily fundamental to the relationships in the data. What is fundamental, in this case, is the distance between each point and the other points in the dataset. A common way to represent this is to use a distance matrix: for $N$ points, we construct an N \times N array such that entry (i, j) contains the distance between point i and point j. Let’s use Scikit-Learn’s efficient pairwise_distances function to do this for our original data: from sklearn.metrics import pairwise_distances D = pairwise_distances(X) D.shapeCode language: Python (python) (1000, 1000) As promised, for our N=1,000 points, we obtain a 1000×1000 matrix, which can be visualised as shown here: plt.imshow(D, zorder=2, cmap='Blues', interpolation='nearest') plt.colorbar()Code language: Python (python) This distance matrix gives us a representation of our data that is invariant to rotations and translations, but the visualisation of the matrix above is not entirely intuitive. In the image shown in this figure, we have lost any visible sign of the impressive structure in the data: the “HELLO” that we saw before. Further, while computing this distance matrix from the (x, y) coordinates is straightforward, transforming the distances back into x and y coordinates is somewhat tricky. This is precisely what the multidimensional scaling algorithm aims to do: given a distance matrix between points, it recovers a $D$-dimensional coordinate representation of the data. Let’s see how it works for our distance matrix, using the precomputed dissimilarity to specify that we are passing a distance matrix: from sklearn.manifold import MDS model = MDS(n_components=2, dissimilarity='precomputed', random_state=1) out = model.fit_transform(D) plt.scatter(out[:, 0], out[:, 1], **colorize) plt.axis('equal')Code language: Python (python) The MDS algorithm recovers one of the possible two-dimensional coordinate representations of our data, using only the N\times N distance matrix describing the relationship between the data points. MDS as Manifold Learning The usefulness of this becomes more apparent when we consider the fact that distance matrices can be computed from data in any dimension. So, for example, instead of simply rotating the data in the two-dimensional plane, we can project it into three dimensions using the following function: def random_projection(X, dimension=3, rseed=42): assert dimension &gt;= X.shape[1] rng = np.random.RandomState(rseed) C = rng.randn(dimension, dimension) e, V = np.linalg.eigh(np.dot(C, C.T)) return np.dot(X, V[:X.shape[1]]) X3 = random_projection(X, 3) X3.shapeCode language: Python (python) (1000, 3) Let’s visualize these points to see what we’re working with: from mpl_toolkits import mplot3d ax = plt.axes(projection='3d') ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2], **colorize) ax.view_init(azim=70, elev=50)Code language: Python (python) We can now ask the MDS estimator to input this three-dimensional data, compute the distance matrix and then determine the optimal two-dimensional embedding for this distance matrix. The result recovers a representation of the original data: model = MDS(n_components=2, random_state=1) out3 = model.fit_transform(X3) plt.scatter(out3[:, 0], out3[:, 1], **colorize) plt.axis('equal')Code language: Python (python) This is essentially the goal of a manifold learning estimator: given high-dimensional embedded data, it seeks a low-dimensional representation of the data that preserves individual relationships within the data. In the case of MDS, the quantity protected is the distance between every pair of points. Manifold Learning Example: Isomap on Faces One place manifold learning is often used is in understanding the relationship between high-dimensional data points. A typical case of high-dimensional data is images: for example, a set of pictures with 1,000 pixels each can be thought of as a collection of points in 1,000 dimensions – the brightness of each pixel in each image defines the coordinate in that dimension. Here let’s apply Isomap on some faces data. I will use the Labeled Faces in the Wild dataset provided by scikit-learn. from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people(min_faces_per_person=30) faces.data.shapeCode language: Python (python) (2370, 2914) We have 2,370 images, each with 2,914 pixels. In other words, the images can be thought of as data points in a 2,914-dimensional space. Let’s quickly visualize several of these images to see what we’re working with: fig, ax = plt.subplots(4, 8, subplot_kw=dict(xticks=[], yticks=[])) for i, axi in enumerate(ax.flat): axi.imshow(faces.images[i], cmap='gray')Code language: Python (python) I would like to plot a low-dimensional embedding of the 2,914-dimensional data to learn the fundamental relationships between the images. One useful way to start is to compute a PCA, and examine the explained variance ratio, which will give us an idea of how many linear features are required to describe the data: from sklearn.decomposition import RandomizedPCA model = RandomizedPCA(100).fit(faces.data) plt.plot(np.cumsum(model.explained_variance_ratio_)) plt.xlabel('n components') plt.ylabel('cumulative variance')Code language: Python (python) We see that for this data, nearly 100 components are required to preserve 90% of the variance: this tells us that the data is intrinsically very high dimensional—it can’t be described linearly with just a few components. When this is the case, nonlinear manifold embeddings like LLE and Isomap can be helpful. We can compute an Isomap embedding on these faces using the same pattern shown before: from sklearn.manifold import Isomap model = Isomap(n_components=2) proj = model.fit_transform(faces.data) proj.shapeCode language: Python (python) (2370, 2) The output is a two-dimensional projection of all the input images. To get a better idea of what the forecast tells us, let’s define a function that will output image thumbnails at the locations of the projections: from matplotlib import offsetbox def plot_components(data, model, images=None, ax=None, thumb_frac=0.05, cmap='gray'): ax = ax or plt.gca() proj = model.fit_transform(data) ax.plot(proj[:, 0], proj[:, 1], '.k') if images is not None: min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2 shown_images = np.array([2 * proj.max(0)]) for i in range(data.shape[0]): dist = np.sum((proj[i] - shown_images) ** 2, 1) if np.min(dist) &lt; min_dist_2: # don't show points that are too close continue shown_images = np.vstack([shown_images, proj[i]]) imagebox = offsetbox.AnnotationBbox( offsetbox.OffsetImage(images[i], cmap=cmap), proj[i]) ax.add_artist(imagebox)Code language: Python (python) Calling this function now, we see the result: fig, ax = plt.subplots(figsize=(10, 10)) plot_components(faces.data, model=Isomap(n_components=2), images=faces.images[:, ::2, ::2]) The result is impressive: the first two Isomap dimensions seem to describe global image features: the overall darkness or lightness of the image from left to right, and the general orientation of the face from bottom to top. This gives us an excellent visual indication of some of the fundamental features in our data. I hope you liked this article on manifold learning in machine learning. Feel free to ask questions in the comments section below on any topic you want. Follow Us: Facebook Instagram";Manifold Learning
2020-07-08 00:31:00;In this article, you will explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.After a brief conceptual discussion of the PCA algorithm, we will see a couple examples of these further applications. I will start with the standard imports:Principal component analysis (PCA) is a fast and flexible unsupervised method for dimensionality reduction in data. Its behavior is easiest to visualize by looking at a two-dimensional dataset. Consider the following 200 points:By eye, it is clear that there is a nearly linear relationship between the x and y variables. But the problem here states: rather than attempting to predict the y values from the x values, the unsupervised learning problem attempts to learn about the relationship between the x and y values.In principal component analysis (PCA), this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn’s PCA estimator, we can compute this as follows:The fit learns some quantities from the data, most importantly the “components” and “explained variance”:To see what these numbers mean, let’s visualize them as vectors over the input data, using the “components” to define the direction of the vector, and the “explained variance” to define the squared-length of the vector:These vectors represent the principal axes of the data, and the length of the vector is an indication of how “important” that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the “principal components” of the data.Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.Here is an example of using PCA as a dimensionality reduction transform:The transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:The light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much “information” is discarded in this reduction of dimensionality.The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data. To see this, let’s take a quick look at the application of PCA to the digits data. I will start by loading the data:To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:We can now plot the first two principal components of each point to learn about the data:the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance. Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised manner—that is, without reference to the labels.A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data. This can be determined by looking at the cumulative explained variance ratio as a function of the number of components:Here we will take a look and explore the Labeled Faces in the Wild dataset made available through Scikit-Learn:Let’s take a look at the principal axes that span this dataset. Because this is a large dataset, we will use RandomizedPCA—it contains a randomized method to approximate the first N principal components much more quickly than the standard PCA estimator, and thus is very useful for high-dimensional data (here, a dimensionality of nearly 3,000). We will take a look at the first 150 components:In this case, it can be interesting to visualize the images associated with the first several principal components:The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips. Let’s take a look at the cumulative variance of these components to see how much of the data information the projection is preserving:We see that these 150 components account for just over 90% of the variance. That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data. To make this more concrete, we can compare the input images with the images reconstructed from these 150 components:The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features. This visualization makes clear why the PCA feature selection used in was so successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image. What this means is that our classification algorithm needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which depending on the particular algorithm we choose, can lead to a much more efficient classification.;https://thecleverprogrammer.com/2020/07/08/pca-in-machine-learning/;['sklearn'];1.0;[];['Supervised Learning', 'Classification', 'Unsupervised learning'];['recogn', 'predict', 'fit', 'supervised learning', 'classif', 'filter', 'train', 'unsupervised learning', 'label'];"In this article, you will explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more. After a brief conceptual discussion of the PCA algorithm, we will see a couple examples of these further applications. I will start with the standard imports: %matplotlib inline import numpy as np import matplotlib.pyplot as plt import seaborn as sns; sns.set()Code language: Python (python) Introducing Principal Component Analysis (PCA) Principal component analysis (PCA) is a fast and flexible unsupervised method for dimensionality reduction in data. Its behavior is easiest to visualize by looking at a two-dimensional dataset. Consider the following 200 points: rng = np.random.RandomState(1) X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T plt.scatter(X[:, 0], X[:, 1]) plt.axis('equal')Code language: Python (python) By eye, it is clear that there is a nearly linear relationship between the x and y variables. But the problem here states: rather than attempting to predict the y values from the x values, the unsupervised learning problem attempts to learn about the relationship between the x and y values. In principal component analysis (PCA), this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn’s PCA estimator, we can compute this as follows: from sklearn.decomposition import PCA pca = PCA(n_components=2) pca.fit(X)Code language: Python (python) PCA(copy=True, n_components=2, whiten=False) The fit learns some quantities from the data, most importantly the “components” and “explained variance”: print(pca.components_)Code language: Python (python) [[ 0.94446029 0.32862557] [ 0.32862557 -0.94446029]] print(pca.explained_variance_)Code language: Python (python) [ 0.75871884 0.01838551] To see what these numbers mean, let’s visualize them as vectors over the input data, using the “components” to define the direction of the vector, and the “explained variance” to define the squared-length of the vector: def draw_vector(v0, v1, ax=None): ax = ax or plt.gca() arrowprops=dict(arrowstyle='-&gt;', linewidth=2, shrinkA=0, shrinkB=0) ax.annotate('', v1, v0, arrowprops=arrowprops) # plot data plt.scatter(X[:, 0], X[:, 1], alpha=0.2) for length, vector in zip(pca.explained_variance_, pca.components_): v = vector * 3 * np.sqrt(length) draw_vector(pca.mean_, pca.mean_ + v) plt.axis('equal')Code language: Python (python) These vectors represent the principal axes of the data, and the length of the vector is an indication of how “important” that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the “principal components” of the data. PCA as Dimensionality Reduction Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance. Here is an example of using PCA as a dimensionality reduction transform: pca = PCA(n_components=1) pca.fit(X) X_pca = pca.transform(X) print(""original shape: "", X.shape) print(""transformed shape:"", X_pca.shape)Code language: Python (python) original shape: (200, 2) transformed shape: (200, 1) The transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data: X_new = pca.inverse_transform(X_pca) plt.scatter(X[:, 0], X[:, 1], alpha=0.2) plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8) plt.axis('equal')Code language: Python (python) The light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much “information” is discarded in this reduction of dimensionality. PCA for visualization: Hand-written digits The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data. To see this, let’s take a quick look at the application of PCA to the digits data. I will start by loading the data: from sklearn.datasets import load_digits digits = load_digits() digits.data.shapeCode language: Python (python) To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two: pca = PCA(2) # project from 64 to 2 dimensions projected = pca.fit_transform(digits.data) print(digits.data.shape) print(projected.shape)Code language: Python (python) (1797, 64) (1797, 2) We can now plot the first two principal components of each point to learn about the data: plt.scatter(projected[:, 0], projected[:, 1], c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('spectral', 10)) plt.xlabel('component 1') plt.ylabel('component 2') plt.colorbar()Code language: Python (python) the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance. Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised manner—that is, without reference to the labels. Choosing the Number of Components A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data. This can be determined by looking at the cumulative explained variance ratio as a function of the number of components: pca = PCA().fit(digits.data) plt.plot(np.cumsum(pca.explained_variance_ratio_)) plt.xlabel('number of components') plt.ylabel('cumulative explained variance')Code language: Python (python) Principal Component Analysis Example: Eigenfaces Here we will take a look and explore the Labeled Faces in the Wild dataset made available through Scikit-Learn: from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people(min_faces_per_person=60) print(faces.target_names) print(faces.images.shape)Code language: Python (python) ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair'] (1348, 62, 47) Let’s take a look at the principal axes that span this dataset. Because this is a large dataset, we will use RandomizedPCA—it contains a randomized method to approximate the first N principal components much more quickly than the standard PCA estimator, and thus is very useful for high-dimensional data (here, a dimensionality of nearly 3,000). We will take a look at the first 150 components: from sklearn.decomposition import RandomizedPCA pca = RandomizedPCA(150) pca.fit(faces.data)Code language: Python (python) RandomizedPCA(copy=True, iterated_power=3, n_components=150, random_state=None, whiten=False) In this case, it can be interesting to visualize the images associated with the first several principal components: fig, axes = plt.subplots(3, 8, figsize=(9, 4), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1)) for i, ax in enumerate(axes.flat): ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')Code language: Python (python) The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips. Let’s take a look at the cumulative variance of these components to see how much of the data information the projection is preserving: plt.plot(np.cumsum(pca.explained_variance_ratio_)) plt.xlabel('number of components') plt.ylabel('cumulative explained variance')Code language: Python (python) We see that these 150 components account for just over 90% of the variance. That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data. To make this more concrete, we can compare the input images with the images reconstructed from these 150 components: pca = RandomizedPCA(150).fit(faces.data) components = pca.transform(faces.data) projected = pca.inverse_transform(components) fig, ax = plt.subplots(2, 10, figsize=(10, 2.5), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1)) for i in range(10): ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r') ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r') ax[0, 0].set_ylabel('full-dim\ninput') ax[1, 0].set_ylabel('150-dim\nreconstruction')Code language: Python (python) The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features. This visualization makes clear why the PCA feature selection used in was so successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image. What this means is that our classification algorithm needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which depending on the particular algorithm we choose, can lead to a much more efficient classification. Follow Us: Facebook Instagram";PCA in Machine Learning
2020-07-09 12:11:11;"One week ago, Dr Cohen started collecting X-ray images of COVID 19 cases and publishing them in the following GitHub repo, for the work of COVID 19 detection.Inside the repo you’ll find example of COVID-19 cases, as well as MERS, SARS, and ARDS.In order to create the COVID 19 X-ray image dataset for this Article, I:In total, that left us with 25 X-ray images of positive COVID-19 cases(figure 2 left above).The next step was to sample X-ray images of healthy patients.To do so, I used Chest X-Ray Images (Pneumonia) dataset and sampled 25 X-ray images from healthy patients. There are a number of problems with  Chest X-Ray dataset, namely noisy/incorrect labels, but it served as a good enough starting point for this proof of concept COVID-19 detector.After gathering our dataset, we are left with 50 total images, equally split with 25 images of COVID-19 positive X-rays and 25 images of healthy patient X-rays.Lets start with importing the librariesTo load our data, we grab all paths to images in the dataset_dir directory . Then, for each imagePath, we:We then scale pixel intensities to the range [0, 1] and convert both our data and labels to NumPy array format.One-hot encoding of labels takes place meaning that our data will be in the following format: [[0. 1.] [0. 1.] [0. 1.] … [1. 0.] [1. 0.] [1. 0.]]Each encoded label consists of a two element array with one of the elements being “hot” (i.e., 1) versus “not” (i.e., 0).Then construct our data split, reserving 80% of the data for training and 20% for testing.In order to ensure that our model generalizes, we perform data augmentation by setting the random image rotation setting to 15 degrees clockwise or counterclockwise. We will Initialize the data augmentation generator object.We will instantiate the VGG16 network with weights pre-trained on ImageNet, leaving off the FC layer head.From there, we construct a new fully-connected layer head consisting of POOL => FC = SOFTMAX layers and append it on top of VGG16.We then freeze the CONV weights of VGG16 such that only the FC layer head will be trained; this completes our fine-tuning setup.compile the network with learning rate decay and the Adam optimizer. Given that this is a 2-class problem, we use “binary_crossentropy” loss rather than categorical crossentropy.To kick off our COVID-19 neural network training process, we make a call to Keras’ fit_generator method, while passing in our chest X-ray data via our data augmentation object.For evaluation, we first make predictions on the testing set and grab the prediction indices.We then generate and print out a classification report using scikit-learn’s helper utility.";https://thecleverprogrammer.com/2020/07/09/covid-19-detection/;['keras', 'sklearn', 'tensorflow', 'imutils'];1.0;[];['NN', 'DL', 'CNN', 'ReLu', 'VGG', 'Classification'];['detect', 'epoch', 'predict', 'fit', 'model', 'loss', 'neural network', 'relu', 'classif', 'layer', 'deep learning', 'ground truth', 'training data', 'convolutional neural network', 'train', 'label', 'vgg'];"One week ago, Dr Cohen started collecting X-ray images of COVID 19 cases and publishing them in the following GitHub repo, for the work of COVID 19 detection. Inside the repo you’ll find example of COVID-19 cases, as well as MERS, SARS, and ARDS. In order to create the COVID 19 X-ray image dataset for this Article, I: Parsed the metadata.csv file found in Dr. Cohen’s repository.Selected all rows that are:Positive for COVID-19 (i.e., ignoring MERS, SARS, and ARDS cases).Posterioranterior (PA) view of the lungs. I used the PA view as, to my knowledge, that was the view used for my “healthy” cases, as discussed below; however, I’m sure that a medical professional will be able clarify and correct me if I am incorrect (which I very well may be, this is just an example). In total, that left us with 25 X-ray images of positive COVID-19 cases(figure 2 left above). The next step was to sample X-ray images of healthy patients. To do so, I used Chest X-Ray Images (Pneumonia) dataset and sampled 25 X-ray images from healthy patients. There are a number of problems with Chest X-Ray dataset, namely noisy/incorrect labels, but it served as a good enough starting point for this proof of concept COVID-19 detector. After gathering our dataset, we are left with 50 total images, equally split with 25 images of COVID-19 positive X-rays and 25 images of healthy patient X-rays. Lets start with importing the libraries import numpy as np # linear algebra import pandas as pd # data processing from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.applications import VGG16 from tensorflow.keras.layers import AveragePooling2D from tensorflow.keras.layers import Dropout from tensorflow.keras.layers import Flatten from tensorflow.keras.layers import Dense from tensorflow.keras.layers import Input from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam from tensorflow.keras.utils import to_categorical from sklearn.preprocessing import LabelBinarizer from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from imutils import paths import matplotlib.pyplot as plt import numpy as np import cv2Code language: Python (python) # initialize the initial learning rate, number of epochs to train for, # and batch size INIT_LR = 1e-3 EPOCHS = 25 BS = 8 #The path to our input dataset of chest X-ray images. dataset_dir = # your path to data set plot_path = # your path to plot.png' model_path = # your path to covid19.model'Code language: Python (python) Load and preprocess our X-ray data: To load our data, we grab all paths to images in the dataset_dir directory . Then, for each imagePath, we: Extract the class label (either covid or normal) from the pathLoad the image, and preprocess it by converting to RGB channel ordering, and resizing it to 224×224 pixels so that it is ready for our Convolutional Neural Network.Update our data and labels lists respectively (Lines 58 and 59). We then scale pixel intensities to the range [0, 1] and convert both our data and labels to NumPy array format. # grab the list of images in our dataset directory, then initialize # the list of data (i.e., images) and class images print(""[INFO] loading images..."") imagePaths = list(paths.list_images(dataset_dir)) data = [] labels = [] # loop over the image paths for imagePath in imagePaths: # extract the class label from the filename label = imagePath.split(os.path.sep)[-2] # load the image, swap color channels, and resize it to be a fixed # 224x224 pixels while ignoring aspect ratio image = cv2.imread(imagePath) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) image = cv2.resize(image, (224, 224)) # update the data and labels lists, respectively data.append(image) labels.append(label) data1 = data.copy() labels1 = labels.copy() # convert the data and labels to NumPy arrays while scaling the pixel # intensities to the range [0, 255] data = np.array(data) / 255.0 labels = np.array(labels)Code language: Python (python) Next we will one-hot encode our labels and create our training/testing splits: One-hot encoding of labels takes place meaning that our data will be in the following format: [[0. 1.] [0. 1.] [0. 1.] … [1. 0.] [1. 0.] [1. 0.]] Each encoded label consists of a two element array with one of the elements being “hot” (i.e., 1) versus “not” (i.e., 0). Then construct our data split, reserving 80% of the data for training and 20% for testing. In order to ensure that our model generalizes, we perform data augmentation by setting the random image rotation setting to 15 degrees clockwise or counterclockwise. We will Initialize the data augmentation generator object. # perform one-hot encoding on the labels lb = LabelBinarizer() labels = lb.fit_transform(labels) labels = to_categorical(labels) print(labels) # partition the data into training and testing splits using 80% of # the data for training and the remaining 20% for testing (trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.20, stratify=labels, random_state=42) # initialize the training data augmentation object trainAug = ImageDataGenerator(rotation_range=15, fill_mode=""nearest"")Code language: Python (python) [[1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [1. 0.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.] [0. 1.]] We will initialize our VGGNet model and set it up for fine-tuning: We will instantiate the VGG16 network with weights pre-trained on ImageNet, leaving off the FC layer head. From there, we construct a new fully-connected layer head consisting of POOL => FC = SOFTMAX layers and append it on top of VGG16. We then freeze the CONV weights of VGG16 such that only the FC layer head will be trained; this completes our fine-tuning setup. # load the VGG16 network, ensuring the head FC layer sets are left # off baseModel = VGG16(weights=""imagenet"", include_top=False, input_tensor=Input(shape=(224, 224, 3))) # construct the head of the model that will be placed on top of the # the base model headModel = baseModel.output headModel = AveragePooling2D(pool_size=(4, 4))(headModel) headModel = Flatten(name=""flatten"")(headModel) headModel = Dense(64, activation=""relu"")(headModel) headModel = Dropout(0.5)(headModel) headModel = Dense(2, activation=""softmax"")(headModel) # place the head FC model on top of the base model (this will become # the actual model we will train) model = Model(inputs=baseModel.input, outputs=headModel) # loop over all layers in the base model and freeze them so they will # *not* be updated during the first training process for layer in baseModel.layers: layer.trainable = FalseCode language: Python (python) Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 58892288/58889256 [==============================] - 1s 0us/step Compile and train our COVID 19 Detection (coronavirus) deep learning model: compile the network with learning rate decay and the Adam optimizer. Given that this is a 2-class problem, we use “binary_crossentropy” loss rather than categorical crossentropy. To kick off our COVID-19 neural network training process, we make a call to Keras’ fit_generator method, while passing in our chest X-ray data via our data augmentation object. # compile our model print(""[INFO] compiling model..."") opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS) model.compile(loss=""binary_crossentropy"", optimizer=opt, metrics=[""accuracy""]) # train the head of the network print(""[INFO] training head..."") H = model.fit_generator( trainAug.flow(trainX, trainY, batch_size=BS), steps_per_epoch=len(trainX) // BS, validation_data=(testX, testY), validation_steps=len(testX) // BS, epochs=EPOCHS)Code language: Python (python) [INFO] compiling model... [INFO] training head... Train for 5 steps, validate on 10 samples Epoch 1/25 5/5 [==============================] - 5s 1s/step - loss: 0.6084 - accuracy: 0.7000 - val_loss: 0.4942 - val_accuracy: 1.0000 Epoch 2/25 5/5 [==============================] - 1s 101ms/step - loss: 0.7425 - accuracy: 0.5250 - val_loss: 0.4818 - val_accuracy: 1.0000 Epoch 3/25 5/5 [==============================] - 1s 107ms/step - loss: 0.6952 - accuracy: 0.5750 - val_loss: 0.4690 - val_accuracy: 1.0000 Epoch 4/25 5/5 [==============================] - 1s 138ms/step - loss: 0.7249 - accuracy: 0.5750 - val_loss: 0.4592 - val_accuracy: 1.0000 Epoch 5/25 5/5 [==============================] - 0s 94ms/step - loss: 0.6418 - accuracy: 0.6250 - val_loss: 0.4466 - val_accuracy: 0.8750 Epoch 6/25 5/5 [==============================] - 0s 93ms/step - loss: 0.6240 - accuracy: 0.6000 - val_loss: 0.4345 - val_accuracy: 0.8750 Epoch 7/25 5/5 [==============================] - 1s 105ms/step - loss: 0.5315 - accuracy: 0.7500 - val_loss: 0.4241 - val_accuracy: 1.0000 Epoch 8/25 5/5 [==============================] - 0s 93ms/step - loss: 0.5350 - accuracy: 0.7000 - val_loss: 0.4102 - val_accuracy: 1.0000 Epoch 9/25 5/5 [==============================] - 0s 98ms/step - loss: 0.5137 - accuracy: 0.8500 - val_loss: 0.4014 - val_accuracy: 0.8750 Epoch 10/25 5/5 [==============================] - 0s 94ms/step - loss: 0.5534 - accuracy: 0.7750 - val_loss: 0.3911 - val_accuracy: 0.8750 Epoch 11/25 5/5 [==============================] - 1s 106ms/step - loss: 0.5366 - accuracy: 0.7500 - val_loss: 0.3815 - val_accuracy: 0.8750 Epoch 12/25 5/5 [==============================] - 0s 97ms/step - loss: 0.4864 - accuracy: 0.8500 - val_loss: 0.3700 - val_accuracy: 0.8750 Epoch 13/25 5/5 [==============================] - 1s 111ms/step - loss: 0.4442 - accuracy: 0.9000 - val_loss: 0.3599 - val_accuracy: 0.8750 Epoch 14/25 5/5 [==============================] - 0s 95ms/step - loss: 0.4352 - accuracy: 0.9250 - val_loss: 0.3496 - val_accuracy: 0.8750 Epoch 15/25 5/5 [==============================] - 0s 99ms/step - loss: 0.4133 - accuracy: 0.8750 - val_loss: 0.3415 - val_accuracy: 0.8750 Epoch 16/25 5/5 [==============================] - 0s 97ms/step - loss: 0.4111 - accuracy: 0.8750 - val_loss: 0.3324 - val_accuracy: 0.8750 Epoch 17/25 5/5 [==============================] - 0s 97ms/step - loss: 0.3939 - accuracy: 0.9250 - val_loss: 0.3233 - val_accuracy: 0.8750 Epoch 18/25 5/5 [==============================] - 0s 95ms/step - loss: 0.3572 - accuracy: 0.9500 - val_loss: 0.3174 - val_accuracy: 0.8750 Epoch 19/25 5/5 [==============================] - 0s 94ms/step - loss: 0.3422 - accuracy: 0.9000 - val_loss: 0.3093 - val_accuracy: 0.8750 Epoch 20/25 5/5 [==============================] - 0s 95ms/step - loss: 0.3999 - accuracy: 0.8750 - val_loss: 0.3050 - val_accuracy: 0.8750 Epoch 21/25 5/5 [==============================] - 0s 94ms/step - loss: 0.3458 - accuracy: 0.9000 - val_loss: 0.3001 - val_accuracy: 0.8750 Epoch 22/25 5/5 [==============================] - 0s 94ms/step - loss: 0.2851 - accuracy: 1.0000 - val_loss: 0.2968 - val_accuracy: 0.8750 Epoch 23/25 5/5 [==============================] - 0s 96ms/step - loss: 0.3796 - accuracy: 0.8750 - val_loss: 0.2887 - val_accuracy: 0.8750 Epoch 24/25 5/5 [==============================] - 1s 108ms/step - loss: 0.2685 - accuracy: 0.9500 - val_loss: 0.2815 - val_accuracy: 0.8750 Epoch 25/25 5/5 [==============================] - 0s 95ms/step - loss: 0.2811 - accuracy: 0.9250 - val_loss: 0.2808 - val_accuracy: 0.8750 Evaluate the COVID 19 Detection model: For evaluation, we first make predictions on the testing set and grab the prediction indices. We then generate and print out a classification report using scikit-learn’s helper utility. # make predictions on the testing set print(""[INFO] evaluating network..."") predIdxs = model.predict(testX, batch_size=BS) # for each image in the testing set we need to find the index of the # label with corresponding largest predicted probability predIdxs = np.argmax(predIdxs, axis=1) # show a nicely formatted classification report print(classification_report(testY.argmax(axis=1), predIdxs, target_names=lb.classes_))Code language: Python (python) [INFO] evaluating network... precision recall f1-score support covid 1.00 0.80 0.89 5 normal 0.83 1.00 0.91 5 accuracy 0.90 10 macro avg 0.92 0.90 0.90 10 weighted avg 0.92 0.90 0.90 10 Plot the predictions rows = 3 columns = 3 fig = plt.figure(figsize=(20, 20)) for m in range(1, 10): if str(predIdxs[m-1]) == ""0"": text = ""NORMAL"" color = (0, 255, 0) elif str(predIdxs[m-1]) == ""1"": text = ""COVID"" color = (255, 0, 0) img = testX[m-1].copy() # Window name in which image is displayed window_name = text # font font = cv2.FONT_HERSHEY_SIMPLEX # org org = (50, 50) # fontScale fontScale = 1 # Line thickness of 2 px thickness = 2 img = cv2.putText(img, text, org, font, fontScale, color, thickness, cv2.LINE_AA) fig.add_subplot(rows, columns, m) plt.imshow(img) plt.title(""Pred: "" + text) plt.axis('off') plt.show()Code language: Python (python) Plot the Ground Truths rows = 3 columns = 3 fig = plt.figure(figsize=(20, 20)) for m in range(1, 10): if str(testY.argmax(axis=1)[m-1]) == ""0"": text = ""NORMAL"" color = (0, 255, 0) elif str(testY.argmax(axis=1)[m-1]) == ""1"": text = ""COVID"" color = (255, 0, 0) img = testX[m-1].copy() # Window name in which image is displayed window_name = text # font font = cv2.FONT_HERSHEY_SIMPLEX # org org = (50, 50) # fontScale fontScale = 1 # Line thickness of 2 px thickness = 2 img = cv2.putText(img, text, org, font, fontScale, color, thickness, cv2.LINE_AA) fig.add_subplot(rows, columns, m) plt.imshow(img) plt.title(""Ground Truth: "" + text) plt.axis('off') plt.show()Code language: Python (python) Compute a confusion matrix for further statistical evaluation: Here we will: Generate a confusion matrixUse the confusion matrix to derive the accuracy, sensitivity, and specificity and print each of these metrics # compute the confusion matrix and and use it to derive the raw # accuracy, sensitivity, and specificity cm = confusion_matrix(testY.argmax(axis=1), predIdxs) total = sum(sum(cm)) acc = (cm[0, 0] + cm[1, 1]) / total sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1]) specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1]) # show the confusion matrix, accuracy, sensitivity, and specificity print(cm) print(""acc: {:.4f}"".format(acc)) print(""sensitivity: {:.4f}"".format(sensitivity)) print(""specificity: {:.4f}"".format(specificity))Code language: Python (python) [[4 1] [0 5]] acc: 0.9000 sensitivity: 0.8000 specificity: 1.0000 Plot our training for COVID 19 Detection We plot our training accuracy/loss history for inspection, outputting the plot to an image file. Finally, we serialize our tf.keras COVID-19 classifier model to disk: # plot the training loss and accuracy N = EPOCHS plt.style.use(""ggplot"") plt.figure() plt.plot(np.arange(0, N), H.history[""loss""], label=""train_loss"") plt.plot(np.arange(0, N), H.history[""val_loss""], label=""val_loss"") plt.plot(np.arange(0, N), H.history[""accuracy""], label=""train_acc"") plt.plot(np.arange(0, N), H.history[""val_accuracy""], label=""val_acc"") plt.title(""Training Loss and Accuracy on COVID-19 Dataset"") plt.xlabel(""Epoch #"") plt.ylabel(""Loss/Accuracy"") plt.legend(loc=""lower left"") plt.savefig(plot_path) # serialize the model to disk print(""[INFO] saving COVID-19 detector model..."") model.save(model_path, save_format=""h5"")Code language: Python (python) I hope you liked this article on COVID 19 Detection using Deep Learning. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";COVID 19 Detection
2020-07-11 11:57:23;Many Business activities are seasonal in nature, where most of the business are dependent on a particular time of festival and holidays. Every business uses sales promotion techniques to increase the demand for their products and services, in order to stay in the market for a longer period. In this article, I am going to do sales forecasting with machine learning by analyzing the historical data with techniques like Time Series Forecasting.;https://thecleverprogrammer.com/2020/07/11/time-series-forecasting/;['statsmodels', 'pattern', 'sklearn'];1.0;[];['ML'];['predict', 'fit', 'model', 'machine learning', 'label'];"Many Business activities are seasonal in nature, where most of the business are dependent on a particular time of festival and holidays. Every business uses sales promotion techniques to increase the demand for their products and services, in order to stay in the market for a longer period. In this article, I am going to do sales forecasting with machine learning by analyzing the historical data with techniques like Time Series Forecasting. Sales Forecast with Time Series Forecasting The data I will use here to predict sales, is a weekly sales data of nine stores and three products. At the end of this article, I will predict sales for next 50 weeks, now to move further with time series forecasting you can download this data that I will use below. Download Now, lets start with importing the standard libraries and reading the dataset: import plotly.express as px from fbprophet import Prophet from sklearn.metrics import mean_squared_error from math import sqrt from statsmodels.distributions.empirical_distribution import ECDF import datetime import pandas as pd import numpy as np df = pd.read_csv('Sales_Product_Price_by_Store.csv') df['Date'] = pd.to_datetime(df['Date']) df['weekly_sales'] = df['Price'] * df['Weekly_Units_Sold'] df.head()Code language: Python (python) StoreProductDateIs_HolidayBase PricePriceWeekly_Units_Soldweekly_sales0112010-02-05False9.997.992451957.551112010-02-12True9.997.994533619.472112010-02-19False9.997.994093267.913112010-02-26False9.997.991911526.094112010-03-05False9.999.991451448.55 df.set_index('Date', inplace=True) df['year'] = df.index.year df['month'] = df.index.month df['day'] = df.index.day df['week_of_year'] = df.index.weekofyear df.head()Code language: Python (python) StoreProductIs_HolidayBase PricePriceWeekly_Units_Soldweekly_salesyearmonthdayweek_of_yearDate2010-02-0511False9.997.992451957.5520102552010-02-1211True9.997.994533619.47201021262010-02-1911False9.997.994093267.91201021972010-02-2611False9.997.991911526.09201022682010-03-0511False9.999.991451448.552010359 Exploratory Data Analysis To get some insights about the continuous variables in data, I will plot and empirical distribution function (ECDF): import matplotlib.pyplot as plt import seaborn as sns sns.set(style = ""ticks"") c = '#386B7F' figure, axes = plt.subplots(nrows=2, ncols=2) figure.tight_layout(pad=2.0) plt.subplot(211) cdf = ECDF(df['Weekly_Units_Sold']) plt.plot(cdf.x, cdf.y, label = ""statmodels"", color = c); plt.xlabel('Weekly Units Sold'); plt.ylabel('ECDF'); plt.subplot(212) cdf = ECDF(df['weekly_sales']) plt.plot(cdf.x, cdf.y, label = ""statmodels"", color = c); plt.xlabel('Weekly sales');Code language: Python (python) The figure above clearly shows that, in a best week for sales, a store managed to sell 2500 units, but about 80 percent of the time, the weekly sales did not crossed 500 units. To see this with numbers let’s look at the statistics of our sales data: df.groupby('Store')['weekly_sales'].describe()Code language: Python (python) countmeanstdmin25%50%75%maxStore1429.01789.414172900.074226769.651208.901659.171957.206816.592429.02469.4474131328.1628841143.481579.212215.082756.559110.003429.0670.924009366.816321229.77459.77619.69730.782650.004429.03078.4621451746.1478721099.451818.182626.613837.5113753.125429.0588.922984242.628977285.87461.23519.74613.532264.976429.02066.7050821163.284768890.191418.581758.402156.407936.037429.0955.115058489.084883389.61649.35857.611041.513270.008429.01352.094056811.326288516.53846.231275.871491.516656.6710429.04093.4072493130.0871911483.652462.883707.814510.4725570.00 df.groupby('Store')['Weekly_Units_Sold'].sum()Code language: Python (python) Store 1 86699 2 121465 3 31689 4 158718 5 27300 6 97698 7 44027 8 65273 10 200924 Name: Weekly_Units_Sold, dtype: int64 Based on the above statistics, we can clearly see that, the store 10 has the highest average weekly sales, and store 5 has the lower average weekly sales among all the stores. The statistics say that store 10 has the most total weekly sales which simply convey that store 10 is the most crowded store among all the stores. g = sns.FacetGrid(df, col=""Is_Holiday"", height=4, aspect=.8) g.map(sns.barplot, ""Product"", ""Price"")Code language: Python (python) g = sns.FacetGrid(df, col=""Is_Holiday"", height=4, aspect=.8) g.map(sns.barplot, ""Product"", ""Weekly_Units_Sold"")Code language: Python (python) Product 2 is the cheapest product among these three products, so, it sells the most. Product 3 is the most expensive product among these three. Product price did not change during holidays. Because we have recorded holidays sales, so we will analyze if holiday also contributed to the sales. g = sns.FacetGrid(df, row=""Is_Holiday"", height=1.7, aspect=4,) g.map(sns.distplot, ""Weekly_Units_Sold"", hist=False, rug=True)Code language: Python (python) sns.factorplot(data= df, x= 'Is_Holiday', y= 'Weekly_Units_Sold', hue= 'Store')Code language: Python (python) sns.factorplot(data= df, x= 'Is_Holiday', y= 'Weekly_Units_Sold', hue= 'Product')Code language: Python (python) From the above figures we can see that holidays do not have a positive impact on the business. For most of the stores, weekly unit sales on the holidays is as same as the normal days, while store 10 also face a decrease in sales during the holidays. Weekly units sold for product 1 had a slightly increase during the holidays, while product 2 and product 3 had a decrease during the holidays. g = sns.FacetGrid(df, col=""Product"", row=""Is_Holiday"", margin_titles=True, height=3) g.map(plt.scatter, ""Price"", ""Weekly_Units_Sold"", color=""#338844"", edgecolor=""white"", s=50, lw=1) g.set(xlim=(0, 30), ylim=(0, 2600));Code language: Python (python) Every product has more than one price, both in holidays and normal days. One price is regular price, and another is a promotional price. However, the price gap for product 3 is huge, it was slashed to almost 50% off during promotions. Product 3 made the most sales during normal days. g = sns.FacetGrid(df, col=""Store"", hue=""Product"", margin_titles=True, col_wrap=3) g.map(plt.scatter, 'Price', 'Weekly_Units_Sold', alpha=.7) g.add_legend()Code language: Python (python) All the stores have the similar price promotion pattern, for some reason, Store 10 sells the most during the promotions. All the products have the regular price and promotion price. Product 3 has the highest discount and sells the most during the promotions. df.groupby(['Product', 'promotion'])['Price', 'Weekly_Units_Sold'].mean()Code language: Python (python) Now, let’s create a heatmap for concluding our all observations: corr_all = df.corr() # Generate a mask for the upper triangle mask = np.zeros_like(corr_all, dtype = np.bool) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure f, ax = plt.subplots(figsize = (11, 9)) # Draw the heatmap with the mask and correct aspect ratio sns.heatmap(corr_all, mask = mask, square = True, linewidths = .5, ax = ax, cmap = ""BuPu"") plt.show();Code language: Python (python) We have a strong positive correlation between price and Base price, weekly units sold and weekly sales, base price and product, price and product. We can also observe a positive correlation between month and week of the year. Observations of our EDA: The most selling and crowded Store is Store 10, and the least crowded store is Store 5.In terms of number of units sold, the most selling product is product 2. In terms of sales dollars, Product 3 posts the highest sales during normal days.Stores do not necessarily run product promotions during holidays. Holidays do not seem to have an impact on Stores’ performance.Product 1 sells a little more during holidays, however, Product 2 and Product 3 sell less on holidays.Product 2 seems to be the cheapest product, and Product 3 is the most expensive product.Most stores have some kind of seasonality and they make the highest sales around July.Product 1 sells a little more in February than the other months, Product 2 sells the most around April and July, and Product 3 sells the most around July.In general, product 2 sells the most at Store 10, but in July, Product 3 has the highest sales in this store.Each product has its regular price and promotional price. There isn’t significant gap between regular price and promotional price on Product 1 and Product 2, however, Product 3’s promotional price can be slashed to 50% of its original price. Although every store makes this kind of price cut for product 3, Store 10 is the one made the highest sales during the price cut.It is nothing unusual to sell more during promotion than the normal days. Store 10’s made Product 3 the best selling product around July. Time Series Forecasting and Sales Prediction Now let’s move to the Time Series Forecasting Part of this article, here we will forecast sales, according to our above observations of exploratory data analysis. # store types sales_1 = df[df.Store == 1]['weekly_sales'] sales_2 = df[df.Store == 2]['weekly_sales'] sales_3 = df[df.Store == 3]['weekly_sales'] sales_4 = df[df.Store == 4]['weekly_sales'] sales_5 = df[df.Store == 5]['weekly_sales'] sales_6 = df[df.Store == 6]['weekly_sales'] sales_7 = df[df.Store == 7]['weekly_sales'] sales_8 = df[df.Store == 8]['weekly_sales'] sales_10 = df[df.Store == 10]['weekly_sales'] f, (ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9) = plt.subplots(9, figsize = (20, 15)) # store types sales_1.plot(color = c, ax = ax1) sales_2.plot(color = c, ax = ax2) sales_3.plot(color = c, ax = ax3) sales_4.plot(color = c, ax = ax4) sales_5.plot(color = c, ax = ax5) sales_6.plot(color = c, ax = ax6) sales_7.plot(color = c, ax = ax7) sales_8.plot(color = c, ax = ax8) sales_10.plot(color = c, ax = ax9)Code language: Python (python) Time Series Forecasting Time Series of the weekly sales: store_10_pro_3 = df[(df.Store == 10) &amp; (df.Product == 3)].loc[:, ['Base Price', 'Price', 'Weekly_Units_Sold', 'weekly_sales']] store_10_pro_3.reset_index(level=0, inplace=True) fig = px.line(store_10_pro_3, x='Date', y='weekly_sales') fig.update_layout(title_text='Time Series of weekly sales') fig.show()Code language: Python (python) Product 2’s seasonality at store 10 is obvious. The sales always peak between July and September during school holiday. Below we are implementing prophet model, forecasting the weekly sales for the future 50 weeks. model = Prophet(interval_width = 0.95) model.fit(store_10_pro_3) future_dates = model.make_future_dataframe(periods = 50, freq='W') future_dates.tail(7)Code language: Python (python) forecast = model.predict(future_dates) # preditions for last week forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(7)Code language: Python (python) dsyhatyhat_loweryhat_upper1862013-08-257160.4536694742.9377109559.6156731872013-09-015542.4347393249.7627127887.3217851882013-09-083702.1683771355.9025665824.5551931892013-09-152427.279755189.5521424693.1589761902013-09-222386.9724287.9734714673.0530271912013-09-293020.451351759.2522365227.6951071922013-10-063157.655085756.0794995603.923897 model.plot(forecast)Code language: Python (python) model.plot_components(forecast)Code language: Python (python) metric_df = forecast.set_index('ds')[['yhat']].join(store_10_pro_3.set_index('ds').y).reset_index() metric_df.dropna(inplace=True) error = mean_squared_error(metric_df.y, metric_df.yhat) print('The RMSE is {}'. format(sqrt(error)))Code language: Python (python) The RMSE is 1190.0962582193933 I hope you liked this article on Time Series Forecasting on Sales Prediction. Feel free to ask your questions about Time Series Forecasting and Analysis or any other topic that you want in the comments section below. Follow Us: Facebook Instagram";Time Series Forecasting
2020-07-12 18:27:24;"This article features the implementation of an employee turnover analysis that is built using Python’s Scikit-Learn library. In this article, I will use Logistic Regression and  Random Forest Machine Learning algorithms. At the end of this article, you would be able to choose the best algorithm for your future projects like Employee Turnover Prediction.Employee Turnover or Employee Turnover ratio is the measurement of the total number of employees who leave an organization in a particular year. Employee Turnover Prediction means to predict whether an employee is going to leave the organization in the coming period. A Company uses this predictive analysis to measure how many employees they will need if the potential employees will leave their organization. A company also uses this predictive analysis to make the workplace better for employees by understanding the core reasons for the high turnover ratio.Now let’s dive into the data to move further with this project on Employee Turnover Prediction. You can download the dataset I have used in this article below.Now let’s import the data and move further with the analysis:Rename column name from “sales” to “department”:The type of the columns can be found out as follows:Our data is pretty clean, with no missing values, so let’s move further and see how many employees work in the organization:The “left” column is the outcome variable recording one and 0. 1 for employees who left the company and 0 for those who didn’t.The department column of the dataset has many categories, and we need to reduce the categories for better modelling. Let’s see all the categories of the department column:Let’s add all the “technical”, “support” and “IT” columns into one column to make our analysis easier.As there are two categorical variables (department, salary) in the dataset and they need to be converted to dummy variables before they can be used for modelling.Now the actual variables need to be removed after the dummy variable have been created. Column names after creating dummy variables for categorical variables:The outcome variable is “left”, and all the other variables are predictors.Let’s use the feature selection method to decide which variables are the best option that can predict employee turnover with great accuracy. There are a total of 18 columns in X, and now let’s see how we can select about 10 from them:You can see that or feature selection chose the 10 variables for us, which are marked True in the support_ array and marked with a choice “1” in the ranking_array. Now lets have a look at these columns:Let’s check the accuracy of our logistic regression model.Now let’s check the accuracy of our Random Forest Classification Model: Random Forest Accuracy: 0.978Now I will construct a confusion matrix to visualize predictions made by our classifier and evaluate the accuracy of our machine learning classification.Random ForestLogistic RegressionThe receiver operating characteristic (ROC) curve is a standard tool used with binary classifiers. The red dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).So, as we can see that the Random Forest Model has proven to be more useful in the prediction of employee turnover, now let’s have a look at the feature importance of our random forest classification model.According to our Random Forest classification model, the above aspects show the most important features which will influence whether an employee will leave the company, in ascending order.I hope you liked this article on Employee Turnover Prediction with Machine Learning. Feel free to ask your valuable questions in the comments section and dont’ forget to subscribe to our daily newsletters below if you like my work.";https://thecleverprogrammer.com/2020/07/12/employee-turnover-prediction/;['sklearn'];1.0;[];['Regression', 'ML', 'Logistic Regression', 'Random Forest', 'Classification'];['regression', 'predict', 'fit', 'model', 'machine learning', 'logistic regression', 'random forest', 'classif', 'rank', 'train', 'label'];"This article features the implementation of an employee turnover analysis that is built using Python’s Scikit-Learn library. In this article, I will use Logistic Regression and Random Forest Machine Learning algorithms. At the end of this article, you would be able to choose the best algorithm for your future projects like Employee Turnover Prediction. What is Employee Turnover? Employee Turnover or Employee Turnover ratio is the measurement of the total number of employees who leave an organization in a particular year. Employee Turnover Prediction means to predict whether an employee is going to leave the organization in the coming period. A Company uses this predictive analysis to measure how many employees they will need if the potential employees will leave their organization. A company also uses this predictive analysis to make the workplace better for employees by understanding the core reasons for the high turnover ratio. Data Preprocessing Now let’s dive into the data to move further with this project on Employee Turnover Prediction. You can download the dataset I have used in this article below. download Now let’s import the data and move further with the analysis: import pandas as pd hr = pd.read_csv('HR.csv') col_names = hr.columns.tolist() print(""Column names:"") print(col_names) print(""\nSample data:"") hr.head()Code language: Python (python) Column names: ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'left', 'promotion_last_5years', 'sales', 'salary'] sample data : Rename column name from “sales” to “department”: hr=hr.rename(columns = {'sales':'department'})Code language: Python (python) The type of the columns can be found out as follows: Our data is pretty clean, with no missing values, so let’s move further and see how many employees work in the organization: hr.shapeCode language: Python (python) The “left” column is the outcome variable recording one and 0. 1 for employees who left the company and 0 for those who didn’t. The department column of the dataset has many categories, and we need to reduce the categories for better modelling. Let’s see all the categories of the department column: hr['department'].unique()Code language: Python (python) array(['sales', 'accounting', 'hr', 'technical', 'support', 'management', 'IT', 'product_mng', 'marketing', 'RandD'], dtype=object) Let’s add all the “technical”, “support” and “IT” columns into one column to make our analysis easier. import numpy as np hr['department']=np.where(hr['department'] =='support', 'technical', hr['department']) hr['department']=np.where(hr['department'] =='IT', 'technical', hr['department'])Code language: Python (python) Creating Variables for Categorical Variables As there are two categorical variables (department, salary) in the dataset and they need to be converted to dummy variables before they can be used for modelling. cat_vars=['department','salary'] for var in cat_vars: cat_list='var'+'_'+var cat_list = pd.get_dummies(hr[var], prefix=var) hr1=hr.join(cat_list) hr=hr1Code language: Python (python) Now the actual variables need to be removed after the dummy variable have been created. Column names after creating dummy variables for categorical variables: hr.drop(hr.columns[[8, 9]], axis=1, inplace=True) hr.columns.valuesCode language: Python (python) array(['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'left', 'promotion_last_5years', 'department_RandD', 'department_accounting', 'department_hr', 'department_management', 'department_marketing', 'department_product_mng', 'department_sales', 'department_technical', 'salary_high', 'salary_low', 'salary_medium'], dtype=object) The outcome variable is “left”, and all the other variables are predictors. hr_vars=hr.columns.values.tolist() y=['left'] X=[i for i in hr_vars if i not in y]Code language: JavaScript (javascript) Feature Selection for Employee Turnover Prediction Let’s use the feature selection method to decide which variables are the best option that can predict employee turnover with great accuracy. There are a total of 18 columns in X, and now let’s see how we can select about 10 from them: from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression model = LogisticRegression() rfe = RFE(model, 10) rfe = rfe.fit(hr[X], hr[y]) print(rfe.support_) print(rfe.ranking_)Code language: Python (python) [True True False False True True True True False True True False False False False True True False] [1 1 3 9 1 1 1 1 5 1 1 6 8 7 4 1 1 2] You can see that or feature selection chose the 10 variables for us, which are marked True in the support_ array and marked with a choice “1” in the ranking_array. Now lets have a look at these columns: ['satisfaction_level', 'last_evaluation', 'time_spend_company', 'Work_accident', 'promotion_last_5years', 'department_RandD', 'department_hr', 'department_management', 'salary_high', 'salary_low'] cols=['satisfaction_level', 'last_evaluation', 'time_spend_company', 'Work_accident', 'promotion_last_5years', 'department_RandD', 'department_hr', 'department_management', 'salary_high', 'salary_low'] X=hr[cols] y=hr['left']Code language: Python (python) Logistic Regression Model to Predict Employee Turnover from sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) from sklearn.linear_model import LogisticRegression from sklearn import metrics logreg = LogisticRegression() logreg.fit(X_train, y_train)Code language: Python (python) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False) Let’s check the accuracy of our logistic regression model. from sklearn.metrics import accuracy_score print('Logistic regression accuracy: {:.3f}'.format(accuracy_score(y_test, logreg.predict(X_test))))Code language: Python (python) Logistic regression accuracy: 0.771 Random Forest Classification Model from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier() rf.fit(X_train, y_train)Code language: Python (python) RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=None, verbose=0, warm_start=False) Now let’s check the accuracy of our Random Forest Classification Model: print('Random Forest Accuracy: {:.3f}'.format(accuracy_score(y_test, rf.predict(X_test))))Code language: Python (python) Random Forest Accuracy: 0.978 Confusion Matrix for our Machine Learning Models Now I will construct a confusion matrix to visualize predictions made by our classifier and evaluate the accuracy of our machine learning classification. Random Forest from sklearn.metrics import classification_report print(classification_report(y_test, rf.predict(X_test)))Code language: Python (python) y_pred = rf.predict(X_test) from sklearn.metrics import confusion_matrix import seaborn as sns forest_cm = metrics.confusion_matrix(y_pred, y_test, [1,0]) sns.heatmap(forest_cm, annot=True, fmt='.2f',xticklabels = [""Left"", ""Stayed""] , yticklabels = [""Left"", ""Stayed""] ) plt.ylabel('True class') plt.xlabel('Predicted class') plt.title('Random Forest')Code language: Python (python) Logistic Regression print(classification_report(y_test, logreg.predict(X_test)))Code language: Python (python) logreg_y_pred = logreg.predict(X_test) logreg_cm = metrics.confusion_matrix(logreg_y_pred, y_test, [1,0]) sns.heatmap(logreg_cm, annot=True, fmt='.2f',xticklabels = [""Left"", ""Stayed""] , yticklabels = [""Left"", ""Stayed""] ) plt.ylabel('True class') plt.xlabel('Predicted class') plt.title('Logistic Regression')Code language: Python (python) Employee Turnover Prediction Curve from sklearn.metrics import roc_auc_score from sklearn.metrics import roc_curve logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test)) fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1]) rf_roc_auc = roc_auc_score(y_test, rf.predict(X_test)) rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1]) plt.figure() plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc) plt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc) plt.plot([0, 1], [0, 1],'r--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic') plt.legend(loc=""lower right"") plt.show()Code language: Python (python) The receiver operating characteristic (ROC) curve is a standard tool used with binary classifiers. The red dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). So, as we can see that the Random Forest Model has proven to be more useful in the prediction of employee turnover, now let’s have a look at the feature importance of our random forest classification model. feature_labels = np.array(['satisfaction_level', 'last_evaluation', 'time_spend_company', 'Work_accident', 'promotion_last_5years', 'department_RandD', 'department_hr', 'department_management', 'salary_high', 'salary_low']) importance = rf.feature_importances_ feature_indexes_by_importance = importance.argsort() for index in feature_indexes_by_importance: print('{}-{:.2f}%'.format(feature_labels[index], (importance[index] *100.0)))Code language: Python (python) promotion_last_5years-0.20% department_management-0.22% department_hr-0.29% department_RandD-0.34% salary_high-0.55% salary_low-1.35% Work_accident-1.46% last_evaluation-19.19% time_spend_company-25.73% satisfaction_level-50.65% According to our Random Forest classification model, the above aspects show the most important features which will influence whether an employee will leave the company, in ascending order. I hope you liked this article on Employee Turnover Prediction with Machine Learning. Feel free to ask your valuable questions in the comments section and dont’ forget to subscribe to our daily newsletters below if you like my work. Follow Us: Facebook Instagram";Employee Turnover Prediction
2020-07-12 23:54:13;"Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as k-means clustering, which is implemented in sklearn.cluster.KMeans.The k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like:Those two assumptions are the basis of the k-means model. We will soon dive into exactly how the algorithm reaches this solution, but for now, let’s take a look at a simple dataset and see the k-means result.First, let’s generate a two-dimensional dataset containing four distinct blobs. To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization.By eye, it is relatively easy to pick out the four clusters. The k-means algorithm does this automatically, and in Scikit-Learn uses the standard estimator API:Let’s visualize the results by plotting the data coloured by these labels. We will also plot the cluster centers as determined by the k-means estimator:The good news is that the k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye. But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the name of data points—an exhaustive search would be very, very costly. Fortunately for us, such a thorough search is not necessary: instead, the typical approach to k-means involves an intuitive, iterative approach known as expectation–maximization.Expectation–maximization (E–M) is a robust algorithm that comes up in a variety of contexts within data science. k-means is a particularly easy-to-understand and straightforward application of the algorithm, and we will walk through it briefly here. In short, the expectation-maximization approach here consists of the following procedure:Here the “E-step” or “Expectation step” is so-named because it involves updating our expectation of which cluster each point belongs to. The “M-step” or “Maximization step” is so-named because it consists in maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.The k-Means algorithm is simple enough that we can write it in a few lines of code. The following is a very basic implementation:To start, let’s take a look at applying k-means on the same simple digits data. Here we will attempt to use k-means to try to identify similar digits without using the original label information; this might be similar to a first step in extracting meaning from a new dataset about which you don’t have any a priori label information.We will start by loading the digits and then finding the KMeans clusters. Recall that the numbers consist of 1,797 samples with 64 features, where each of the 64 elements is the brightness of one pixel in an 8×8 image:Now let’s do the clustering:The result is 10 clusters in 64 dimensions. Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the “typical” digit within the cluster. Let’s see what these cluster centers look like:We see that even without the labels, KMeans can find clusters whose centers are recognizable digits, with perhaps the exception of 1 and 8.Because k-means knows nothing about the identity of the cluster, the 0–9 labels may be permuted. We can fix this by matching each learned cluster label with the correct names found in them:Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:With just a simple k-means algorithm, we discovered the correct grouping for 80% of the input digits! Let’s check the confusion matrix for this:As we might expect from the cluster centers we visualized before, the main point of confusion is between the eights and ones. But this still shows that using k-means, we can essentially build a digit classifier without reference to any known labels.I hope you liked this article on k-means algorithm. Feel free to ask your valuable questions in the comments section. Don’t forget to subscribe for my daily newsletters below to get email notifications if you like my work.";https://thecleverprogrammer.com/2020/07/12/k-means-in-machine-learning/;['sklearn'];1.0;[];['K-Means', 'Clustering', 'Classification'];['clustering', 'recogn', 'k-means', 'unlabeled', 'predict', 'fit', 'model', 'classif', 'label'];"Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as k-means clustering, which is implemented in sklearn.cluster.KMeans. Introduction The k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like: The “cluster centre” is the arithmetic mean of all the points belonging to the cluster.Each point is closer to its cluster centre than to other cluster centres. Those two assumptions are the basis of the k-means model. We will soon dive into exactly how the algorithm reaches this solution, but for now, let’s take a look at a simple dataset and see the k-means result. First, let’s generate a two-dimensional dataset containing four distinct blobs. To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization. import matplotlib.pyplot as plt import seaborn as sns; sns.set() # for plot styling import numpy as np from sklearn.datasets.samples_generator import make_blobs X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0) plt.scatter(X[:, 0], X[:, 1], s=50)Code language: Python (python) By eye, it is relatively easy to pick out the four clusters. The k-means algorithm does this automatically, and in Scikit-Learn uses the standard estimator API: from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=4) kmeans.fit(X) y_kmeans = kmeans.predict(X)Code language: Python (python) Let’s visualize the results by plotting the data coloured by these labels. We will also plot the cluster centers as determined by the k-means estimator: plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis') centers = kmeans.cluster_centers_ plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)Code language: Python (python) The good news is that the k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye. But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the name of data points—an exhaustive search would be very, very costly. Fortunately for us, such a thorough search is not necessary: instead, the typical approach to k-means involves an intuitive, iterative approach known as expectation–maximization. k-Means Algorithm: Expectation–Maximization Expectation–maximization (E–M) is a robust algorithm that comes up in a variety of contexts within data science. k-means is a particularly easy-to-understand and straightforward application of the algorithm, and we will walk through it briefly here. In short, the expectation-maximization approach here consists of the following procedure: Guess some cluster centersRepeat until convergedE-Step: assign points to the nearest cluster centerM-Step: set the cluster centers to the mean Here the “E-step” or “Expectation step” is so-named because it involves updating our expectation of which cluster each point belongs to. The “M-step” or “Maximization step” is so-named because it consists in maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster. The k-Means algorithm is simple enough that we can write it in a few lines of code. The following is a very basic implementation: from sklearn.metrics import pairwise_distances_argmin def find_clusters(X, n_clusters, rseed=2): # 1. Randomly choose clusters rng = np.random.RandomState(rseed) i = rng.permutation(X.shape[0])[:n_clusters] centers = X[i] while True: # 2a. Assign labels based on closest center labels = pairwise_distances_argmin(X, centers) # 2b. Find new centers from means of points new_centers = np.array([X[labels == i].mean(0) for i in range(n_clusters)]) # 2c. Check for convergence if np.all(centers == new_centers): break centers = new_centers return centers, labels centers, labels = find_clusters(X, 4) plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')Code language: Python (python) Example: K-Means Algorithm on Digits To start, let’s take a look at applying k-means on the same simple digits data. Here we will attempt to use k-means to try to identify similar digits without using the original label information; this might be similar to a first step in extracting meaning from a new dataset about which you don’t have any a priori label information. We will start by loading the digits and then finding the KMeans clusters. Recall that the numbers consist of 1,797 samples with 64 features, where each of the 64 elements is the brightness of one pixel in an 8×8 image: from sklearn.datasets import load_digits digits = load_digits() digits.data.shapeCode language: Python (python) (1797, 64) Now let’s do the clustering: kmeans = KMeans(n_clusters=10, random_state=0) clusters = kmeans.fit_predict(digits.data) kmeans.cluster_centers_.shapeCode language: Python (python) (10, 64) The result is 10 clusters in 64 dimensions. Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the “typical” digit within the cluster. Let’s see what these cluster centers look like: fig, ax = plt.subplots(2, 5, figsize=(8, 3)) centers = kmeans.cluster_centers_.reshape(10, 8, 8) for axi, center in zip(ax.flat, centers): axi.set(xticks=[], yticks=[]) axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)Code language: Python (python) We see that even without the labels, KMeans can find clusters whose centers are recognizable digits, with perhaps the exception of 1 and 8. Because k-means knows nothing about the identity of the cluster, the 0–9 labels may be permuted. We can fix this by matching each learned cluster label with the correct names found in them: from scipy.stats import mode labels = np.zeros_like(clusters) for i in range(10): mask = (clusters == i) labels[mask] = mode(digits.target[mask])[0]Code language: Python (python) Now we can check how accurate our unsupervised clustering was in finding similar digits within the data: from sklearn.metrics import accuracy_score accuracy_score(digits.target, labels)Code language: Python (python) 0.79354479688369506 With just a simple k-means algorithm, we discovered the correct grouping for 80% of the input digits! Let’s check the confusion matrix for this: from sklearn.metrics import confusion_matrix mat = confusion_matrix(digits.target, labels) sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=digits.target_names, yticklabels=digits.target_names) plt.xlabel('true label') plt.ylabel('predicted label')Code language: Python (python) As we might expect from the cluster centers we visualized before, the main point of confusion is between the eights and ones. But this still shows that using k-means, we can essentially build a digit classifier without reference to any known labels. I hope you liked this article on k-means algorithm. Feel free to ask your valuable questions in the comments section. Don’t forget to subscribe for my daily newsletters below to get email notifications if you like my work. Follow Us: Facebook Instagram";K-Means in Machine Learning
2020-07-13 10:47:52;According to the report of Centers of Disease Control and Prevention about one in seven adults in the United States have Diabetes. But by next few years this rate can move higher. With this in mind today, In this article, I will show you how you can use machine learning to Predict Diabetes using Python.Let’s straightaway get into the data, you can download the data I have used in this article to Predict Diabetes below.Now let’s import the data and gets started:The diabetes data set consists of 768 data points, with 9 features each:dimension of diabetes data: (768, 9)“Outcome” is the feature we are going to predict, 0 means No diabetes, 1 means diabetes. Of these 768 data points, 500 are labeled as 0 and 268 as 1:The k-Nearest Neighbors algorithm is arguably the simplest machine learning algorithm. Building the model consists only of storing the training data set. To make a prediction for a new point in the dataset, the algorithm finds the closest data points in the training data set — its “nearest neighbors.”First, Let’s investigate whether we can confirm the connection between model complexity and accuracy:Let’s check the accuracy score of the k-nearest neighbors algorithm to predict diabetes.Accuracy of K-NN classifier on training set: 0.79Accuracy of K-NN classifier on test set: 0.78Accuracy on training set: 1.000Accuracy on test set: 0.714The accuracy on the training set with Decision Tree Classifier is 100%, while the test set accuracy is much worse. This is an indicative that the tree is overfitting and not generalizing well to new data. Therefore, we need to apply pre-pruning to the tree.Now, I will do this again by doing set max_depth=3, limiting the depth of the tree decreases overfitting. This leads to a lower accuracy on the training set, but an improvement on the test set.Accuracy on training set: 0.773Accuracy on test set: 0.740Feature importance shows how important each feature is for the decision a decision tree classifier makes. It is a number between 0 and 1 for each feature, where 0 means “not used at all” and 1 means “perfectly predicts the target”. The feature importance always sum to 1:Feature importances: [ 0.04554275 0.6830362 0. 0. 0. 0.27142106 0. 0. ]Now lets visualize the feature importance of decision tree to predict diabetes.So the Glucose feature is used the most to predict diabetes.Lets train a deep learning model to predict diabetes:Accuracy on training set: 0.71Accuracy on test set: 0.67The accuracy of the Multilayer perceptrons (MLP) is not as good as the other models at all, this is likely due to scaling of the data. Deep learning algorithms also expect all input features to vary in a similar way, and ideally to have a mean of 0, and a variance of 1. Now I will re-scale our data so that it fulfills these requirements to predict diabetes with a good accuracy.Accuracy on training set: 0.823Accuracy on test set: 0.802Now let’s increase the number of iterations, alpha parameter and add stronger parameters to the weights of the model:Accuracy on training set: 0.795Accuracy on test set: 0.792The result is good, but we are not able to increase the test accuracy further. Therefore, our best model so far is default deep learning model after scaling. Now I will  plot a heat map of the first layer weights in a neural network learned on the to predict diabetes using the data set.I hope you liked this article to predict diabetes with Machine Learning. Feel free to ask your valuable questions in the comments section. Don’t forget to subscribe for the daily newsletters below to receive daily posts notifications if you like my work.;https://thecleverprogrammer.com/2020/07/13/predict-diabetes-with-machine-learning/;['sklearn'];1.0;['ML', 'NN'];['NN', 'DL', 'ML', 'Decision Tree', 'KNN', 'Classification'];['predict', 'fit', 'model', 'machine learning', 'neural network', 'classif', 'k-nearest neighbor', 'layer', 'deep learning', 'training data', 'train', 'label', 'decision tree'];"According to the report of Centers of Disease Control and Prevention about one in seven adults in the United States have Diabetes. But by next few years this rate can move higher. With this in mind today, In this article, I will show you how you can use machine learning to Predict Diabetes using Python. Let’s straightaway get into the data, you can download the data I have used in this article to Predict Diabetes below. download Now let’s import the data and gets started: import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline diabetes = pd.read_csv('diabetes.csv') print(diabetes.columns)Code language: Python (python) Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'], dtype='object') diabetes.head()Code language: Python (python) The diabetes data set consists of 768 data points, with 9 features each: print(""dimension of diabetes data: {}"".format(diabetes.shape))Code language: Python (python) dimension of diabetes data: (768, 9) “Outcome” is the feature we are going to predict, 0 means No diabetes, 1 means diabetes. Of these 768 data points, 500 are labeled as 0 and 268 as 1: print(diabetes.groupby('Outcome').size())Code language: Python (python) import seaborn as sns sns.countplot(diabetes['Outcome'],label=""Count"")Code language: Python (python) diabetes.info()Code language: Python (python) K-Nearest Neighbors to Predict Diabetes The k-Nearest Neighbors algorithm is arguably the simplest machine learning algorithm. Building the model consists only of storing the training data set. To make a prediction for a new point in the dataset, the algorithm finds the closest data points in the training data set — its “nearest neighbors.” First, Let’s investigate whether we can confirm the connection between model complexity and accuracy: from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(diabetes.loc[:, diabetes.columns != 'Outcome'], diabetes['Outcome'], stratify=diabetes['Outcome'], random_state=66) from sklearn.neighbors import KNeighborsClassifier training_accuracy = [] test_accuracy = [] # try n_neighbors from 1 to 10 neighbors_settings = range(1, 11) for n_neighbors in neighbors_settings: # build the model knn = KNeighborsClassifier(n_neighbors=n_neighbors) knn.fit(X_train, y_train) # record training set accuracy training_accuracy.append(knn.score(X_train, y_train)) # record test set accuracy test_accuracy.append(knn.score(X_test, y_test)) plt.plot(neighbors_settings, training_accuracy, label=""training accuracy"") plt.plot(neighbors_settings, test_accuracy, label=""test accuracy"") plt.ylabel(""Accuracy"") plt.xlabel(""n_neighbors"") plt.legend()Code language: Python (python) Let’s check the accuracy score of the k-nearest neighbors algorithm to predict diabetes. knn = KNeighborsClassifier(n_neighbors=9) knn.fit(X_train, y_train) print('Accuracy of K-NN classifier on training set: {:.2f}'.format(knn.score(X_train, y_train))) print('Accuracy of K-NN classifier on test set: {:.2f}'.format(knn.score(X_test, y_test)))Code language: Python (python) Accuracy of K-NN classifier on training set: 0.79Accuracy of K-NN classifier on test set: 0.78 Decision Tree Classifier from sklearn.tree import DecisionTreeClassifier tree = DecisionTreeClassifier(random_state=0) tree.fit(X_train, y_train) print(""Accuracy on training set: {:.3f}"".format(tree.score(X_train, y_train))) print(""Accuracy on test set: {:.3f}"".format(tree.score(X_test, y_test)))Code language: Python (python) Accuracy on training set: 1.000Accuracy on test set: 0.714 The accuracy on the training set with Decision Tree Classifier is 100%, while the test set accuracy is much worse. This is an indicative that the tree is overfitting and not generalizing well to new data. Therefore, we need to apply pre-pruning to the tree. Now, I will do this again by doing set max_depth=3, limiting the depth of the tree decreases overfitting. This leads to a lower accuracy on the training set, but an improvement on the test set. tree = DecisionTreeClassifier(max_depth=3, random_state=0) tree.fit(X_train, y_train) print(""Accuracy on training set: {:.3f}"".format(tree.score(X_train, y_train))) print(""Accuracy on test set: {:.3f}"".format(tree.score(X_test, y_test)))Code language: Python (python) Accuracy on training set: 0.773Accuracy on test set: 0.740 Feature Importance in Decision Trees Feature importance shows how important each feature is for the decision a decision tree classifier makes. It is a number between 0 and 1 for each feature, where 0 means “not used at all” and 1 means “perfectly predicts the target”. The feature importance always sum to 1: print(""Feature importances:\n{}"".format(tree.feature_importances_))Code language: Python (python) Feature importances: [ 0.04554275 0.6830362 0. 0. 0. 0.27142106 0. 0. ] Now lets visualize the feature importance of decision tree to predict diabetes. def plot_feature_importances_diabetes(model): plt.figure(figsize=(8,6)) n_features = 8 plt.barh(range(n_features), model.feature_importances_, align='center') plt.yticks(np.arange(n_features), diabetes_features) plt.xlabel(""Feature importance"") plt.ylabel(""Feature"") plt.ylim(-1, n_features) plot_feature_importances_diabetes(tree)Code language: Python (python) So the Glucose feature is used the most to predict diabetes. Deep Learning to Predict Diabetes Lets train a deep learning model to predict diabetes: from sklearn.neural_network import MLPClassifier mlp = MLPClassifier(random_state=42) mlp.fit(X_train, y_train) print(""Accuracy on training set: {:.2f}"".format(mlp.score(X_train, y_train))) print(""Accuracy on test set: {:.2f}"".format(mlp.score(X_test, y_test)))Code language: Python (python) Accuracy on training set: 0.71Accuracy on test set: 0.67 The accuracy of the Multilayer perceptrons (MLP) is not as good as the other models at all, this is likely due to scaling of the data. Deep learning algorithms also expect all input features to vary in a similar way, and ideally to have a mean of 0, and a variance of 1. Now I will re-scale our data so that it fulfills these requirements to predict diabetes with a good accuracy. from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.fit_transform(X_test) mlp = MLPClassifier(random_state=0) mlp.fit(X_train_scaled, y_train) print(""Accuracy on training set: {:.3f}"".format( mlp.score(X_train_scaled, y_train))) print(""Accuracy on test set: {:.3f}"".format(mlp.score(X_test_scaled, y_test)))Code language: Python (python) Accuracy on training set: 0.823Accuracy on test set: 0.802 Now let’s increase the number of iterations, alpha parameter and add stronger parameters to the weights of the model: mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0) mlp.fit(X_train_scaled, y_train) print(""Accuracy on training set: {:.3f}"".format( mlp.score(X_train_scaled, y_train))) print(""Accuracy on test set: {:.3f}"".format(mlp.score(X_test_scaled, y_test)))Code language: Python (python) Accuracy on training set: 0.795Accuracy on test set: 0.792 The result is good, but we are not able to increase the test accuracy further. Therefore, our best model so far is default deep learning model after scaling. Now I will plot a heat map of the first layer weights in a neural network learned on the to predict diabetes using the data set. plt.figure(figsize=(20, 5)) plt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis') plt.yticks(range(8), diabetes_features) plt.xlabel(""Columns in weight matrix"") plt.ylabel(""Input feature"") plt.colorbar()Code language: Python (python) I hope you liked this article to predict diabetes with Machine Learning. Feel free to ask your valuable questions in the comments section. Don’t forget to subscribe for the daily newsletters below to receive daily posts notifications if you like my work. Follow Us: Facebook Instagram";Predict Diabetes with Machine Learning
2020-07-14 11:54:36;I have trained and developed a lot of Machine Learning models, if you are a student in Machine Learning, you must have also developed models. In this article, I will train and Deploy a Machine Learning Model using Flask step by step. I will first train a model, then I will work to serve our model, and at the end I will deploy our machine learning model.When you train a machine learning model, also think about how you will deploy a machine learning model to serve your trained model to the available users. You will get a lot of websites who are teaching to train a machine learning model but nobody goes beyond to deploy a machine learning model. Because training and deploying a machine learning model are very different from each other. But it’s not difficult. Training a model is the most important part in machine learning. But deploying a model is a different art because you have to think  a lot in the process how you will make your machine learning application to your users. Let’s do this step by step. I will train a SMS Spam detection machine learning model. I will not explain the procedure of training a machine learning model as you will get a lot of articles in this website about training model. My primary focus for today is to teach you how to deploy a machine learning model.You can download the dataset I will use in training my machine learning model below:Now let’s train our model for sms spam detection:After you train your model, you must have wondered how could you use that model without training the model. So you can save the model for you future use, there is no need of training the model every time while you the model. You can save your machine learning model as follows:And the next time you need the same model, you can load this machine learning model as follows:After training your sms spam detection classification, it’s time to deploy our machine learning model. I will create a simple web application which will consist a web page which will look like a form where we can write or paste our sms. The web page will consist a submit button, when we will click the submit button it will use our machine learning model to classify whether the sms is spam or ham (not spam).First, I will create a folder for this project called SMS-Message-Spam-Detector , this is the directory tree inside the folder:The sub-directory templates is the directory in which Flask will look for static HTML files for rendering in the web browser, in our case, we have two html files: home.html and result.html.The app.py file will contain the main code that will be executed by the python to run our Flask web application. It includes the Machine Learning code for classifying our sms messages as spam or ham.The following are the contents of the home.html file that will create a text form where a user can enter a message:In the header section of or html file (home.html), we loaded style.css file. CSS is to determine the appearance and style of HTML documents. style.css has to be saved in a sub-directory called static, which is the default directory where Flask looks for static files such as CSS.Now, I will create a result.html file that will give us a result of our model prediction. It is the final step when we deploy a machine learning model.After following all the steps to deploy a machine learning model, now you can simply run this program using your app.py file.You will see you output as follows:I hope, you like this article on Train and deploy a machine learning model. Feel free to ask your valuable questions in the comments section. Don’t forget to subscribe for my daily newsletters below to get emails notification of our daily newsletters is you like my work.;https://thecleverprogrammer.com/2020/07/14/deploy-a-machine-learning-model/;['sklearn'];1.0;['ML'];['ML', 'Naive Bayes', 'Classification'];['detect', 'predict', 'fit', 'model', 'machine learning', 'classif', 'naive bayes', 'train', 'label'];"I have trained and developed a lot of Machine Learning models, if you are a student in Machine Learning, you must have also developed models. In this article, I will train and Deploy a Machine Learning Model using Flask step by step. I will first train a model, then I will work to serve our model, and at the end I will deploy our machine learning model. Train and Deploy a Machine Learning Model When you train a machine learning model, also think about how you will deploy a machine learning model to serve your trained model to the available users. You will get a lot of websites who are teaching to train a machine learning model but nobody goes beyond to deploy a machine learning model. Because training and deploying a machine learning model are very different from each other. But it’s not difficult. Training a model is the most important part in machine learning. But deploying a model is a different art because you have to think a lot in the process how you will make your machine learning application to your users. Let’s do this step by step. Training a Machine Learning Model I will train a SMS Spam detection machine learning model. I will not explain the procedure of training a machine learning model as you will get a lot of articles in this website about training model. My primary focus for today is to teach you how to deploy a machine learning model. You can download the dataset I will use in training my machine learning model below: download Now let’s train our model for sms spam detection: import pandas as pd import numpy as np from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import classification_report df = pd.read_csv('spam.csv', encoding=""latin-1"") df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True) df['label'] = df['class'].map({'ham': 0, 'spam': 1}) X = df['message'] y = df['label'] cv = CountVectorizer() X = cv.fit_transform(X) # Fit the Data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) #Naive Bayes Classifier clf = MultinomialNB() clf.fit(X_train,y_train) clf.score(X_test,y_test) y_pred = clf.predict(X_test) print(classification_report(y_test, y_pred))​x import pandas as pdimport numpy as npfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.metrics import classification_report​df = pd.read_csv('spam.csv', encoding=""latin-1"")df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)df['label'] = df['class'].map({'ham': 0, 'spam': 1})X = df['message']y = df['label']cv = CountVectorizer()X = cv.fit_transform(X) # Fit the DataX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)#Naive Bayes Classifierclf = MultinomialNB()clf.fit(X_train,y_train)clf.score(X_test,y_test)y_pred = clf.predict(X_test)print(classification_report(y_test, y_pred)) Saving our Machine Learning Model After you train your model, you must have wondered how could you use that model without training the model. So you can save the model for you future use, there is no need of training the model every time while you the model. You can save your machine learning model as follows: from sklearn.externals import joblib joblib.dump(clf, 'NB_spam_model.pkl') from sklearn.externals import joblibjoblib.dump(clf, 'NB_spam_model.pkl') And the next time you need the same model, you can load this machine learning model as follows: NB_spam_model = open('NB_spam_model.pkl','rb') clf = joblib.load(NB_spam_model) NB_spam_model = open('NB_spam_model.pkl','rb')clf = joblib.load(NB_spam_model) Deploying a Machine Learning Model into a Web Application After training your sms spam detection classification, it’s time to deploy our machine learning model. I will create a simple web application which will consist a web page which will look like a form where we can write or paste our sms. The web page will consist a submit button, when we will click the submit button it will use our machine learning model to classify whether the sms is spam or ham (not spam). First, I will create a folder for this project called SMS-Message-Spam-Detector , this is the directory tree inside the folder: spam.csv app.py templates/ home.html result.html static/ style.css The sub-directory templates is the directory in which Flask will look for static HTML files for rendering in the web browser, in our case, we have two html files: home.html and result.html. app.py The app.py file will contain the main code that will be executed by the python to run our Flask web application. It includes the Machine Learning code for classifying our sms messages as spam or ham. from flask import Flask,render_template,url_for,request import pandas as pd import pickle from sklearn.feature_extraction.text import CountVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.externals import joblib app = Flask(__name__) @app.route('/') def home(): return render_template('home.html') @app.route('/predict',methods=['POST']) def predict(): df= pd.read_csv(""spam.csv"", encoding=""latin-1"") df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True) # Features and Labels df['label'] = df['class'].map({'ham': 0, 'spam': 1}) X = df['message'] y = df['label'] # Extract Feature With CountVectorizer cv = CountVectorizer() X = cv.fit_transform(X) # Fit the Data from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) #Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB clf = MultinomialNB() clf.fit(X_train,y_train) clf.score(X_test,y_test) #Alternative Usage of Saved Model # joblib.dump(clf, 'NB_spam_model.pkl') # NB_spam_model = open('NB_spam_model.pkl','rb') # clf = joblib.load(NB_spam_model) if request.method == 'POST': message = request.form['message'] data = [message] vect = cv.transform(data).toarray() my_prediction = clf.predict(vect) return render_template('result.html',prediction = my_prediction) if __name__ == '__main__': app.run(debug=True) from flask import Flask,render_template,url_for,requestimport pandas as pd import picklefrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.externals import joblib​​app = Flask(__name__)​@app.route('/')def home(): return render_template('home.html')​@app.route('/predict',methods=['POST'])def predict(): df= pd.read_csv(""spam.csv"", encoding=""latin-1"") df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True) # Features and Labels df['label'] = df['class'].map({'ham': 0, 'spam': 1}) X = df['message'] y = df['label'] # Extract Feature With CountVectorizer cv = CountVectorizer() X = cv.fit_transform(X) # Fit the Data from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) #Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB​ clf = MultinomialNB() clf.fit(X_train,y_train) clf.score(X_test,y_test) #Alternative Usage of Saved Model # joblib.dump(clf, 'NB_spam_model.pkl') # NB_spam_model = open('NB_spam_model.pkl','rb') # clf = joblib.load(NB_spam_model)​ if request.method == 'POST': message = request.form['message'] data = [message] vect = cv.transform(data).toarray() my_prediction = clf.predict(vect) return render_template('result.html',prediction = my_prediction)​​​if __name__ == '__main__': app.run(debug=True)​ home.html The following are the contents of the home.html file that will create a text form where a user can enter a message: <!DOCTYPE html> <html> <head> <title>Home</title> <!-- <link rel=""stylesheet"" type=""text/css"" href=""../static/css/styles.css""> --> <link rel=""stylesheet"" type=""text/css"" href=""{{ url_for('static', filename='css/styles.css') }}""> </head> <body> <header> <div class=""container""> <div id=""brandname""> Machine Learning App with Flask </div> <h2>Spam Detector For SMS Messages</h2> </div> </header> <div class=""ml-container""> <form action=""{{ url_for('predict')}}"" method=""POST""> <p>Enter Your Message Here</p> <!-- <input type=""text"" name=""comment""/> --> <textarea name=""message"" rows=""4"" cols=""50""></textarea> <br/> <input type=""submit"" class=""btn-info"" value=""predict""> </form> </div> </body> </html> <!DOCTYPE html><html><head> <title>Home</title> <!-- <link rel=""stylesheet"" type=""text/css"" href=""../static/css/styles.css""> --> <link rel=""stylesheet"" type=""text/css"" href=""{{ url_for('static', filename='css/styles.css') }}""></head><body>​ <header> <div class=""container""> <div id=""brandname""> Machine Learning App with Flask </div> <h2>Spam Detector For SMS Messages</h2> </div> </header>​ <div class=""ml-container"">​ <form action=""{{ url_for('predict')}}"" method=""POST""> <p>Enter Your Message Here</p> <!-- <input type=""text"" name=""comment""/> --> <textarea name=""message"" rows=""4"" cols=""50""></textarea> <br/>​ <input type=""submit"" class=""btn-info"" value=""predict""> </form> </div>​ ​</body></html> style.css In the header section of or html file (home.html), we loaded style.css file. CSS is to determine the appearance and style of HTML documents. style.css has to be saved in a sub-directory called static, which is the default directory where Flask looks for static files such as CSS. body{ font:15px/1.5 Arial, Helvetica,sans-serif; padding: 0px; background-color:#f4f3f3; } .container{ width:100%; margin: auto; overflow: hidden; } header{ background:#03A9F4;#35434a; border-bottom:#448AFF 3px solid; height:120px; width:100%; padding-top:30px; } .main-header{ text-align:center; background-color: blue; height:100px; width:100%; margin:0px; } #brandname{ float:left; font-size:30px; color: #fff; margin: 10px; } header h2{ text-align:center; color:#fff; } .btn-info {background-color: #2196F3; height:40px; width:100px;} /* Blue */ .btn-info:hover {background: #0b7dda;} .resultss{ border-radius: 15px 50px; background: #345fe4; padding: 20px; width: 200px; height: 150px; } body{ font:15px/1.5 Arial, Helvetica,sans-serif; padding: 0px; background-color:#f4f3f3;}​.container{ width:100%; margin: auto; overflow: hidden;}​header{ background:#03A9F4;#35434a; border-bottom:#448AFF 3px solid; height:120px; width:100%; padding-top:30px;​}​.main-header{ text-align:center; background-color: blue; height:100px; width:100%; margin:0px; }#brandname{ float:left; font-size:30px; color: #fff; margin: 10px;}​header h2{ text-align:center; color:#fff;​}​​​.btn-info {background-color: #2196F3; height:40px; width:100px;} /* Blue */.btn-info:hover {background: #0b7dda;}​​.resultss{ border-radius: 15px 50px; background: #345fe4; padding: 20px; width: 200px; height: 150px;}​ result.html Now, I will create a result.html file that will give us a result of our model prediction. It is the final step when we deploy a machine learning model. <!DOCTYPE html> <html> <head> <title></title> <link rel=""stylesheet"" type=""text/css"" href=""{{ url_for('static', filename='css/styles.css') }}""> </head> <body> <header> <div class=""container""> <div id=""brandname""> ML App </div> <h2>Spam Detector For SMS Messages</h2> </div> </header> <p style=""color:blue;font-size:20;text-align: center;""><b>Results for Comment</b></p> <div class=""results""> {% if prediction == 1%} <h2 style=""color:red;"">Spam</h2> {% elif prediction == 0%} <h2 style=""color:blue;"">Not a Spam (It is a Ham)</h2> {% endif %} </div> </body> </html> <!DOCTYPE html><html><head> <title></title> <link rel=""stylesheet"" type=""text/css"" href=""{{ url_for('static', filename='css/styles.css') }}""></head><body>​ <header> <div class=""container""> <div id=""brandname""> ML App </div> <h2>Spam Detector For SMS Messages</h2> </div> </header> <p style=""color:blue;font-size:20;text-align: center;""><b>Results for Comment</b></p> <div class=""results"">​​ {% if prediction == 1%} <h2 style=""color:red;"">Spam</h2> {% elif prediction == 0%} <h2 style=""color:blue;"">Not a Spam (It is a Ham)</h2> {% endif %}​ </div>​</body></html> After following all the steps to deploy a machine learning model, now you can simply run this program using your app.py file. You will see you output as follows: I hope, you like this article on Train and deploy a machine learning model. Feel free to ask your valuable questions in the comments section. Don’t forget to subscribe for my daily newsletters below to get emails notification of our daily newsletters is you like my work. Follow Us: Facebook Instagram";Deploy a Machine Learning Model
2020-07-16 17:35:34;Natural Language Processing or NLP is a field of Artificial Intelligence which focuses on enabling the systems for understanding and processing the human languages. In this article, I will use NLP to analyze my WhatsApp Chats. For some privacy reasons, I will use Person 1, Person 2 and so on in my WhatsApp Chats.If you have never exported your whatsapp chats before, don’t worry it’s very easy. For NLP of WhatsApp chats, you need to extract the whatsapp chats from your smartphone. You just need to open any chat in your whatsapp then select the export chat option. The text file you will get as a return will look like this:I will use two different approaches for the NLP of WhatsApp Chats. First, by focusing on the fundamentals of NLP and the other is by using the datetime stamp at the starting of every conversation.To analyze our whatsapp conversations, initially, our conversation needs to be formatted in the form of data. This involved a few basic steps in achieving the formation of data by creating a dictionary, constructed within two keys with each of the respective values with a list of the person tokenized conversations.The classification of tokenized conversations will ne be achieved by training a Naive Bayes Classification model or the training set with some pre-categorized chat styles conversations:Our trained model can be tested by using a test set or even by user input. Our model is trained in a way that can classify any tokenized sentence into different categories like Greetings, Statements, Emotions, questions, etc.‘Greet’ynQuestion’Now let’s run the model on WhatsApp data for counting the occurrences of each category of the tokenized conversations:We all use emojis, everyone, not only on WhatsApp but with any other chatting platform. Now let’s see what emojis are being used in most of the conversations.It’s very interesting to visualize how one person uses more emojis than the other person. This is the only way we express our emotions while having a whatsapp conversation.The plotting of sentiments against the datetime is not as easy as it looks. As there are many different sentiments on the same day, so the first step is to calculate the mean sentiment for each day and then grouping by datetime. So let’s see how we can do this:Even plotting the average of sentiments for each day will prove to be very messy. So let’s simply take a rolling average of 10 days, and then plot the average sentiment score:Now let’s have a look at the frequency of whatsapp chats which is not a part of NLP for Whatsapp but it is a part of time series analysis. We can use time series here to see the frequency of chats. First, need to create a colour pallete ordered by the total number of messages for each day.Ordered list of days according to total message countThis is essentially the current order of colours:Reorder colours according to their index position in the ‘days_freq’ list:I hope you liked this article on NLP for WhatsApp chats. Feel free to ask your valuable questions in the comments section. Don’t forget to subscribe for my daily newsletters below to get email notifications if you like my work.;https://thecleverprogrammer.com/2020/07/16/nlp-for-whatsapp-chats/;['nltk'];1.0;['NLP'];['NLP', 'Naive Bayes', 'Classification', 'AI'];['artificial intelligence', 'model', 'classif', 'natural language processing', 'naive bayes', 'rank', 'train', 'label'];"Natural Language Processing or NLP is a field of Artificial Intelligence which focuses on enabling the systems for understanding and processing the human languages. In this article, I will use NLP to analyze my WhatsApp Chats. For some privacy reasons, I will use Person 1, Person 2 and so on in my WhatsApp Chats. Get The Whatsapp Data for NLP If you have never exported your whatsapp chats before, don’t worry it’s very easy. For NLP of WhatsApp chats, you need to extract the whatsapp chats from your smartphone. You just need to open any chat in your whatsapp then select the export chat option. The text file you will get as a return will look like this: [""[02/07/2017, 5:47:33 pm] Person_1: Hey there! This is the first message"", ""[02/07/2017, 5:48:24 pm] Person_1: This is the second message"", ""[02/07/2017, 5:48:44 pm] Person_1: Third…"", ""[02/07/2017, 8:10:52 pm] Person_2: Hey Person_1! This is the fourth message"", ""[02/07/2017, 8:14:11 pm] Person_2: Fifth …etc""]Code language: Python (python) I will use two different approaches for the NLP of WhatsApp Chats. First, by focusing on the fundamentals of NLP and the other is by using the datetime stamp at the starting of every conversation. Formatting Whatsapp Chats for NLP To analyze our whatsapp conversations, initially, our conversation needs to be formatted in the form of data. This involved a few basic steps in achieving the formation of data by creating a dictionary, constructed within two keys with each of the respective values with a list of the person tokenized conversations. ppl=defaultdict(list) for line in content: try: person = line.split(':')[2][7:] text = nltk.sent_tokenize(':'.join(line.split(':')[3:])) ppl[person].extend(text) # If key exists (person), extend list with value (text), # if not create a new key, with value added to list except: print(line) # in case reading a line fails, examine why passCode language: Python (python) ppl = {'Person_1' : ['This is message 1', 'Another message', 'Hi Person_2', ... , 'My last tokenised message in the chat'] , 'Person_2':['Hello Person_1!', 'How's it going?', 'Another messsage', ...]}Code language: Python (python) Classification of Dialogues The classification of tokenized conversations will ne be achieved by training a Naive Bayes Classification model or the training set with some pre-categorized chat styles conversations: posts = nltk.corpus.nps_chat.xml_posts() def extract_features(post): features = {} for word in nltk.word_tokenize(post): features['contains({})'.format(word.lower())] = True return features fposts = [(extract_features(p.text), p.get('class')) for p in posts] test_size = int(len(fposts) * 0.1) train_set, test_set = fposts[test_size:], fposts[:test_size] classifier = nltk.NaiveBayesClassifier.train(train_set)Code language: Python (python) Our trained model can be tested by using a test set or even by user input. Our model is trained in a way that can classify any tokenized sentence into different categories like Greetings, Statements, Emotions, questions, etc. classifier.classify(extract_features('Hi there!'))Code language: Python (python) ‘Greet’ classifier.classify(extract_features('Do you want to watch a film later?'))Code language: Python (python) ynQuestion’ Now let’s run the model on WhatsApp data for counting the occurrences of each category of the tokenized conversations: ax = df.T.plot(kind='bar', figsize=(10, 7), legend=True, fontsize=16, color=['y','g']) ax.set_title(""Frequency of Message Categories"", fontsize= 18) ax.set_xlabel(""Message Category"", fontsize=14) ax.set_ylabel(""Frequency"", fontsize=14) #plt.savefig('plots/cat_message') # uncomment to save plt.show()Code language: Python (python) NLP for WhatsApp Chats Emotions We all use emojis, everyone, not only on WhatsApp but with any other chatting platform. Now let’s see what emojis are being used in most of the conversations. def extract_emojis(str): return ''.join(c for c in str if c in emoji.UNICODE_EMOJI) for key, val in ppl.items(): emojis=extract_emojis(str(ppl[key])) count = Counter(emojis).most_common()[:10] print(""{}'s emojis:\n {} \n"".format(key, emojis)) print(""Most common: {}\n\n"".format(count))Code language: Python (python) Person_1's emojis: 😏🕺🏼🍻😮🤤😭😏💁🏼😏👏🙏🐳🐋😏😱🙄😳☺😭🚀💫⭐✨💥🍕🍕😏😊😘🙄💭😭😭😭😭😏✅😱😏😭🙄😘😘😘😘😭😭😭😭😭😭🍸😘😘😅😘😭👏💪😭🙅♂🙆♂🙋♂💁♂😘🎉🎉🎉🎉🎉🎉🎉🎉🎉😊😘🙄😴😉🕺🏼😭😎😭🙄😘😘😘👏😩😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭📞🎉😘😀😚😱👏🏏😏🚂🤓👏🙄🙌😘😘😏😭😭🙌😏😔😭😘🤰🏼😘🙄🙄😰🙋🏼♀😭🙄😍🤓👏😭😭😭😭😭😘🍕💩☹🙋🏼♀😘😴🚲😘😘😘😭☹😗😙😚😚🤔🤝🍻🎂✈😘👌😰😘🔺🔥😩😘💨😚😱😢😭😭😭😭😭😭😭😭😭😭😗🤔🤔🤔🤔🤔🤔🤔👀👏😇😗😚😘🙄☹😘😩😚😇⚡💥🔥☹😭😩😭😰😱😅😅😍😞👏👏👏👏👏👏😘😘😊😘😘😍😘🙄😏😘😘🙄😘👀😘😘👀😘😘😘🥕😘😘😘😘😘😘😘😘😭😘😘🖕🏻😘🌇😘😘😘🙄😪🤧😘🥚😘😘😘😘😘😘😘😘😘😘😱😘😭😭😘🆘❌‼⭕♨🚫⛔🚷🖍📌📍✂📕📮🔻☎⏰🚨🚒🚗🥊🏓🍷🌶🍅🍎☄🌹🎒👠⛑😎😘😘😘😘😙👀🙄😭😭😭😭😭😭😘😘😘🥚😘🙄🙄😘 Most common: [('😘', 77), ('😭', 68), ('🙄', 16), ('👏', 13), ('😏', 11), ('🎉', 10), ('🤔', 8), ('🏼', 6), ('😱', 6), ('😚', 6)] Person_2's emojis: 😁🙂🤓😅😀👍😂😬👻😁😂✌😴😬😬🙄🎉✌😂😪😒😬😐😬😁😬😁😏🤢😁😒😁😏😘😒😅😂💪👊😬😏💁♂😴😬😅😏😆🐬🙁😬🐬😁😁✌😁😁👊👮😕✌😁😁😐✌😱😩😬✌✌😂😘💇♂😁😁😁😅🙂😬🙁😁😁😕😴😁😏😁😘😅😴🙂🎉🎉🎉😁🚀🚀🚀😁😱✌🍕🍕😏👍😂😁😑😘🙄😁😘😬😂😁🎉🎉🎉✌☺😑😁😬🙂😱😂✌☺😁👊😁👊👍😏💁🏼😅😁😁😁😕✌🤓😂😘😁😁✌✌😘🙁😘😁🎉✌😘😘😘😘😅😁😁😁😁😂🙁😏😔✌😘😁😐😁✌🙂👍😘😬😁✌😂🙋🏼😎😁🤓💩😂😘😐😏✌🙂✌😘✌😁🤔✌🏋🏼♀😬🙂😁👊😁✌😁😁😏🤜🤛☹⚡😬🎯💪😁☹😞👋🙂😘😴😁😁🎉😁✌🙂😘😬✌👍😁💃👍👍👍👍😢☹🙁🙁👋😏😬😁✌😘🙁👍🙌🤓😏🎉💁♂😁😑😁😁😁🎉😁☹😕😢😬✌😞😬✌😬👍😁😏😁👍👍👊😁😧😘😪😁🎉🎉🎉😕👍😁👉😁👊😏😁😁😂😂😂🤳👌😁👌🙋🏼♀👋😐😐😁🙁😕👊😁🤔🤗🤙👍😬🤔🎉🎅🏻👍😁😁😁🤚😘🤚👍👊🙁🙁🙁🙄😘🙋🏼♀🤣😘🎉😬🙁😖💁♂😂😒🎉😗👏🤔🤐🙄👊😘😉😘🙂☹💰😏🎉😑😬👍👍👎🙋♂💁♂😁😁🙂☹🤔🦄🦄😬😆😴😁😁😁😍🏄♀👀😁🏄♀👍😬👊😬🤔😁🙄👌👍😫☹🤗😩👀😁💰🤔👍😁😰😳😣😟😘👀🤗🙂😅👍🤔🙂😁😁😣🕺😮🙂☹☹😑🤘☹😬🍳😘😬😘🤘🙋♂🙁🍓😢😁😂😂😂😁😘🐑😚😚😚🤞😁🙄😁🙋♂😴😘👍😁👊😑😒👍😑😬👍👍👍😕☹😟💇♀👏🎉😏😁😚🤔👍👍😁😏👍😁😚😁🎉😬🙂😬😁🔥🤝☹🙌😏💁♂😁😁😁😁😁🙁😭🙂😬😘🙂😁😬👍☺🙁😂👀👌🙌😁💁🏼♀😁😬👍😕🙂😗😁😕🙁👀😁👏🎉😩😕🙁😊😴🤞😚😩😩😩😁😬👍👍😬😚😁😱👻👽😑😁😴🤒😁🙁👊🤓☹😁🤙😁👽👊😊🤙😁☹🙄😇🙂😁😩☹😚😏👍🙁👋😟😁☹😚🤔😧🙁☹🙃🙂👋🙂👍👍😁🤙👍💰🙂😢🤙💰😚👍🤔🤣🤣🤣🎉😢😏😬🤓👊💁♂😁😁😁👍🔥🤙😁👉😗😁⚡💆♀⚡👏😚😘🤔☹🤝😢😳😳😉👍☺👊☹⚡⚡⚡☹☹☹👍☹😚🔥🔥😢💰😁😬👊🤔👻🙌💁🏼♀😒😫👍👊😇🙂🤔🤙☹😪😉👍😁💪😭😁💩🤤😚☹☹👊🤙😚😘🙏🤥😁👍👍😚🤗😁🙄🙄😁👍😁😯😚👍🙄🙌🤔😁😘👍👊😱😏👍😘😁🎉😭😁😚😘😴👍😏🤔🤔😏🤢😘😭😭😚😬👍😘👊👌😘😁😁😚👋😁✋☝😭🤔👍😘🤙💁🏼♀😘😘👍👀👋😘😘😘😘😘😘😘😘🙁🙁👍😘😁😚👊👍😬👍👍🎉👍😋😘😘😘😘😘😘😘😘☹😘😁👍😁🤙👏👍😚😘😘😘😘😘😘👍💁🏼♀👍😘😏🤔👍👍👍😘👍😁😘👊👍👍👍👍☹👍👍👍👍😘👍😴🤙😘😘😘😘😘😘😘😘😕👊👍👍😁😘😚👆💁♀😴😘👊😥👊👍😅🙂👊🤙😘😘😘😘😘😘😘😘😲😘👍🤔😫🤣🍳😎😚😢😯💃👍🙄👍👍💇♂👊😚😚😘👍🙄😘😚😘😢🛎😚🙏😂😘😘😘👌👍🤷♂😂👍😕👍😘😘😘😘👏👊😅😉💤👍😁😚👍🤙🤓🤗😘😁💃😏😘😘😬💁♂😂☹😁👍😘 Most common: [('😁', 138), ('😘', 103), ('👍', 91), ('😬', 42), ('👊', 29), ('☹', 29), ('😚', 28), ('✌', 27), ('😏', 25), ('🙂', 24)] It’s very interesting to visualize how one person uses more emojis than the other person. This is the only way we express our emotions while having a whatsapp conversation. Sentiment Against Time The plotting of sentiments against the datetime is not as easy as it looks. As there are many different sentiments on the same day, so the first step is to calculate the mean sentiment for each day and then grouping by datetime. So let’s see how we can do this: df= pd.DataFrame(final).T # convert dictionary to a dataframe, makes process of plotting straightforward df.columns = ['pol', 'name', 'date', 'token'] df['pol'] = df['pol'].apply(lambda x : float(x)) # convert polarity to a float df3 = df.groupby(['date'], as_index=False).agg('mean') df3['name'] = 'Combined' final =pd.concat([df2, df3]) final['date'] = pd.to_datetime(final.date, format='%d/%m/%Y') # need to chnage 'date' to a datetime object final = final.sort_values('date') final['x'] = final['date'].rank(method='dense', ascending=True).astype(int) final[:6]Code language: Python (python) datenamepolx172017-07-02Combined0.3211621342017-07-02Person_10.2984901352017-07-02Person_20.3417731592017-07-03Person_20.2494892292017-07-03Combined0.2714582582017-07-03Person_10.3373672 Even plotting the average of sentiments for each day will prove to be very messy. So let’s simply take a rolling average of 10 days, and then plot the average sentiment score: sns.set_style(""whitegrid"") fig, ax = plt.subplots(figsize=(12,8)) colours=['b','y','g'] i=0 for label, df in final.groupby('name'): new=df.reset_index() new['rol'] = new['pol'].rolling(10).mean() # rolling mean calculation on a 10 day basis g = new.plot(x='date', y='rol', ax=ax, label=label, color=colours[i]) # rolling mean plot plt.scatter(df['date'].tolist(), df['pol'], color=colours[i], alpha=0.2) # underlying scatter plot i+=1 ax.set_ybound(lower=-0.1, upper=0.4) ax.set_xlabel('Date', fontsize=15) ax.set_ylabel('Compound Sentiment', fontsize=15) g.set_title('10 Day Rolling Mean Sentiment', fontsize=18)Code language: Python (python) Frequency of Chats Now let’s have a look at the frequency of whatsapp chats which is not a part of NLP for Whatsapp but it is a part of time series analysis. We can use time series here to see the frequency of chats. First, need to create a colour pallete ordered by the total number of messages for each day. pal = sns.cubehelix_palette(7, rot=-.25, light=.7)​x pal = sns.cubehelix_palette(7, rot=-.25, light=.7) Ordered list of days according to total message count days_freq = list(df.day.value_counts().index) days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']Code language: Python (python) This is essentially the current order of colours: lst = list(zip(days, pal[::-1])) lstCode language: Python (python) [('Monday', [0.12071162840208301, 0.14526386650440642, 0.2463679091477368]), ('Tuesday', [0.18152581198633005, 0.24364059111738742, 0.37281834227732574]), ('Wednesday', [0.2426591079772084, 0.3511228226876375, 0.4852103253459974]), ('Thursday', [0.30463866738797124, 0.45571986933681846, 0.5751187147066701]), ('Friday', [0.37810168111401876, 0.5633546614344814, 0.6530658354036274]), ('Saturday', [0.46091631066717925, 0.662287611911293, 0.7165315069314769]), ('Sunday', [0.5632111255041908, 0.758620966612444, 0.7764634182455044])] Reorder colours according to their index position in the ‘days_freq’ list: pal_reorder=[] for i in days: #print(i) j=0 for day in days_freq: if i == day: #print(lst[j][1]) pal_reorder.append(lst[j][1]) j+=1 pal_reorder # colours ordered according to total message count for the dayCode language: Python (python) [[0.30463866738797124, 0.45571986933681846, 0.5751187147066701], [0.18152581198633005, 0.24364059111738742, 0.37281834227732574], [0.12071162840208301, 0.14526386650440642, 0.2463679091477368], [0.2426591079772084, 0.3511228226876375, 0.4852103253459974], [0.37810168111401876, 0.5633546614344814, 0.6530658354036274], [0.5632111255041908, 0.758620966612444, 0.7764634182455044], [0.46091631066717925, 0.662287611911293, 0.7165315069314769]] sns.set(style=""white"", rc={""axes.facecolor"": (0, 0, 0, 0)}) pal = sns.cubehelix_palette(7, rot=-.25, light=.7) g = sns.FacetGrid(df[(df.float_time &gt; 8)], row=""day"", hue=""day"", # change ""day"" to year_month if required aspect=10, size=1.5, palette=pal_reorder, xlim=(7,24)) # Draw the densities in a few steps g.map(sns.kdeplot, ""float_time"", clip_on=False, shade=True, alpha=1, lw=1.5, bw=.2) g.map(sns.kdeplot, ""float_time"", clip_on=False, color=""w"", lw=3, bw=.2) g.map(plt.axhline, y=0, lw=1, clip_on=False) # Define and use a simple function to label the plot in axes coordinates def label(x, color, label): ax = plt.gca() ax.text(0, 0.1, label, fontweight=""bold"", color=color, ha=""left"", va=""center"", transform=ax.transAxes, size=18) g.map(label, ""float_time"") g.set_xlabels('Time of Day', fontsize=30) g.set_xticklabels(fontsize=20) # Set the subplots to overlap g.fig.subplots_adjust(hspace=-0.5) g.fig.suptitle('Message Density by Time and Day of the Week, Shaded by Total Message Count', fontsize=22) g.set_titles("""") g.set(yticks=[]) g.despine(bottom=True, left=True)Code language: Python (python) I hope you liked this article on NLP for WhatsApp chats. Feel free to ask your valuable questions in the comments section. Don’t forget to subscribe for my daily newsletters below to get email notifications if you like my work. Follow Us: Facebook Instagram";NLP For WhatsApp Chats
2020-07-17 10:54:51;In this article, I will take you through a very powerful algorithm in Machine Learning, which is the Grid Search Algorithm. It is mostly used in hyperparameters tuning and models selection in Machine Learning. Here I will teach you how to implement this algorithm using python. At the end of this article, you will learn how to apply it in a real-life problem and how to choose the most effective hyperparameters for our Machine Learning model for great accuracy.;https://thecleverprogrammer.com/2020/07/17/grid-search-for-model-tuning/;['keras', 'sklearn'];1.0;['CV'];['CV', 'DL', 'ML', 'ReLu', 'Classification'];['epoch', 'fit', 'model', 'loss', 'machine learning', 'relu', 'classif', 'layer', 'deep learning', 'training data', 'train', 'label'];"In this article, I will take you through a very powerful algorithm in Machine Learning, which is the Grid Search Algorithm. It is mostly used in hyperparameters tuning and models selection in Machine Learning. Here I will teach you how to implement this algorithm using python. At the end of this article, you will learn how to apply it in a real-life problem and how to choose the most effective hyperparameters for our Machine Learning model for great accuracy. What is Grid Search? Grid Search is used in Fine-tuning a Machine Learning model. Let’s assume that you have a shortlist of promising models. You now need to fine-tune them. So how you will do that? One option would be to fiddle with the hyperparameters manually until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations. Instead, you can use the Grid Search Algorithm to look for you. All you need to do is tell it which hyperparameters you want it to experiment with and what values to try out, and it will use cross-validation to evaluate all the possible combinations of hyperparameters values. You should also know that you can treat some of the data preparation steps as hyperparameters. For example, the grid search will automatically find out whether or not to add a feature you were not sure about. It may similarly be used to automatically find the best way to handle outliers, missing features, feature selection, and more. Training a Deep Learning Model Now let’s see how we can implement the Grid Search Algorithm in Deep Learning. The dataset I will use here is based on diabetes, which can be easily downloaded from here. Now let’s start with importing the necessary libraries: from sklearn.model_selection import GridSearchCV, KFold from keras.models import Sequential from keras.layers import Dense, Dropout from keras.wrappers.scikit_learn import KerasClassifier from keras.optimizers import Adam import sys import pandas as pd import numpy as npCode language: Python (python) Now let’s import and read the dataset. columns = ['num_pregnant', 'glucose_concentration', 'blood_pressure', 'skin_thickness', 'serum_insulin', 'BMI', 'pedigree_function', 'age', 'class'] data_path = pd.read_csv(""pima-indians-diabetes.csv"") df = pd.read_csv(data_path, names=columns) df.head()Code language: Python (python) Now let’s remove the unnecessary rows and replace the NaN values with 0. # Remove first 9 non-data rows df = df.iloc[9:] # Replace NaN (Not a Number) values with 0 in each column for col in columns: df[col].replace(0, np.NaN, inplace=True) df.dropna(inplace=True) # Drop all rows with missing values dataset = df.values # Convert dataframe to numpy arrayCode language: Python (python) Now I will divide the dataset into features and labels by using the Standard Scalar method. X = dataset[:,0:8] Y = dataset[:, 8].astype(int) # Normalize the data using sklearn StandardScaler from sklearn.preprocessing import StandardScaler scaler = StandardScaler().fit(X) # Transform and display the training data X_standardized = scaler.transform(X) data = pd.DataFrame(X_standardized)Code language: Python (python) Now I will create function to create a deep learning model. def create_model(learn_rate, dropout_rate): # Create model model = Sequential() model.add(Dense(8, input_dim=8, kernel_initializer='normal', activation='relu')) model.add(Dropout(dropout_rate)) model.add(Dense(4, input_dim=8, kernel_initializer='normal', activation='relu')) model.add(Dropout(dropout_rate)) model.add(Dense(1, activation='sigmoid')) # Compile the model adam = Adam(lr=learn_rate) model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy']) return modelCode language: Python (python) Now, I will implement a grid search algorithm but to understand it better let’s first train our model without implementing it. # Declare parameter values dropout_rate = 0.1 epochs = 1 batch_size = 20 learn_rate = 0.001 # Create the model object by calling the create_model function we created above model = create_model(learn_rate, dropout_rate) # Fit the model onto the training data model.fit(X_standardized, Y, batch_size=batch_size, epochs=epochs, verbose=1)Code language: Python (python) Epoch 1/1 130/130 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.6000 So, we have got an accuracy of 60 percent without implementing the grid search algorithm let’s see how much we can improve the accuracy after implementing the algorithm in our deep learning model. Implementing Grid Search Algorithm First I will modify the function that I created above because to use our Algorithm we need to apply some parameters to the create_model function. def create_model(learn_rate, dropout_rate): # Create model model = Sequential() model.add(Dense(8, input_dim=8, kernel_initializer='normal', activation='relu')) model.add(Dropout(dropout_rate)) model.add(Dense(4, input_dim=8, kernel_initializer='normal', activation='relu')) model.add(Dropout(dropout_rate)) model.add(Dense(1, activation='sigmoid')) # Compile the model adam = Adam(lr=learn_rate) model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy']) return model # Create the model model = KerasClassifier(build_fn=create_model, verbose=1)Code language: Python (python) Now let’s apply our Algorithm and fit the data on it: # Define the parameters that you wish to use in your Grid Search along # with the list of values that you wish to try out learn_rate = [0.001, 0.02, 0.2] dropout_rate = [0.0, 0.2, 0.4] batch_size = [10, 20, 30] epochs = [1, 5, 10] seed = 42 # Make a dictionary of the grid search parameters param_grid = dict(learn_rate=learn_rate, dropout_rate=dropout_rate, batch_size=batch_size, epochs=epochs ) # Build and fit the GridSearchCV grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=KFold(random_state=seed), verbose=10) grid_results = grid.fit(X_standardized, Y) # Summarize the results in a readable format print(""Best: {0}, using {1}"".format(grid_results.best_score_, grid_results.best_params_)) means = grid_results.cv_results_['mean_test_score'] stds = grid_results.cv_results_['std_test_score'] params = grid_results.cv_results_['params'] for mean, stdev, param in zip(means, stds, params): print('{0} ({1}) with: {2}'.format(mean, stdev, param))Code language: Python (python) Best: 0.7959183612648322, using {'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'learn_rate': 0.02} In the output, you can see it provides us with the best hyperparameters combination which improves the accuracy of our model that is 75 percent. I hope you liked this article on the Grid Search Algorithm in Deep Learning. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Grid Search for Model Tuning
2020-07-18 21:55:20;"Fake News is one of the major concerns in our society right now. It is a very widespread issue that even the most leading media sometimes gets with the trap of Fake News. If it’s difficult for media channels to detect fake news then it’s next to difficult for a general citizen. As a part of a Machine Learning project, in this article, I will show you Fake News Detection with Machine Learning. I will use all the misinformation that we heard from some previous months about coronavirus. So at the end of this article, you will be able to create a fake news detection model on coronavirus.The data I will use in this article is collected from more than 1000 news articles and posts on social media platforms on coronavirus. You can download the dataset from here.Now I will Import all the libraries that we need for Fake News Detection; then I will Import the dataset using the pandas library in Python. Then I will prepare the data using the python pandas:I will create a number of new features based on the titles and body of news articles. Now let’s go through all the features one by one.Now I will count the number of capital letters in the Title of each article; then I will count the percentage of each capital letter in the body of all the articles. Counting the Capital letters will allow is to know what is the Title based on so that we can use that data for use as a categorical feature.On average, the fake news has a huge amount of words in capital letters in the title of the articles. This gives the impression that the people who write fake news try to impress the audience by the titles of their articles.Stop Words are the words that the search engines are programmed to ignore. For example – a the, at, on, which etc. Now I will count the number of stop words present in the title of each article:The titles of the articles of fake news contain fewer stop words then the titles of real news.Now I will count the number of proper noun in the title of each article:The titles of the fake news articles have more use of proper nouns than the titles of real news.";https://thecleverprogrammer.com/2020/07/18/fake-news-detection-model/;['sklearn', 'nltk'];1.0;['NN'];['ML', 'Classification', 'NN'];['detect', 'fit', 'model', 'machine learning', 'classif', 'train', 'label'];"Fake News is one of the major concerns in our society right now. It is a very widespread issue that even the most leading media sometimes gets with the trap of Fake News. If it’s difficult for media channels to detect fake news then it’s next to difficult for a general citizen. As a part of a Machine Learning project, in this article, I will show you Fake News Detection with Machine Learning. I will use all the misinformation that we heard from some previous months about coronavirus. So at the end of this article, you will be able to create a fake news detection model on coronavirus. The data I will use in this article is collected from more than 1000 news articles and posts on social media platforms on coronavirus. You can download the dataset from here. Now I will Import all the libraries that we need for Fake News Detection; then I will Import the dataset using the pandas library in Python. Then I will prepare the data using the python pandas: from nltk.corpus import stopwords stop_words = set(stopwords.words('english')) from nltk.tag import pos_tag from nltk import word_tokenize from collections import Counter import textstat from lexicalrichness import LexicalRichness import plotly.express as px import plotly.figure_factory as ff import plotly.graph_objects as go from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score from sklearn.svm import LinearSVC from sklearn.preprocessing import StandardScaler from sklearn.model_selection import cross_val_score pd.set_option('display.max_columns', 500) df = pd.read_csv('data/corona_fake.csv') df.loc[df['label'] == 'Fake', ['label']] = 'FAKE' df.loc[df['label'] == 'fake', ['label']] = 'FAKE' df.loc[df['source'] == 'facebook', ['source']] = 'Facebook' df.text.fillna(df.title, inplace=True) df.loc[5]['label'] = 'FAKE' df.loc[15]['label'] = 'TRUE' df.loc[43]['label'] = 'FAKE' df.loc[131]['label'] = 'TRUE' df.loc[242]['label'] = 'FAKE' df = df.sample(frac=1).reset_index(drop=True) df.title.fillna('missing', inplace=True) df.source.fillna('missing', inplace=True)Code language: Python (python) I will create a number of new features based on the titles and body of news articles. Now let’s go through all the features one by one. Capital Letters in the Title Now I will count the number of capital letters in the Title of each article; then I will count the percentage of each capital letter in the body of all the articles. Counting the Capital letters will allow is to know what is the Title based on so that we can use that data for use as a categorical feature. df['title_num_uppercase'] = df['title'].str.count(r'[A-Z]') df['text_num_uppercase'] = df['text'].str.count(r'[A-Z]') df['text_len'] = df['text'].str.len() df['text_pct_uppercase'] = df.text_num_uppercase.div(df.text_len) x1 = df.loc[df['label']=='TRUE']['title_num_uppercase'] x2 = df.loc[df['label'] == 'FAKE']['title_num_uppercase'] group_labels = ['TRUE', 'FAKE'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Distribution of Uppercase in title', template=""plotly_white"") fig.show()Code language: Python (python) fig = go.Figure() fig.add_trace(go.Box(y=x1, name='TRUE', marker_color = 'rgb(0, 0, 100)')) fig.add_trace(go.Box(y=x2, name = 'FAKE', marker_color = 'rgb(0, 200, 200)')) fig.update_layout(title_text='Box plot of Capital Letter in title', template=""plotly_white"") fig.show()Code language: Python (python) On average, the fake news has a huge amount of words in capital letters in the title of the articles. This gives the impression that the people who write fake news try to impress the audience by the titles of their articles. Stop Words Stop Words are the words that the search engines are programmed to ignore. For example – a the, at, on, which etc. Now I will count the number of stop words present in the title of each article: df['title_num_stop_words'] = df['title'].str.split().apply(lambda x: len(set(x) &amp; stop_words)) df['text_num_stop_words'] = df['text'].str.split().apply(lambda x: len(set(x) &amp; stop_words)) df['text_word_count'] = df['text'].apply(lambda x: len(str(x).split())) df['text_pct_stop_words'] = df['text_num_stop_words'] / df['text_word_count'] x1 = df.loc[df['label']=='TRUE']['title_num_stop_words'] x2 = df.loc[df['label'] == 'FAKE']['title_num_stop_words'] group_labels = ['TRUE', 'FAKE'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Distribution of Stop Words in title', template=""plotly_white"") fig.show()Code language: Python (python) fig = go.Figure() fig.add_trace(go.Box(y=x1, name='TRUE', marker_color = 'rgb(0, 0, 100)')) fig.add_trace(go.Box(y=x2, name = 'FAKE', marker_color = 'rgb(0, 200, 200)')) fig.update_layout(title_text='Box plot of Stop Words in title', template=""plotly_white"") fig.show()Code language: Python (python) The titles of the articles of fake news contain fewer stop words then the titles of real news. Proper Noun Now I will count the number of proper noun in the title of each article: df.drop(['text_num_uppercase', 'text_len', 'text_num_stop_words', 'text_word_count'], axis=1, inplace=True) df['token'] = df.apply(lambda row: nltk.word_tokenize(row['title']), axis=1) df['pos_tags'] = df.apply(lambda row: nltk.pos_tag(row['token']), axis=1) tag_count_df = pd.DataFrame(df['pos_tags'].map(lambda x: Counter(tag[1] for tag in x)).to_list()) df = pd.concat([df, tag_count_df], axis=1).fillna(0).drop(['pos_tags', 'token'], axis=1) df = df[['title', 'text', 'source', 'label', 'title_num_uppercase', 'text_pct_uppercase', 'title_num_stop_words', 'text_pct_stop_words', 'NNP']].rename(columns={'NNP': 'NNP_title'}) x1 = df.loc[df['label']=='TRUE']['NNP_title'] x2 = df.loc[df['label'] == 'FAKE']['NNP_title'] group_labels = ['TRUE', 'FAKE'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Number of Proper nouns in title', template=""plotly_white"") fig.show()Code language: Python (python) fig = go.Figure() fig.add_trace(go.Box(y=x1, name='TRUE', marker_color = 'rgb(0, 0, 100)')) fig.add_trace(go.Box(y=x2, name = 'FAKE', marker_color = 'rgb(0, 200, 200)')) fig.update_layout(title_text='Box plot of Proper nouns in title', template=""plotly_white"") fig.show()Code language: Python (python) The titles of the fake news articles have more use of proper nouns than the titles of real news. Analysis of Titles of Fake News Articles The above analysis showed us that the authors of fake news write very less amount of stop words and focus on a catchy title by increasing the number of proper nouns in their titles. Classifying Features To classify the features of fake news and real news, we need to compute a lot of content-based features within the body of articles, let’s go through all the features one by one. I will use a part-of-speech tagger and keep a count on how many times each tag is written in the articles: df['token'] = df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1) df['pos_tags'] = df.apply(lambda row: nltk.pos_tag(row['token']), axis=1) tag_count_df = pd.DataFrame(df['pos_tags'].map(lambda x: Counter(tag[1] for tag in x)).to_list()) df = pd.concat([df, tag_count_df], axis=1).fillna(0).drop(['pos_tags', 'token'], axis=1)Code language: Python (python) Number of negations and interrogatives in the body of each article: df['num_negation'] = df['text'].str.lower().str.count(""no|not|never|none|nothing|nobody|neither|nowhere|hardly|scarcely|barely|doesn't|isn't|wasn't|shouldn't|wouldn't|couldn't|won't|can't|don't"") df['num_interrogatives_title'] = df['title'].str.lower().str.count(""what|who|when|where|which|why|how"") df['num_interrogatives_text'] = df['text'].str.lower().str.count(""what|who|when|where|which|why|how"")Code language: Python (python) Now, I will use textstat library in python to store the readability score of each article: reading_ease = [] for doc in df['text']: reading_ease.append(textstat.flesch_reading_ease(doc)) smog = [] for doc in df['text']: smog.append(textstat.smog_index(doc)) kincaid_grade = [] for doc in df['text']: kincaid_grade.append(textstat.flesch_kincaid_grade(doc)) liau_index = [] for doc in df['text']: liau_index.append(textstat.coleman_liau_index(doc)) readability_index = [] for doc in df['text']: readability_index.append(textstat.automated_readability_index(doc)) readability_score = [] for doc in df['text']: readability_score.append(textstat.dale_chall_readability_score(doc)) difficult_words = [] for doc in df['text']: difficult_words.append(textstat.difficult_words(doc)) write_formula = [] for doc in df['text']: write_formula.append(textstat.linsear_write_formula(doc)) gunning_fog = [] for doc in df['text']: gunning_fog.append(textstat.gunning_fog(doc)) text_standard = [] for doc in df['text']: text_standard.append(textstat.text_standard(doc)) df['flesch_reading_ease'] = reading_ease df['smog_index'] = smog df['flesch_kincaid_grade'] = kincaid_grade df['automated_readability_index'] = readability_index df['dale_chall_readability_score'] = readability_score df['difficult_words'] = difficult_words df['linsear_write_formula'] = write_formula df['gunning_fog'] = gunning_fog df['text_standard'] = text_standardCode language: Python (python) Now I will calculate the Type-Token ratio, which is the total number of unique words in an article: ttr = [] for doc in df['text']: lex = LexicalRichness(doc) ttr.append(lex.ttr) df['ttr'] = ttrCode language: Python (python) Now, I will store the amount of type of words, including – tentative words, casual words, emotional words, and powerful words in each article: df['num_powerWords_text'] = df['text'].str.lower().str.count('improve|trust|immediately|discover|profit|learn|know|understand|powerful|best|win|more|bonus|exclusive|extra|you|free|health|guarantee|new|proven|safety|money|now|today|results|protect|help|easy|amazing|latest|extraordinary|how to|worst|ultimate|hot|first|big|anniversary|premiere|basic|complete|save|plus|create') df['num_casualWords_text'] = df['text'].str.lower().str.count('make|because|how|why|change|use|since|reason|therefore|result') df['num_tentativeWords_text'] = df['text'].str.lower().str.count('may|might|can|could|possibly|probably|it is likely|it is unlikely|it is possible|it is probable|tends to|appears to|suggests that|seems to') df['num_emotionWords_text'] = df['text'].str.lower().str.count('ordeal|outrageous|provoke|repulsive|scandal|severe|shameful|shocking|terrible|tragic|unreliable|unstable|wicked|aggravate|agony|appalled|atrocious|corruption|damage|disastrous|disgusted|dreadful|eliminate|harmful|harsh|inconsiderate|enraged|offensive|aggressive|frustrated|controlling|resentful|anger|sad|fear|malicious|infuriated|critical|violent|vindictive|furious|contrary|condemning|sarcastic|poisonous|jealous|retaliating|desperate|alienated|unjustified|violated')Code language: Python (python) Exploratory Data Analysis for Fake News Capital Letters in the body of each article: x1 = df.loc[df['label']=='TRUE']['text_pct_uppercase'] x2 = df.loc[df['label'] == 'FAKE']['text_pct_uppercase'] group_labels = ['TRUE', 'FAKE'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Percentage of Capital Letter in Article body', template=""plotly_white"") fig.show()Code language: Python (python) Stop Words in the body of each article: x1 = df.loc[df['label']=='TRUE']['text_pct_stop_words'] x2 = df.loc[df['label'] == 'FAKE']['text_pct_stop_words'] group_labels = ['TRUE', 'FAKE'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Percentage of Stop Words in Text', template=""plotly_white"") fig.show()Code language: Python (python) The Number of proper nouns used in the body of each article: x1 = df.loc[df['label']=='TRUE']['NNP'] x2 = df.loc[df['label'] == 'FAKE']['NNP'] group_labels = ['TRUE', 'FAKE'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Number of Proper noun in Article Body', template=""plotly_white"") fig.show()Code language: Python (python) The Number of Negative Words in the body of each article: x1 = df.loc[df['label']=='TRUE']['num_negation'] x2 = df.loc[df['label'] == 'FAKE']['num_negation'] group_labels = ['TRUE', 'FAKE'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Number of Negations in Text', template=""plotly_white"") fig.show()Code language: Python (python) Type-Token Ratio of each article: x1 = df.loc[df['label']=='TRUE']['ttr'] x2 = df.loc[df['label'] == 'FAKE']['ttr'] group_labels = ['TRUE', 'FAKE'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Type-token ratio in Article Bodies', template=""plotly_white"") fig.show()Code language: Python (python) Harvard Health Publishing Vs. Natural News Always remember that the Natural News is a conspiracy theory. I will label all the collected article as Harvard News and Natural news to explore more features, as the features I counted above are not that much helpful, so we will classify more features: x1 = df.loc[df['source']=='https://www.health.harvard.edu/']['text_pct_stop_words'] x2 = df.loc[df['source']=='https://www.naturalnews.com/']['text_pct_stop_words'] group_labels = ['Health Harvard', 'Natural News'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Percentage of Stop Words in Article Bodies', template=""plotly_white"") fig.show()Code language: Python (python) Within my expectations, The article of the Natural News use very less stop words than the Harvard Health publishing. x1 = df.loc[df['source']=='https://www.health.harvard.edu/']['ttr'] x2 = df.loc[df['source'] == 'https://www.naturalnews.com/']['ttr'] group_labels = ['Harvard', 'Natural News'] colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)'] fig = ff.create_distplot( [x1, x2], group_labels,colors=colors) fig.update_layout(title_text='Type-token ratio in Article Bodies', template=""plotly_white"") fig.show()Code language: Python (python) Fake News Detection Model Now, I will use a Support Vector Machine classification model to fit all the features that we have gone throughout this article to build a Fake News Detection model. X, y = df.drop(['title', 'text', 'source', 'label', 'text_standard'], axis = 1), df['label'] scaler = StandardScaler() scaler.fit(X) X = scaler.transform(X) svc=LinearSVC(dual=False) scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy') print(scores)Code language: Python (python) [0.77777778 0.88888889 0.79487179 0.86324786 0.80172414 0.87931034 0.85344828 0.87068966 0.86206897 0.82758621] print(scores.mean())Code language: Python (python) 0.841961391099322 When 10-fold cross validation is done we can see 10 different score in each iteration, and then we compute the mean score. Take all the values of C parameter and check out the accuracy score: C_range=list(range(1,26)) acc_score=[] for c in C_range: svc = LinearSVC(dual=False, C=c) scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy') acc_score.append(scores.mean()) C_values=list(range(1,26)) fig = go.Figure(data=go.Scatter(x=C_values, y=acc_score)) fig.update_layout(xaxis_title='Value of C for SVC', yaxis_title='Cross Validated Accuracy', template='plotly_white',xaxis = dict(dtick = 1)) fig.show()Code language: Python (python) The Figure above shows that our model has built with a great accuracy of 84.2 percent, then it drops down to 83.8 percent and remains constant. Also, Read: Training Models Across Multiple Devices. So this was a Fake News Detection model that I trained using the Support Vector Machine algorithm in Machine Learning. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Fake News Detection Model
2020-07-19 01:30:39;Birds inspired us to fly, nature inspired us to countless inventions. It seems logical, then to look at the brain’s architecture for inspiration on how to build an Intelligent Machine. This is the logic that sparked Artificial Neural Networks (ANN). ANN is a Machine Learning Model inspired by the networks of biological neurons found in our brains. However, although planes were inspired by birds, they don’t have to flap their wings. Similarly, ANN have gradually become quite different from their biological cousins. In this Article, I will build an Image Classification model with ANN to show you how ANN works.;https://thecleverprogrammer.com/2020/07/19/image-classification-with-ann/;['keras', 'sklearn', 'tensorflow'];1.0;['ML', 'NN', 'ANN'];['NN', 'ML', 'ANN', 'ReLu', 'Classification'];['detect', 'artificial neural network', 'relu', 'train', 'label', 'output layer', 'image classification', 'gradient descent', 'hidden layer', 'epoch', 'fit', 'model', 'loss', 'activation function', 'predict', 'machine learning', 'neural network', 'classif', 'layer', 'neuron'];"Birds inspired us to fly, nature inspired us to countless inventions. It seems logical, then to look at the brain’s architecture for inspiration on how to build an Intelligent Machine. This is the logic that sparked Artificial Neural Networks (ANN). ANN is a Machine Learning Model inspired by the networks of biological neurons found in our brains. However, although planes were inspired by birds, they don’t have to flap their wings. Similarly, ANN have gradually become quite different from their biological cousins. In this Article, I will build an Image Classification model with ANN to show you how ANN works. Building an Image Classification with ANN First, we need to load a dataset. In this Image Classification model we will tackle Fashion MNIST. It has a format of 60,000 grayscale images of 28 x 28 pixels each, with 10 classes. Let’s import some necessary libraries to start with this task: # Python ≥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= ""0.20"" try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass # TensorFlow ≥2.0 is required import tensorflow as tf assert tf.__version__ &gt;= ""2.0"" # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12)Code language: Python (python) Using Keras to Load the Dataset Keras provide some quality functions to fetch and load common datasets, including MNIST, Fashion MNIST, and the California housing dataset. Let’s start by loading the fashion MNIST dataset to create an Image Classification model. Keras has a number of functions to load popular datasets in keras.datasets. The dataset is already split for you between a training set and a test set, but it can be useful to split the training set further to have a validation set: import tensorflow as tf from tensorflow import keras fashion_mnist = keras.datasets.fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()Code language: Python (python) When loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one important difference is that every image is represented as a 28 x 28 array rather than a 1D array of size 784. Moreover, the pixel intensities are represented as integers rather than the floats. Let’s take a look at the shape and data type of the training set: X_train_full.shapeCode language: Python (python) (60000, 28, 28) X_train_full.dtypeCode language: Python (python) dtype(‘uint8’) Note that the dataset is already split into a training set and a test set, but there is no validation set, so we’ll create one now. Additionally, since we are going to train the ANN using Gradient Descent, we must scale the input features. For simplicity, I will scale the pixel intensities down to the 0-1 range by dividing them by 255.0: X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255. y_valid, y_train = y_train_full[:5000], y_train_full[5000:] X_test = X_test / 255.Code language: Python (python) You can plot an image using Matplotlib’s imshow() function, with a 'binary' color map: plt.imshow(X_train[0], cmap=""binary"") plt.axis('off') plt.show()Code language: Python (python) The labels are the class IDs (represented as uint8), from 0 to 9: y_trainCode language: Python (python) array([4, 0, 7, …, 3, 0, 5], dtype=uint8) With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5. easy. For Fashion MNIST, however, we need the list of class names to know what we are dealing with: class_names = [""T-shirt/top"", ""Trouser"", ""Pullover"", ""Dress"", ""Coat"", ""Sandal"", ""Shirt"", ""Sneaker"", ""Bag"", ""Ankle boot""]Code language: Python (python) For example, the first image in the training set represents a coat: class_names[y_train[0]]Code language: Python (python) Coat The validation set contains 5,000 images, and the test set contains 10,000 images: X_valid.shapeCode language: Python (python) (5000, 28, 28) X_test.shapeCode language: Python (python) (10000, 28, 28) Let’s take a look at a sample of the images in the dataset: n_rows = 4 n_cols = 10 plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2)) for row in range(n_rows): for col in range(n_cols): index = n_cols * row + col plt.subplot(n_rows, n_cols, index + 1) plt.imshow(X_train[index], cmap=""binary"", interpolation=""nearest"") plt.axis('off') plt.title(class_names[y_train[index]], fontsize=12) plt.subplots_adjust(wspace=0.2, hspace=0.5) save_fig('fashion_mnist_plot', tight_layout=False) plt.show()Code language: Python (python) Image Classification Model using Sequential API Now, let’s build the neural network. Here is a classification MLP with two hidden layers: model = keras.models.Sequential() model.add(keras.layers.Flatten(input_shape=[28, 28])) model.add(keras.layers.Dense(300, activation=""relu"")) model.add(keras.layers.Dense(100, activation=""relu"")) model.add(keras.layers.Dense(10, activation=""softmax"")) keras.backend.clear_session() np.random.seed(42) tf.random.set_seed(42)Code language: Python (python) Let’s go through the above code line by line: The first line creates a Sequential model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the Sequential API.Next, we build the first layer and add it to the model. It is Flatten layer whose role is to convert each input image into a 1D array. If it receives input data X, it computes X.reshape(-1,1). This layer does not have any parameters, it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape, which doesn’t include the batch size, only the shape of the instances. Alternatively, you could add a keras.layers.InputLayer as the first layer, setting input _shape = [28,28].Next we add a Dense hidden layer with 300 neurons.It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias term.Then we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.Finally, we add a Dense output layer with 10 neurons, using the softmax qctivation function. Instead of adding the layers one by one as we just did, you can pass a list of layers when creating the Sequential model: model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=""relu""), keras.layers.Dense(100, activation=""relu""), keras.layers.Dense(10, activation=""softmax"") ]) model.layersCode language: Python (python) The model’s summary() method will display all the model’s layers. including each layer’s name, it’s output shape, and it’s number of parameters, including trainable and non-trainable parameters. model.summary()Code language: Python (python) Model: ""sequential"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 300) 235500 _________________________________________________________________ dense_1 (Dense) (None, 100) 30100 _________________________________________________________________ dense_2 (Dense) (None, 10) 1010 ================================================================= Total params: 266,610 Trainable params: 266,610 Non-trainable params: 0 keras.utils.plot_model(model, ""my_fashion_mnist_model.png"", show_shapes=True)Code language: Python (python) hidden1 = model.layers[1] hidden1.nameCode language: Python (python) dense model.get_layer(hidden1.name) is hidden1Code language: Python (python) True weights, biases = hidden1.get_weights() Code language: Python (python) array([[ 0.02448617, -0.00877795, -0.02189048, ..., -0.02766046, 0.03859074, -0.06889391], [ 0.00476504, -0.03105379, -0.0586676 , ..., 0.00602964, -0.02763776, -0.04165364], [-0.06189284, -0.06901957, 0.07102345, ..., -0.04238207, 0.07121518, -0.07331658], ..., [-0.03048757, 0.02155137, -0.05400612, ..., -0.00113463, 0.00228987, 0.05581069], [ 0.07061854, -0.06960931, 0.07038955, ..., -0.00384101, 0.00034875, 0.02878492], [-0.06022581, 0.01577859, -0.02585464, ..., -0.00527829, 0.00272203, -0.06793761]], dtype=float32) biasesCode language: Python (python) array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32) Compiling the Image Classification Model After a model is created, you must call its compile() methid to specify that the loss function and the optimizer to use. Optionally, you can specify a list of extra metrices to compute during training and evaluation: model.compile(loss=""sparse_categorical_crossentropy"", optimizer=""sgd"", metrics=[""accuracy""])Code language: Python (python) Training and Evaluating the Image Classification Model Now the model is ready to be trained. For this we simply need to call its fit() method: history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))Code language: Python (python) The fit() method returns a History object containing the training parameters, the list of epochs it went through, and most importantly a dictionary containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set. If you use this dictionary to create a pandsa DataFrame and call its plot(), then you can see the learning curves of our trained model: import pandas as pd pd.DataFrame(history.history).plot(figsize=(8, 5)) plt.grid(True) plt.gca().set_ylim(0, 1) save_fig(""keras_learning_curves_plot"") plt.show()Code language: Python (python) You can see that both the training accuracy and the validation accuracy steadily increase during training, while the training loss and the validation loss decrease. Once you are satisfied with your model’s validation accuracy, you should evaluate it on a test set to estimate the generalization error before you deploy it to the production. You can easily do this using the evaluate() method: model.evaluate(X_test, y_test)Code language: Python (python) 313/313 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.8822 [0.3381877839565277, 0.8822000026702881] Use the Model to Make Predictions Next, we can use the model’s predict() method to make predictions on new instances. Since we don’t have actual new instances, we will just use the first three instances of the test set: X_new = X_test[:3] y_proba = model.predict(X_new) y_proba.round(2)Code language: Python (python) array([[0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.03, 0. , 0.96], [0. , 0. , 0.99, 0. , 0.01, 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32) y_pred = model.predict_classes(X_new) y_predCode language: Python (python) array([9, 2, 1]) np.array(class_names)[y_pred]Code language: Python (python) array([‘Ankle boot’, ‘Pullover’, ‘Trouser’], dtype='<U11′) Here, the classification model actually classified all three images correctly: y_new = y_test[:3] plt.figure(figsize=(7.2, 2.4)) for index, image in enumerate(X_new): plt.subplot(1, 3, index + 1) plt.imshow(image, cmap=""binary"", interpolation=""nearest"") plt.axis('off') plt.title(class_names[y_test[index]], fontsize=12) plt.subplots_adjust(wspace=0.2, hspace=0.5) save_fig('fashion_mnist_images_plot', tight_layout=False) plt.show()Code language: Python (python) Also, Read – Fake News Detection Model. I hope you liked this article on Image Classification with Artificial Neural Networks (ANN). Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Image Classification with ANN
2020-07-20 01:05:06;Binary Classification is a type of classification model that have two label of classes. For example an email spam detection model contains two label of classes as spam or not spam. Most of the times the tasks of binary classification includes one label in a normal state, and another label in an abnormal state. In this article I will take you through Binary Classification in Machine Learning using Python.I will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. This set has been studied so much that it is often called the “hello world” of Machine Learning.Whenever people come up with new classification algorithm they are curious to see how it will perform on MNIST, and anyone who learns Machine Learning tackles this dataset sooner or later. So let’s import some libraries to start with our Binary Classification model:Scikit-Learn provides many helper functions to download popular datasets. MNIST is one of them. The following code fetches the MNIST dataset:Now Let’s look at the data:(70000, 784)(70000,)784There are 70,000 images, and each image has 784 features. This is because each image is 28×28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255(black). Let’s take a peak at one digit from the dataset. All you need to do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using Matplotlib’s imshow() function:This looks like a 5, and indeed that’s what the label tells us:5Note that the label is a string. Most Machine Learning Algorithms expect numbers, so let’s cast y to integer:Now before training a Binary Classification model, let’ have a look at the digits:You should always create a test set and set it aside before inspecting the data closely. The MNIST dataset is actually already split into a training set and a test set:;https://thecleverprogrammer.com/2020/07/20/binary-classification-model/;['sklearn'];1.0;[];['ML', 'Classification'];['detect', 'predict', 'fit', 'model', 'loss', 'machine learning', 'classif', 'train', 'label', 'gradient descent'];"Binary Classification is a type of classification model that have two label of classes. For example an email spam detection model contains two label of classes as spam or not spam. Most of the times the tasks of binary classification includes one label in a normal state, and another label in an abnormal state. In this article I will take you through Binary Classification in Machine Learning using Python. MNIST Dataset I will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. This set has been studied so much that it is often called the “hello world” of Machine Learning. Whenever people come up with new classification algorithm they are curious to see how it will perform on MNIST, and anyone who learns Machine Learning tackles this dataset sooner or later. So let’s import some libraries to start with our Binary Classification model: # Python ≥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= ""0.20"" # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12)Code language: Python (python) Scikit-Learn provides many helper functions to download popular datasets. MNIST is one of them. The following code fetches the MNIST dataset: from sklearn.datasets import fetch_openml mnist = fetch_openml('mnist_784', version=1) mnist.keys()Code language: Python (python) dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'DESCR', 'details', 'categories', 'url']) Now Let’s look at the data: X, y = mnist[""data""], mnist[""target""] X.shapeCode language: Python (python) (70000, 784) y.shapeCode language: Python (python) (70000,) 28 * 28Code language: Python (python) 784 There are 70,000 images, and each image has 784 features. This is because each image is 28×28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255(black). Let’s take a peak at one digit from the dataset. All you need to do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using Matplotlib’s imshow() function: %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt some_digit = X[0] some_digit_image = some_digit.reshape(28, 28) plt.imshow(some_digit_image, cmap=mpl.cm.binary) plt.axis(""off"") save_fig(""some_digit_plot"") plt.show()Code language: Python (python) This looks like a 5, and indeed that’s what the label tells us: y[0]Code language: Python (python) 5 Note that the label is a string. Most Machine Learning Algorithms expect numbers, so let’s cast y to integer: y = y.astype(np.uint8)Code language: Python (python) Now before training a Binary Classification model, let’ have a look at the digits: def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = mpl.cm.binary, interpolation=""nearest"") plt.axis(""off"") def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = mpl.cm.binary, **options) plt.axis(""off"") plt.figure(figsize=(9,9)) example_images = X[:100] plot_digits(example_images, images_per_row=10) save_fig(""more_digits_plot"") plt.show()Code language: Python (python) You should always create a test set and set it aside before inspecting the data closely. The MNIST dataset is actually already split into a training set and a test set: X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]Code language: Python (python) Training a Binary Classification Model Let’s simply the problem for now and only try to identify one digit. For example, the number 5. This “5 detector” will be an example of a binary classification, capable of distinguishing between just two classes, 5 and not 5. Let’s create the target vectors for the classification task: y_train_5 = (y_train == 5) y_test_5 = (y_test == 5)Code language: Python (python) Now let’s pick a classification model and train it. A good place to start is with a Stochastic Gradient Descent (SGD) deals with training instances independently, one at a time, as we will se later in our binary classification model. Let’s build a binary classification using the SGDClassifier and train it on the whole training set: from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) sgd_clf.fit(X_train, y_train_5)Code language: Python (python) SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5, random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) sgd_clf.predict([some_digit])Code language: Python (python) array([ True]) The classifier guesses that this image represents a 5 (True). Looks like it guessed right in this particular case. Now let’s evaluate the performance of our binary classification model. Performance Measures Evaluating a Classifier is often trickier than evaluating a regressor, so we will spend some more part of this article to evaluate our binary classification model. Implementing Cross-Validation on Binary Classification Model Occasionally you will need more control over the cross-validation process than what scikit-learn provides off the shelf. In these cases, you can implement cross-validation yourself. The following code does roughly the same thing as Scikit-learn’s cross_val_score() function does, and it prints the same result: from sklearn.model_selection import StratifiedKFold from sklearn.base import clone skfolds = StratifiedKFold(n_splits=3, random_state=42) for train_index, test_index in skfolds.split(X_train, y_train_5): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_5[train_index] X_test_fold = X_train[test_index] y_test_fold = y_train_5[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred = clone_clf.predict(X_test_fold) n_correct = sum(y_pred == y_test_fold) print(n_correct / len(y_pred))Code language: Python (python) 0.95035 0.96035 0.9604 The StratifiedKFold class performs stratified sampling to produce folds that contain a representative ratio of each class. At each iteration the code creates a clone of the classification model, trains that clone on the training folds, and make predictions on the test fold. Then it counts the number of correct predictions and outputs the ratio of correct predictions. Let’s use the cross_val_score() function to evaluate our SGDClassifier model, using K-fold cross-validation with three folds. Remember that K-fold cross-validation means splitting the training set into K folds, then making predictions and evaluating them on each fold using a model trained on the remaining folds: from sklearn.model_selection import cross_val_score cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=""accuracy"")Code language: Python (python) array([0.95035, 0.96035, 0.9604 ]) Wow! Above 93% accuracy on all cross-validation folds. Well, before you get too exited, let’s look at a very dumb classifier that just classifies every single image in the “not 5” class: from sklearn.base import BaseEstimator class Never5Classifier(BaseEstimator): def fit(self, X, y=None): pass def predict(self, X): return np.zeros((len(X), 1), dtype=bool) never_5_clf = Never5Classifier() cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=""accuracy"")Code language: Python (python) array([0.91125, 0.90855, 0.90915]) Also, Read: Generate WordClouds with Python. That’s right it has over 90% accuracy. this is simply because only about 10% of the images are 5s, so if you always guess that an image is not a 5, you will be right about 90% of the time. So I hope you liked this article on Binary Classification Model in Machine Learning. Feel free to ask you valuable questions in the comments section below. Follow Us: Facebook Instagram";Binary Classification Model
2020-07-20 14:36:26;Data Augmentation is a technique in Deep Learning which helps in adding value to our base dataset by adding the gathered information from various sources to improve the quality of data of an organisation. Data Augmentation is one of the most important processes that makes the data very much informational. Improving the data is very important for every business because data is considered as the oil of business. Data Augmentation can be applied to any form of the dataset, which mainly includes text, images, and audio.Data Augmentation helps in the study of the insights based on customers, product sales, and various other departments where the use of additional information can help a business to study the insights of the data very deeply. In this article, I will show you the most used field of Data Augmentation that is Image Augmentation.;https://thecleverprogrammer.com/2020/07/20/data-augmentation-in-deep-learning/;['keras', 'tensorflow'];1.0;[];['NN', 'DL', 'ML', 'ReLu', 'Classification'];['epoch', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'relu', 'deep learning', 'train', 'label'];"Data Augmentation is a technique in Deep Learning which helps in adding value to our base dataset by adding the gathered information from various sources to improve the quality of data of an organisation. Data Augmentation is one of the most important processes that makes the data very much informational. Improving the data is very important for every business because data is considered as the oil of business. Data Augmentation can be applied to any form of the dataset, which mainly includes text, images, and audio. Data Augmentation helps in the study of the insights based on customers, product sales, and various other departments where the use of additional information can help a business to study the insights of the data very deeply. In this article, I will show you the most used field of Data Augmentation that is Image Augmentation. Data Augmentation Here I will show you some manual image augmentation and manipulation using TensorFlow. In Deep Learning, Data Augmentation is a very common technique to improve the results and overfitting. To move further in this article, you need to install a package using the pip installer command: pip install tensorflow_docs. Now, let’s import all the libraries we need for this task: import urllib import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras import layers AUTOTUNE = tf.data.experimental.AUTOTUNE import tensorflow_docs as tfdocs import tensorflow_docs.plots import tensorflow_datasets as tfds import PIL.Image import matplotlib.pyplot as plt import matplotlib as mpl mpl.rcParams['figure.figsize'] = (12, 5) import numpy as npCode language: Python (python) Now, let’s go through all the data augmentation features using an image, and later I will apply those features in the whole dataset to train a Deep Learning Model. The image that I will use in this article, can be downloaded from here. Now let’s read the image and have a quick look at it. image_path = tf.keras.utils.get_file(""cat.jpg"") PIL.Image.open(image_path)Code language: Python (python) Now I will simply read and decode the above image in the format of tensor: image_string=tf.io.read_file(image_path) image=tf.image.decode_jpeg(image_string,channels=3)Code language: Python (python) Now, I will create a function to visualize the comparison between the original image and the augmented image: def visualize(original, augmented): fig = plt.figure() plt.subplot(1,2,1) plt.title('Original image') plt.imshow(original) plt.subplot(1,2,2) plt.title('Augmented image') plt.imshow(augmented)Code language: Python (python) Data Augmentation on a Single Image Let’s start with flipping the image, here I will flip the image either horizontally or vertically: flipped = tf.image.flip_left_right(image) visualize(image, flipped)Code language: Python (python) Flipping the Image Now, let’s move further by applying the grayscale features to the image: grayscaled = tf.image.rgb_to_grayscale(image) visualize(image, tf.squeeze(grayscaled)) plt.colorbar()Code language: Python (python) Grayscale the Image Now, I will move further with adding the Saturation factor to the image: saturated = tf.image.adjust_saturation(image, 3) visualize(image, saturated)Code language: Python (python) Saturation Now, I will move further with changing the brightness levels of the image: bright = tf.image.adjust_brightness(image, 0.4) visualize(image, bright)Code language: Python (python) Brightness Now, I will rotate the image with 90 degree angle: rotated = tf.image.rot90(image) visualize(image, rotated)Code language: Python (python) Rotation Now, before applying all the above features on the dataset, let’s have a look at one more feature that is cropping the image: cropped = tf.image.central_crop(image, central_fraction=0.5) visualize(image,cropped)Code language: Python (python) Cropping Data Augmentation on a Dataset and Training a Model Now, I will apply the data augmentation features on a dataset, and then use that augmented dataset for training a model. dataset, info = tfds.load('mnist', as_supervised=True, with_info=True) train_dataset, test_dataset = dataset['train'], dataset['test'] num_train_examples= info.splits['train'].num_examplesCode language: Python (python) Now, I will create a function to augment the images in the dataset, and then I will map the function on the dataset. This will return a dataset that will augment the data on the fly. def convert(image, label): image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1] return image, label def augment(image,label): image,label = convert(image, label) image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1] image = tf.image.resize_with_crop_or_pad(image, 34, 34) # Add 6 pixels of padding image = tf.image.random_crop(image, size=[28, 28, 1]) # Random crop back to 28x28 image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness return image,label BATCH_SIZE = 64 # Only use a subset of the data so it's easier to overfit, for this tutorial NUM_EXAMPLES = 2048Code language: Python (python) Now, I will create a augmented dataset: augmented_train_batches = ( train_dataset # Only train on a subset, so you can quickly see the effect. .take(NUM_EXAMPLES) .cache() .shuffle(num_train_examples//4) # The augmentation is added here. .map(augment, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .prefetch(AUTOTUNE) ) Code language: Python (python) And, now I will create a non-augmented dataset to draw comparisons: non_augmented_train_batches = ( train_dataset # Only train on a subset, so you can quickly see the effect. .take(NUM_EXAMPLES) .cache() .shuffle(num_train_examples//4) # No augmentation. .map(convert, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .prefetch(AUTOTUNE) ) Code language: Python (python) Now, I will set up a validation set. Note that a validation set will never change whether you are using augmentation in your model or not: validation_batches = ( test_dataset .map(convert, num_parallel_calls=AUTOTUNE) .batch(2*BATCH_SIZE) )Code language: Python (python) Now I will create and compile a fully connected neural networks model with two layers: def make_model(): model = tf.keras.Sequential([ layers.Flatten(input_shape=(28, 28, 1)), layers.Dense(4096, activation='relu'), layers.Dense(4096, activation='relu'), layers.Dense(10) ]) model.compile(optimizer = 'adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) return modelCode language: Python (python) Training the Model Now, I will first train the model without data augmentation: model_without_aug = make_model() no_aug_history = model_without_aug.fit(non_augmented_train_batches, epochs=50, validation_data=validation_batches)Code language: Python (python) Epoch 1/50 32/32 [==============================] - 3s 102ms/step - loss: 0.7278 - accuracy: 0.7769 - val_loss: 0.3283 - val_accuracy: 0.9029 Epoch 2/50 32/32 [==============================] - 3s 99ms/step - loss: 0.1549 - accuracy: 0.9526 - val_loss: 0.3250 - val_accuracy: 0.9104 Epoch 3/50 32/32 [==============================] - 3s 95ms/step - loss: 0.1067 - accuracy: 0.9663 - val_loss: 0.2956 - val_accuracy: 0.9217 Epoch 4/50 32/32 [==============================] - 3s 93ms/step - loss: 0.0550 - accuracy: 0.9819 - val_loss: 0.3056 - val_accuracy: 0.9262 Epoch 5/50 32/32 [==============================] - 3s 96ms/step - loss: 0.0270 - accuracy: 0.9922 - val_loss: 0.3168 - val_accuracy: 0.9280 Epoch 6/50 32/32 [==============================] - 3s 97ms/step - loss: 0.0177 - accuracy: 0.9937 - val_loss: 0.3839 - val_accuracy: 0.9214 Epoch 7/50 32/32 [==============================] - 3s 95ms/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.4449 - val_accuracy: 0.9086 Epoch 8/50 32/32 [==============================] - 3s 94ms/step - loss: 0.0545 - accuracy: 0.9854 - val_loss: 0.6276 - val_accuracy: 0.8819 Epoch 9/50 32/32 [==============================] - 3s 93ms/step - loss: 0.0681 - accuracy: 0.9800 - val_loss: 0.3126 - val_accuracy: 0.9298 Epoch 10/50 32/32 [==============================] - 3s 96ms/step - loss: 0.0335 - accuracy: 0.9897 - val_loss: 0.4641 - val_accuracy: 0.9058 Epoch 11/50 32/32 [==============================] - 3s 97ms/step - loss: 0.0657 - accuracy: 0.9800 - val_loss: 0.4502 - val_accuracy: 0.9053 Epoch 12/50 32/32 [==============================] - 3s 97ms/step - loss: 0.0912 - accuracy: 0.9751 - val_loss: 0.4676 - val_accuracy: 0.9055 Epoch 13/50 32/32 [==============================] - 3s 94ms/step - loss: 0.0359 - accuracy: 0.9873 - val_loss: 0.3761 - val_accuracy: 0.9262 Epoch 14/50 32/32 [==============================] - 3s 94ms/step - loss: 0.0131 - accuracy: 0.9937 - val_loss: 0.4399 - val_accuracy: 0.9217 Epoch 15/50 32/32 [==============================] - 3s 96ms/step - loss: 0.0068 - accuracy: 0.9971 - val_loss: 0.3926 - val_accuracy: 0.9294 Epoch 16/50 32/32 [==============================] - 3s 95ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.3854 - val_accuracy: 0.9314 Epoch 17/50 32/32 [==============================] - 3s 98ms/step - loss: 0.0212 - accuracy: 0.9966 - val_loss: 0.4145 - val_accuracy: 0.9364 Epoch 18/50 32/32 [==============================] - 3s 97ms/step - loss: 0.0101 - accuracy: 0.9961 - val_loss: 0.4364 - val_accuracy: 0.9272 Epoch 19/50 32/32 [==============================] - 3s 94ms/step - loss: 0.0278 - accuracy: 0.9922 - val_loss: 0.5049 - val_accuracy: 0.9229 Epoch 20/50 32/32 [==============================] - 3s 95ms/step - loss: 0.0622 - accuracy: 0.9849 - val_loss: 0.4006 - val_accuracy: 0.9273 Epoch 21/50 32/32 [==============================] - 3s 97ms/step - loss: 0.0055 - accuracy: 0.9976 - val_loss: 0.4556 - val_accuracy: 0.9296 Epoch 22/50 32/32 [==============================] - 3s 96ms/step - loss: 0.0109 - accuracy: 0.9961 - val_loss: 0.4552 - val_accuracy: 0.9260 Epoch 23/50 32/32 [==============================] - 3s 97ms/step - loss: 0.0356 - accuracy: 0.9902 - val_loss: 0.5692 - val_accuracy: 0.9145 Epoch 24/50 32/32 [==============================] - 3s 95ms/step - loss: 0.0432 - accuracy: 0.9902 - val_loss: 0.4462 - val_accuracy: 0.9287 Epoch 25/50 32/32 [==============================] - 3s 97ms/step - loss: 0.0151 - accuracy: 0.9961 - val_loss: 0.4916 - val_accuracy: 0.9259 Epoch 26/50 32/32 [==============================] - 3s 97ms/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 0.4660 - val_accuracy: 0.9258 Epoch 27/50 32/32 [==============================] - 3s 95ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.3922 - val_accuracy: 0.9380 Epoch 28/50 32/32 [==============================] - 3s 94ms/step - loss: 4.4921e-04 - accuracy: 1.0000 - val_loss: 0.4107 - val_accuracy: 0.9382 Epoch 29/50 32/32 [==============================] - 3s 95ms/step - loss: 9.6380e-04 - accuracy: 0.9995 - val_loss: 0.4102 - val_accuracy: 0.9375 Epoch 30/50 32/32 [==============================] - 3s 95ms/step - loss: 1.4121e-04 - accuracy: 1.0000 - val_loss: 0.4050 - val_accuracy: 0.9376 Epoch 31/50 32/32 [==============================] - 3s 94ms/step - loss: 5.3728e-05 - accuracy: 1.0000 - val_loss: 0.4047 - val_accuracy: 0.9386 Epoch 32/50 32/32 [==============================] - 3s 98ms/step - loss: 3.7863e-05 - accuracy: 1.0000 - val_loss: 0.4060 - val_accuracy: 0.9390 Epoch 33/50 32/32 [==============================] - 3s 96ms/step - loss: 3.3242e-05 - accuracy: 1.0000 - val_loss: 0.4072 - val_accuracy: 0.9391 Epoch 34/50 32/32 [==============================] - 3s 94ms/step - loss: 3.0050e-05 - accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 0.9396 Epoch 35/50 32/32 [==============================] - 3s 97ms/step - loss: 2.7223e-05 - accuracy: 1.0000 - val_loss: 0.4097 - val_accuracy: 0.9398 Epoch 36/50 32/32 [==============================] - 3s 96ms/step - loss: 2.4499e-05 - accuracy: 1.0000 - val_loss: 0.4112 - val_accuracy: 0.9398 Epoch 37/50 32/32 [==============================] - 3s 96ms/step - loss: 2.1698e-05 - accuracy: 1.0000 - val_loss: 0.4132 - val_accuracy: 0.9402 Epoch 38/50 32/32 [==============================] - 3s 95ms/step - loss: 1.9247e-05 - accuracy: 1.0000 - val_loss: 0.4153 - val_accuracy: 0.9402 Epoch 39/50 32/32 [==============================] - 3s 95ms/step - loss: 1.7082e-05 - accuracy: 1.0000 - val_loss: 0.4168 - val_accuracy: 0.9402 Epoch 40/50 32/32 [==============================] - 3s 96ms/step - loss: 1.5243e-05 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.9404 Epoch 41/50 32/32 [==============================] - 3s 96ms/step - loss: 1.3809e-05 - accuracy: 1.0000 - val_loss: 0.4200 - val_accuracy: 0.9403 Epoch 42/50 32/32 [==============================] - 3s 99ms/step - loss: 1.2567e-05 - accuracy: 1.0000 - val_loss: 0.4208 - val_accuracy: 0.9407 Epoch 43/50 32/32 [==============================] - 3s 96ms/step - loss: 1.1590e-05 - accuracy: 1.0000 - val_loss: 0.4224 - val_accuracy: 0.9409 Epoch 44/50 32/32 [==============================] - 3s 95ms/step - loss: 1.0728e-05 - accuracy: 1.0000 - val_loss: 0.4231 - val_accuracy: 0.9411 Epoch 45/50 32/32 [==============================] - 3s 96ms/step - loss: 9.9013e-06 - accuracy: 1.0000 - val_loss: 0.4251 - val_accuracy: 0.9410 Epoch 46/50 32/32 [==============================] - 3s 96ms/step - loss: 9.2411e-06 - accuracy: 1.0000 - val_loss: 0.4253 - val_accuracy: 0.9412 Epoch 47/50 32/32 [==============================] - 3s 95ms/step - loss: 8.6455e-06 - accuracy: 1.0000 - val_loss: 0.4268 - val_accuracy: 0.9414 Epoch 48/50 32/32 [==============================] - 3s 94ms/step - loss: 8.1020e-06 - accuracy: 1.0000 - val_loss: 0.4275 - val_accuracy: 0.9411 Epoch 49/50 32/32 [==============================] - 3s 98ms/step - loss: 7.6395e-06 - accuracy: 1.0000 - val_loss: 0.4288 - val_accuracy: 0.9412 Epoch 50/50 32/32 [==============================] - 3s 95ms/step - loss: 7.1791e-06 - accuracy: 1.0000 - val_loss: 0.4297 - val_accuracy: 0.9412 And, Now I will train the model with Augmentation: model_with_aug = make_model() aug_history = model_with_aug.fit(augmented_train_batches, epochs=50, validation_data=validation_batches)Code language: Python (python) Epoch 1/50 32/32 [==============================] - 3s 97ms/step - loss: 2.3180 - accuracy: 0.2847 - val_loss: 1.1624 - val_accuracy: 0.6775 Epoch 2/50 32/32 [==============================] - 3s 94ms/step - loss: 1.3399 - accuracy: 0.5381 - val_loss: 0.8038 - val_accuracy: 0.7815 Epoch 3/50 32/32 [==============================] - 3s 99ms/step - loss: 0.9384 - accuracy: 0.6826 - val_loss: 0.5355 - val_accuracy: 0.8389 Epoch 4/50 32/32 [==============================] - 3s 97ms/step - loss: 0.8110 - accuracy: 0.7310 - val_loss: 0.4035 - val_accuracy: 0.8878 Epoch 5/50 32/32 [==============================] - 3s 94ms/step - loss: 0.6298 - accuracy: 0.7871 - val_loss: 0.3538 - val_accuracy: 0.8846 Epoch 6/50 32/32 [==============================] - 3s 97ms/step - loss: 0.5988 - accuracy: 0.8037 - val_loss: 0.3261 - val_accuracy: 0.9021 Epoch 7/50 32/32 [==============================] - 3s 97ms/step - loss: 0.5618 - accuracy: 0.8223 - val_loss: 0.3066 - val_accuracy: 0.8979 Epoch 8/50 32/32 [==============================] - 3s 96ms/step - loss: 0.4548 - accuracy: 0.8530 - val_loss: 0.3066 - val_accuracy: 0.9012 Epoch 9/50 32/32 [==============================] - 3s 96ms/step - loss: 0.4961 - accuracy: 0.8423 - val_loss: 0.2741 - val_accuracy: 0.9128 Epoch 10/50 32/32 [==============================] - 3s 93ms/step - loss: 0.4198 - accuracy: 0.8569 - val_loss: 0.2845 - val_accuracy: 0.9084 Epoch 11/50 32/32 [==============================] - 3s 96ms/step - loss: 0.4199 - accuracy: 0.8687 - val_loss: 0.2223 - val_accuracy: 0.9291 Epoch 12/50 32/32 [==============================] - 3s 96ms/step - loss: 0.3781 - accuracy: 0.8696 - val_loss: 0.2124 - val_accuracy: 0.9300 Epoch 13/50 32/32 [==============================] - 3s 96ms/step - loss: 0.3615 - accuracy: 0.8804 - val_loss: 0.2269 - val_accuracy: 0.9278 Epoch 14/50 32/32 [==============================] - 3s 96ms/step - loss: 0.3543 - accuracy: 0.8809 - val_loss: 0.2238 - val_accuracy: 0.9306 Epoch 15/50 32/32 [==============================] - 3s 97ms/step - loss: 0.3862 - accuracy: 0.8755 - val_loss: 0.2006 - val_accuracy: 0.9364 Epoch 16/50 32/32 [==============================] - 3s 94ms/step - loss: 0.3346 - accuracy: 0.8965 - val_loss: 0.1717 - val_accuracy: 0.9455 Epoch 17/50 32/32 [==============================] - 3s 100ms/step - loss: 0.2652 - accuracy: 0.9170 - val_loss: 0.1905 - val_accuracy: 0.9390 Epoch 18/50 32/32 [==============================] - 3s 96ms/step - loss: 0.2810 - accuracy: 0.9146 - val_loss: 0.1646 - val_accuracy: 0.9472 Epoch 19/50 32/32 [==============================] - 3s 95ms/step - loss: 0.2845 - accuracy: 0.9053 - val_loss: 0.1897 - val_accuracy: 0.9416 Epoch 20/50 32/32 [==============================] - 3s 96ms/step - loss: 0.2635 - accuracy: 0.9048 - val_loss: 0.1781 - val_accuracy: 0.9436 Epoch 21/50 32/32 [==============================] - 3s 94ms/step - loss: 0.3179 - accuracy: 0.8994 - val_loss: 0.1534 - val_accuracy: 0.9520 Epoch 22/50 32/32 [==============================] - 3s 95ms/step - loss: 0.2240 - accuracy: 0.9233 - val_loss: 0.1753 - val_accuracy: 0.9423 Epoch 23/50 32/32 [==============================] - 3s 95ms/step - loss: 0.2690 - accuracy: 0.9194 - val_loss: 0.1811 - val_accuracy: 0.9428 Epoch 24/50 32/32 [==============================] - 3s 94ms/step - loss: 0.2374 - accuracy: 0.9263 - val_loss: 0.1654 - val_accuracy: 0.9482 Epoch 25/50 32/32 [==============================] - 3s 94ms/step - loss: 0.2740 - accuracy: 0.9131 - val_loss: 0.1659 - val_accuracy: 0.9492 Epoch 26/50 32/32 [==============================] - 3s 95ms/step - loss: 0.2468 - accuracy: 0.9233 - val_loss: 0.1644 - val_accuracy: 0.9501 Epoch 27/50 32/32 [==============================] - 3s 96ms/step - loss: 0.2514 - accuracy: 0.9199 - val_loss: 0.1597 - val_accuracy: 0.9479 Epoch 28/50 32/32 [==============================] - 3s 97ms/step - loss: 0.1930 - accuracy: 0.9370 - val_loss: 0.1561 - val_accuracy: 0.9509 Epoch 29/50 32/32 [==============================] - 3s 97ms/step - loss: 0.2026 - accuracy: 0.9355 - val_loss: 0.1688 - val_accuracy: 0.9500 Epoch 30/50 32/32 [==============================] - 3s 98ms/step - loss: 0.2216 - accuracy: 0.9268 - val_loss: 0.1661 - val_accuracy: 0.9490 Epoch 31/50 32/32 [==============================] - 3s 97ms/step - loss: 0.2534 - accuracy: 0.9209 - val_loss: 0.1772 - val_accuracy: 0.9464 Epoch 32/50 32/32 [==============================] - 3s 100ms/step - loss: 0.2998 - accuracy: 0.9023 - val_loss: 0.1512 - val_accuracy: 0.9514 Epoch 33/50 32/32 [==============================] - 3s 100ms/step - loss: 0.2218 - accuracy: 0.9224 - val_loss: 0.1733 - val_accuracy: 0.9447 Epoch 34/50 32/32 [==============================] - 3s 97ms/step - loss: 0.2016 - accuracy: 0.9380 - val_loss: 0.1539 - val_accuracy: 0.9530 Epoch 35/50 32/32 [==============================] - 3s 96ms/step - loss: 0.1676 - accuracy: 0.9438 - val_loss: 0.1465 - val_accuracy: 0.9556 Epoch 36/50 32/32 [==============================] - 3s 98ms/step - loss: 0.1608 - accuracy: 0.9458 - val_loss: 0.1570 - val_accuracy: 0.9546 Epoch 37/50 32/32 [==============================] - 3s 100ms/step - loss: 0.1625 - accuracy: 0.9438 - val_loss: 0.1807 - val_accuracy: 0.9462 Epoch 38/50 32/32 [==============================] - 3s 107ms/step - loss: 0.1715 - accuracy: 0.9438 - val_loss: 0.1639 - val_accuracy: 0.9525 Epoch 39/50 32/32 [==============================] - 3s 95ms/step - loss: 0.1760 - accuracy: 0.9390 - val_loss: 0.1534 - val_accuracy: 0.9521 Epoch 40/50 32/32 [==============================] - 3s 94ms/step - loss: 0.1685 - accuracy: 0.9473 - val_loss: 0.1546 - val_accuracy: 0.9532 Epoch 41/50 32/32 [==============================] - 3s 97ms/step - loss: 0.1442 - accuracy: 0.9526 - val_loss: 0.1524 - val_accuracy: 0.9550 Epoch 42/50 32/32 [==============================] - 3s 98ms/step - loss: 0.1863 - accuracy: 0.9370 - val_loss: 0.1653 - val_accuracy: 0.9480 Epoch 43/50 32/32 [==============================] - 3s 95ms/step - loss: 0.1574 - accuracy: 0.9468 - val_loss: 0.1570 - val_accuracy: 0.9557 Epoch 44/50 32/32 [==============================] - 3s 95ms/step - loss: 0.1691 - accuracy: 0.9438 - val_loss: 0.1502 - val_accuracy: 0.9551 Epoch 45/50 32/32 [==============================] - 3s 96ms/step - loss: 0.1629 - accuracy: 0.9429 - val_loss: 0.1501 - val_accuracy: 0.9555 Epoch 46/50 32/32 [==============================] - 3s 101ms/step - loss: 0.1672 - accuracy: 0.9409 - val_loss: 0.1530 - val_accuracy: 0.9548 Epoch 47/50 32/32 [==============================] - 3s 97ms/step - loss: 0.1924 - accuracy: 0.9429 - val_loss: 0.1564 - val_accuracy: 0.9534 Epoch 48/50 32/32 [==============================] - 3s 94ms/step - loss: 0.1623 - accuracy: 0.9453 - val_loss: 0.1431 - val_accuracy: 0.9574 Epoch 49/50 32/32 [==============================] - 3s 97ms/step - loss: 0.1680 - accuracy: 0.9443 - val_loss: 0.1384 - val_accuracy: 0.9593 Epoch 50/50 32/32 [==============================] - 3s 100ms/step - loss: 0.1736 - accuracy: 0.9414 - val_loss: 0.1370 - val_accuracy: 0.9596 So, Our Augmented model produced the accuracy of 95 percent on the validation set, which is slightly higher than the accuracy of non-augmented model which is 94 percent. plotter = tfdocs.plots.HistoryPlotter() plotter.plot({""Augmented"": aug_history, ""Non-Augmented"": no_aug_history}, metric = ""accuracy"") plt.title(""Accuracy"") plt.ylim([0.75,1])Code language: Python (python) Also Read: Binary Classification Model in Machine Learning. With respect to the loss, the non-augmented model is overfitting the dataset. Whereas the Augmented model, is fitting and training on the dataset very well. I hope you liked this article on Data Augmentation in Deep Learning. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Data Augmentation in Deep Learning
2020-07-20 20:51:31;"Most of the keyboards in smartphones give next word prediction features; google also uses next word prediction based on our browsing history. So a preloaded data is also stored in the keyboard function of our smartphones to predict the next word correctly. In this article, I will train a Deep Learning model for next word prediction using Python. I will use the Tensorflow and Keras library in Python for next word prediction model.For making a Next Word Prediction model, I will train a Recurrent Neural Network (RNN). So let’s start with this task now without wasting any time.Also, Read – 100+ Machine Learning Projects Solved and Explained.";https://thecleverprogrammer.com/2020/07/20/next-word-prediction-model/;['keras', 'nltk'];1.0;['RNN', 'NN'];['LSTM', 'RNN', 'NN', 'DL', 'ML'];['epoch', 'recurrent neural network', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'layer', 'deep learning', 'train', 'lstm', 'label'];"Most of the keyboards in smartphones give next word prediction features; google also uses next word prediction based on our browsing history. So a preloaded data is also stored in the keyboard function of our smartphones to predict the next word correctly. In this article, I will train a Deep Learning model for next word prediction using Python. I will use the Tensorflow and Keras library in Python for next word prediction model. For making a Next Word Prediction model, I will train a Recurrent Neural Network (RNN). So let’s start with this task now without wasting any time. Also, Read – 100+ Machine Learning Projects Solved and Explained. Next Word Prediction Model To start with our next word prediction model, let’s import some all the libraries we need for this task: import numpy as np from nltk.tokenize import RegexpTokenizer from keras.models import Sequential, load_model from keras.layers import LSTM from keras.layers.core import Dense, Activation from keras.optimizers import RMSprop import matplotlib.pyplot as plt import pickle import heapq​x import numpy as npfrom nltk.tokenize import RegexpTokenizerfrom keras.models import Sequential, load_modelfrom keras.layers import LSTMfrom keras.layers.core import Dense, Activationfrom keras.optimizers import RMSpropimport matplotlib.pyplot as pltimport pickleimport heapq Code language: Python (python) As I told earlier, Google uses our browsing history to make next word predictions, smartphones, and all the keyboards that are trained to predict the next word are trained using some data. So I will also use a dataset. You can download the dataset from here. Now let’s load the data and have a quick look at what we are going to work with: path = '1661-0.txt' text = open(path).read().lower() print('corpus length:', len(text)) path = '1661-0.txt'text = open(path).read().lower()print('corpus length:', len(text)) Code language: Python (python) corpus length: 581887 Now I will split the dataset into each word in order but without the presence of some special characters. tokenizer = RegexpTokenizer(r'w+') words = tokenizer.tokenize(text) tokenizer = RegexpTokenizer(r'w+')words = tokenizer.tokenize(text) Code language: Python (python) ['project', 'gutenberg', 's', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', ............................... , 'our', 'email', 'newsletter', 'to', 'hear', 'about', 'new', 'ebooks'] Now the next process will be performing the feature engineering in our data. For this purpose, we will require a dictionary with each word in the data within the list of unique words as the key, and it’s significant portions as value. unique_words = np.unique(words) unique_word_index = dict((c, i) for i, c in enumerate(unique_words)) unique_words = np.unique(words)unique_word_index = dict((c, i) for i, c in enumerate(unique_words)) Code language: Python (python) Feature Engineering Feature Engineering means taking whatever information we have about our problem and turning it into numbers that we can use to build our feature matrix. If you want a detailed tutorial of feature engineering, you can learn it from here. Here I will define a Word length which will represent the number of previous words that will determine our next word. I will define prev words to keep five previous words and their corresponding next words in the list of next words. WORD_LENGTH = 5 prev_words = [] next_words = [] for i in range(len(words) - WORD_LENGTH): prev_words.append(words[i:i + WORD_LENGTH]) next_words.append(words[i + WORD_LENGTH]) print(prev_words[0]) print(next_words[0]) WORD_LENGTH = 5prev_words = []next_words = []for i in range(len(words) - WORD_LENGTH): prev_words.append(words[i:i + WORD_LENGTH]) next_words.append(words[i + WORD_LENGTH])print(prev_words[0])print(next_words[0]) Code language: Python (python) ['project', 'gutenberg', 's', 'the', 'adventures'] Now I will create two numpy arrays x for storing the features and y for storing its corresponding label. I will iterate x and y if the word is available so that the corresponding position becomes 1. X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool) Y = np.zeros((len(next_words), len(unique_words)), dtype=bool) for i, each_words in enumerate(prev_words): for j, each_word in enumerate(each_words): X[i, j, unique_word_index[each_word]] = 1 Y[i, unique_word_index[next_words[i]]] = 1 X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)for i, each_words in enumerate(prev_words): for j, each_word in enumerate(each_words): X[i, j, unique_word_index[each_word]] = 1 Y[i, unique_word_index[next_words[i]]] = 1 Code language: Python (python) Now before moving forward, have a look at a single sequence of words: print(X[0][0]) print(X[0][0]) Code language: Python (python) [False False False … False False False] Building the Recurrent Neural network As I stated earlier, I will use the Recurrent Neural networks for next word prediction model. Here I will use the LSTM model, which is a very powerful RNN. model = Sequential() model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words)))) model.add(Dense(len(unique_words))) model.add(Activation('softmax')) model = Sequential()model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words))))model.add(Dense(len(unique_words)))model.add(Activation('softmax')) Code language: Python (python) Training the Next Word Prediction Model I will be training the next word prediction model with 20 epochs: optimizer = RMSprop(lr=0.01) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) history = model.fit(X, Y, validation_split=0.05, batch_size=128, epochs=2, shuffle=True).history optimizer = RMSprop(lr=0.01)model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])history = model.fit(X, Y, validation_split=0.05, batch_size=128, epochs=2, shuffle=True).history Code language: Python (python) Now we have successfully trained our model, before moving forward to evaluating our model, it will be better to save this model for our future use. model.save('keras_next_word_model.h5') pickle.dump(history, open(""history.p"", ""wb"")) model = load_model('keras_next_word_model.h5') history = pickle.load(open(""history.p"", ""rb"")) model.save('keras_next_word_model.h5')pickle.dump(history, open(""history.p"", ""wb""))model = load_model('keras_next_word_model.h5')history = pickle.load(open(""history.p"", ""rb"")) Code language: Python (python) Evaluating the Next Word Prediction Model Now let’s have a quick look at how our model is going to behave based on its accuracy and loss changes while training: plt.plot(history['acc']) plt.plot(history['val_acc']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.plot(history['acc'])plt.plot(history['val_acc'])plt.title('model accuracy')plt.ylabel('accuracy')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left') Code language: Python (python) Code language: Python (python) plt.plot(history['loss']) plt.plot(history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.plot(history['loss'])plt.plot(history['val_loss'])plt.title('model loss')plt.ylabel('loss')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left') Testing Next Word Prediction Model Now let’s build a python program to predict the next word using our trained model. For this, I will define some essential functions that will be used in the process. def prepare_input(text): x = np.zeros((1, SEQUENCE_LENGTH, len(chars))) for t, char in enumerate(text): x[0, t, char_indices[char]] = 1. return x def prepare_input(text): x = np.zeros((1, SEQUENCE_LENGTH, len(chars))) for t, char in enumerate(text): x[0, t, char_indices[char]] = 1. return x Code language: Python (python) Now before moving forward, let’s test the function, make sure you use a lower() function while giving input : prepare_input(""This is an example of input for our LSTM"".lower()) prepare_input(""This is an example of input for our LSTM"".lower()) Code language: Python (python) array([[[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.]]]) Note that the sequences should be 40 characters (not words) long so that we could easily fit it in a tensor of the shape (1, 40, 57). Not before moving forward, let’s check if the created function is working correctly. def prepare_input(text): x = np.zeros((1, WORD_LENGTH, len(unique_words))) for t, word in enumerate(text.split()): print(word) x[0, t, unique_word_index[word]] = 1 return x prepare_input(""It is not a lack"".lower()) def prepare_input(text): x = np.zeros((1, WORD_LENGTH, len(unique_words))) for t, word in enumerate(text.split()): print(word) x[0, t, unique_word_index[word]] = 1 return xprepare_input(""It is not a lack"".lower()) Code language: Python (python) array([[[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.]]]) Now I will create a function to return samples: def sample(preds, top_n=3): preds = np.asarray(preds).astype('float64') preds = np.log(preds) exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) return heapq.nlargest(top_n, range(len(preds)), preds.take) def sample(preds, top_n=3): preds = np.asarray(preds).astype('float64') preds = np.log(preds) exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds)​ return heapq.nlargest(top_n, range(len(preds)), preds.take) Code language: Python (python) And now I will create a function for next word prediction: def predict_completion(text): original_text = text generated = text completion = '' while True: x = prepare_input(text) preds = model.predict(x, verbose=0)[0] next_index = sample(preds, top_n=1)[0] next_char = indices_char[next_index] text = text[1:] + next_char completion += next_char if len(original_text + completion) + 2 &amp;amp;gt; len(original_text) and next_char == ' ': return completion def predict_completion(text): original_text = text generated = text completion = '' while True: x = prepare_input(text) preds = model.predict(x, verbose=0)[0] next_index = sample(preds, top_n=1)[0] next_char = indices_char[next_index] text = text[1:] + next_char completion += next_char if len(original_text + completion) + 2 &amp;amp;gt; len(original_text) and next_char == ' ': return completion Code language: Python (python) This function is created to predict the next word until space is generated. It will do this by iterating the input, which will ask our RNN model and extract instances from it. Now I will modify the above function to predict multiple characters: def predict_completions(text, n=3): x = prepare_input(text) preds = model.predict(x, verbose=0)[0] next_indices = sample(preds, n) return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices] def predict_completions(text, n=3): x = prepare_input(text) preds = model.predict(x, verbose=0)[0] next_indices = sample(preds, n) return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices] Code language: Python (python) Now I will use the sequence of 40 characters that we can use as a base for our predictions. quotes = [ ""It is not a lack of love, but a lack of friendship that makes unhappy marriages."", ""That which does not kill us makes us stronger."", ""I'm not upset that you lied to me, I'm upset that from now on I can't believe you."", ""And those who were seen dancing were thought to be insane by those who could not hear the music."", ""It is hard enough to remember my opinions, without also remembering my reasons for them!"" ] quotes = [ ""It is not a lack of love, but a lack of friendship that makes unhappy marriages."", ""That which does not kill us makes us stronger."", ""I'm not upset that you lied to me, I'm upset that from now on I can't believe you."", ""And those who were seen dancing were thought to be insane by those who could not hear the music."", ""It is hard enough to remember my opinions, without also remembering my reasons for them!""] Code language: Python (python) Now finally, we can use the model to predict the next word: for q in quotes: seq = q[:40].lower() print(seq) print(predict_completions(seq, 5)) print() for q in quotes: seq = q[:40].lower() print(seq) print(predict_completions(seq, 5)) print() Code language: Python (python) it is not a lack of love, but a lack of ['the ', 'an ', 'such ', 'man ', 'present, '] that which does not kill us makes us str ['ength ', 'uggle ', 'ong ', 'ange ', 'ive '] i'm not upset that you lied to me, i'm u ['nder ', 'pon ', 'ses ', 't ', 'uder '] and those who were seen dancing were tho ['se ', 're ', 'ugh ', ' servated ', 't ']it is hard enough to remember my opinion [' of ', 's ', ', ', 'nof ', 'ed '] Also Read: Data Augmentation in Deep Learning. I hope you liked this article of Next Word Prediction Model, feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Next Word Prediction Model
2020-07-21 01:10:19;Where Binary Classification distinguish between two classes, Multiclass Classification or Multinomial Classification can distinguish between more than two classes.Some algorithms such as SGD classifiers, Random Forest Classifiers, and Naive Bayes classification are capable of handling multiple classes natively. Others such as Logistic Regression or Support Vector Machine Classifiers are strictly binary classifiers. However, there are various strategies that you can use to perform multiclass classification with multiple binary classifiers.;https://thecleverprogrammer.com/2020/07/21/multiclass-classification/;['sklearn'];1.0;[];['Regression', 'ML', 'Logistic Regression', 'Random Forest', 'Naive Bayes', 'Classification'];['detect', 'regression', 'predict', 'fit', 'model', 'machine learning', 'random forest', 'classif', 'naive bayes', 'train', 'logistic regression'];"Where Binary Classification distinguish between two classes, Multiclass Classification or Multinomial Classification can distinguish between more than two classes. Some algorithms such as SGD classifiers, Random Forest Classifiers, and Naive Bayes classification are capable of handling multiple classes natively. Others such as Logistic Regression or Support Vector Machine Classifiers are strictly binary classifiers. However, there are various strategies that you can use to perform multiclass classification with multiple binary classifiers. Techniques of Multiclass Classification There are two Techniques of Multiclass Classification, OvO and OvR, let’s go through both these techniques one by one: OvR Strategy One way to create a system that can classify the digit imsges into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit ( a 0 – detector, a 1 – detector, and so on). Then when you want to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outputs the highest score. This is called the one-versus-the-rest (OvR) strategy also known as one-versus-all. OvO Strategy Another strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy. If there are N classes, you need to train N × (N – 1)/2 classifiers. For the MNIST problem, this means training 45 binary classifiers. When you want to classify an image, you have to run the image through all 45 classifiers and see which class wins the most duels. The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish. Training a Multiclass Classification Model Some algorithms such as Support Vector Machine classifiers scale poorly with the size of the training set. For these algorithms OvO is preferred because it is faster to train many classifiers on small training sets than to train few classifiers on large training sets. For most binary classification algorithms, however, OvR is preferred. Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvR or OvO, depending on the algorithm. Let’s try this with a Support Vector Machine classifier, but before I suggest you to go through my article on Binary Classification, because I will use the same classification problem so that you can understand the difference between training a binary classification and a multiclass classification. I will not start the code here from beginning, you can continue this code from the end of your binary classification model: from sklearn.svm import SVC svm_clf = SVC(gamma=""auto"", random_state=42) svm_clf.fit(X_train[:1000], y_train[:1000]) # y_train, not y_train_5 svm_clf.predict([some_digit])Code language: Python (python) array([5], dtype=uint8) That was easy, this code trains the SVC on the training set using the original target class from 0 to 9 (y_train), instead of the 5-versus-the-rest target classes (y_train_5). Then it makes a prediction (a correct one in this case). Under the hood, Scikit-Learn actually used the OvO strategy: it trained 45 binary classifiers, got their decision scores for the image, and selected the class that won the most duels. If you call the decision_function() method, you will see that it returns 10 scores per instance (instead of just 1). That’s one score per class: some_digit_scores = svm_clf.decision_function([some_digit]) some_digit_scoresCode language: Python (python) array([[ 2.92492871, 7.02307409, 3.93648529, 0.90117363, 5.96945908, 9.5 , 1.90718593, 8.02755089, -0.13202708, 4.94216947]]) The highest score is indeed the one corresponding to class 5: np.argmax(some_digit_scores)Code language: Python (python) 5 If you want to force Scikit-Learn to use one-versus-one or one-versus-the-rest, you can use the OneVsOneClassifier of OneVsRestClassifier classes. Simply create an instance and pass a Classifier to its constructor. For example, this code creates a multiclass classification using the OvR strategy, based on SVC: from sklearn.multiclass import OneVsRestClassifier ovr_clf = OneVsRestClassifier(SVC(gamma=""auto"", random_state=42)) ovr_clf.fit(X_train[:1000], y_train[:1000]) ovr_clf.predict([some_digit])Code language: Python (python) array([5], dtype=uint8) len(ovr_clf.estimators_)Code language: Python (python) 10 Training an SGDClassifier is just as easy: sgd_clf.fit(X_train, y_train) sgd_clf.predict([some_digit])Code language: Python (python) array([5], dtype=uint8) This time Scikit-Learn did not have to run OvR or OvO because SGD classifiers can directly classify instances into multiple classes. The decision_function() method now returns one value per class. Let’s look at the score that SGD classifier assigned to each class: sgd_clf.decision_function([some_digit])Code language: Python (python) array([[-15955.22627845, -38080.96296175, -13326.66694897, 573.52692379, -17680.6846644 , 2412.53175101, -25526.86498156, -12290.15704709, -7946.05205023, -10631.35888549]]) Now of course you want to evaluate this multiclass classification. I will use the cross-validation function to evaluate the SGDClassifier’s accuracy: cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=""accuracy"")Code language: Python (python) array([0.8489802 , 0.87129356, 0.86988048]) It gets over 84 percent on all test folds. If you used a random classifier, you would get 10 percent accuracy, so this is not such a bad score, but you can still do much better. Simply scaling the inputs increases accuracy above 89 percent: from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train.astype(np.float64)) cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=""accuracy"")Code language: Python (python) array([0.89707059, 0.8960948 , 0.90693604]) Error Analysis of Multiclass Classification Now, let’s look at the confusion matrix first. You need to make predictions using the cross_val_predict() function, then call the confusion_matrix() function: y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) conf_mx = confusion_matrix(y_train, y_train_pred) conf_mxCode language: Python (python) array([[5578, 0, 22, 7, 8, 45, 35, 5, 222, 1], [ 0, 6410, 35, 26, 4, 44, 4, 8, 198, 13], [ 28, 27, 5232, 100, 74, 27, 68, 37, 354, 11], [ 23, 18, 115, 5254, 2, 209, 26, 38, 373, 73], [ 11, 14, 45, 12, 5219, 11, 33, 26, 299, 172], [ 26, 16, 31, 173, 54, 4484, 76, 14, 482, 65], [ 31, 17, 45, 2, 42, 98, 5556, 3, 123, 1], [ 20, 10, 53, 27, 50, 13, 3, 5696, 173, 220], [ 17, 64, 47, 91, 3, 125, 24, 11, 5421, 48], [ 24, 18, 29, 67, 116, 39, 1, 174, 329, 5152]]) That’s a lot of numbers. It’s often more convenient to look at an image representing of the confusion matrix, using Matplotlib’s matshow() function: # since sklearn 0.22, you can use sklearn.metrics.plot_confusion_matrix() def plot_confusion_matrix(matrix): """"""If you prefer color and a colorbar"""""" fig = plt.figure(figsize=(8,8)) ax = fig.add_subplot(111) cax = ax.matshow(matrix) fig.colorbar(cax) plt.matshow(conf_mx, cmap=plt.cm.gray) plt.show()Code language: Python (python) Let’s focus the plot on errors. First we need to divide each value in the confusion matrix by the number of images in the corresponding class so that you can campare error rates instead of absolute numbers of errors: row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums np.fill_diagonal(norm_conf_mx, 0) plt.matshow(norm_conf_mx, cmap=plt.cm.gray) save_fig(""confusion_matrix_errors_plot"", tight_layout=False) plt.show()Code language: Python (python) Analyzing individual errors can also be a good way to gain insights on what your classifier is doing and why it is failing, but it is more difficult and time consuming. For example, let’s plot examples of 3s and 5s: cl_a, cl_b = 3, 5 X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)] X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)] X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)] X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)] plt.figure(figsize=(8,8)) plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5) plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5) plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5) plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5) save_fig(""error_analysis_digits_plot"") plt.show()Code language: Python (python) Also Read: 10 Machine Learning Projects to Boost your Portfolio. The main difference between the 3s and 5s is the position of the small line that joins the top line to bottom arc. If you draw a 3 with the junction slightly shifted to the left, the classifier might classify it as 5, and vice versa. So I hope you liked this article on Multiclass Classification. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Multiclass Classification
2020-07-21 20:19:05;"Machine Learning Pipelines performs a complete workflow with an ordered sequence of the process involved in a Machine Learning task. In most of the functions in Machine Learning, the data that you work with is barely in a format for training the model with it’s the best performance.There are several steps in the process of training a machine learning model, like encoding, categorical variables, feature scaling, and normalization. The preprocessing package of Scikit-Learn provides all these functions that can be easily used as transformations.But, in a typical workflow of a Machine Learning task, you need to apply all the processes of transformations at least two times. The first time when you train the model and then when you use the trained model on the new data. On the other hand, you can create a function to apply all the transformations and reuse on the original data by calling the function, but you would still need to run this first and call the model separately. So to tackle this, we have Machine Learning Pipelines that is a method to simplify this process. The most essential benefits that Machine Learning Pipelines provides are:In this article, I will take you through the implementation of Machine Learning Pipelines in a Machine Learning Project. First, I will transform the dataset according to our needs; then, I will move towards the implementation of the Machine Learning Pipelines.";https://thecleverprogrammer.com/2020/07/21/pipelines-in-machine-learning/;['sklearn'];1.0;['CV'];['ML', 'Random Forest', 'CV', 'Classification'];['predict', 'fit', 'model', 'loss', 'machine learning', 'random forest', 'classif', 'training data', 'train'];"Machine Learning Pipelines performs a complete workflow with an ordered sequence of the process involved in a Machine Learning task. In most of the functions in Machine Learning, the data that you work with is barely in a format for training the model with it’s the best performance. There are several steps in the process of training a machine learning model, like encoding, categorical variables, feature scaling, and normalization. The preprocessing package of Scikit-Learn provides all these functions that can be easily used as transformations. But, in a typical workflow of a Machine Learning task, you need to apply all the processes of transformations at least two times. The first time when you train the model and then when you use the trained model on the new data. On the other hand, you can create a function to apply all the transformations and reuse on the original data by calling the function, but you would still need to run this first and call the model separately. So to tackle this, we have Machine Learning Pipelines that is a method to simplify this process. The most essential benefits that Machine Learning Pipelines provides are: Machine Learning Pipelines will make the workflow of your task very much easier to read and understand.The Pipelines in Machine Learning enforce robust implementation of the process involved in your task.In the end, it will make your work more reproducible. Source – datanami In this article, I will take you through the implementation of Machine Learning Pipelines in a Machine Learning Project. First, I will transform the dataset according to our needs; then, I will move towards the implementation of the Machine Learning Pipelines. Data Preparation (Transformation) First I will transform the data by using the pandas package in Python. The data that I have used in this article can be easily downloaded from here. import pandas as pd train = pd.read_csv('train.csv') test = pd.read_csv('test.csv') train = train.drop('Loan_ID', axis=1) train.dtypesCode language: Python (python) Gender object Married object Dependents object Education object Self_Employed object ApplicantIncome int64 CoapplicantIncome float64 LoanAmount float64 Loan_Amount_Term float64 Credit_History float64 Property_Area object Loan_Status object dtype: object Before building a Machine Learning Pipeline, I will split the training data into train and test sets to validate the performance of our model. X = train.drop('Loan_Status', axis=1) y = train['Loan_Status'] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)Code language: Python (python) Building Machine Learning Pipelines The first step in building a pipeline is to define the type of each transformer. In simple words it means to create transformers according to the type of their variables. from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder numeric_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]) categorical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])Code language: Python (python) Now I will use a Column Transformer to apply all the transformations to their respective columns in the dataframe. numeric_features = train.select_dtypes(include=['int64', 'float64']).columns categorical_features = train.select_dtypes(include=['object']).drop(['Loan_Status'], axis=1).columns from sklearn.compose import ColumnTransformer preprocessor = ColumnTransformer( transformers=[ ('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, categorical_features)])Code language: Python (python) Fitting Classifiers in Machine Learning Pipelines The next step is to build a pipeline that can easily combine the transformations created above with a Classifier. In this task I will choose a Random Forest Classifier. from sklearn.ensemble import RandomForestClassifier rf = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', RandomForestClassifier())])Code language: Python (python) Now you can easily call the fit() method on raw data, all the preprocessing process will be applied by doing so: rf.fit(X_train, y_train)Code language: Python (python) Pipeline(memory=None, steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3, transformer_weights=None, transformers=[('num', Pipeline(memory=None, steps=[('imputer', SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy='median', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean... RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))], verbose=False) Now to predict it on new data, it is straightforward. You just need to call the predict() method and all the process of preprocessing will be applied to it: y_pred = rf.predict(X_test)Code language: Python (python) Model Selection with Machine Learning Pipelines The Pipelines can also be used in the process of Model Selection. Below I will loop the code through a number of classification models provided by Scikit-Learn, for applying the transformations and training the Machine Learning model. from sklearn.metrics import accuracy_score, log_loss from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC, LinearSVC, NuSVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis classifiers = [ KNeighborsClassifier(3), SVC(kernel=""rbf"", C=0.025, probability=True), NuSVC(probability=True), DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier() ] for classifier in classifiers: pipe = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)]) pipe.fit(X_train, y_train) print(classifier) print(""model score: %.3f"" % pipe.score(X_test, y_test))Code language: Python (python) KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=3, p=2, weights='uniform') model score: 0.780 SVC(C=0.025, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False) model score: 0.659 NuSVC(break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf', max_iter=-1, nu=0.5, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False) model score: 0.797 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best') model score: 0.724 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) model score: 0.780 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0, n_estimators=50, random_state=None) model score: 0.805 GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, presort='deprecated', random_state=None, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) model score: 0.789 The Pipelines can also be used in finding the best performing parameters using the grid search algorithm. If you don’t know how grid search works, you can learn it from here. Now I will apply the pipeline with the grid search algorithm: param_grid = { 'classifier__n_estimators': [200, 500], 'classifier__max_features': ['auto', 'sqrt', 'log2'], 'classifier__max_depth' : [4,5,6,7,8], 'classifier__criterion' :['gini', 'entropy']} from sklearn.model_selection import GridSearchCV CV = GridSearchCV(rf, param_grid, n_jobs= 1) CV.fit(X_train, y_train) print(CV.best_params_) print(CV.best_score_)Code language: Python (python) {‘classifier__criterion’: ‘gini’, ‘classifier__max_depth’: 4, ‘classifier__max_features’: ‘auto’, ‘classifier__n_estimators’: 200} 0.8124922696351268 I work on a lot of Machine Learning Projects. At the initial phase of my career, I used to ignore pipelines in my tasks. But since I started using the pipelines in my models, I find it easy to work whenever I see the same kind of dataset. I hope you liked this article on Machine Learning Pipelines. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Pipelines in Machine Learning
2020-07-21 11:35:20;TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualize the computation graphs, analyze training statistics, view images generated by your model, visualize complex multidimensional data automatically when you install TensorFlow, so you already have it.;https://thecleverprogrammer.com/2020/07/21/tensorboard-for-visualizations/;['keras', 'tensorflow'];1.0;[];['ML', 'ReLu', 'Classification'];['epoch', 'predict', 'fit', 'model', 'loss', 'machine learning', 'relu', 'classif', 'layer', 'train'];"TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualize the computation graphs, analyze training statistics, view images generated by your model, visualize complex multidimensional data automatically when you install TensorFlow, so you already have it. Getting Started with TensorBoard To use TensorBoard, you must modify your program so that it outputs the data you want to visualize to special binary log files called event files. Each binary data record is called a summary. The TensorBoard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations. This allows you to visualize live data such as learning curves during training. In general, you want to point the TensorBoard server to a root log directory and configure your program so that it writes to a different subdirectory every time it runs. this way, the same TensorBoard server instance will allow you to visualize and compare data from multiple runs of your program, without getting mixed up. Let’s get started by loading the TensorBoard notebook extension: # Load the TensorBoard notebook extension %load_ext tensorboard import tensorflow as tf import datetime # Clear any logs from previous runs !rm -rf ./logs/ Code language: Python (python) I will use the MNIST dataset in this article, I will normalize the data and then write a function that will create a Keras model to classify the images into 10 classes: mnist = tf.keras.datasets.mnist (x_train, y_train),(x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 def create_model(): return tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(512, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation='softmax') ])Code language: Python (python) Using TensorBoard with Keras When we train with a Keral’s Model.fit(), by adding the tf.keras.callbacks. The TensorBoard callbacks will ensure that all the logs have been created and stored successfully. It also allows histogram computations with every epoch where histogram_freq=1. Also Read: 10 Machine Learning Projects to Boost your Portfolio. Now, lets place the logs in a timestamped subdirectory to enable easy selection of multiple training runs: model = create_model() model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) log_dir = ""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"") tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) model.fit(x=x_train, y=y_train, epochs=5, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])Code language: Python (python) Now, start the TensorBoard through the command line or within a notebook experience. The two interfaces are generally the same. In notebooks, use the %tensorboard line magic. On the command line, run the same command without “%”. %tensorboard --logdir logs/fit Using TensorBoard with Other Methods When training with methods such as tf.GradientTape(), use tf.summary to log the required information. Use the same dataset as above, but convert it to tf.data.Dataset to take advantage of batching capabilities: train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)) train_dataset = train_dataset.shuffle(60000).batch(64) test_dataset = test_dataset.batch(64) loss_object = tf.keras.losses.SparseCategoricalCrossentropy() optimizer = tf.keras.optimizers.Adam()Code language: Python (python) Create stateful metrics that can be used to accumulate values during training and logged at any point: # Define our metrics train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32) train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy') test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32) test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')Code language: Python (python) Define the training and test functions: def train_step(model, optimizer, x_train, y_train): with tf.GradientTape() as tape: predictions = model(x_train, training=True) loss = loss_object(y_train, predictions) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables)) train_loss(loss) train_accuracy(y_train, predictions) def test_step(model, x_test, y_test): predictions = model(x_test) loss = loss_object(y_test, predictions) test_loss(loss) test_accuracy(y_test, predictions)Code language: Python (python) To set up the summary writers to write the summaries to the disk in a different logs directory: current_time = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"") train_log_dir = 'logs/gradient_tape/' + current_time + '/train' test_log_dir = 'logs/gradient_tape/' + current_time + '/test' train_summary_writer = tf.summary.create_file_writer(train_log_dir) test_summary_writer = tf.summary.create_file_writer(test_log_dir)Code language: Python (python) Now let’s start training by using the tf.summary.scalar() to log the metrics during the tarining or testing with the scope of summary writers to write the summary in the disk. It’s up to you to select which metrics to log and how how many times to repeat it, you have full control over it. model = create_model() # reset our model EPOCHS = 5 for epoch in range(EPOCHS): for (x_train, y_train) in train_dataset: train_step(model, optimizer, x_train, y_train) with train_summary_writer.as_default(): tf.summary.scalar('loss', train_loss.result(), step=epoch) tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch) for (x_test, y_test) in test_dataset: test_step(model, x_test, y_test) with test_summary_writer.as_default(): tf.summary.scalar('loss', test_loss.result(), step=epoch) tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch) template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}' print (template.format(epoch+1, train_loss.result(), train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100)) # Reset metrics every epoch train_loss.reset_states() test_loss.reset_states() train_accuracy.reset_states() test_accuracy.reset_states()Code language: Python (python) Epoch 1, Loss: 0.24321186542510986, Accuracy: 92.84333801269531, Test Loss: 0.13006582856178284, Test Accuracy: 95.9000015258789 Epoch 2, Loss: 0.10446818172931671, Accuracy: 96.84833526611328, Test Loss: 0.08867532759904861, Test Accuracy: 97.1199951171875 Epoch 3, Loss: 0.07096975296735764, Accuracy: 97.80166625976562, Test Loss: 0.07875105738639832, Test Accuracy: 97.48999786376953 Epoch 4, Loss: 0.05380449816584587, Accuracy: 98.34166717529297, Test Loss: 0.07712937891483307, Test Accuracy: 97.56999969482422 Epoch 5, Loss: 0.041443776339292526, Accuracy: 98.71833038330078, Test Loss: 0.07514958828687668, Test Accuracy: 97.5 Now, let’s open TensorBoard again, but this time point it to another directory: %tensorboard --logdir logs/gradient_tape​x %tensorboard --logdir logs/gradient_tape Also Read: Multiclass Classification in Machine Learning. So, I hope you liked this article on TensorBoard. Feel free to ask you valuable questions in the comment section below. Follow Us: Facebook Instagram";TensorBoard for Visualizations
2020-07-22 11:57:18;Have you ever thought how Snapchat manage to apply amazing filters according to your face? It have been programmed to detect some marks on your face to project a filter according to those marks. In Machine Learning those marks are known as Face Landmarks. In this article I will guide you how you can detect face Landmarks with Machine Learning.Now, I will simply start with importing all the libraries we need for this task. I will use PyTorch in this article to face landmarks detection with Deep Learning. Let’s import all the libraries:The dataset I will choose here to detect Face Landmarks in an official DLIB dataset which consists of over 6666 images of different dimensions. The code below will download the dataset and unzip for further exploration:Now, let’s have a look at what we are working with, to see all the data cleaning and preprocessing opportunities that we need to go through. Here is an example of an image from the dataset we have taken for this task. You can see that the face covers very less amount of space in the image. If we will use this image in the neural network it will take the background also. So like we prepare a text data we will prepare this image dataset for further exploration.;https://thecleverprogrammer.com/2020/07/22/face-landmarks-detection/;['imutils', 'skimage'];1.0;['DL'];['ResNet', 'NN', 'DL', 'ML', 'Image Segmentation'];['detect', 'epoch', 'image segmentation', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'layer', 'filter', 'deep learning', 'resnet', 'anomaly', 'train', 'label'];"Have you ever thought how Snapchat manage to apply amazing filters according to your face? It have been programmed to detect some marks on your face to project a filter according to those marks. In Machine Learning those marks are known as Face Landmarks. In this article I will guide you how you can detect face Landmarks with Machine Learning. Now, I will simply start with importing all the libraries we need for this task. I will use PyTorch in this article to face landmarks detection with Deep Learning. Let’s import all the libraries: import time import cv2 import os import random import numpy as np import matplotlib.pyplot as plt from PIL import Image import imutils import matplotlib.image as mpimg from collections import OrderedDict from skimage import io, transform from math import * import xml.etree.ElementTree as ET import torch import torchvision import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision.transforms.functional as TF from torchvision import datasets, models, transforms from torch.utils.data import Dataset from torch.utils.data import DataLoaderCode language: Python (python) Download the DLIB Dataset The dataset I will choose here to detect Face Landmarks in an official DLIB dataset which consists of over 6666 images of different dimensions. The code below will download the dataset and unzip for further exploration: %%capture if not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'): !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz !tar -xvzf 'ibug_300W_large_face_landmark_dataset.tar.gz' !rm -r 'ibug_300W_large_face_landmark_dataset.tar.gz'Code language: JavaScript (javascript) Visualize the Dataset Now, let’s have a look at what we are working with, to see all the data cleaning and preprocessing opportunities that we need to go through. Here is an example of an image from the dataset we have taken for this task. file = open('ibug_300W_large_face_landmark_dataset/helen/trainset/100032540_1.pts') points = file.readlines()[3:-1] landmarks = [] for point in points: x,y = point.split(' ') landmarks.append([floor(float(x)), floor(float(y[:-1]))]) landmarks = np.array(landmarks) plt.figure(figsize=(10,10)) plt.imshow(mpimg.imread('ibug_300W_large_face_landmark_dataset/helen/trainset/100032540_1.jpg')) plt.scatter(landmarks[:,0], landmarks[:,1], s = 5, c = 'g') plt.show()Code language: Python (python) You can see that the face covers very less amount of space in the image. If we will use this image in the neural network it will take the background also. So like we prepare a text data we will prepare this image dataset for further exploration. Creating Dataset Classes Now Let’s dig deeper into the classes and labels in the dataset. The labels_ibug_300W_train.xml consists of the input images and landmarks and bounding box to crop the face. I will store all these values in the list so that we could easily access them during the training process. class Transforms(): def __init__(self): pass def rotate(self, image, landmarks, angle): angle = random.uniform(-angle, +angle) transformation_matrix = torch.tensor([ [+cos(radians(angle)), -sin(radians(angle))], [+sin(radians(angle)), +cos(radians(angle))] ]) image = imutils.rotate(np.array(image), angle) landmarks = landmarks - 0.5 new_landmarks = np.matmul(landmarks, transformation_matrix) new_landmarks = new_landmarks + 0.5 return Image.fromarray(image), new_landmarks def resize(self, image, landmarks, img_size): image = TF.resize(image, img_size) return image, landmarks def color_jitter(self, image, landmarks): color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1) image = color_jitter(image) return image, landmarks def crop_face(self, image, landmarks, crops): left = int(crops['left']) top = int(crops['top']) width = int(crops['width']) height = int(crops['height']) image = TF.crop(image, top, left, height, width) img_shape = np.array(image).shape landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]]) landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]]) return image, landmarks def __call__(self, image, landmarks, crops): image = Image.fromarray(image) image, landmarks = self.crop_face(image, landmarks, crops) image, landmarks = self.resize(image, landmarks, (224, 224)) image, landmarks = self.color_jitter(image, landmarks) image, landmarks = self.rotate(image, landmarks, angle=10) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) return image, landmarksCode language: Python (python) class FaceLandmarksDataset(Dataset): def __init__(self, transform=None): tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml') root = tree.getroot() self.image_filenames = [] self.landmarks = [] self.crops = [] self.transform = transform self.root_dir = 'ibug_300W_large_face_landmark_dataset' for filename in root[2]: self.image_filenames.append(os.path.join(self.root_dir, filename.attrib['file'])) self.crops.append(filename[0].attrib) landmark = [] for num in range(68): x_coordinate = int(filename[0][num].attrib['x']) y_coordinate = int(filename[0][num].attrib['y']) landmark.append([x_coordinate, y_coordinate]) self.landmarks.append(landmark) self.landmarks = np.array(self.landmarks).astype('float32') assert len(self.image_filenames) == len(self.landmarks) def __len__(self): return len(self.image_filenames) def __getitem__(self, index): image = cv2.imread(self.image_filenames[index], 0) landmarks = self.landmarks[index] if self.transform: image, landmarks = self.transform(image, landmarks, self.crops[index]) landmarks = landmarks - 0.5 return image, landmarks dataset = FaceLandmarksDataset(Transforms())Code language: Python (python) Visualize Train Transforms: Now let’s have a quick look at what we have done until now. I will just visualize the dataset by performing the transformation that the above classes will provide to the dataset: image, landmarks = dataset[0] landmarks = (landmarks + 0.5) * 224 plt.figure(figsize=(10, 10)) plt.imshow(image.numpy().squeeze(), cmap='gray'); plt.scatter(landmarks[:,0], landmarks[:,1], s=8);Code language: Python (python) Split the Dataset for Training and Prediction of Face Landmarks Now, to move further, I will split the dataset into a train and a valid dataset: # split the dataset into validation and test sets len_valid_set = int(0.1*len(dataset)) len_train_set = len(dataset) - len_valid_set print(""The length of Train set is {}"".format(len_train_set)) print(""The length of Valid set is {}"".format(len_valid_set)) train_dataset , valid_dataset, = torch.utils.data.random_split(dataset , [len_train_set, len_valid_set]) # shuffle and batch the datasets train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4) valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=True, num_workers=4)Code language: Python (python) The length of Train set is 6000 The length of Valid set is 666 Testing the shape of input data: images, landmarks = next(iter(train_loader)) print(images.shape) print(landmarks.shape)Code language: Python (python) torch.Size([64, 1, 224, 224]) torch.Size([64, 68, 2]) Define the Face Landmarks Detection Model Now I will use the ResNet18 as our fundamental framework. I will modify the first and last layers so that the layers will fit easily for our purpose: class Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name='resnet18' self.model=models.resnet18() self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features, num_classes) def forward(self, x): x=self.model(x) return xCode language: Python (python) Helper Functions: import sys def print_overwrite(step, total_step, loss, operation): sys.stdout.write('\r') if operation == 'train': sys.stdout.write(""Train Steps: %d/%d Loss: %.4f "" % (step, total_step, loss)) else: sys.stdout.write(""Valid Steps: %d/%d Loss: %.4f "" % (step, total_step, loss)) sys.stdout.flush()Code language: Python (python) Training the Neural Network for Face Landmarks Detection I will now use the Mean Squared Error between the true and predicted face Landmarks: torch.autograd.set_detect_anomaly(True) network = Network() network.cuda() criterion = nn.MSELoss() optimizer = optim.Adam(network.parameters(), lr=0.0001) loss_min = np.inf num_epochs = 10 start_time = time.time() for epoch in range(1,num_epochs+1): loss_train = 0 loss_valid = 0 running_loss = 0 network.train() for step in range(1,len(train_loader)+1): images, landmarks = next(iter(train_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # clear all the gradients before calculating them optimizer.zero_grad() # find the loss for the current step loss_train_step = criterion(predictions, landmarks) # calculate the gradients loss_train_step.backward() # update the parameters optimizer.step() loss_train += loss_train_step.item() running_loss = loss_train/step print_overwrite(step, len(train_loader), running_loss, 'train') network.eval() with torch.no_grad(): for step in range(1,len(valid_loader)+1): images, landmarks = next(iter(valid_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # find the loss for the current step loss_valid_step = criterion(predictions, landmarks) loss_valid += loss_valid_step.item() running_loss = loss_valid/step print_overwrite(step, len(valid_loader), running_loss, 'valid') loss_train /= len(train_loader) loss_valid /= len(valid_loader) print('\n--------------------------------------------------') print('Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid)) print('--------------------------------------------------') if loss_valid &lt; loss_min: loss_min = loss_valid torch.save(network.state_dict(), '/content/face_landmarks.pth') print(""\nMinimum Validation Loss of {:.4f} at epoch {}/{}"".format(loss_min, epoch, num_epochs)) print('Model Saved\n') print('Training Complete') print(""Total Elapsed Time : {} s"".format(time.time()-start_time))Code language: Python (python) Face Landmarks Prediction Now let’s use the model that we trained above on the unseen images in the dataset: Also Read: 10 Machine Learning Projects to Boost your Portfolio. start_time = time.time() with torch.no_grad(): best_network = Network() best_network.cuda() best_network.load_state_dict(torch.load('/content/face_landmarks.pth')) best_network.eval() images, landmarks = next(iter(valid_loader)) images = images.cuda() landmarks = (landmarks + 0.5) * 224 predictions = (best_network(images).cpu() + 0.5) * 224 predictions = predictions.view(-1,68,2) plt.figure(figsize=(10,40)) for img_num in range(8): plt.subplot(8,1,img_num+1) plt.imshow(images[img_num].cpu().numpy().transpose(1,2,0).squeeze(), cmap='gray') plt.scatter(predictions[img_num,:,0], predictions[img_num,:,1], c = 'r', s = 5) plt.scatter(landmarks[img_num,:,0], landmarks[img_num,:,1], c = 'g', s = 5) print('Total number of test images: {}'.format(len(valid_dataset))) end_time = time.time() print(""Elapsed Time : {}"".format(end_time - start_time)) Code language: Python (python) Also Read: Image Segmentation with Deep Learning. I hope you liked this article, Feel free to ask you valuable questions in the comments section below. Don’t forget to subscribe to my daily newsletters if you like my works. Follow Us: Facebook Instagram";Face Landmarks Detection
2020-07-22 00:39:06;Being a practitioner in Machine Learning, you must have gone through an image classification, where the goal is to assign a label or a class to the input image. Now, suppose you want to get where the object is present inside the image, the shape of the object, or what pixel represents what object. In such a case, you have to play with the segment of the image, from which I mean to say to give a label to each pixel of the image. The goal of Image Segmentation is to train a Neural Network which can return a pixel-wise mask of the image.In the real world, Image Segmentation helps in many applications in medical science, self-driven cars, imaging of satellites and many more. Image Segmentation works by studying the image at the lowest level. In this article, I will take you through Image Segmentation with Deep Learning. Now let’s learn about Image Segmentation by digging deeper into it. I will start by merely importing the libraries that we need for Image Segmentation.I will use the Oxford-IIIT Pets dataset, that is already included in Tensorflow:The code below performs a simple image augmentation. Like we prepare the data before doing any machine learning task based on text analysis. Here I am just preparing the images for Image Segmentation:In the dataset, we already have the required number of training and test sets. So I will continue to use that split of training and test sets:Now let’s have a quick look at an image and it’s mask from the data:;https://thecleverprogrammer.com/2020/07/22/image-segmentation/;['keras', 'tensorflow'];1.0;['NN', 'ANN', 'AI'];['NN', 'DL', 'AI', 'ML', 'ANN', 'ReLu', 'Image Segmentation', 'Pix2Pix', 'U-Net', 'Classification'];['epoch', 'image segmentation', 'predict', 'fit', 'u-net', 'model', 'image classification', 'loss', 'machine learning', 'classif', 'layer', 'neural network', 'relu', 'deep learning', 'pix2pix', 'train', 'label'];"Being a practitioner in Machine Learning, you must have gone through an image classification, where the goal is to assign a label or a class to the input image. Now, suppose you want to get where the object is present inside the image, the shape of the object, or what pixel represents what object. In such a case, you have to play with the segment of the image, from which I mean to say to give a label to each pixel of the image. The goal of Image Segmentation is to train a Neural Network which can return a pixel-wise mask of the image. In the real world, Image Segmentation helps in many applications in medical science, self-driven cars, imaging of satellites and many more. Image Segmentation works by studying the image at the lowest level. In this article, I will take you through Image Segmentation with Deep Learning. Now let’s learn about Image Segmentation by digging deeper into it. I will start by merely importing the libraries that we need for Image Segmentation. import tensorflow as tf from tensorflow_examples.models.pix2pix import pix2pix import tensorflow_datasets as tfds tfds.disable_progress_bar() from IPython.display import clear_output import matplotlib.pyplot as pltCode language: Python (python) I will use the Oxford-IIIT Pets dataset, that is already included in Tensorflow: dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)Code language: Python (python) The code below performs a simple image augmentation. Like we prepare the data before doing any machine learning task based on text analysis. Here I am just preparing the images for Image Segmentation: def normalize(input_image, input_mask): input_image = tf.cast(input_image, tf.float32) / 255.0 input_mask -= 1 return input_image, input_mask @tf.function def load_image_train(datapoint): input_image = tf.image.resize(datapoint['image'], (128, 128)) input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128)) if tf.random.uniform(()) &gt; 0.5: input_image = tf.image.flip_left_right(input_image) input_mask = tf.image.flip_left_right(input_mask) input_image, input_mask = normalize(input_image, input_mask) return input_image, input_mask def load_image_test(datapoint): input_image = tf.image.resize(datapoint['image'], (128, 128)) input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128)) input_image, input_mask = normalize(input_image, input_mask) return input_image, input_maskCode language: Python (python) In the dataset, we already have the required number of training and test sets. So I will continue to use that split of training and test sets: TRAIN_LENGTH = info.splits['train'].num_examples BATCH_SIZE = 64 BUFFER_SIZE = 1000 STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE) test = dataset['test'].map(load_image_test) train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat() train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE) test_dataset = test.batch(BATCH_SIZE)Code language: Python (python) Now let’s have a quick look at an image and it’s mask from the data: def display(display_list): plt.figure(figsize=(15, 15)) title = ['Input Image', 'True Mask', 'Predicted Mask'] for i in range(len(display_list)): plt.subplot(1, len(display_list), i+1) plt.title(title[i]) plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i])) plt.axis('off') plt.show() for image, mask in train.take(1): sample_image, sample_mask = image, mask display([sample_image, sample_mask])Code language: Python (python) Define the Image SegmentationModel The model that I will use here is a modified U-Net. A U-Net contains an encoder and a decoder. In order to learn the robust features, and reducing all the trainable parameters, a pretrained model can be used efficiently as an encoder. OUTPUT_CHANNELS = 3Code language: Python (python) As I already mentioned above, our encoder is a pretrained model which is available and ready to use in tf.keras.applications. This encoder contains some specific outputs from the intermediate layers of the model. Please note that the encoder will not be trained during the process of training. base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False) # Use the activations of these layers layer_names = [ 'block_1_expand_relu', # 64x64 'block_3_expand_relu', # 32x32 'block_6_expand_relu', # 16x16 'block_13_expand_relu', # 8x8 'block_16_project', # 4x4 ] layers = [base_model.get_layer(name).output for name in layer_names] # Create the feature extraction model down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers) down_stack.trainable = FalseCode language: Python (python) The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples: up_stack = [ pix2pix.upsample(512, 3), # 4x4 -&gt; 8x8 pix2pix.upsample(256, 3), # 8x8 -&gt; 16x16 pix2pix.upsample(128, 3), # 16x16 -&gt; 32x32 pix2pix.upsample(64, 3), # 32x32 -&gt; 64x64 ] def unet_model(output_channels): inputs = tf.keras.layers.Input(shape=[128, 128, 3]) x = inputs # Downsampling through the model skips = down_stack(x) x = skips[-1] skips = reversed(skips[:-1]) # Upsampling and establishing the skip connections for up, skip in zip(up_stack, skips): x = up(x) concat = tf.keras.layers.Concatenate() x = concat([x, skip]) # This is the last layer of the model last = tf.keras.layers.Conv2DTranspose( output_channels, 3, strides=2, padding='same') #64x64 -&gt; 128x128 x = last(x) return tf.keras.Model(inputs=inputs, outputs=x)Code language: Python (python) Training the Image Segmentation Model model = unet_model(OUTPUT_CHANNELS) model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])Code language: Python (python) Now before moving forward let’s have a quick look at the resulting output of the trained model: tf.keras.utils.plot_model(model, show_shapes=True)Code language: Python (python) Let’s try out the model to see what it predicts before training: def create_mask(pred_mask): pred_mask = tf.argmax(pred_mask, axis=-1) pred_mask = pred_mask[..., tf.newaxis] return pred_mask[0] def show_predictions(dataset=None, num=1): if dataset: for image, mask in dataset.take(num): pred_mask = model.predict(image) display([image[0], mask[0], create_mask(pred_mask)]) else: display([sample_image, sample_mask, create_mask(model.predict(sample_image[tf.newaxis, ...]))]) show_predictions()Code language: Python (python) Now, Let’s observe how the Image Segmentation model improves while it is training. To accomplish this task, a callback function is defined below: class DisplayCallback(tf.keras.callbacks.Callback): def on_epoch_end(self, epoch, logs=None): clear_output(wait=True) show_predictions() print ('\nSample Prediction after epoch {}\n'.format(epoch+1)) EPOCHS = 20 VAL_SUBSPLITS = 5 VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS model_history = model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, validation_steps=VALIDATION_STEPS, validation_data=test_dataset, callbacks=[DisplayCallback()])Code language: Python (python) Now, let’s have a quick look on the performance of the model: loss = model_history.history['loss'] val_loss = model_history.history['val_loss'] epochs = range(EPOCHS) plt.figure() plt.plot(epochs, loss, 'r', label='Training loss') plt.plot(epochs, val_loss, 'bo', label='Validation loss') plt.title('Training and Validation Loss') plt.xlabel('Epoch') plt.ylabel('Loss Value') plt.ylim([0, 1]) plt.legend() plt.show()Code language: Python (python) Make Predictions using Image Segmentation Let’s make some predictions. In the interest of saving time, the number of epochs was kept small, but you may set this higher to achieve more accurate results: Also Read: Pipelines in Machine Learning. show_predictions(test_dataset, 3)Code language: Python (python) Also Read: 10 Machine Learning Projects to Boost your Portfolio. I hope you liked this article on Image Segmentation with Deep Learning. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Image Segmentation
2020-07-23 23:18:07;In this article, I will take you through a brief explanation of Image Segmentation in Deep Learning. I will only explain the concept behind the image segmentation here in this article. If you want to go through the practical part of Image Segmentation you can see it’s tutorial here.;https://thecleverprogrammer.com/2020/07/23/what-is-image-segmentation/;['keras'];1.0;['NN', 'CNN'];['NN', 'Object Detection', 'DL', 'ML', 'CNN', 'Image Segmentation', 'Classification'];['detect', 'image segmentation', 'machine learning', 'classif', 'layer', 'deep learning', 'object detection', 'train'];"In this article, I will take you through a brief explanation of Image Segmentation in Deep Learning. I will only explain the concept behind the image segmentation here in this article. If you want to go through the practical part of Image Segmentation you can see it’s tutorial here. Image Segmentation in Deep Learning In Image segmentation, each pixel is classified according to the class of the object it belongs to (e.g., road, car, pedestrian, building, etc.), as shown in the figure below. Note that different objects of the same class are not distinguished. For example, all the bicycles on the right side of the segmented image end up as one big lump of pixels. The main difficulty in this task is that when images go through a regular CNN, they gradually lose their spatial resolution ( due to the layers with strides higher than ); so, a regular CNN may end up knowing that there’s a person somewhere in the bottom left of the image, but it will not be much more precise than that. Just like object detection, there are many different approaches to tackle this problem, some quite complex. However, a reasonably simple solution was proposed in the 2015 paper by Jonathan Long et al. The author starts by taking a pre-trained CNN and turning it into an FCN. The CNN applies an overall stride of 32 to the input image, meaning the last layer outputs feature maps 32 times smaller than the input image. This is too coarse, so they add a single upsampling layer that multiplies the resolution by 32. There are several solutions available for upsampling ( increasing the size of an image), such as bilinear interpolation, but that only works reasonably well up to * 4 or *8 instead, they use a transposed convolutional layer. It is equivalent to first stretching the image by inserting empty rows and columns ( full of zeros) and performing a regular convolution. Also Read: 10 Machine Learning Projects to Boost your Portfolio. Alternatively, some people prefer to think of it as a consistent convolutional layer that can be initialized to perform something close to linear interpolation. Still, since it is a trainable layer, it will learn to do better during training. In tf.keras, you can use the conv2DTranspose layer. I hope you liked this article on the concept of Image Segmentation in deep learning. This was a theoretical explanation, you can see the practical guide from here. Follow Us: Facebook Instagram";What is Image Segmentation?
2020-07-26 01:09:34;OpenAI Gym is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical simulations, and so on), so you can train agents, compare them, or develop new Machine Learning algorithms (Reinforcement Learning). OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal is to promote and develop friendly AIs that will benefit humanity (rather than exterminate it).;https://thecleverprogrammer.com/2020/07/26/openai-gym-in-machine-learning/;['keras', 'sklearn', 'tensorflow'];1.0;['ML', 'NN', 'CNN', 'AI'];['NN', 'AI', 'ML', 'CNN', 'RL'];['detect', 'artificial intelligence', 'reinforcement learning', 'fit', 'machine learning', 'reward', 'filter', 'train', 'label'];"OpenAI Gym is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical simulations, and so on), so you can train agents, compare them, or develop new Machine Learning algorithms (Reinforcement Learning). OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal is to promote and develop friendly AIs that will benefit humanity (rather than exterminate it). Installing OpenAI Gym In this article, I will be using the OpenAI gym, a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning agents to interact with. Before installing the toolkit, if you created an isolated environment using virtualenv, you first need to activate it: $ cd $ML_PATH # Your ML working directory (e.g., $HOME/ml) $ source my_env/bin/activate # on Linux or MacOS $ .my_envScriptsactivate # on WindowsCode language: PHP (php) Next, install OpenAI Gym (if you are not using a virtual environment, you will need to add the –user option, or have administrator rights): $ python3 -m pip install -U gym Depending on your system, you may also need to install the Mesa OpenGL Utility (GLU) library (e.g., on Ubuntu 18.04 you need to run apt install libglu1-mesa). This library will be needed to render the first environment. Next, open up a Python shell or a Jupyter notebook or Google Colab and I will first import all the necessary libraries and then I will create an environment with make(): # Python ≥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= ""0.20"" try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x !apt update &amp;&amp; apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb !pip install -q -U tf-agents-nightly pyvirtualdisplay gym[atari] IS_COLAB = True except Exception: IS_COLAB = False # TensorFlow ≥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= ""2.0"" if not tf.config.list_physical_devices('GPU'): print(""No GPU was detected. CNNs can be very slow without a GPU."") if IS_COLAB: print(""Go to Runtime &gt; Change runtime and select a GPU hardware accelerator."") # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) tf.random.set_seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12) # To get smooth animations import matplotlib.animation as animation mpl.rc('animation', html='jshtml') import gymCode language: Python (python) Let’s list all the available environments: gym.envs.registry.all()Code language: Python (python) The Cart-Pole is a very simple environment composed of a cart that can move left or right, and a pole placed vertically on top of it. The agent must move the cart left or right to keep the pole upright. env = gym.make('CartPole-v1')Code language: Python (python) Let’s initialize the environment by calling is a reset() method. This returns an observation: env.seed(42) obs = env.reset()Code language: Python (python) Observations vary depending on the environment. In this case it is a 1D NumPy array composed of 4 floats: they represent the cart’s horizontal position, its velocity, the angle of the pole (0 = vertical), and the angular velocity. obsCode language: Python (python) array([-0.01258566, -0.00156614, 0.04207708, -0.00180545]) An environment can be visualized by calling its render() method, and you can pick the rendering mode (the rendering options depend on the environment). env.render()Code language: Python (python) In this example, we will set mode=”rgb_array” to get an image of the environment as a NumPy array: img = env.render(mode=""rgb_array"") img.shapeCode language: Python (python) (400, 600, 3) def plot_environment(env, figsize=(5,4)): plt.figure(figsize=figsize) img = env.render(mode=""rgb_array"") plt.imshow(img) plt.axis(""off"") return img plot_environment(env) plt.show()Code language: Python (python) Let’s see how to interact with the OpenAI Gym environment. Your agent will need to select an action from an “action space” (the set of possible actions). Let’s see what this environment’s action space looks like: env.action_spaceCode language: Python (python) Discrete(2) Discrete(2) means that the possible actions are integers 0 and 1, which represent accelerating left (0) or right (1). Other environments may have additional discrete actions, or other kinds of actions (e.g., continuous). Since the pole is leaning toward the right (obs[2] > 0), let’s accelerate the cart toward the right: action = 1 # accelerate right obs, reward, done, info = env.step(action) obsCode language: Python (python) array([-0.01261699, 0.19292789, 0.04204097, -0.28092127]) Notice that the cart is now moving toward the right (obs[1] > 0). The pole is still tilted toward the right (obs[2] > 0), but its angular velocity is now negative (obs[3] < 0), so it will likely be tilted toward the left after the next step. plot_environment(env)Code language: Python (python) Looks like it’s doing what we’re telling it to do! The environment also tells the agent how much reward it got during the last step: rewardCode language: Python (python) 1.0 When the game is over, the environment returns done=True: doneCode language: Python (python) False Finally, info is an environment-specific dictionary that can provide some extra information that you may find useful for debugging or for training. For example, in some games, it may indicate how many lives the agent has. infoCode language: Python (python) {} The sequence of steps between the moment the environment is reset until it is done is called an “episode”. At the end of an episode (i.e., when step() returns done=True), you should reset the environment before you continue to use it. if done: obs = env.reset()Code language: Python (python) Hardcoding OpenAI Gym using Simple Policy Algorithm Let’s hardcode a simple policy that accelerates left when the pole is leaning toward the left and accelerates right when the pole is leaning toward the right. We will run this policy to see the average rewards it gets over 500 episodes: env.seed(42) def basic_policy(obs): angle = obs[2] return 0 if angle &lt; 0 else 1 totals = [] for episode in range(500): episode_rewards = 0 obs = env.reset() for step in range(200): action = basic_policy(obs) obs, reward, done, info = env.step(action) episode_rewards += reward if done: break totals.append(episode_rewards)Code language: Python (python) This code is hopefully self-explanatory. Let’s look at the result: np.mean(totals), np.std(totals), np.min(totals), np.max(totals)Code language: Python (python) (41.718, 8.858356280936096, 24.0, 68.0) Well, as expected, this strategy is a bit too basic: the best it did was to keep the poll up for only 68 steps. This environment is considered solved when the agent keeps the poll up for 200 steps. env.seed(42) frames = [] obs = env.reset() for step in range(200): img = env.render(mode=""rgb_array"") frames.append(img) action = basic_policy(obs) obs, reward, done, info = env.step(action) if done: breakCode language: Python (python) Now show the animation: def update_scene(num, frames, patch): patch.set_data(frames[num]) return patch, def plot_animation(frames, repeat=False, interval=40): fig = plt.figure() patch = plt.imshow(frames[0]) plt.axis('off') anim = animation.FuncAnimation( fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval) plt.close() return anim plot_animation(frames)Code language: Python (python) Also, Read: Image Filtering with Machine Learning. I hope you liked this article on OpenAI Gym. If you want me to explore this topic more for you then just mention it in the comments section. Also, follow me on Medium to read more amazing articles. Follow Us: Facebook Instagram";OpenAI Gym in Machine Learning
2020-07-26 23:29:51;The Receiver Operating Characteristic (ROC) curve is a popular tool used with binary classifiers. It is very similar to the precision/recall curve. Still, instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate (FPR). The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to 1 – the true negative rate (TNR), which is the ratio of negative cases that are correctly classified as negative. The TNR is also called specificity.;https://thecleverprogrammer.com/2020/07/26/roc-curve-in-machine-learning/;['sklearn'];1.0;[];['Random Forest', 'Classification'];['predict', 'random forest', 'classif', 'train', 'label'];"The Receiver Operating Characteristic (ROC) curve is a popular tool used with binary classifiers. It is very similar to the precision/recall curve. Still, instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate (FPR). The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to 1 – the true negative rate (TNR), which is the ratio of negative cases that are correctly classified as negative. The TNR is also called specificity. Plotting The ROC Curve I will explain this by using the classification that I did in my article on Binary Classification. I will continue this from where I left in the Binary Classification. To plot the ROC curve, you first use the roc_curve() function to compute the TPR and FPR for various threshold values: from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)​x from sklearn.metrics import roc_curvefpr, tpr, thresholds = roc_curve(y_train_5, y_scores) Then you can plot the FPR against the TPR using Matplotlib: def plot_roc_curve(fpr, tpr, label=None): plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal plt.axis([0, 1, 0, 1]) # Not shown in the book plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown plt.ylabel('True Positive Rate (Recall)', fontsize=16) # Not shown plt.grid(True) # Not shown plt.figure(figsize=(8, 6)) # Not shown plot_roc_curve(fpr, tpr) plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], ""r:"") # Not shown plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], ""r:"") # Not shown plt.plot([4.837e-3], [0.4368], ""ro"") # Not shown save_fig(""roc_curve_plot"") # Not shown plt.show()Code language: Python (python) So there is a trade-off: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). In the output above, The ROC plots, the false positive rate against the true positive rate for all possible thresholds; the red circle highlights the chosen ratio (at 43.68% recall). One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC: from sklearn.metrics import roc_auc_score roc_auc_score(y_train_5, y_scores)Code language: Python (python) 0.9604938554008616 Since the ROC curve is so similar to the precision/recall (PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives. Otherwise, use the ROC curve. Using The ROC Curve in Classification Let’s now train a RandomForestClassifier and compare its ROC curve and ROC AUC score to those of the SGDClassifier. First, you need to get scores for each instance in the training set: from sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(n_estimators=100, random_state=42) y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method=""predict_proba"")Code language: Python (python) The roc_curve() function expects labels and scores, but instead of scores, you can give it class probabilities. Let’s use the positive class’s probability as the score: y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)Code language: Python (python) Now you are ready to plot the ROC. It is useful to plot the first ROC curve as well as to see how they compare: plt.figure(figsize=(8, 6)) plt.plot(fpr, tpr, ""b:"", linewidth=2, label=""SGD"") plot_roc_curve(fpr_forest, tpr_forest, ""Random Forest"") plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], ""r:"") plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], ""r:"") plt.plot([4.837e-3], [0.4368], ""ro"") plt.plot([4.837e-3, 4.837e-3], [0., 0.9487], ""r:"") plt.plot([4.837e-3], [0.9487], ""ro"") plt.grid(True) plt.legend(loc=""lower right"", fontsize=16) save_fig(""roc_curve_comparison_plot"") plt.show()Code language: Python (python) As you can see in the output above, the RandomForestClassifier’s curve looks much better than the SGDClassifier’s: it comes much closer to the top-left corner. As a result, its ROC AUC score is also significantly better: roc_auc_score(y_train_5, y_scores_forest)Code language: Python (python) 0.9983436731328145 I hope you liked this article. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium, to read more amazing articles. Follow Us: Facebook Instagram";ROC Curve in Machine Learning
2020-07-27 11:54:15;Gradient Descent is an optimisation algorithm which is capable of providing optimal performance to a wide range of tasks in Machine Learning. The idea behind a Gradient Descent algorithm is to tweak the parameters using loops to minimize the cost function. In this article, I will take you through the Gradient Descent algorithm in Machine Learning.One of the most important parameters of the Gradient Descent algorithm is the size of its steps which is determined by a Hyperparameter known as the learning rate. If the learning rate is very small then our algorithm will need to cover a lot of loops which may take a long time, and if the learning rate is very high then our algorithm may fail to find the best solution.When you use a Gradient Descent algorithm, you should make sure that all the features we use should have a similar scale, otherwise, it will take very long to converge. One thing you should know that this algorithm is evolved from a linear model. So before diving into how we can use this algorithm, I will first create a linear model which is necessary for our task.;https://thecleverprogrammer.com/2020/07/27/gradient-descent-algorithm-in-machine-learning/;['sklearn'];1.0;[];['ML', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'rank', 'label', 'gradient descent'];"Gradient Descent is an optimisation algorithm which is capable of providing optimal performance to a wide range of tasks in Machine Learning. The idea behind a Gradient Descent algorithm is to tweak the parameters using loops to minimize the cost function. In this article, I will take you through the Gradient Descent algorithm in Machine Learning. One of the most important parameters of the Gradient Descent algorithm is the size of its steps which is determined by a Hyperparameter known as the learning rate. If the learning rate is very small then our algorithm will need to cover a lot of loops which may take a long time, and if the learning rate is very high then our algorithm may fail to find the best solution. When you use a Gradient Descent algorithm, you should make sure that all the features we use should have a similar scale, otherwise, it will take very long to converge. One thing you should know that this algorithm is evolved from a linear model. So before diving into how we can use this algorithm, I will first create a linear model which is necessary for our task. Preparing a Linear Model for Gradient Descent # Python ≥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= ""0.20"" # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12) import numpy as np X = 2 * np.random.rand(100, 1) y = 4 + 3 * X + np.random.randn(100, 1) X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance linear = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) X_new = np.array([[0], [2]]) X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance y_predict = X_new_b.dot(linear) from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X, y) lin_reg.intercept_, lin_reg.coef_ theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6) np.linalg.pinv(X_b).dot(y)Code language: Python (python) array([[4.21509616], [2.77011339]]) If you want to go through the above code with a detailed explanation you can refer to this article. Implementing a Gradient Descent Algorithm To implement a Gradient Descent algorithm you first need to calculate the Gradient cost concerning the parameters of each model. Now let’s go through the implementation of this algorithm: eta = 0.1 # learning rate n_iterations = 1000 m = 100 theta = np.random.randn(2,1) # random initialization for iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradientsCode language: Python (python) The above algorithm doesn’t look much hard now let’s have a quick look at the resulting theta: thetaCode language: Python (python) array([[4.21509616], [9.75532293]]) So this is exactly what a gradient descent algorithm does, we just found some perfect results. But what if we will use a different learning rate known as eta. Let’s use a different learning rate and also see how we can plot this algorithm using Matplotlib: theta_path_bgd = [] def plot_gradient_descent(theta, eta, theta_path=None): m = len(X_b) plt.plot(X, y, ""b."") n_iterations = 1000 for iteration in range(n_iterations): if iteration &lt; 10: y_predict = X_new_b.dot(theta) style = ""b-"" if iteration &gt; 0 else ""r--"" plt.plot(X_new, y_predict, style) gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients if theta_path is not None: theta_path.append(theta) plt.xlabel(""$x_1$"", fontsize=18) plt.axis([0, 2, 0, 15]) plt.title(r""$eta = {}$"".format(eta), fontsize=16) np.random.seed(42) theta = np.random.randn(2,1) # random initialization plt.figure(figsize=(10,4)) plt.subplot(131); plot_gradient_descent(theta, eta=0.02) plt.ylabel(""$y$"", rotation=0, fontsize=18) plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd) plt.subplot(133); plot_gradient_descent(theta, eta=0.5) save_fig(""gradient_descent_plot"") plt.show()Code language: Python (python) Let’s Go Through The Above Gradient Descent Algorithm The plot above shows our algorithm with various learning rates: One the left, the learning rate is very low. It shows that the algorithm will reach the solution but it will take a long time because of the low learning rate. In the middle, the learning rate looks very good. It shows that the algorithm will reach the solutions using a few numbers of iterations. The right one shows that the learning rate is very high. Here the algorithm dives all over the place which shows that our algorithm is getting away from the solution with every step. Now you may think how to set the number of iterations. If the iterations are very less you will be far from reaching the solution and if the iterations are top high then you will just waste your time because the model parameters will never change with that. A simple solution is to set a very large number of iterations and to interrupt your gradient descent algorithm when the Gradient vectors become small. I hope you liked this article on Gradient Descent algorithm in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to read more amazing articles. Follow Us: Facebook Instagram";Gradient Descent Algorithm in Machine Learning
2020-07-27 22:27:03;"What will you do if your data is a bit more complicated than a straight line? A good alternative for you is that you can use a linear model to fit in a nonlinear data. You can use the add powers of ever feature as the new features, and then you can use the new set of features to train a Linear Model. In Machine Learning, this technique is known as Polynomial Regression.Let’s understand Polynomial Regression from an example. I will first generate a nonlinear data which is based on a quadratic equation. A quadratic equation is in the form of ax2+bx+c; I will first import all the necessary libraries then I will create a quadratic equation:Now let’s make a quadratic equation:";https://thecleverprogrammer.com/2020/07/27/polynomial-regression-algorithm/;['sklearn'];1.0;[];['ML', 'Linear Regression', 'Regression'];['regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'training data', 'train', 'validation data', 'label', 'gradient descent'];"What will you do if your data is a bit more complicated than a straight line? A good alternative for you is that you can use a linear model to fit in a nonlinear data. You can use the add powers of ever feature as the new features, and then you can use the new set of features to train a Linear Model. In Machine Learning, this technique is known as Polynomial Regression. Let’s understand Polynomial Regression from an example. I will first generate a nonlinear data which is based on a quadratic equation. A quadratic equation is in the form of ax2+bx+c; I will first import all the necessary libraries then I will create a quadratic equation: # Python ≥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= ""0.20"" # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12) import numpy as np import numpy.random as rnd np.random.seed(42)Code language: Python (python) Now let’s make a quadratic equation: m = 100 X = 6 * np.random.rand(m, 1) - 3 y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1) plt.plot(X, y, ""b."") plt.xlabel(""$x_1$"", fontsize=18) plt.ylabel(""$y$"", rotation=0, fontsize=18) plt.axis([-3, 3, 0, 10]) save_fig(""quadratic_data_plot"") plt.show()Code language: Python (python) Polynomial Regression A straight line will never fit on a nonlinear data like this. Now, I will use the Polynomial Features algorithm provided by Scikit-Learn to transfer the above training data by adding the square all features present in our training data as new features for our model: from sklearn.preprocessing import PolynomialFeatures poly_features = PolynomialFeatures(degree=2, include_bias=False) X_poly = poly_features.fit_transform(X) X[0]Code language: Python (python) array([-0.75275929]) X_poly[0]Code language: Python (python) array([-0.75275929, 0.56664654]) Now X_poly contains the original features of X plus the square of the features. Now we can use the Linear Regression algorithm to fit in our new training data: from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X_poly, y) lin_reg.intercept_, lin_reg.coef_Code language: Python (python) (array([1.78134581]), array([[0.93366893, 0.56456263]])) X_new=np.linspace(-3, 3, 100).reshape(100, 1) X_new_poly = poly_features.transform(X_new) y_new = lin_reg.predict(X_new_poly) plt.plot(X, y, ""b."") plt.plot(X_new, y_new, ""r-"", linewidth=2, label=""Predictions"") plt.xlabel(""$x_1$"", fontsize=18) plt.ylabel(""$y$"", rotation=0, fontsize=18) plt.legend(loc=""upper left"", fontsize=14) plt.axis([-3, 3, 0, 10]) save_fig(""quadratic_predictions_plot"") plt.show()Code language: Python (python) You must know that when we have multiple features, the Polynomial Regression is very much capable of finding the relationships between all the features in the data. This is possible because the Polynomial Features adds all the combinations of features up to a provided degree. Learning Curves in Polynomial Regression If you use a high-degree Polynomial Regression, you will end up fitting the training data in a much better way than a Linear Regression. Let’s understand this with an example: from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline for style, width, degree in ((""g-"", 1, 300), (""b--"", 2, 2), (""r-+"", 2, 1)): polybig_features = PolynomialFeatures(degree=degree, include_bias=False) std_scaler = StandardScaler() lin_reg = LinearRegression() polynomial_regression = Pipeline([ (""poly_features"", polybig_features), (""std_scaler"", std_scaler), (""lin_reg"", lin_reg), ]) polynomial_regression.fit(X, y) y_newbig = polynomial_regression.predict(X_new) plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width) plt.plot(X, y, ""b."", linewidth=3) plt.legend(loc=""upper left"") plt.xlabel(""$x_1$"", fontsize=18) plt.ylabel(""$y$"", rotation=0, fontsize=18) plt.axis([-3, 3, 0, 10]) save_fig(""high_degree_polynomials_plot"") plt.show()Code language: Python (python) The high-degree Polynomial Regression model is overfitting the training data, where a linear model is underfitting it. So the model that will perform the best, in this case, is quadratic because the data is generated using a quadratic equation. But you never know, what function is used in creating the data. So how you will decide how complex your model should be? How to analyse whether your model is overfitting or underfitting the data? Also, Read: Gradient Descent Algorithm in Machine Learning. A right way to generalise the performance of our model is to look at the learning curves. Learning curves are plots of the performance of a model on the training set and the validation set as a function of the size of the training set. To generate learning curves, train the model several times on different size of subsets of the training data. Now let’s understand this with an example. The code below defines a function that can plot the learning curves of a model by using the training data: from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split def plot_learning_curves(model, X, y): X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10) train_errors, val_errors = [], [] for m in range(1, len(X_train)): model.fit(X_train[:m], y_train[:m]) y_train_predict = model.predict(X_train[:m]) y_val_predict = model.predict(X_val) train_errors.append(mean_squared_error(y_train[:m], y_train_predict)) val_errors.append(mean_squared_error(y_val, y_val_predict)) plt.plot(np.sqrt(train_errors), ""r-+"", linewidth=2, label=""train"") plt.plot(np.sqrt(val_errors), ""b-"", linewidth=3, label=""val"") plt.legend(loc=""upper right"", fontsize=14) plt.xlabel(""Training set size"", fontsize=14) plt.ylabel(""RMSE"", fontsize=14) Code language: Python (python) Now let’s look at the learning curves of our model using the function that I created above: lin_reg = LinearRegression() plot_learning_curves(lin_reg, X, y) plt.axis([0, 80, 0, 3]) save_fig(""underfitting_learning_curves_plot"") plt.show() Code language: Python (python) So the output resulted in underfitting data. If your data is underfitting the training data, adding more instances will not help. You need to use a more sophisticated machine learning model or come up with some better features. Now let’s look at the learning curves of a 10th-degree polynomial regression model on the same data: from sklearn.pipeline import Pipeline polynomial_regression = Pipeline([ (""poly_features"", PolynomialFeatures(degree=10, include_bias=False)), (""lin_reg"", LinearRegression()), ]) plot_learning_curves(polynomial_regression, X, y) plt.axis([0, 80, 0, 3]) save_fig(""learning_curves_plot"") plt.show() Code language: Python (python) The learning rate are looking a bit likely to the previous ones, but there are some major differences here: The error on the training data is very much lower than the previous learning curves we explored using a linear regression model.There is a gap between the curves, which means that the performance of the model is very much better on the training data than the validation data. I hope you liked this article on Polynomial Regression and learning curves in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to read more amazing articles. Follow Us: Facebook Instagram";Polynomial Regression Algorithm
2020-07-28 08:14:08;Audio Feature Extraction has been one of the significant focus of Machine Learning over the years. The most frequent common state of data is a text where we can perform feature extraction quite smoothly. Then we have Feature Extraction for the image, which is a challenging task. Now I will show you Audio Feature Extraction, which is a bit more complicated task in Machine Learning. Feature Extraction is the process of reducing the number of features in the data by creating new features using the existing ones. The new extracted features must be able to summarise most of the information contained in the original set of elements in the data.;https://thecleverprogrammer.com/2020/07/28/audio-feature-extraction/;['tensorflow'];1.0;[];['ML', 'K-Means', 'Clustering', 'Regression'];['clustering', 'k-means', 'epoch', 'regression', 'model', 'machine learning', 'train', 'label'];"Audio Feature Extraction has been one of the significant focus of Machine Learning over the years. The most frequent common state of data is a text where we can perform feature extraction quite smoothly. Then we have Feature Extraction for the image, which is a challenging task. Now I will show you Audio Feature Extraction, which is a bit more complicated task in Machine Learning. Feature Extraction is the process of reducing the number of features in the data by creating new features using the existing ones. The new extracted features must be able to summarise most of the information contained in the original set of elements in the data. Audio Feature Extraction Audio Feature Extraction plays a significant part in analyzing the audios. The idea is to extract those powerful features that can help in characterizing all the complex nature of audio signals which at the end will help in to identify the discriminatory subspaces of audio and all the keys that you need to analyze sound signals. Now let’s start with importing all the libraries that we need for this task: import tensorflow as tf import numpy as np import pandas as pd from pyAudioAnalysis import audioBasicIO from pyAudioAnalysis import audioFeatureExtraction import matplotlib.pyplot as plt import osCode language: Python (python) Audio Basic IO is used to extract the audio data like a data frame and creating sample data for audio signals. Audio Feature Extraction is responsible for obtaining all the features from the signals of audio that we need for this task. Now I will define a utility function that will help us in taking a file name as argument: def preProcess( fileName ): [Fs, x] = audioBasicIO.readAudioFile(fileName) if( len( x.shape ) &amp;gt; 1 and x.shape[1] == 2 ): x = np.mean( x, axis = 1, keepdims = True ) else: x = x.reshape( x.shape[0], 1 ) F, f_names = audioFeatureExtraction.stFeatureExtraction( x[ :, 0 ], Fs, 0.050*Fs, 0.025*Fs ) return (f_names, F)Code language: Python (python) Now I would like to use only the chronogram feature from the audio signals, so I will now separate the data from our function: def getChromagram( audioData ): temp_data = audioData[ 21 ].reshape( 1, audioData[ 21 ].shape[0] ) chronograph = temp_data for i in range( 22, 33 ): temp_data = audioData[ i ].reshape( 1, audioData[ i ].shape[0] )just say you love me chronograph = np.vstack( [ chronograph, temp_data ] ) return chronographjust say you love meCode language: Python (python) Now I will create a function that will be used to find the best note in each window, and then we can easily find the frequencies from the audio signals: def getNoteFrequency( chromagram ): numberOfWindows = chromagram.shape[1] freqVal = chromagram.argmax( axis = 0 ) histogram, bin = np.histogram( freqVal, bins = 12 ) normalized_hist = histogram.reshape( 1, 12 ).astype( float ) / numberOfWindows #D return normalized_histCode language: Python (python) Now I will create a function to iterate over the files in the path of our directory. Here I will be using a pandas data frame to store our feature vectors: fileList = [] def getDataset( filePath ): X = pd.DataFrame( ) columns=[ ""G#"", ""G"", ""F#"", ""F"", ""E"", ""D#"", ""D"", ""C#"", ""C"", ""B"", ""A#"", ""A"" ] for root, dirs, filenames in os.walk( filePath ): for file in filenames: fileList.append( file ) feature_name, features = preProcess(filePath + file ) chromagram = getChromagram( features ) noteFrequency = getNoteFrequency( chromagram ) x_new = pd.Series(noteFrequency[ 0, : ]) X = pd.concat( [ X, x_new ], axis = 1 ) data = X.T.copy() data.columns = columns data.index = [ i for i in range( 0, data.shape[ 0 ] ) ] return data dataCode language: Python (python) G#GF#FED#DC#CBA#A00.0000000.00.00.0000000.00.01.0000000.00.00.00.00.00000010.9917130.00.00.0000000.00.00.0000000.00.00.00.00.00828720.0218340.00.00.0000000.00.00.0000000.00.00.00.00.97816630.0153850.00.00.0000000.00.00.0000000.00.00.00.00.98461540.0909090.00.00.0909090.00.00.5454550.00.00.00.00.27272750.0270270.00.00.0000000.00.00.0000000.00.00.00.00.97297360.0000000.00.00.0000000.00.01.0000000.00.00.00.00.00000070.0975610.00.00.0000000.00.00.0000000.00.00.00.00.90243980.0000000.00.00.0000000.00.01.0000000.00.00.00.00.00000090.0000000.00.00.0000000.00.01.0000000.00.00.00.00.000000 In the data frame above each row represents a data point, and each column represents the features. So we have 19 files and 12 features each in our audio signals. Also, Read: Polynomial Regression Algorithm in Machine Learning. Machine Learning Algorithm for Audio Feature Extraction Here I will use the K-means clustering algorithm. Now I will define the hyperparameters for our Machine Learning Algorithm. Here K will represent the number of clusters, and epochs represent the number of iterations our Machine Learning Algorithm will run for: k = 4 epochs = 1000Code language: Python (python) Now I will make a function to select the k data points as initial centroids: def initilizeCentroids( data, k ): centroids = data[ 0: k ] return centroidsCode language: Python (python) Now, I will define tensors that will represent the placeholders of our data. Here X is a representation of the data, C is the list of k centroids, and C_labels is the index of the centroids that we have assigned to our each data point: X = tf.placeholder( dtype = tf.float32 ) C = tf.placeholder( dtype = tf.float32 ) C_labels = tf.placeholder( dtype = tf.int32 )Code language: Python (python) Now I will prepare our data for audio feature extraction with Machine Learning: expanded_vectors = tf.expand_dims( X, 0 ) expanded_centroids = tf.expand_dims( C, 1 ) distance = tf.reduce_sum( tf.square( tf.subtract( expanded_vectors, expanded_centroids ) ), axis = 2 ) getCentroidsOp = tf.argmin( distance, 0 )Code language: Python (python) Now I will compute the new centroids from our assigned labels and data values: sums = tf.unsorted_segment_sum( X, C_labels, k ) counts = tf.unsorted_segment_sum( tf.ones_like( X ), C_labels, k ) reCalculateCentroidsOp = tf.divide( sums, counts ) Driver Code Now I will define the driver code for our algorithm. Using this function, we will feed the necessary data so that we could train it using our Machine Learning Algorithm: data_labels = [] centroids = [] with tf.Session() as sess: sess.run( tf.global_variables_initializer() ) centroids = initilizeCentroids( data, k ) for epoch in range( epochs ): data_labels = sess.run( getCentroidsOp, feed_dict = { X: data, C: centroids } ) centroids = sess.run( reCalculateCentroidsOp, feed_dict = { X: data, C_labels: data_labels } ) print( data_labels ) print( centroids )Code language: Python (python) [0 1 2 2 0 2 0 2 0 0] [[0.01818182 0. 0. 0.01818182 0. 0. 0.9090909 0. 0. 0. 0. 0.05454545] [0.9917127 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.00828729] [0.04045167 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.95954835]] Now we have trained the model for audio feature extraction. Let’s have a look at our output: final_labels = pd.DataFrame( { ""Labels"": data_labels, ""File Names"": fileList } ) final_labelsCode language: Python (python) File NamesLabels0c3_1.wav01c4_1.wav12c1_2.wav23c2_2.wav24c3_2.wav05c2_3.wav26c2_4.wav07c1_1.wav28c2_1.wav09c4_2.wav0 I hope you liked this article on Audio Feature Extraction using the k-means clustering algorithm. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to read more amazing articles. Follow Us: Facebook Instagram";Audio Feature Extraction
2020-07-28 12:37:22;Machine Translation is one of the most challenging tasks in Artificial Intelligence that works by investigating the use of software to translate a text or speech from one language to another. In this article, I will take you through Machine Translation using Neural networks.At the end of this article, you will learn to develop a machine translation model using Neural networks and python. I will use the English language as an input and we will train our Machine Translation model to give the output in the French language. Now let’s start with importing all the libraries that we need for this task:I will first create two functions to load the data and another function to test our data:Now let’s load the data and have a look at some insights from the data, the dataset that I am using here contains an English phrase with its translation:Dataset LoadedAs we are doing the translation of a language so the complexity of this problem will be determined by the complexity of the vocabulary. The more complex is the vocabulary of our language is the more complex our problem will be. Let’s look at the data to see what complex data we are dealing with:;https://thecleverprogrammer.com/2020/07/28/machine-translation-model/;['keras', 'vocabulary'];1.0;['RNN', 'NN'];['RNN', 'NN', 'AI', 'ML', 'GRU', 'Machine Translation'];['artificial intelligence', 'epoch', 'gru', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'layer', 'machine translation', 'train', 'label'];"Machine Translation is one of the most challenging tasks in Artificial Intelligence that works by investigating the use of software to translate a text or speech from one language to another. In this article, I will take you through Machine Translation using Neural networks. At the end of this article, you will learn to develop a machine translation model using Neural networks and python. I will use the English language as an input and we will train our Machine Translation model to give the output in the French language. Now let’s start with importing all the libraries that we need for this task: import collections import helper import numpy as np import project_tests as tests from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.models import Model from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional from keras.layers.embeddings import Embedding from keras.optimizers import Adam from keras.losses import sparse_categorical_crossentropyCode language: Python (python) I will first create two functions to load the data and another function to test our data: import os def load_data(path): """""" Load dataset """""" input_file = os.path.join(path) with open(input_file, ""r"") as f: data = f.read() return data.split('\n') def _test_model(model, input_shape, output_sequence_length, french_vocab_size): if isinstance(model, Sequential): model = model.model assert model.input_shape == (None, *input_shape[1:]),\ 'Wrong input shape. Found input shape {} using parameter input_shape={}'.format(model.input_shape, input_shape) assert model.output_shape == (None, output_sequence_length, french_vocab_size),\ 'Wrong output shape. Found output shape {} using parameters output_sequence_length={} and french_vocab_size={}'\ .format(model.output_shape, output_sequence_length, french_vocab_size) assert len(model.loss_functions) &amp;gt; 0,\ 'No loss function set. Apply the `compile` function to the model.' assert sparse_categorical_crossentropy in model.loss_functions,\ 'Not using `sparse_categorical_crossentropy` function for loss.Code language: Python (python) Now let’s load the data and have a look at some insights from the data, the dataset that I am using here contains an English phrase with its translation: english_sentences = helper.load_data('data/small_vocab_en') french_sentences = helper.load_data('data/small_vocab_fr') print('Dataset Loaded')Code language: Python (python) Dataset Loaded for sample_i in range(2): print('small_vocab_en Line {}: {}'.format(sample_i + 1, english_sentences[sample_i])) print('small_vocab_fr Line {}: {}'.format(sample_i + 1, french_sentences[sample_i]))Code language: Python (python) small_vocab_en Line 1: new jersey is sometimes quiet during autumn , and it is snowy in april . small_vocab_fr Line 1: new jersey est parfois calme pendant l' automne , et il est neigeux en avril . small_vocab_en Line 2: the united states is usually chilly during july , and it is usually freezing in november . small_vocab_fr Line 2: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre . As we are doing the translation of a language so the complexity of this problem will be determined by the complexity of the vocabulary. The more complex is the vocabulary of our language is the more complex our problem will be. Let’s look at the data to see what complex data we are dealing with: english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()]) french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()]) print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()]))) print('{} unique English words.'.format(len(english_words_counter))) print('10 Most common words in the English dataset:') print('""' + '"" ""'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '""') print() print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()]))) print('{} unique French words.'.format(len(french_words_counter))) print('10 Most common words in the French dataset:') print('""' + '"" ""'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '""')Code language: Python (python) 1823250 English words. 227 unique English words. 10 Most common words in the English dataset: ""is"" "","" ""."" ""in"" ""it"" ""during"" ""the"" ""but"" ""and"" ""sometimes"" 1961295 French words. 355 unique French words. 10 Most common words in the French dataset: ""est"" ""."" "","" ""en"" ""il"" ""les"" ""mais"" ""et"" ""la"" ""parfois"" Preprocessing the Data In Machine Learning wherever we are dealing with any sort of text values we first need to convert the text values into sequences of integers by using two primary methods like Tokenize and Padding. Now let’s start with Tokenization: def tokenize(x): x_tk = Tokenizer(char_level = False) x_tk.fit_on_texts(x) return x_tk.texts_to_sequences(x), x_tk text_sentences = [ 'The quick brown fox jumps over the lazy dog .', 'By Jove , my quick study of lexicography won a prize .', 'This is a short sentence .'] text_tokenized, text_tokenizer = tokenize(text_sentences) print(text_tokenizer.word_index) print() for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)): print('Sequence {} in x'.format(sample_i + 1)) print(' Input: {}'.format(sent)) print(' Output: {}'.format(token_sent))Code language: Python (python) {'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21} Sequence 1 in x Input: The quick brown fox jumps over the lazy dog . Output: [1, 2, 4, 5, 6, 7, 1, 8, 9] Sequence 2 in x Input: By Jove , my quick study of lexicography won a prize . Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17] Sequence 3 in x Input: This is a short sentence . Output: [18, 19, 3, 20, 21] Now let’s use the padding method to make all the sequences of the same length: def pad(x, length=None): if length is None: length = max([len(sentence) for sentence in x]) return pad_sequences(x, maxlen = length, padding = 'post') tests.test_pad(pad) # Pad Tokenized output test_pad = pad(text_tokenized) for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)): print('Sequence {} in x'.format(sample_i + 1)) print(' Input: {}'.format(np.array(token_sent))) print(' Output: {}'.format(pad_sent))Code language: Python (python) Sequence 1 in x Input: [1 2 4 5 6 7 1 8 9] Output: [1 2 4 5 6 7 1 8 9 0] Sequence 2 in x Input: [10 11 12 2 13 14 15 16 3 17] Output: [10 11 12 2 13 14 15 16 3 17] Sequence 3 in x Input: [18 19 3 20 21] Output: [18 19 3 20 21 0 0 0 0 0] Preprocessing a Pipeline for Machine Translation Now let’s define a preprocessing function to create a Pipeline for the task of Machine Translation so that we could use this model in future also: def preprocess(x, y): preprocess_x, x_tk = tokenize(x) preprocess_y, y_tk = tokenize(y) preprocess_x = pad(preprocess_x) preprocess_y = pad(preprocess_y) # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1) return preprocess_x, preprocess_y, x_tk, y_tk preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\ preprocess(english_sentences, french_sentences) max_english_sequence_length = preproc_english_sentences.shape[1] max_french_sequence_length = preproc_french_sentences.shape[1] english_vocab_size = len(english_tokenizer.word_index) french_vocab_size = len(french_tokenizer.word_index) print('Data Preprocessed') print(""Max English sentence length:"", max_english_sequence_length) print(""Max French sentence length:"", max_french_sequence_length) print(""English vocabulary size:"", english_vocab_size) print(""French vocabulary size:"", french_vocab_size)Code language: Python (python) Data Preprocessed Max English sentence length: 15 Max French sentence length: 21 English vocabulary size: 199 French vocabulary size: 344 Training a Neural Network for Machine Translation Now, here I will train a model using Neural networks. Let’s start by creating a helper function: def logits_to_text(logits, tokenizer): index_to_words = {id: word for word, id in tokenizer.word_index.items()} index_to_words[0] = '&amp;lt;PAD&amp;gt;' return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)]) print('`logits_to_text` function loaded.')Code language: Python (python) `logits_to_text` function loaded. Now I will train a RNN model which will act as a very good base for our sequences that can translate English language to French: def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size): learning_rate = 1e-3 input_seq = Input(input_shape[1:]) rnn = GRU(64, return_sequences = True)(input_seq) logits = TimeDistributed(Dense(french_vocab_size))(rnn) model = Model(input_seq, Activation('softmax')(logits)) model.compile(loss = sparse_categorical_crossentropy, optimizer = Adam(learning_rate), metrics = ['accuracy']) return model tests.test_simple_model(simple_model) tmp_x = pad(preproc_english_sentences, max_french_sequence_length) tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1)) # Train the neural network simple_rnn_model = simple_model( tmp_x.shape, max_french_sequence_length, english_vocab_size, french_vocab_size) simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2) # Print prediction(s) print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))Code language: Python (python) Train on 110288 samples, validate on 27573 samples Epoch 1/10 110288/110288 [==============================] - 9s 82us/step - loss: 3.5162 - acc: 0.4027 - val_loss: nan - val_acc: 0.4516 Epoch 2/10 110288/110288 [==============================] - 7s 64us/step - loss: 2.4823 - acc: 0.4655 - val_loss: nan - val_acc: 0.4838 Epoch 3/10 110288/110288 [==============================] - 7s 63us/step - loss: 2.2427 - acc: 0.5016 - val_loss: nan - val_acc: 0.5082 Epoch 4/10 110288/110288 [==============================] - 7s 64us/step - loss: 2.0188 - acc: 0.5230 - val_loss: nan - val_acc: 0.5428 Epoch 5/10 110288/110288 [==============================] - 7s 64us/step - loss: 1.8418 - acc: 0.5542 - val_loss: nan - val_acc: 0.5685 Epoch 6/10 110288/110288 [==============================] - 7s 64us/step - loss: 1.7258 - acc: 0.5731 - val_loss: nan - val_acc: 0.5811 Epoch 7/10 110288/110288 [==============================] - 7s 64us/step - loss: 1.6478 - acc: 0.5871 - val_loss: nan - val_acc: 0.5890 Epoch 8/10 110288/110288 [==============================] - 7s 64us/step - loss: 1.5850 - acc: 0.5940 - val_loss: nan - val_acc: 0.5977 Epoch 9/10 110288/110288 [==============================] - 7s 64us/step - loss: 1.5320 - acc: 0.5996 - val_loss: nan - val_acc: 0.6027 Epoch 10/10 110288/110288 [==============================] - 7s 64us/step - loss: 1.4874 - acc: 0.6037 - val_loss: nan - val_acc: 0.6039 new jersey est parfois parfois en en et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> The RNN model gave us an accuracy of only 60 per cent, let’s use a more complex neural network to train our model with better accuracy. I will now train our model using RNN with embedding. Embedding represents a vector of a word that is very close to a similar word in the n-dimensional world. The n here represents the size of the vectors of embedding: from keras.models import Sequential def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size): learning_rate = 1e-3 rnn = GRU(64, return_sequences=True, activation=""tanh"") embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1]) logits = TimeDistributed(Dense(french_vocab_size, activation=""softmax"")) model = Sequential() #em can only be used in first layer --&amp;gt; Keras Documentation model.add(embedding) model.add(rnn) model.add(logits) model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy']) return model tests.test_embed_model(embed_model) tmp_x = pad(preproc_english_sentences, max_french_sequence_length) tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2])) embeded_model = embed_model( tmp_x.shape, max_french_sequence_length, english_vocab_size, french_vocab_size) embeded_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2) print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], french_tokenizer))Code language: Python (python) Train on 110288 samples, validate on 27573 samples Epoch 1/10 110288/110288 [==============================] - 8s 68us/step - loss: 3.7877 - acc: 0.4018 - val_loss: nan - val_acc: 0.4093 Epoch 2/10 110288/110288 [==============================] - 7s 65us/step - loss: 2.7258 - acc: 0.4382 - val_loss: nan - val_acc: 0.5152 Epoch 3/10 110288/110288 [==============================] - 7s 65us/step - loss: 2.0359 - acc: 0.5453 - val_loss: nan - val_acc: 0.6068 Epoch 4/10 110288/110288 [==============================] - 7s 65us/step - loss: 1.4586 - acc: 0.6558 - val_loss: nan - val_acc: 0.6967 Epoch 5/10 110288/110288 [==============================] - 7s 65us/step - loss: 1.1346 - acc: 0.7308 - val_loss: nan - val_acc: 0.7561 Epoch 6/10 110288/110288 [==============================] - 7s 65us/step - loss: 0.9358 - acc: 0.7681 - val_loss: nan - val_acc: 0.7825 Epoch 7/10 110288/110288 [==============================] - 7s 65us/step - loss: 0.8057 - acc: 0.7917 - val_loss: nan - val_acc: 0.7993 Epoch 8/10 110288/110288 [==============================] - 7s 65us/step - loss: 0.7132 - acc: 0.8095 - val_loss: nan - val_acc: 0.8173 Epoch 9/10 110288/110288 [==============================] - 7s 65us/step - loss: 0.6453 - acc: 0.8229 - val_loss: nan - val_acc: 0.8313 Epoch 10/10 110288/110288 [==============================] - 7s 64us/step - loss: 0.5893 - acc: 0.8355 - val_loss: nan - val_acc: 0.8401 new jersey est parfois calme au l'automne et il il est neigeux en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Also, Read: Audio Feature Extraction in Machine Learning. So our RNN model with embedding resulted in a very good accuracy of 84 per cent. I hope you liked this article on Machine Translation using Neural networks and python. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to read more amazing articles. Follow Us: Facebook Instagram";Machine Translation Model
2020-07-29 23:06:20;The general goal of a classification model is to find a decision boundary. The purpose of the decision boundaries is to identify those regions of the input class space that corresponds to each class. In this article, I will take you through the concept of decision boundary in machine learning.To explain the concept of decision boundaries in machine learning, I will first create a Logistic Regression model. So now let’s import some libraries and get started with the task:;https://thecleverprogrammer.com/2020/07/29/decision-boundary-in-machine-learning/;['sklearn'];1.0;[];['Regression', 'ML', 'Linear Regression', 'Anomaly Detection', 'Logistic Regression', 'Classification'];['detect', 'regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'logistic regression', 'classif', 'anomaly', 'anomaly detection', 'train', 'label'];"The general goal of a classification model is to find a decision boundary. The purpose of the decision boundaries is to identify those regions of the input class space that corresponds to each class. In this article, I will take you through the concept of decision boundary in machine learning. To explain the concept of decision boundaries in machine learning, I will first create a Logistic Regression model. So now let’s import some libraries and get started with the task: # Python ≥3.5 is required import sys assert sys.version_info &amp;gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &amp;gt;= ""0.20"" # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12)Code language: Python (python) Decision Boundaries with Logistic Regression I will use the iris dataset to fit a Linear Regression model. Iris is a very famous dataset among machine learning practitioners for classification tasks. It contains the sepal and petal length with width of 150 iris flowers of three different species; Iris setosa, Iris versicolor, and Iris Virginica. Now I will try to build a classification model to detect the Iris virginica type based only on the width of a petal: t = np.linspace(-10, 10, 100) sig = 1 / (1 + np.exp(-t)) plt.figure(figsize=(9, 3)) plt.plot([-10, 10], [0, 0], ""k-"") plt.plot([-10, 10], [0.5, 0.5], ""k:"") plt.plot([-10, 10], [1, 1], ""k:"") plt.plot([0, 0], [-1.1, 1.1], ""k-"") plt.plot(t, sig, ""b-"", linewidth=2, label=r""$\sigma(t) = \frac{1}{1 + e^{-t}}$"") plt.xlabel(""t"") plt.legend(loc=""upper left"", fontsize=20) plt.axis([-10, 10, -0.1, 1.1]) plt.show()Code language: Python (python) Now let’s train our Logistic Regression model to frame a decision boundary: from sklearn import datasets iris = datasets.load_iris() X = iris[""data""][:, 3:] # petal width y = (iris[""target""] == 2).astype(np.int) # 1 if Iris virginica, else 0 from sklearn.linear_model import LogisticRegression log_reg = LogisticRegression(solver=""lbfgs"", random_state=42) log_reg.fit(X, y)Code language: Python (python) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=42, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) Decision Boundary using One Feature (Petal Width) Now let’s have a quick look at our trained model’s estimated probabilities for flowers with the petal widths that vary from 0 cm to 3 cm: X_new = np.linspace(0, 3, 1000).reshape(-1, 1) y_proba = log_reg.predict_proba(X_new) decision_boundary = X_new[y_proba[:, 1] &amp;gt;= 0.5][0] plt.figure(figsize=(8, 3)) plt.plot(X[y==0], y[y==0], ""bs"") plt.plot(X[y==1], y[y==1], ""g^"") plt.plot([decision_boundary, decision_boundary], [-1, 2], ""k:"", linewidth=2) plt.plot(X_new, y_proba[:, 1], ""g-"", linewidth=2, label=""Iris virginica"") plt.plot(X_new, y_proba[:, 0], ""b--"", linewidth=2, label=""Not Iris virginica"") plt.text(decision_boundary+0.02, 0.15, ""Decision boundary"", fontsize=14, color=""k"", ha=""center"") plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b') plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g') plt.xlabel(""Petal width (cm)"", fontsize=14) plt.ylabel(""Probability"", fontsize=14) plt.legend(loc=""center left"", fontsize=14) plt.axis([0, 3, -0.02, 1.02]) plt.show()Code language: Python (python) The output above shows that the petal width of Iris virginica ranges from 1.4 cm to 2.5 cm, while the other flowers are having a small petal width which is ranging from 0.1 cm to 1.8 cm. If we will use the predict() method to predict the class of the flower, it will return the class that mostly falls in this category. There is a decision boundary at around 1.6 cm where both the probabilities are 50 percent, which conveys that if the petal width is higher than 1.6 cm, then our classification model will predict that the input class is an Iris virginica, and otherwise the model will predict that it is not iris virginica. Decision Boundary using Two Features (Petal Width & Petal Length) Now let’s plot a little bit complex decision boundary which will be based on two features petal width and petal length. Now we will train the model based on two features to predict whether the flower is Iris virginica: from sklearn.linear_model import LogisticRegression X = iris[""data""][:, (2, 3)] # petal length, petal width y = (iris[""target""] == 2).astype(np.int) log_reg = LogisticRegression(solver=""lbfgs"", C=10**10, random_state=42) log_reg.fit(X, y) x0, x1 = np.meshgrid( np.linspace(2.9, 7, 500).reshape(-1, 1), np.linspace(0.8, 2.7, 200).reshape(-1, 1), ) X_new = np.c_[x0.ravel(), x1.ravel()] y_proba = log_reg.predict_proba(X_new) plt.figure(figsize=(10, 4)) plt.plot(X[y==0, 0], X[y==0, 1], ""bs"") plt.plot(X[y==1, 0], X[y==1, 1], ""g^"") zz = y_proba[:, 1].reshape(x0.shape) contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg) left_right = np.array([2.9, 7]) boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1] plt.clabel(contour, inline=1, fontsize=12) plt.plot(left_right, boundary, ""k--"", linewidth=3) plt.text(3.5, 1.5, ""Not Iris virginica"", fontsize=14, color=""b"", ha=""center"") plt.text(6.5, 2.3, ""Iris virginica"", fontsize=14, color=""g"", ha=""center"") plt.xlabel(""Petal length"", fontsize=14) plt.ylabel(""Petal width"", fontsize=14) plt.axis([2.9, 7, 0.8, 2.7]) plt.show()Code language: Python (python) In the output above the dashed line is representing the points where our Logistic Regression model predicts a probability of 50 percent, this line is the decision boundary for our classification model. One thing to note here is that it is a Linear decision boundary. Also, Read: Anomaly Detection with Machine Learning. Here each parallel line is representing the points where the output of our model shows a specific probability from 15 percent to 90 percent. All the flowers lying beyond the top right line have over 90 percent probability of being Iris Virginia. I hope you liked this article on Decision Boundary in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to read more amazing articles. Follow Us: Facebook Instagram";Decision Boundary in Machine Learning
2020-07-31 23:52:53;In Machine Learning, one way to use the same training algorithm for more prediction models and to train them on different sets of the data is known as Bagging and Pasting. Bagging means to perform sampling with replacement and when the process of bagging is done without replacement then this is known as Pasting.;https://thecleverprogrammer.com/2020/07/31/bagging-and-pasting-in-machine-learning/;['sklearn'];1.0;[];['ML', 'Classification', 'Decision Tree'];['predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'label', 'decision tree'];"In Machine Learning, one way to use the same training algorithm for more prediction models and to train them on different sets of the data is known as Bagging and Pasting. Bagging means to perform sampling with replacement and when the process of bagging is done without replacement then this is known as Pasting. Why Bagging and Pasting? Generally speaking, both bagging and pasting allow the training samples for sampling a lot of time across multiple prediction models, but only bagging can allow the training samples for sampling a lot of time on the same prediction model. Now I will import some libraries to start with a prediction model. I will first train a machine learning model, then we will see how we can process the machine learning algorithm using the concept of Bagging. So let’s start with importing some necessary libraries: # Python ≥3.5 is required import sys assert sys.version_info &amp;gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &amp;gt;= ""0.20"" # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12)Code language: Python (python) Now let’s train a Prediction Model: from sklearn.model_selection import train_test_split from sklearn.datasets import make_moons X, y = make_moons(n_samples=500, noise=0.30, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)Code language: Python (python) Note that the predictors are trained parallelly, by the medium of CPU cores or even different servers. In the same way, we can also make predictions parallelly. This is one of the important reasons why bagging and pasting are an important concept of machine learning as they scale the algorithm very well. Bagging and Pasting in Machine Learning In Machine Learning, scikit-learn provides an API for the process of both bagging and pasting. We have BaggingClassifier in scikit-learn. Now let’s go through the process: from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier bag_clf = BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=500, max_samples=100, bootstrap=True, random_state=42) bag_clf.fit(X_train, y_train) y_pred = bag_clf.predict(X_test)Code language: Python (python) from sklearn.metrics import accuracy_score print(accuracy_score(y_test, y_pred))Code language: Python (python) 0.904 tree_clf = DecisionTreeClassifier(random_state=42) tree_clf.fit(X_train, y_train) y_pred_tree = tree_clf.predict(X_test) print(accuracy_score(y_test, y_pred_tree))Code language: Python (python) 0.856 from matplotlib.colors import ListedColormap def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True): x1s = np.linspace(axes[0], axes[1], 100) x2s = np.linspace(axes[2], axes[3], 100) x1, x2 = np.meshgrid(x1s, x2s) X_new = np.c_[x1.ravel(), x2.ravel()] y_pred = clf.predict(X_new).reshape(x1.shape) custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0']) plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap) if contour: custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50']) plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8) plt.plot(X[:, 0][y==0], X[:, 1][y==0], ""yo"", alpha=alpha) plt.plot(X[:, 0][y==1], X[:, 1][y==1], ""bs"", alpha=alpha) plt.axis(axes) plt.xlabel(r""$x_1$"", fontsize=18) plt.ylabel(r""$x_2$"", fontsize=18, rotation=0) fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True) plt.sca(axes[0]) plot_decision_boundary(tree_clf, X, y) plt.title(""Decision Tree"", fontsize=14) plt.sca(axes[1]) plot_decision_boundary(bag_clf, X, y) plt.title(""Decision Trees with Bagging"", fontsize=14) plt.ylabel("""") plt.show()Code language: Python (python) The BaggingClassifier will automatically use the voting classifier to estimate the class predictions. The output above simply compares the decision boundary of a Decision Tree with the decision boundary of a bagging classifier of 500 trees. We can see that the BaggingClassifier is generalizing very much better than the predictions of the Decision Tree. Overall, BaggingClassifier often results in better models. I hope you liked this article on BaggingClassifier and Pasting in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to read more amazing articles. Follow Us: Facebook Instagram";Bagging and Pasting in Machine Learning
2020-07-31 00:44:52;Suppose you have trained a lot of classification models, and your each model is achieving the accuracy of 85 percent. A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classification is known as a voting classifier.In this article, I will take you through the voting classifier in Machine Learning. I will first start with importing the necessary libraries:;https://thecleverprogrammer.com/2020/07/31/voting-classifier-in-machine-learning/;['sklearn'];1.0;[];['ML', 'Classification', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'label'];"Suppose you have trained a lot of classification models, and your each model is achieving the accuracy of 85 percent. A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classification is known as a voting classifier. In this article, I will take you through the voting classifier in Machine Learning. I will first start with importing the necessary libraries: import sys assert sys.version_info &amp;gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &amp;gt;= ""0.20"" # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12)Code language: Python (python) Example of a Voting Classifier Now, let’s move forward to create our voting classifier: heads_proba = 0.51 coin_tosses = (np.random.rand(10000, 10) &amp;lt; heads_proba).astype(np.int32) cumulative_heads_ratio = np.cumsum(coin_tosses, axis=0) / np.arange(1, 10001).reshape(-1, 1) plt.figure(figsize=(8,3.5)) plt.plot(cumulative_heads_ratio) plt.plot([0, 10000], [0.51, 0.51], ""k--"", linewidth=2, label=""51%"") plt.plot([0, 10000], [0.5, 0.5], ""k-"", label=""50%"") plt.xlabel(""Number of coin tosses"") plt.ylabel(""Heads ratio"") plt.legend(loc=""lower right"") plt.axis([0, 10000, 0.42, 0.58]) save_fig(""law_of_large_numbers_plot"") plt.show()Code language: Python (python) The above output shows if you are tossing a slightly biased coin that is having 51 per cent chances of showing heads and 49 per cent chances of showing tails. If you toss such a coin, you will get heads 510 times and tails 490 times similarly if you have an ensemble with 1000 classification models that shows accuracy only 51 per cent of times. If you predict the classes with such models, you can receive up to 75 per cent accuracy rate. Training a Voting Classifier The methods of voting classifier work best when the predictions are independent of each other—the only way to diversify the classification models to train them using different algorithms. Also, Read: Scraping Instagram with Python. Now let’s create and train a voting classifier in Machine Learning using Scikit-Learn, which will include three classification models. from sklearn.model_selection import train_test_split from sklearn.datasets import make_moons X, y = make_moons(n_samples=500, noise=0.30, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)Code language: Python (python) from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC log_clf = LogisticRegression(solver=""lbfgs"", random_state=42) rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42) svm_clf = SVC(gamma=""scale"", random_state=42) voting_clf = VotingClassifier( estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='hard') voting_clf.fit(X_train, y_train)Code language: Python (python) from sklearn.metrics import accuracy_score for clf in (log_clf, rnd_clf, svm_clf, voting_clf): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(clf.__class__.__name__, accuracy_score(y_test, y_pred))Code language: Python (python) LogisticRegression 0.864 RandomForestClassifier 0.896 SVC 0.896 VotingClassifier 0.912 In the output, we can see that all the classification models performed with an accuracy rate of more than 85 per cent, and the voting classification model which used the predictions of all the three models gave us an accuracy of over 90 per cent. So this is how we can use a voting classification model in Machine Learning classification models. I hope you liked this article. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to read more amazing articles. Follow Us: Facebook Instagram";Voting Classifier in Machine Learning
2020-08-01 19:39:42;Machine Learning and Artificial Intelligence are the most searched content on the Internet among the programmers coming from different programming languages. The popularity of Machine Learning has led to a lot of research that today we have even reached to the concept of AutoML, where we can automate machine learning tasks by automating some of the complex processes of Machine Learning.Now we have some interfaces which can help to automate machine learning code that can make our task a little bit easy, but you still need to know about Data Science and Machine Learning to look at your task, whether it is going in a right way or not.;https://thecleverprogrammer.com/2020/08/01/automate-machine-learning-with-h2o-automl/;['h2o'];1.0;['ML', 'AutoML'];['ML', 'AutoML', 'AI'];['artificial intelligence', 'predict', 'model', 'machine learning', 'train'];"Machine Learning and Artificial Intelligence are the most searched content on the Internet among the programmers coming from different programming languages. The popularity of Machine Learning has led to a lot of research that today we have even reached to the concept of AutoML, where we can automate machine learning tasks by automating some of the complex processes of Machine Learning. Now we have some interfaces which can help to automate machine learning code that can make our task a little bit easy, but you still need to know about Data Science and Machine Learning to look at your task, whether it is going in a right way or not. H2O AutoML With the packages provided by AutoML to Automate Machine Learning code, one useful package is H2O AutoML, which will automate machine learning code by automating the whole process involved in model selection and hyperparameters tuning. In this article, we will look at how we can use H2O AutoML to Automate Machine Learning code. Also, Read – Machine Learning Projects for Beginners. Installing this package is as easy as installing all other packages in python. You just need to write – pip install h2o, in your terminal. If you use google colab you can install any package while writing the pip command in the cell itself using – !pip install h20. Automate Machine Learning with H2O: Example The dataset I will use for this task is based on the data of advertising, which consists of the sales of the Company as a dependent variable and it consists of features like Radio, Newspaper, and Television. You can download this dataset from here. Now let’s import the necessary libraries and have a look at the data: import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.read_csv('Advertising.csv') df.head()Code language: Python (python) I hope you have installed the h20 package successfully, now I will simply import the h2o package to automate our machine learning code: import h2o h2o.init()Code language: Python (python) Checking whether there is an H2O instance running at http://localhost:54321 ..... not found. Attempting to start a local H2O server... Java Version: openjdk version ""11.0.8"" 2020-07-14; OpenJDK Runtime Environment (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1); OpenJDK 64-Bit Server VM (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1, mixed mode, sharing) Starting server from /usr/local/lib/python3.6/dist-packages/h2o/backend/bin/h2o.jar Ice root: /tmp/tmp04nu4_h6 JVM stdout: /tmp/tmp04nu4_h6/h2o_unknownUser_started_from_python.out JVM stderr: /tmp/tmp04nu4_h6/h2o_unknownUser_started_from_python.err Server is running at http://127.0.0.1:54321 Connecting to H2O server at http://127.0.0.1:54321 ... successful. H2O_cluster_uptime:	02 secs H2O_cluster_timezone:	Etc/UTC H2O_data_parsing_timezone:	UTC H2O_cluster_version:	3.30.0.7 H2O_cluster_version_age:	10 days H2O_cluster_name:	H2O_from_python_unknownUser_vvwlgf H2O_cluster_total_nodes:	1 H2O_cluster_free_memory:	3.180 Gb H2O_cluster_total_cores:	2 H2O_cluster_allowed_cores:	2 H2O_cluster_status:	accepting new members, healthy H2O_connection_url:	http://127.0.0.1:54321 H2O_connection_proxy:	{""http"": null, ""https"": null} H2O_internal_security:	False H2O_API_Extensions:	Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 Python_version:	3.6.9 final Now, I will convert our dataset to an H2OFrame, which is like a pandas data frame but it has some more properties: adver_df = h2o.H2OFrame(df) adver_df.describe()Code language: Python (python) Now, I will split the above data into training set and text set: train, test = adver_df.split_frame(ratios=[.50]) x = train.columns y = ""Sales"" x.remove(y)Code language: Python (python) Now, I will import the AutoML model provided by H2O to automate our machine learning task: from h2o.automl import H2OAutoML aml = H2OAutoML(max_runtime_secs=600, seed=1, balance_classes=False, project_name='Advertising' ) %time aml.train(x=x, y=y, training_frame=train)Code language: Python (python) The above code will pass our data from various machine learning models, in the fixed time limit of 600 seconds. In these 600 seconds, our data will store the performance of all the models through which our AutoML model has passed through. Now I will generate a leaderboard to see which machine learning model has performed the best among all. lb = aml.leaderboard lb.head()Code language: Python (python) Now, I will choose the best performing model, and find the best variable which is the most important one for our dependent variable: se = aml.leader #Loading Stack Ensambled Metelearner model metalearner = h2o.get_model(se.metalearner()['name']) metalearner.varimp()Code language: Python (python) Now, let’s analyze our AutoML model: model = h2o.get_model('DeepLearning_grid__1_AutoML_20200731_222821_model_1') model.model_performance(test)Code language: Python (python) Now, let’s have a look at the most important feature our model used for our dependent variable: model.varimp_plot(num_of_features=3)Code language: Python (python) Here we can clearly see that ‘TV’ is the most important feature in the predictions of Sales. Now, let’s visualize its dependence on Sales: model.partial_plot(train, cols=[""TV""], figsize=(5,5))Code language: Python (python) I hope you liked this article on AutoML H2O to automate our machine learning code. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Automate Machine Learning with H2O AutoML
2020-08-02 18:15:45;Indeed, two people can never have the same fingerprints, it is unique in every human. But using fingerprints we can classify gender, whether it’s male or female. In this article, I will take you through a Gender Classification Model which I will train using Deep Learning and Convolutional Neural Networks.Now you must be thinking about how I will do Gender Classification. I will build a Convolutional Neural Network that can classify the gender using the fingerprints. For this task, I will use a dataset that contains over 55000 images of fingerprints of each finger.Also, Read – Work on Artificial Intelligence Projects.;https://thecleverprogrammer.com/2020/08/02/gender-classification-model/;['keras', 'tensorflow'];1.0;['NN', 'CNN'];['Recommender', 'NN', 'DL', 'AI', 'CNN', 'ReLu', 'Classification'];['artificial intelligence', 'epoch', 'fit', 'model', 'loss', 'neural network', 'recommend', 'classif', 'layer', 'relu', 'deep learning', 'convolutional neural network', 'train', 'label'];"Indeed, two people can never have the same fingerprints, it is unique in every human. But using fingerprints we can classify gender, whether it’s male or female. In this article, I will take you through a Gender Classification Model which I will train using Deep Learning and Convolutional Neural Networks. Now you must be thinking about how I will do Gender Classification. I will build a Convolutional Neural Network that can classify the gender using the fingerprints. For this task, I will use a dataset that contains over 55000 images of fingerprints of each finger. Also, Read – Work on Artificial Intelligence Projects. Gender Classification using Fingerprints I will first start with converting those images into pixels value to extract the features from the fingerprints. Then I will split the data into training, testing and validation sets. Now let’s start with importing the libraries we need for this task and performing some steps of data preparation: import numpy as np import pandas as pd import seaborn as sns import tensorflow as tf import os import cv2 import matplotlib.pyplot as pltCode language: Python (python) Creating a Helper Function: def extract_label(img_path,train = True): filename, _ = os.path.splitext(os.path.basename(img_path)) subject_id, etc = filename.split('__') if train: gender, lr, finger, _, _ = etc.split('_') else: gender, lr, finger, _ = etc.split('_') gender = 0 if gender == 'M' else 1 lr = 0 if lr == 'Left' else 1 if finger == 'thumb': finger = 0 elif finger == 'index': finger = 1 elif finger == 'middle': finger = 2 elif finger == 'ring': finger = 3 elif finger == 'little': finger = 4 return np.array([gender], dtype=np.uint16)Code language: Python (python) The above function will help us in extracting the features from the fingerprints. This function will work by iterating through the labels of the images that we will assign in the function. The function will return an array 0 and 1. The zeros will represent males and ones will be representing the females. Now our next step is to load the image path to the function we created to iterate all over the images to find labels, you can download the dataset from here: img_size = 96 def loading_data(path,train): print(""loading data from: "",path) data = [] for img in os.listdir(path): try: img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE) img_resize = cv2.resize(img_array, (img_size, img_size)) label = extract_label(os.path.join(path, img),train) data.append([label[0], img_resize ]) except Exception as e: pass data return dataCode language: Python (python) Now, the next step is to assign various directories or folders to use the loading data function on all the images: Real_path = ""../input/aman/web/Real"" Easy_path = ""../input/aman/web/Altered-Easy"" Medium_path = ""../input/aman/web/Altered-Medium"" Hard_path = ""../input/aman/web/Altered-Hard"" Easy_data = loading_data(Easy_path, train = True) Medium_data = loading_data(Medium_path, train = True) Hard_data = loading_data(Hard_path, train = True) test = loading_data(Real_path, train = False) data = np.concatenate([Easy_data, Medium_data, Hard_data], axis=0) del Easy_data, Medium_data, Hard_dataCode language: Python (python) Now let’s randomize the data and test the arrays to see what our data looks like: import random random.shuffle(data) random.shuffle(test)Code language: Python (python) array([[1, array([[160, 158, 158, ..., 0, 0, 0], [160, 105, 121, ..., 0, 0, 0], [160, 105, 255, ..., 0, 0, 0], ..., [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0]], dtype=uint8)], [1, array([[160, 158, 158, ..., 0, 0, 0], [160, 105, 121, ..., 0, 0, 0], [160, 105, 255, ..., 0, 0, 0], ..., [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0]], dtype=uint8)], [1, array([[160, 158, 158, ..., 0, 0, 0], [160, 105, 121, ..., 0, 0, 0], [160, 105, 255, ..., 0, 0, 0], ..., [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0]], dtype=uint8)], ..., [0, array([[160, 158, 158, ..., 0, 0, 0], [160, 105, 121, ..., 0, 0, 0], [160, 105, 255, ..., 0, 0, 0], ..., [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0]], dtype=uint8)], [0, array([[160, 158, 158, ..., 0, 0, 0], [160, 105, 121, ..., 0, 0, 0], [160, 105, 255, ..., 0, 0, 0], ..., [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0]], dtype=uint8)], [1, array([[160, 158, 158, ..., 0, 0, 0], [160, 105, 121, ..., 0, 0, 0], [160, 105, 255, ..., 0, 0, 0], ..., [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0]], dtype=uint8)]], dtype=object) The output above is showing how our images are present in the data. The first array is representing the label value of 0 and then we have an array of pixel values. Don’t get confused, the above array is the real representation of how the computer sees an image. For example: Now I will split the image arrays and image labels: img, labels = [], [] for label, feature in data: labels.append(label) img.append(feature) train_data = np.array(img).reshape(-1, img_size, img_size, 1) train_data = train_data / 255.0Code language: Python (python) Building CNN for Gender Classification Model Now, here I will build a Convolutional Neural Network for Gender classification model: from tensorflow.keras import Sequential from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten from tensorflow.keras import layers from tensorflow.keras import optimizers model = Sequential([ Conv2D(32, 3, padding='same', activation='relu',kernel_initializer='he_uniform', input_shape = [96, 96, 1]), MaxPooling2D(2), Conv2D(32, 3, padding='same', kernel_initializer='he_uniform', activation='relu'), MaxPooling2D(2), Flatten(), Dense(128, kernel_initializer='he_uniform',activation = 'relu'), Dense(2, activation = 'softmax'), ])Code language: Python (python) Now, before moving forward, let’s have a quick look at the summary of our CNN Model: model.summary()Code language: Python (python) Model: ""sequential"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 96, 96, 32) 320 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 48, 48, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 48, 48, 32) 9248 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32) 0 _________________________________________________________________ flatten (Flatten) (None, 18432) 0 _________________________________________________________________ dense (Dense) (None, 128) 2359424 _________________________________________________________________ dense_1 (Dense) (None, 2) 258 ================================================================= Total params: 2,369,250 Trainable params: 2,369,250 Non-trainable params: 0 Now, I will compile the model using Adam optimizers, with a learning rate of 10%, and to prevent our CNN model from overfitting, I will be using the early_stopping_call method: model.compile(optimizer = optimizers.Adam(1e-3), loss = 'categorical_crossentropy', metrics = ['accuracy']) early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)Code language: Python (python) Now, let’s fit our Gender Classification Model, We are going to train the model for 30 epochs. history = model.fit(train_data, train_labels, batch_size = 128, epochs = 30, validation_split = 0.2, callbacks = [early_stopping_cb], verbose = 1)Code language: Python (python) You will see a very long output for this of 30 epochs. It will use GPU, So I will recommend you to use Google colab for better performance. It will give train loss and accuracy and also validation loss and accuracy per epochs. Now, we have successfully trained our CNN for Gender Classification Model with a training accuracy of 99 per cent and Validation accuracy of 98 per cent. Now, let’s have a look at the performance of our Gender Classification Model: import pandas as pd import matplotlib.pyplot as plt pd.DataFrame(history.history).plot(figsize = (8,5)) plt.grid(True) plt.gca().set_ylim(0,1)Code language: Python (python) Testing Gender Classification Model As we split the training images into image labels and image arrays while training our model, we need to repeat the same process on the test images: test_images, test_labels = [], [] for label, feature in test: test_images.append(feature) test_labels.append(label) test_images = np.array(test_images).reshape(-1, img_size, img_size, 1) test_images = test_images / 255.0 del test test_labels = to_categorical(test_labels, num_classes = 2)Code language: Python (python) Now, let’s evaluate the performance of our CNN model on the test set: model.evaluate(test_images, test_labels)​x model.evaluate(test_images, test_labels) 6000/6000 [==============================] - 1s 134us/sample - loss: 0.0231 - accuracy: 0.9972 [0.023149466800407026, 0.9971667]Code language: Python (python) So, we got an accuracy of 99.72 per cent and a loss value of 0.0126 on our gender classification model. I hope you liked this article, feel free to ask your valuable questions in the comments section below. Don’t forget to subscribe for my daily newsletters if you like my work. Follow Us: Facebook Instagram";Gender Classification Model
2020-08-03 13:24:30;The Random Forest algorithm is an ensemble of the Decision Trees algorithm. A Decision Tree model is generally trained using the Bagging Classifier. If you don’t want to use a bagging classifier algorithm to pass it through the Decision Tree Classification model, you can use a Random Forest algorithm as it is more convenient and better optimized for Decision Tree Classification. In this article, I will take you through the Random Forest algorithm in Machine Learning.I will use all the CPU cores to train a RandomForestClassifier algorithm with 500 trees. But first, let’s start with importing the necessary libraries and data preparation to fit into a RandomForestClassifier algorithm:;https://thecleverprogrammer.com/2020/08/03/random-forest-algorithm/;['sklearn'];1.0;[];['AI', 'ML', 'Decision Tree', 'Random Forest', 'Classification'];['artificial intelligence', 'predict', 'fit', 'model', 'machine learning', 'random forest', 'classif', 'train', 'label', 'decision tree'];"The Random Forest algorithm is an ensemble of the Decision Trees algorithm. A Decision Tree model is generally trained using the Bagging Classifier. If you don’t want to use a bagging classifier algorithm to pass it through the Decision Tree Classification model, you can use a Random Forest algorithm as it is more convenient and better optimized for Decision Tree Classification. In this article, I will take you through the Random Forest algorithm in Machine Learning. I will use all the CPU cores to train a RandomForestClassifier algorithm with 500 trees. But first, let’s start with importing the necessary libraries and data preparation to fit into a RandomForestClassifier algorithm: import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= ""0.20"" # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12)Code language: Python (python) Data Preparation Now, I will load the data, and split it into training and test sets: from sklearn.model_selection import train_test_split from sklearn.datasets import make_moons X, y = make_moons(n_samples=500, noise=0.30, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)Code language: Python (python) Now I will prepare the data using the bagging classifier and the decision tree classification model: from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier bag_clf = BaggingClassifier( DecisionTreeClassifier(splitter=""random"", max_leaf_nodes=16, random_state=42), n_estimators=500, max_samples=1.0, bootstrap=True, random_state=42) bag_clf.fit(X_train, y_train) y_pred = bag_clf.predict(X_test)Code language: Python (python) Random Forest Algorithm in Machine Learning With a very few parameters, a RandomForestClassifier uses all the hyperparameters of a Decision Tree Classification model and bagging classifier algorithm. Now let’s see how we can do this: from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42) rnd_clf.fit(X_train, y_train) y_pred_rf = rnd_clf.predict(X_test) np.sum(y_pred == y_pred_rf) / len(y_pred)Code language: Python (python) 0.976 The RandomForestClassifier algorithm works by introducing extra randomness while producing decision trees. Instead of searching for the best features, it works by searching for the best features among the random sets of features. Also, Read – Work on Artificial Intelligence Projects. from sklearn.datasets import load_iris iris = load_iris() rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42) rnd_clf.fit(iris[""data""], iris[""target""]) for name, score in zip(iris[""feature_names""], rnd_clf.feature_importances_): print(name, score)Code language: Python (python) sepal length (cm) 0.11249225099876375 sepal width (cm) 0.02311928828251033 petal length (cm) 0.4410304643639577 petal width (cm) 0.4233579963547682 rnd_clf.feature_importances_Code language: Python (python) array([0.11249225, 0.02311929, 0.44103046, 0.423358 ]) Decision Boundary for Random Forest Now I will plot a decision boundary on our model. For this purpose, I will first create a function to plot the decision boundary. If you don’t know what a decision boundary is, you can learn it from here. Now let’s create the function: from matplotlib.colors import ListedColormap def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True): x1s = np.linspace(axes[0], axes[1], 100) x2s = np.linspace(axes[2], axes[3], 100) x1, x2 = np.meshgrid(x1s, x2s) X_new = np.c_[x1.ravel(), x2.ravel()] y_pred = clf.predict(X_new).reshape(x1.shape) custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0']) plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap) if contour: custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50']) plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8) plt.plot(X[:, 0][y==0], X[:, 1][y==0], ""yo"", alpha=alpha) plt.plot(X[:, 0][y==1], X[:, 1][y==1], ""bs"", alpha=alpha) plt.axis(axes) plt.xlabel(r""$x_1$"", fontsize=18) plt.ylabel(r""$x_2$"", fontsize=18, rotation=0)Code language: Python (python) plt.figure(figsize=(6, 4)) for i in range(15): tree_clf = DecisionTreeClassifier(max_leaf_nodes=16, random_state=42 + i) indices_with_replacement = np.random.randint(0, len(X_train), len(X_train)) tree_clf.fit(X[indices_with_replacement], y[indices_with_replacement]) plot_decision_boundary(tree_clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.02, contour=False) plt.show()Code language: Python (python) The Random Forest algorithm results in higher tree diversification, which trades with a very higher level of bias for lower variance. Overall, it leads to a better prediction model. I hope you liked this article, feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Random Forest Algorithm
2020-08-04 01:04:12;Fraud is one of the major issues we come up majorly in banks, life insurance, health insurance, and many others. These major frauds are dependent on the person who is trying to sell you the fake product or service, if you are matured enough to decide what is wrong then you will never get into any fraud transactions. But one such fraud that has been increasing a lot these days is fraud in making payments. In this article, I will take you through a solution to fraud detection with machine learning.The dataset that I will use for this task can be easily downloaded from here. The dataset that I am using is transaction data for online purchases collected from an e-commerce retailer. The dataset contains more than 39000 transactions, each transaction contains 5 features that will describe the nature of the transactions. So let’s start with importing all the necessary libraries we need for Fraud Detection with Machine Learning:;https://thecleverprogrammer.com/2020/08/04/fraud-detection-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Logistic Regression', 'Classification', 'Regression'];['detect', 'recogn', 'regression', 'predict', 'fit', 'model', 'machine learning', 'logistic regression', 'classif', 'ground truth', 'train', 'label'];Fraud is one of the major issues we come up majorly in banks, life insurance, health insurance, and many others. These major frauds are dependent on the person who is trying to sell you the fake product or service, if you are matured enough to decide what is wrong then you will never get into any fraud transactions. But one such fraud that has been increasing a lot these days is fraud in making payments. In this article, I will take you through a solution to fraud detection with machine learning. The dataset that I will use for this task can be easily downloaded from here. The dataset that I am using is transaction data for online purchases collected from an e-commerce retailer. The dataset contains more than 39000 transactions, each transaction contains 5 features that will describe the nature of the transactions. So let’s start with importing all the necessary libraries we need for Fraud Detection with Machine Learning: import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrixCode language: Python (python) Payment Fraud Detection Model Fortunately, the dataset I am using is already structured very well with no missing values in it, and I don’t find any scope of data cleaning in it. So without wasting any time, I will dive into building our machine learning model. So, now I will start by importing the data: from google.colab import files uploaded = files.upload() df = pd.read_csv('payment_fraud.csv') df.head()Code language: Python (python) Now, I will split the data into training and test sets: # Split dataset up into train and test sets X_train, X_test, y_train, y_test = train_test_split( df.drop('label', axis=1), df['label'], test_size=0.33, random_state=17)Code language: Python (python) As this is a problem of binary classification, I will use a Logistic Regression algorithm, as it is one of the most powerful algorithms for a binary classification model. If you don’t know what Binary Classification means, you can learn it from here. Now let’s simply train the fraud detection model using logistic regression algorithm and have a look at the accuracy score that we will get by using this algorithm: clf = LogisticRegression().fit(X_train, y_train) # Make predictions on test set y_pred = clf.predict(X_test) from sklearn.metrics import accuracy_score print(accuracy_score(y_pred, y_test))Code language: Python (python) 1.0 Well, what was the last time when you got an accuracy of 100 per cent. Our fraud detection model gave an accuracy of 100 per cent by using the logistic regression algorithm. Evaluating the Fraud Detection Model Now, let’s evaluate the performance of our model. I will use the confusion matrix algorithm to evaluate the performance of our model. We can use the confusion matrix algorithm with a one-line code only: # Compare test set predictions with ground truth labels print(confusion_matrix(y_test, y_pred))Code language: Python (python) [[12753 0] [ 0 190]] So out of all the transaction in the dataset,190 transactions are correctly recognized as fraud, and 12753 transactions are recognized as not fraudulent transactions. I hope you liked this article on Fraud Detection with Machine Learning. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram;Fraud Detection with Machine Learning
2020-08-04 21:46:09;Named Entity means anything that is a real-world object such as a person, a place, any organisation, any product which has a name. For example – “My name is Aman, and I and a Machine Learning Trainer”. In this sentence the name “Aman”, the field or subject “Machine Learning” and the profession “Trainer” are named entities. In Machine Learning Named Entity Recognition (NER) is a task of Natural Language Processing to identify the named entities in a certain piece of text.Have you ever used software known as Grammarly? It identifies all the incorrect spellings and punctuations in the text and corrects it. But it does not do anything with the named entities, as it is also using the same technique. In this article, I will take you through the task of Named Entity Recognition (NER) with Machine Learning.;https://thecleverprogrammer.com/2020/08/04/named-entity-recognition-ner/;['spacy', 'keras', 'sklearn', 'tensorflow'];1.0;['NER'];['LSTM', 'NN', 'AI', 'ML', 'NLP', 'ReLu', 'NER'];['artificial intelligence', 'recogn', 'epoch', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'layer', 'relu', 'named entity recognition', 'natural language processing', 'train', 'lstm', 'label'];"Named Entity means anything that is a real-world object such as a person, a place, any organisation, any product which has a name. For example – “My name is Aman, and I and a Machine Learning Trainer”. In this sentence the name “Aman”, the field or subject “Machine Learning” and the profession “Trainer” are named entities. In Machine Learning Named Entity Recognition (NER) is a task of Natural Language Processing to identify the named entities in a certain piece of text. Have you ever used software known as Grammarly? It identifies all the incorrect spellings and punctuations in the text and corrects it. But it does not do anything with the named entities, as it is also using the same technique. In this article, I will take you through the task of Named Entity Recognition (NER) with Machine Learning. Loading the Data for Named Entity Recognition (NER) The dataset, that I will use for this task can be easily downloaded from here. Now the first thing I will fo is to load the data and have a look at it to know what I am working with. So let’s simply import the pandas library and load the data: from google.colab import files uploaded = files.upload() import pandas as pd data = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape') data.head()Code language: Python (python) In the data, we can see that the words are broken into columns which will represent our feature X, and the Tag column in the right will represent our label Y. Data Preparation for Neural Networks I will train a Neural Network for the task of Named Entity Recognition (NER). So we need to do some modifications in the data to prepare it in such a manner so that it can easily fit into a neutral network. I will start this step by extracting the mappings that are required to train the neural network: from itertools import chain def get_dict_map(data, token_or_tag): tok2idx = {} idx2tok = {} if token_or_tag == 'token': vocab = list(set(data['Word'].to_list())) else: vocab = list(set(data['Tag'].to_list())) idx2tok = {idx:tok for idx, tok in enumerate(vocab)} tok2idx = {tok:idx for idx, tok in enumerate(vocab)} return tok2idx, idx2tok token2idx, idx2token = get_dict_map(data, 'token') tag2idx, idx2tag = get_dict_map(data, 'tag')Code language: Python (python) Now I will transform the columns in the data to extract the sequential data for our neural network: data['Word_idx'] = data['Word'].map(token2idx) data['Tag_idx'] = data['Tag'].map(tag2idx) data_fillna = data.fillna(method='ffill', axis=0) # Groupby and collect columns data_group = data_fillna.groupby( ['Sentence #'],as_index=False )['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))Code language: Python (python) Now I will split the data into training and test sets. I will create a function for splitting the data because the LSTM layers accept sequences of the same length only. So every sentence that appears as integer in the data must be padded with the same length: from sklearn.model_selection import train_test_split from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical def get_pad_train_test_val(data_group, data): #get max token and tag length n_token = len(list(set(data['Word'].to_list()))) n_tag = len(list(set(data['Tag'].to_list()))) #Pad tokens (X var) tokens = data_group['Word_idx'].tolist() maxlen = max([len(s) for s in tokens]) pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1) #Pad Tags (y var) and convert it into one hot encoding tags = data_group['Tag_idx'].tolist() pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[""O""]) n_tags = len(tag2idx) pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags] #Split train, test and validation set tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020) train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020) print( 'train_tokens length:', len(train_tokens), '\ntrain_tokens length:', len(train_tokens), '\ntest_tokens length:', len(test_tokens), '\ntest_tags:', len(test_tags), '\nval_tokens:', len(val_tokens), '\nval_tags:', len(val_tags), ) return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)Code language: Python (python) train_tokens length: 32372 train_tokens length: 32372 test_tokens length: 4796 test_tags: 4796 val_tokens: 10791 val_tags: 10791 Training Neural Network for Named Entity Recognition (NER) Now, I will proceed with training the neural network architecture of our model. So let’s start with importing all the packages we need for training our neural network: import numpy as np import tensorflow from tensorflow.keras import Sequential, Model, Input from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional from tensorflow.keras.utils import plot_model from numpy.random import seed seed(1) tensorflow.random.set_seed(2)Code language: Python (python) The layer below will take the dimensions from the LSTM layer and will give the maximum length and maximum tags as an output: input_dim = len(list(set(data['Word'].to_list())))+1 output_dim = 64 input_length = max([len(s) for s in data_group['Word_idx'].tolist()]) n_tags = len(tag2idx)Code language: Python (python) Now I will create a helper function which will help us in giving the summary of every layer of the neural network model for Named Entity Recognition (NER): def get_bilstm_lstm_model(): model = Sequential() # Add Embedding layer model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length)) # Add bidirectional LSTM model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat')) # Add LSTM model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)) # Add timeDistributed Layer model.add(TimeDistributed(Dense(n_tags, activation=""relu""))) #Optimiser # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.summary() return modelCode language: Python (python) Now I will create a helper function to train the Named Entity Recognition model: def train_model(X, y, model): loss = list() for i in range(25): # fit model for one epoch on this sequence hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2) loss.append(hist.history['loss'][0]) return lossCode language: Python (python) Driver code: results = pd.DataFrame() model_bilstm_lstm = get_bilstm_lstm_model() plot_model(model_bilstm_lstm) results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)Code language: Python (python) The model will give the final output after running for 25 epochs. So it will take some time to run. Testing the Named Entity Recognition (NER) Model: Now let’s test our model on a piece of text: import spacy from spacy import displacy nlp = spacy.load('en_core_web_sm') text = nlp('Hi, My name is Aman Kharwal \n I am from India \n I want to work with Google \n Steve Jobs is My Inspiration') displacy.render(text, style = 'ent', jupyter=True)Code language: Python (python) So we can see a very good result from our model. I hope you liked this article on Named Entity Recognition (NER) with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Also Read: Artificial Intelligence Projects to Boost your Portfolio. Follow Us: Facebook Instagram";Named Entity Recognition (NER)
2020-08-06 17:12:41;AdaBoost Algorithm is a boosting method that works by combining weak learners into strong learners. A good way for a prediction model to correct its predecessor is to give more attention to the training samples where the predecessor did not fit well. This can result in a new prediction model which will focus much on the hard instances. This technique is used by an AdaBoost Algorithm. In this article, I will take you through the AdaBoost Algorithm in Machine Learning.;https://thecleverprogrammer.com/2020/08/06/adaboost-algorithm/;['sklearn'];1.0;[];['ML', 'Classification', 'Decision Tree'];['predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'label', 'decision tree'];"AdaBoost Algorithm is a boosting method that works by combining weak learners into strong learners. A good way for a prediction model to correct its predecessor is to give more attention to the training samples where the predecessor did not fit well. This can result in a new prediction model which will focus much on the hard instances. This technique is used by an AdaBoost Algorithm. In this article, I will take you through the AdaBoost Algorithm in Machine Learning. Training a Base Classifier To use an AdaBoost classification algorithm, we first need to train a base classification model. So, to explain this algorithm, I will first train a Decision Tree algorithm as our base classification model. I will start by importing the necessary packages to train a DecisionTreeClassifier: import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= ""0.20"" # Common imports import numpy as np import os # to make this notebook's output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc('axes', labelsize=14) mpl.rc('xtick', labelsize=12) mpl.rc('ytick', labelsize=12)Code language: Python (python) Now I will use the Decision Tree algorithm to train a base classification: from sklearn.model_selection import train_test_split from sklearn.datasets import make_moons X, y = make_moons(n_samples=500, noise=0.30, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)Code language: Python (python) AdaBoost Algorithm in Machine Learning The AdaBoost Algorithm increases the relative weight of less classified training samples. Then it trains another classification model by using the new updates weights of classified training samples and again predicts on the training set. Let’s have a look at how we can implement this algorithm: from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier ada_clf = AdaBoostClassifier( DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=""SAMME.R"", learning_rate=0.5, random_state=42) ada_clf.fit(X_train, y_train)Code language: Python (python) To move further, I will create a helper function to plot a decision boundary. If you want to learn about a decision boundary, you can learn about it from here. Now, Let’s create the function: from matplotlib.colors import ListedColormap def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True): x1s = np.linspace(axes[0], axes[1], 100) x2s = np.linspace(axes[2], axes[3], 100) x1, x2 = np.meshgrid(x1s, x2s) X_new = np.c_[x1.ravel(), x2.ravel()] y_pred = clf.predict(X_new).reshape(x1.shape) custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0']) plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap) if contour: custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50']) plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8) plt.plot(X[:, 0][y==0], X[:, 1][y==0], ""yo"", alpha=alpha) plt.plot(X[:, 0][y==1], X[:, 1][y==1], ""bs"", alpha=alpha) plt.axis(axes) plt.xlabel(r""$x_1$"", fontsize=18) plt.ylabel(r""$x_2$"", fontsize=18, rotation=0) plot_decision_boundary(ada_clf, X, y)Code language: Python (python) Now, let’s plot the results of our classification model using a decision boundary: from sklearn.svm import SVC m = len(X_train) fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True) for subplot, learning_rate in ((0, 1), (1, 0.5)): sample_weights = np.ones(m) plt.sca(axes[subplot]) for i in range(5): svm_clf = SVC(kernel=""rbf"", C=0.05, gamma=""scale"", random_state=42) svm_clf.fit(X_train, y_train, sample_weight=sample_weights) y_pred = svm_clf.predict(X_train) sample_weights[y_pred != y_train] *= (1 + learning_rate) plot_decision_boundary(svm_clf, X, y, alpha=0.2) plt.title(""learning_rate = {}"".format(learning_rate), fontsize=16) if subplot == 0: plt.text(-0.7, -0.65, ""1"", fontsize=14) plt.text(-0.6, -0.10, ""2"", fontsize=14) plt.text(-0.5, 0.10, ""3"", fontsize=14) plt.text(-0.4, 0.55, ""4"", fontsize=14) plt.text(-0.3, 0.90, ""5"", fontsize=14) else: plt.ylabel("""") plt.show()Code language: Python (python) The new classification algorithm does a better job in the same instances as used by the decision tree algorithm. The figure on the right represents the sequence of predictions used by the AdaBoost Algorithm. The learning rate is halved because the less classified weights are boosted half at every step of iterations. Also, Read – WhatsApp Group Chat Analysis. As you can see, the AdaBoost Algorithm added the predictions to the model and at the end made it performed better. If your model is overfitting on the training set, you can reduce the number of estimators. I hope you liked this article. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Also, Read – Machine Learning Algorithms that are Mostly Used. Follow Us: Facebook Instagram";AdaBoost Algorithm
2020-08-06 12:07:04;So I am a part of a WhatsApp group named as “Data Science Community”, recently I thought to explore the chat of this group and do some analysis on it. So, here in this article, I will take you through a  WhatsApp group chat Analysis with Data Science.If you don’t know how to extract the messages from any chat then just open any chat click on the 3 dots above, select more and then select explore chat, and share it with any means, most preferable your email.The chat you will get at the end does not need any cleaning and preparation it can be used directly for the task. Now let’s start with this WhatsApp group chat analysis, I will simply import the required packages and get started with the task:;https://thecleverprogrammer.com/2020/08/06/whatsapp-group-chat-analysis/;['pattern'];1.0;['NER', 'RL'];['ML', 'NER', 'RL'];['detect', 'recogn', 'machine learning', 'filter', 'named entity recognition'];"So I am a part of a WhatsApp group named as “Data Science Community”, recently I thought to explore the chat of this group and do some analysis on it. So, here in this article, I will take you through a WhatsApp group chat Analysis with Data Science. If you don’t know how to extract the messages from any chat then just open any chat click on the 3 dots above, select more and then select explore chat, and share it with any means, most preferable your email. The chat you will get at the end does not need any cleaning and preparation it can be used directly for the task. Now let’s start with this WhatsApp group chat analysis, I will simply import the required packages and get started with the task: import regex import pandas as pd import numpy as np import emoji from collections import Counter import matplotlib.pyplot as plt from os import path from PIL import Image from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator % matplotlib inlineCode language: Python (python) WhatsApp Group Chat Analysis Although, the data is ready to use we still need to change the format of the date and time of messages which can be done easily. For this I will define a function that can detect whether each line starts with a date as it states that it is a unique message: def startsWithDateAndTime(s): pattern = '^([0-9]+)(\/)([0-9]+)(\/)([0-9]+), ([0-9]+):([0-9]+)[ ]?(AM|PM|am|pm)? -' result = re.match(pattern, s) if result: return True return FalseCode language: Python (python) Now I will create a function to extract the usernames in the chats as Authors: def FindAuthor(s): s=s.split("":"") if len(s)==2: return True else: return FalseCode language: Python (python) Now, I will create a function to separate all the information from each other so that we could easily use the information as a pandas dataframe: def getDataPoint(line): splitLine = line.split(' - ') dateTime = splitLine[0] date, time = dateTime.split(', ') message = ' '.join(splitLine[1:]) if FindAuthor(message): splitMessage = message.split(': ') author = splitMessage[0] message = ' '.join(splitMessage[1:]) else: author = None return date, time, author, messageCode language: Python (python) The code below will help you to get the data, if you are using an IDE or Jupyter notebook or Google Colab on anything, you can use the code below, you just need to make sure that you write the complete path of your dataset if you are not using Colab or Notebook: from google.colab import files uploaded = files.upload() data = [] # List to keep track of data so it can be used by a Pandas dataframe conversation = 'WhatsApp Chat (1).txt' with open(conversation, encoding=""utf-8"") as fp: fp.readline() # Skipping first line of the file because contains information related to something about end-to-end encryption messageBuffer = [] date, time, author = None, None, None while True: line = fp.readline() if not line: break line = line.strip() if startsWithDateAndTime(line): if len(messageBuffer) &amp;gt; 0: parsedData.append([date, time, author, ' '.join(messageBuffer)]) messageBuffer.clear() date, time, author, message = getDataPoint(line) messageBuffer.append(message) else: messageBuffer.append(line)Code language: Python (python) Now, let’s put the data into a dataframe and have a look at the data: df = pd.DataFrame(parsedData, columns=['Date', 'Time', 'Author', 'Message']) # Initialising a pandas Dataframe. df[""Date""] = pd.to_datetime(df[""Date""]) df.tail(20)Code language: Python (python) The above dataframe looks good. Now let’s start with our WhatsApp group chat analysis. Also, Read – Named Entity Recognition (NER) To Get All The Authors: The Authors are representing all the participants of the WhatsApp group, now let’s use how we can extract names of all the authors: df.Author.unique()Code language: Python (python) array([None, ‘Aman Kharwal’, ‘Sahil Pansare’, ‘+91 97386 30266’, ‘+91 97217 95958’, ‘+91 83696 21916’, ‘+91 88064 51751’, ‘+91 96627 78558’, ‘+91 90252 51204’, ‘+91 70665 40498’, ‘+91 84471 85093’, ‘+91 79065 56743’, ‘+60 11-5689 2040’, ‘+91 99150 15281’, ‘+91 93983 18393’, ‘+91 95612 77706’, ‘+91 98224 35433’, ‘+91 98673 74287’, ‘+91 74474 80190’, ‘+91 87288 48041’, ‘+91 86106 90461’, ‘+91 76200 14058’, ‘+91 98507 34912’, ‘+91 77868 68987’, ‘+91 77387 12804’, ‘+91 98119 14741’, ‘+91 99724 91453’, ‘+91 70382 50701’, ‘+91 83448 26314’, ‘+91 95000 28536’, ‘+91 93703 49063’, ‘+91 93808 22645’, ‘+91 99165 66683’, ‘+91 70424 73460’, ‘Sumehar’, ‘+91 86002 94761’], dtype=object) So, I got only 3 numbers saved out of all the participants. I will use the data as it is and will explore the 3 numbers with names. WhatsApp Group Chat Analysis: Group Wise Status Now let’s have some analysis by looking at the statistics. I will first create a function which will split the text and other media files from each other including emojis: media_messages = df[df['Message'] == '&amp;lt;Media omitted&amp;gt;'].shape[0] def split_count(text): emoji_list = [] data = regex.findall(r'\X', text) for word in data: if any(char in emoji.UNICODE_EMOJI for char in word): emoji_list.append(word) return emoji_list df[""emoji""] = df[""Message""].apply(split_count) emojis = sum(df['emoji'].str.len()) URLPATTERN = r'(https?://\S+)' df['urlcount'] = df.Message.apply(lambda x: re.findall(URLPATTERN, x)).str.len() links = np.sum(df.urlcount) print(""Data Science Community"") print(""Messages:"",total_messages) print(""Media:"",media_messages) print(""Emojis:"",emojis) print(""Links:"",links)Code language: Python (python) Data Science Community Messages: 2201 Media: 470 Emojis: 613 Links: 437 Now we will look at the author wise status from the WhatsApp group chat: media_messages_df = df[df['Message'] == '&amp;lt;Media omitted&amp;gt;'] messages_df = df.drop(media_messages_df.index) messages_df['Letter_Count'] = messages_df['Message'].apply(lambda s : len(s)) messages_df['Word_Count'] = messages_df['Message'].apply(lambda s : len(s.split(' '))) messages_df[""MessageCount""]=1 l = [""Aman Kharwal"", ""Sahil Pansare"", ""Sumehar""] for i in range(len(l)): # Filtering out messages of particular user req_df= messages_df[messages_df[""Author""] == l[i]] # req_df will contain messages of only one particular user print(f'Stats of {l[i]} -') # shape will print number of rows which indirectly means the number of messages print('Messages Sent', req_df.shape[0]) #Word_Count contains of total words in one message. Sum of all words/ Total Messages will yield words per message words_per_message = (np.sum(req_df['Word_Count']))/req_df.shape[0] print('Words per message', words_per_message) #media conists of media messages media = media_messages_df[media_messages_df['Author'] == l[i]].shape[0] print('Media Messages Sent', media) # emojis conists of total emojis emojis = sum(req_df['emoji'].str.len()) print('Emojis Sent', emojis) #links consist of total links links = sum(req_df[""urlcount""]) print('Links Sent', links) print()Code language: Python (python) Stats of Aman Kharwal – Messages Sent 431 Words per message 5.907192575406032 Media Messages Sent 17 Emojis Sent 83 Links Sent 245 Stats of Sahil Pansare – Messages Sent 306 Words per message 20.81045751633987 Media Messages Sent 12 Emojis Sent 195 Links Sent 52 Stats of Sumehar – Messages Sent 52 Words per message 4.826923076923077 Media Messages Sent 0 Emojis Sent 8 Links Sent 0 Now let’s have a look at the most emojis used in the group: total_emojis_list = list([a for b in messages_df.emoji for a in b]) emoji_dict = dict(Counter(total_emojis_list)) emoji_dict = sorted(emoji_dict.items(), key=lambda x: x[1], reverse=True) for i in emoji_dict: print(i)Code language: Python (python) (‘👍🏻’, 118) (‘😊’, 81) (‘💯’, 60) (‘🤝’, 39) (‘👌🏿’, 28) (‘👍🏽’, 28) (‘👌’, 24) (‘👏🏾’, 20) (‘😂’, 16) (‘‼️’, 16) (‘👉’, 16) (‘⭐’, 12) (‘👍🏼’, 12) (‘👍’, 12) (‘✌🏻’, 8) (‘✌️’, 8) (‘😅’, 8) (‘🌹’, 8) (‘\U0001f973’, 8) (‘😇’, 8) (‘😉’, 6) (‘🙌🏻’, 6) (‘\U0001f929’, 5) (‘😄’, 4) (‘😶’, 4) (‘☕’, 4) (‘\U0001f9e1’, 4) (‘\U0001f91f🏻’, 4) (‘🙂’, 4) (‘👏’, 4) (‘🔔’, 4) (‘✨’, 4) (‘😬’, 4) (‘👌🏾’, 4) (‘😍’, 4) (‘❤’, 4) (‘🙏🏻’, 3) (‘🤣’, 3) (‘🔥’, 2) (‘👻’, 2) (‘😁’, 1) (‘👏🏻’, 1) (‘🙌’, 1) (‘🤔’, 1) WhatsApp Group Chat Analysis: Word Cloud Now, I will create a Word Cloud for our WhatsApp Group Chat Analysis, to see what the group is based on. A Word Cloud is a graph of words which shows the most used words by representing the most used words bigger than the rest: text = "" "".join(review for review in messages_df.Message) print (""There are {} words in all the messages."".format(len(text))) stopwords = set(STOPWORDS) # Generate a word cloud image wordcloud = WordCloud(stopwords=stopwords, background_color=""white"").generate(text) # Display the generated image: # the matplotlib way: plt.figure( figsize=(10,5)) plt.imshow(wordcloud, interpolation='bilinear') plt.axis(""off"") plt.show()Code language: Python (python) The above Word Cloud is based on the chats of the whole group. Now I will look at the Author wise WordCloud: l = [""Aman Kharwal"", ""Sahil Pansare"", ""Sumehar""] for i in range(len(l)): dummy_df = messages_df[messages_df['Author'] == l[i]] text = "" "".join(review for review in dummy_df.Message) stopwords = set(STOPWORDS) #Generate a word cloud image print('Author name',l[i]) wordcloud = WordCloud(stopwords=stopwords, background_color=""white"").generate(text) #Display the generated image plt.figure( figsize=(10,5)) plt.imshow(wordcloud, interpolation='bilinear') plt.axis(""off"") plt.show()Code language: Python (python) Author name Aman Kharwal Author name Sahil Pansare Author name Sumehar The above WhatsApp Group Chat analysis clearly shows that the group is not a friendship group, it is based on beginners of Machine Learning and programming. I hope you liked this article on WhatsApp Group Chat Analysis. Feel free to ask your valuable questions in the comments section below. You can follow me on Medium, to learn every topic of Machine Learning. Also, Read – Machine Learning Algorithms That Are Mostly Used. Follow Us: Facebook Instagram";WhatsApp Group Chat Analysis
2020-08-08 13:56:57;The Classes in Python are the main tool used for Object-Oriented Programming (OOP). A Class is a coding structure and a specific tool to implement new kinds of objects in Python that supports inheritance. OOP offers a very different and effective way of programming. We can use OOP for Machine Learning also to build Models.The use of OOP is entirely optional in Machine Learning as we already have libraries like Scikit-learn and TensorFlow from where we can easily use algorithms. So learning Object-Oriented Programming for Machine Learning is not that necessary, but as a programmer, you should not limit yourself. So in this article, I will take you through how you can use OOP for Machine Learning to build models.;https://thecleverprogrammer.com/2020/08/08/oop-for-machine-learning/;['sklearn'];1.0;[];['ML', 'Linear Regression', 'Regression'];['regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'train'];"The Classes in Python are the main tool used for Object-Oriented Programming (OOP). A Class is a coding structure and a specific tool to implement new kinds of objects in Python that supports inheritance. OOP offers a very different and effective way of programming. We can use OOP for Machine Learning also to build Models. The use of OOP is entirely optional in Machine Learning as we already have libraries like Scikit-learn and TensorFlow from where we can easily use algorithms. So learning Object-Oriented Programming for Machine Learning is not that necessary, but as a programmer, you should not limit yourself. So in this article, I will take you through how you can use OOP for Machine Learning to build models. Object-Oriented Programming (OOP) for Machine Learning To illustrate the process of using OOP for Machine Learning, I will use a dataset which is based on Weather, can be easily downloaded from here. As we are using Object Oriented Programming so we don’t need any of the machine learning packages except pandas, which we will use to read the data, and Scikit-Learn to use a Machine Learning Algorithm and Numpy for Numerical Python. So let’s import pandas and get started with OOP for Machine Learning. import pandas as pd from google.colab import files uploaded = files.upload() class Model: def __init__(self, datafile = ""weatherHistory (1).csv""): self.df = pd.read_csv(datafile) if __name__ == '__main__': model_instance = Model() print(model_instance.df.head())Code language: Python (python) Formatted Date ... Daily Summary 0 2006-04-01 00:00:00.000 +0200 ... Partly cloudy throughout the day. 1 2006-04-01 01:00:00.000 +0200 ... Partly cloudy throughout the day. 2 2006-04-01 02:00:00.000 +0200 ... Partly cloudy throughout the day. 3 2006-04-01 03:00:00.000 +0200 ... Partly cloudy throughout the day. 4 2006-04-01 04:00:00.000 +0200 ... Partly cloudy throughout the day. The next thing to do here is defining a class to call a machine learning model. Just to keep it simple, I will use a Linear Regression model for this task: import pandas as pd from sklearn.model_selection import train_test_split import numpy as np from sklearn.linear_model import LinearRegression class Model: def __init__(self, datafile = ""weatherHistory (1).csv""): self.df = pd.read_csv(datafile) self.linear_reg = LinearRegression()Code language: Python (python) The next step is to split the data into a training set and a test set. For this, I will import train_test_split provided by Scikit-Learn, and then I will create a function to split the data into training and test sets: from sklearn.model_selection import train_test_split import numpy as np def split(self, test_size): X = np.array(self.df[['Humidity', 'Pressure (millibars)']]) y = np.array(self.df['Temperature (C)']) self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size = test_size, random_state = 42)Code language: Python (python) Now, I will define a function to fit the data into our machine learning model: def fit(self): self.model = self.linear_reg.fit(self.X_train, self.y_train)Code language: Python (python) Now let’s create a function to predict using our machine learning model: def predict(self): result = self.linear_reg.predict(self.X_test) return resultCode language: Python (python) Now, let’s run the complete file with training set as 80 per cent, and the test set as 20 per cent. For this, I will write the whole code with proper indentation. And at the end I will also print the accuracy of our model: import pandas as pd from sklearn.model_selection import train_test_split import numpy as np from sklearn.linear_model import LinearRegression class Model: def __init__(self, datafile = ""weatherHistory (1).csv""): self.df = pd.read_csv(datafile) self.linear_reg = LinearRegression() def split(self, test_size): X = np.array(self.df[['Humidity', 'Pressure (millibars)']]) y = np.array(self.df['Temperature (C)']) self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size = test_size, random_state = 42) def fit(self): self.model = self.linear_reg.fit(self.X_train, self.y_train) def predict(self): result = self.linear_reg.predict(self.X_test) return result if __name__ == '__main__': model_instance = Model() model_instance.split(0.2) model_instance.fit() print(model_instance.predict()) print(""Accuracy: "", model_instance.model.score(model_instance.X_test, model_instance.y_test))Code language: Python (python) [18.27024238 8.9966463 14.55962648 … 18.26962999 17.96249378 16.41528188] Accuracy: 0.39578560465686424 Also, Read – Web Scraping to Create CSV. So in this way, we can easily use Object-Oriented Programming for Machine Learning. I hope you liked this article on using OOP for Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Also, Read – How to Start with Machine Learning? Follow Us: Facebook Instagram";OOP for Machine Learning
2020-08-11 13:05:09;The LSTM Network model stands for Long Short Term Memory networks. These are a special kind of Neural Networks which are generally capable of understanding long term dependencies. LSTM model was generally designed to prevent the problems of long term dependencies which they generally do in a very good manner. In this article I will take you through how we can use LSTMs in Time Series Forecasting.The LSTM Network models generally have potential to remove or add data carefully which is regulated by a special structure known as gates. The first step in processing LSTMs is to determine what information we need to throw from the cell. The next step is deciding what new information we should store in the cell. At last we decide what we want as an output. The output is generally based on the state of cells. Let’s understand the process of LSTM using an example of Time Series Forecasting to predict stock prices.;https://thecleverprogrammer.com/2020/08/11/lstm-in-machine-learning/;['keras', 'sklearn', 'tensorflow'];1.0;['RNN', 'NN'];['ML', 'LSTM', 'RNN', 'NN'];['hidden layer', 'epoch', 'output layer', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'layer', 'train', 'lstm', 'label', 'test data', 'neuron'];"The LSTM Network model stands for Long Short Term Memory networks. These are a special kind of Neural Networks which are generally capable of understanding long term dependencies. LSTM model was generally designed to prevent the problems of long term dependencies which they generally do in a very good manner. In this article I will take you through how we can use LSTMs in Time Series Forecasting. The LSTM Network models generally have potential to remove or add data carefully which is regulated by a special structure known as gates. The first step in processing LSTMs is to determine what information we need to throw from the cell. The next step is deciding what new information we should store in the cell. At last we decide what we want as an output. The output is generally based on the state of cells. Let’s understand the process of LSTM using an example of Time Series Forecasting to predict stock prices. Stock Price Prediction using LSTM Let’s see how we can use the LSTM model to predict stock prices using Time Series Forecasting. For this task I will scrape the data from yahoo finance using the pandas_datareader library. So before doing so let’s start with importing all the packages we need for this task: import math import matplotlib.pyplot as plt import keras import pandas as pd import numpy as np from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.layers import Dropout from keras.layers import * from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_error from sklearn.model_selection import train_test_split from keras.callbacks import EarlyStopping import pandas_datareader as webCode language: Python (python) To get the data: data = web.DataReader("""", data_source=""yahoo"", start=None, end=None) data.reset_index(inplace=True) data.head()Code language: Python (python) The next step is to divide the data into training and testing sets to avoid overfitting and to be able to study the generalizability of our model: training_set = data.iloc[:800, 1:2].values test_set = data.iloc[800:, 1:2].valuesCode language: Python (python) The target value to be predicted will be the value of the “Close” share price. It is a good idea to normalize the data before fitting the model. This will increase performance. Let’s create the input features with a 1-day lag: # Feature Scaling sc = MinMaxScaler(feature_range = (0, 1)) training_set_scaled = sc.fit_transform(training_set) # Creating a data structure with 60 time-steps and 1 output X_train = [] y_train = [] for i in range(60, 800): X_train.append(training_set_scaled[i-60:i, 0]) y_train.append(training_set_scaled[i, 0]) X_train, y_train = np.array(X_train), np.array(y_train) X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))Code language: Python (python) Building LSTM Model Now it’s time to build the Long-Short Term Memory model, I will build the neural network with 50 neurons and four hidden layers: model = Sequential() #Adding the first LSTM layer and some Dropout regularisation model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1))) model.add(Dropout(0.2)) # Adding a second LSTM layer and some Dropout regularisation model.add(LSTM(units = 50, return_sequences = True)) model.add(Dropout(0.2)) # Adding a third LSTM layer and some Dropout regularisation model.add(LSTM(units = 50, return_sequences = True)) model.add(Dropout(0.2)) # Adding a fourth LSTM layer and some Dropout regularisation model.add(LSTM(units = 50)) model.add(Dropout(0.2)) # Adding the output layer model.add(Dense(units = 1)) # Compiling the RNN model.compile(optimizer = 'adam', loss = 'mean_squared_error') # Fitting the RNN to the Training set model.fit(X_train, y_train, epochs = 100, batch_size = 32)Code language: Python (python) The model will take some time to run, after the execution you will see an output like this: Epoch 1/100 24/24 [==============================] - 2s 99ms/step - loss: 0.0496 Epoch 2/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0085 Epoch 3/100 24/24 [==============================] - 3s 109ms/step - loss: 0.0057 Epoch 4/100 24/24 [==============================] - 3s 105ms/step - loss: 0.0057 Epoch 5/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0054 Epoch 6/100 24/24 [==============================] - 3s 106ms/step - loss: 0.0062 Epoch 7/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0055 Epoch 8/100 24/24 [==============================] - 3s 105ms/step - loss: 0.0050 Epoch 9/100 24/24 [==============================] - 3s 113ms/step - loss: 0.0044 Epoch 10/100 24/24 [==============================] - 3s 105ms/step - loss: 0.0053 Epoch 11/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0048 Epoch 12/100 24/24 [==============================] - 3s 118ms/step - loss: 0.0051 Epoch 13/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0054 Epoch 14/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0046 Epoch 15/100 24/24 [==============================] - 3s 106ms/step - loss: 0.0046 Epoch 16/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0045 Epoch 17/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0044 Epoch 18/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0042 Epoch 19/100 24/24 [==============================] - 3s 112ms/step - loss: 0.0039 Epoch 20/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0042 Epoch 21/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0043 Epoch 22/100 24/24 [==============================] - 3s 109ms/step - loss: 0.0038 Epoch 23/100 24/24 [==============================] - 3s 113ms/step - loss: 0.0037 Epoch 24/100 24/24 [==============================] - 3s 106ms/step - loss: 0.0039 Epoch 25/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0038 Epoch 26/100 24/24 [==============================] - 3s 114ms/step - loss: 0.0040 Epoch 27/100 24/24 [==============================] - 3s 117ms/step - loss: 0.0034 Epoch 28/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0039 Epoch 29/100 24/24 [==============================] - 3s 109ms/step - loss: 0.0035 Epoch 30/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0055 Epoch 31/100 24/24 [==============================] - 3s 106ms/step - loss: 0.0038 Epoch 32/100 24/24 [==============================] - 3s 113ms/step - loss: 0.0033 Epoch 33/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0035 Epoch 34/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0033 Epoch 35/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0035 Epoch 36/100 24/24 [==============================] - 3s 114ms/step - loss: 0.0036 Epoch 37/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0033 Epoch 38/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0039 Epoch 39/100 24/24 [==============================] - 3s 109ms/step - loss: 0.0033 Epoch 40/100 24/24 [==============================] - 3s 109ms/step - loss: 0.0032 Epoch 41/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0031 Epoch 42/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0028 Epoch 43/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0038 Epoch 44/100 24/24 [==============================] - 3s 106ms/step - loss: 0.0034 Epoch 45/100 24/24 [==============================] - 3s 106ms/step - loss: 0.0034 Epoch 46/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0032 Epoch 47/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0033 Epoch 48/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0046 Epoch 49/100 24/24 [==============================] - 3s 115ms/step - loss: 0.0032 Epoch 50/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0028 Epoch 51/100 24/24 [==============================] - 3s 106ms/step - loss: 0.0028 Epoch 52/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0025 Epoch 53/100 24/24 [==============================] - 3s 105ms/step - loss: 0.0025 Epoch 54/100 24/24 [==============================] - 3s 105ms/step - loss: 0.0027 Epoch 55/100 24/24 [==============================] - 3s 116ms/step - loss: 0.0033 Epoch 56/100 24/24 [==============================] - 3s 109ms/step - loss: 0.0029 Epoch 57/100 24/24 [==============================] - 3s 112ms/step - loss: 0.0026 Epoch 58/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0026 Epoch 59/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0029 Epoch 60/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0028 Epoch 61/100 24/24 [==============================] - 3s 106ms/step - loss: 0.0026 Epoch 62/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0030 Epoch 63/100 24/24 [==============================] - 3s 105ms/step - loss: 0.0027 Epoch 64/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0030 Epoch 65/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0024 Epoch 66/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0028 Epoch 67/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0024 Epoch 68/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0022 Epoch 69/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0023 Epoch 70/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0024 Epoch 71/100 24/24 [==============================] - 3s 115ms/step - loss: 0.0022 Epoch 72/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0022 Epoch 73/100 24/24 [==============================] - 3s 105ms/step - loss: 0.0025 Epoch 74/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0020 Epoch 75/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0021 Epoch 76/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0022 Epoch 77/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0021 Epoch 78/100 24/24 [==============================] - 3s 112ms/step - loss: 0.0022 Epoch 79/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0020 Epoch 80/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0020 Epoch 81/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0021 Epoch 82/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0021 Epoch 83/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0019 Epoch 84/100 24/24 [==============================] - 3s 110ms/step - loss: 0.0020 Epoch 85/100 24/24 [==============================] - 3s 105ms/step - loss: 0.0019 Epoch 86/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0018 Epoch 87/100 24/24 [==============================] - 3s 109ms/step - loss: 0.0018 Epoch 88/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0020 Epoch 89/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0027 Epoch 90/100 24/24 [==============================] - 3s 109ms/step - loss: 0.0024 Epoch 91/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0021 Epoch 92/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0016 Epoch 93/100 24/24 [==============================] - 3s 106ms/step - loss: 0.0017 Epoch 94/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0018 Epoch 95/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0019 Epoch 96/100 24/24 [==============================] - 3s 109ms/step - loss: 0.0018 Epoch 97/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0020 Epoch 98/100 24/24 [==============================] - 3s 107ms/step - loss: 0.0017 Epoch 99/100 24/24 [==============================] - 3s 108ms/step - loss: 0.0019 Epoch 100/100 24/24 [==============================] - 3s 111ms/step - loss: 0.0018 <tensorflow.python.keras.callbacks.History at 0x7ffa67b17b00> Now, let’s reshape the test data: dataset_train = data.iloc[:800, 1:2] dataset_test = data.iloc[800:, 1:2] dataset_total = pd.concat((dataset_train, dataset_test), axis = 0) inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values inputs = inputs.reshape(-1,1) inputs = sc.transform(inputs) X_test = [] for i in range(60, 519): X_test.append(inputs[i-60:i, 0]) X_test = np.array(X_test) X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))Code language: Python (python) Now, let’s make predictions using the LSTM model on the test set: predicted_stock_price = model.predict(X_test) predicted_stock_price = sc.inverse_transform(predicted_stock_price)Code language: Python (python) Now, let’s have a look at our predictions: plt.plot(df.loc[800:, 'Date'],dataset_test.values, color = 'red', label = 'Real TESLA Stock Price') plt.plot(df.loc[800:, 'Date'],predicted_stock_price, color = 'blue', label = 'Predicted TESLA Stock Price') plt.xticks(np.arange(0,459,50)) plt.title('TESLA Stock Price Prediction') plt.xlabel('Time') plt.ylabel('TESLA Stock Price') plt.legend() plt.show()Code language: Python (python) Also, Read – What is Data Mining? We can see that our model has worked very well. It can accurately track most unanticipated jumps/declines; however, for the most recent timestamps, we can see that the model expects lower (predicted) values ​​compared to the actual price values ​​of the action. I hope you liked this article on LSTM in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Also, Read – Scrape Wikipedia with Python. Follow Us: Facebook Instagram";LSTM in Machine Learning
2020-08-11 18:34:33;Due to the affordable computing power and the importance of data in decision making Machine Learning in Finance is getting very useful to make financial and investment decisions with great accuracy. The use of Machine Learning in Finance is reshaping the financial industry to make as perfect decisions as never before.There is a wide variety of Machine Learning Algorithms that fit perfectly in the financial data. Although, Machine Learning can be used in every field to make predictions for better decision making, but making accurate predictions about the financial returns is an art. This is where machine learning plays a major role in Finance to guide financial returns by using historical data. In this article, I will take you through some highly used applications of Machine Learning in finance sectors that you should be perfect at if you have a good interest in Finance.;https://thecleverprogrammer.com/2020/08/11/machine-learning-in-finance/;['pattern'];1.0;[];['ML', 'LSTM'];['predict', 'fit', 'model', 'machine learning', 'lstm'];Due to the affordable computing power and the importance of data in decision making Machine Learning in Finance is getting very useful to make financial and investment decisions with great accuracy. The use of Machine Learning in Finance is reshaping the financial industry to make as perfect decisions as never before. There is a wide variety of Machine Learning Algorithms that fit perfectly in the financial data. Although, Machine Learning can be used in every field to make predictions for better decision making, but making accurate predictions about the financial returns is an art. This is where machine learning plays a major role in Finance to guide financial returns by using historical data. In this article, I will take you through some highly used applications of Machine Learning in finance sectors that you should be perfect at if you have a good interest in Finance. Useful Applications of Machine Learning in Finance Now, let’s go through some very useful applications of Machine Learning in finance: Candlestick Chart Candlestick Chart is a powerful way to visualize the trends and changes in the stock market and other financial instruments. Most people use a Candlestick chart to visualize the trading patterns. To visualize our data in the form of Candlesticks, we must be having data that comprises open price, high price, low price, and close price. It is mainly used in financial analysis. Japanese commodity Traders created this technique to build this type of chart, and initially, they were known as the Japanese Candlesticks. You can learn the practical implementation of this chart from here. Stock Price Prediction with Facebook Prophet Model Stock Price Prediction means to determine the future value of the stocks or other financial instruments of an organisation. If you master the art to predict stock prices, you can earn a lot by investing and selling at the right time, and you can even earn by mentoring other people who want to explore trading. Facebook Prophet is an algorithm developed by Facebook’s Core Data Science team. It is used in the applications of time series forecasting. It is very much used when there is a possibility of seasonal effects. The Time Series Forecasting is very much used in Stock Price Prediction. You can learn about its practical implementation from here. Trading Strategy with Machine Learning Trading Strategy with Machine Learning, which can be used to determine when you should buy stocks and when you should sell. Generally, it is not easy to predict the stock market, but building a Trading Strategy using Machine Learning in Finance can make it easy. This strategy can be used to generate the signals conveying when to invest in the Stock market or when to sell. It uses the strategy of three moving averages – one fast/short, second middle/medium, one slow/long. You can learn its practical implementation from here. GDP Analysis Gross domestic product (GDP) at current prices is the GDP at the market value of goods and services produced in a country during a year. In other words, GDP measures the monetary value of final goods and services produced by a country/state in a given period of time. GDP can be broadly divided into goods and services produced by three sectors: the primary sector (agriculture), the secondary sector (industry), and the tertiary sector (services). It is also known as nominal GDP. More technically, (real) GDP takes into account the price change that may have occurred due to inflation. This means that the real GDP is nominal GDP adjusted for inflation. You can learn to analyse GDP from here. Also, Read – LSTM in Machine Learning. I hope you liked this article on the application of Machine Learning in Finance. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Also, Read – What is Data Mining? Follow Us: Facebook Instagram;Machine Learning in Finance
2020-08-11 00:40:49;The combined knowledge of statistics, data mining, and machine learning plays a major role in understanding the data and describing the data features to find the relationships and patterns between the data so that we can build a model for further predictions. You must sometimes get confused about how to use these techniques to identify and solve a business problem.;https://thecleverprogrammer.com/2020/08/11/what-is-data-mining/;['pattern'];1.0;[];['ML', 'Classification'];['detect', 'predict', 'model', 'machine learning', 'classif', 'train'];The combined knowledge of statistics, data mining, and machine learning plays a major role in understanding the data and describing the data features to find the relationships and patterns between the data so that we can build a model for further predictions. You must sometimes get confused about how to use these techniques to identify and solve a business problem. The Importance of Data Mining in Machine Learning Most of the data mining techniques are structured to build machine learning models for predictive analysis. Data scientists combine all these techniques to study the data with great expertise. Using data mining and statistical tools helps a data scientist improve the capabilities to predict future outcomes with greater accuracy. We can’t predict the future of the business by using only a single technique. In simple words, it is one of the most important technique to study data in the process of training Machine Learning Models. What is Data Mining and How It Works? It is based on the principles of statistics, which means exploring and analyzing large quantities of data to discover patterns in that data. The algorithms are used to find relationships and patterns in the data. Then, the information from the machine learning model is used to make predictions and forecasts. It is used to solve a range of trade issues, such as fraud detection, market basket analysis, and customer churn rate analysis. Traditionally, organizations used a large volume of data mining and structured data tools, such as customer relationship management databases or inventories of aircraft parts. The primary purpose is to explain and to understand data. It is not intended to make predictions or to support assumptions. Typically, it aims to extract data from a larger dataset for classification or predictions. In data mining, data is framed together into groups. For example, a marketer might be interested in people who responded to a promotional offer versus those who did not respond to the promotion. In this example, mining would be used to extract the necessary information according to the two different classes and analyze each class’s characteristics. A marketer could be interested in predicting who will respond to a promotion. Also, Read – Scrape Wikipedia Using Python. Data mining tools are intended to support humans’ decision-making process. Therefore, it is intended to show models that can be used by humans. On the other hand, machine learning automates the identification process models used to make predictions. I hope you liked this article on the Importance of Data Mining in Machine Learning. Feel Free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Also, Read – Translate Languages Using Python. Follow Us: Facebook Instagram;What is Data Mining?
2020-08-12 10:03:08;The most likely way for attackers to gain access to your infrastructure is through the network. Network security is the general practice of protecting computer networks and devices accessible to the network against malicious intent, misuse and denial. In this article, I will take you through techniques of Network Security Analysis with Machine Learning.Exploring patterns is one of the main strengths of machine learning, and there are many inherent patterns to discover in the network traffic data. At first glance, network packet capture data may appear sporadic and random, but most communication flows follow strict network protocol.Live network data capture is the primary way to record network activity for online or offline analysis. Like a CCTV camera at a traffic intersection, packet analyzers intercept and record network traffic. Now let’s create a network attack classifier from scratch using machine learning.;https://thecleverprogrammer.com/2020/08/12/network-security-with-machine-learning/;['pattern', 'sklearn'];1.0;[];['ML', 'Classification'];['detect', 'predict', 'fit', 'model', 'loss', 'machine learning', 'classif', 'training data', 'train', 'label', 'test data'];The most likely way for attackers to gain access to your infrastructure is through the network. Network security is the general practice of protecting computer networks and devices accessible to the network against malicious intent, misuse and denial. In this article, I will take you through techniques of Network Security Analysis with Machine Learning. Exploring patterns is one of the main strengths of machine learning, and there are many inherent patterns to discover in the network traffic data. At first glance, network packet capture data may appear sporadic and random, but most communication flows follow strict network protocol. Live network data capture is the primary way to record network activity for online or offline analysis. Like a CCTV camera at a traffic intersection, packet analyzers intercept and record network traffic. Now let’s create a network attack classifier from scratch using machine learning. Building a Predictive Model to Classify Network Security Attacks The dataset we will be using is the NSLKDD dataset, which is an improvement over traditional network intrusion detection. This dataset widely used by security data science professionals to classify problems of Network Security. You can download the dataset from here. Let’s start this task by importing some necessary libraries: import os from collections import defaultdict import pandas as pd import numpy as np import matplotlib.pyplot as pltCode language: Python (python) Data Exploration: Let’s look at the preliminary data to get some insight into the data. Let’s take a look at the breakdown of categories first: dataset_root = 'datasets/nsl-kdd' train_file = os.path.join(dataset_root, 'KDDTrain+.txt') test_file = os.path.join(dataset_root, 'KDDTest+.txt') header_names = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type', 'success_pred'] col_names = np.array(header_names) nominal_idx = [1, 2, 3] binary_idx = [6, 11, 13, 14, 20, 21] numeric_idx = list(set(range(41)).difference(nominal_idx).difference(binary_idx)) nominal_cols = col_names[nominal_idx].tolist() binary_cols = col_names[binary_idx].tolist() numeric_cols = col_names[numeric_idx].tolist() category = defaultdict(list) category['benign'].append('normal') with open('datasets/training_attack_types.txt', 'r') as f: for line in f.readlines(): attack, cat = line.strip().split(' ') category[cat].append(attack) attack_mapping = dict((v,k) for k in category for v in category[k])Code language: Python (python) Now, here is the data that we will be using: train_df = pd.read_csv(train_file, names=header_names) train_df['attack_category'] = train_df['attack_type'] \ .map(lambda x: attack_mapping[x]) train_df.drop(['success_pred'], axis=1, inplace=True) test_df = pd.read_csv(test_file, names=header_names) test_df['attack_category'] = test_df['attack_type'] \ .map(lambda x: attack_mapping[x]) test_df.drop(['success_pred'], axis=1, inplace=True) train_attack_types = train_df['attack_type'].value_counts() train_attack_cats = train_df['attack_category'].value_counts() test_attack_types = test_df['attack_type'].value_counts() test_attack_cats = test_df['attack_category'].value_counts() train_attack_types.plot(kind='barh', figsize=(20,10), fontsize=20)Code language: Python (python) train_attack_cats.plot(kind='barh', figsize=(20,10), fontsize=30)Code language: Python (python) Data Preparation The NSL-KDD dataset is a useful dataset for education and experimentation with data mining and machine learning classification because it strikes a balance between simplicity and sophistication. Let’s start by splitting the test and training DataFrames into data and labels: train_Y = train_df['attack_category'] train_x_raw = train_df.drop(['attack_category','attack_type'], axis=1) test_Y = test_df['attack_category'] test_x_raw = test_df.drop(['attack_category','attack_type'], axis=1)Code language: Python (python) In typical cases, we will have complete knowledge of all categorical variables either because we defined them or because the dataset provided this information. In this case of Network Security Analysis, the dataset is not provided with a list of possible values ​​of each categorical variable, so we can preprocess as follows: combined_df_raw = pd.concat([train_x_raw, test_x_raw]) combined_df = pd.get_dummies(combined_df_raw, columns=nominal_cols, drop_first=True) train_x = combined_df[:len(train_x_raw)] test_x = combined_df[len(train_x_raw):] # Store dummy variable feature names dummy_variables = list(set(train_x)-set(combined_df_raw))Code language: Python (python) Now let’s apply the Standard Scalar Algorithm on this data to scale the dataset: # Experimenting with StandardScaler on the single 'duration' feature from sklearn.preprocessing import StandardScaler durations = train_x['duration'].values.reshape(-1, 1) standard_scaler = StandardScaler().fit(durations) scaled_durations = standard_scaler.transform(durations) pd.Series(scaled_durations.flatten()).describe()Code language: Python (python) count 1.259730e+05 mean 2.549477e-17 std 1.000004e+00 min -1.102492e-01 25% -1.102492e-01 50% -1.102492e-01 75% -1.102492e-01 max 1.636428e+01 dtype: float64 You can choose to use MinMaxScaler on StandardScaler if you want the scaling operation to keep the small standard deviations of the original series, or if you want to keep zero entries in the sparse data. Here’s how MinMaxScaler transforms the duration function: from sklearn.preprocessing import MinMaxScaler min_max_scaler = MinMaxScaler().fit(durations) min_max_scaled_durations = min_max_scaler.transform(durations) pd.Series(min_max_scaled_durations.flatten()).describe()Code language: Python (python) count 125973.000000 mean 0.006692 std 0.060700 min 0.000000 25% 0.000000 50% 0.000000 75% 0.000000 max 1.000000 dtype: float64 Outliers in your data can seriously and negatively skew the results of standard scaling and normalization. If the data contains outliers, sklearn.preprocessing.RobustScaler will be more suitable for this problem of Network Security. RobustScaler uses robust estimates such as median and quartile ranges, so it won’t be affected as much by outliers: from sklearn.preprocessing import RobustScaler min_max_scaler = RobustScaler().fit(durations) robust_scaled_durations = min_max_scaler.transform(durations) pd.Series(robust_scaled_durations.flatten()).describe()Code language: Python (python) count 125973.00000 mean 287.14465 std 2604.51531 min 0.00000 25% 0.00000 50% 0.00000 75% 0.00000 max 42908.00000 dtype: float64 We complete the data preprocessing phase by standardizing training and test data: standard_scaler = StandardScaler().fit(train_x[numeric_cols]) train_x[numeric_cols] = \ standard_scaler.transform(train_x[numeric_cols]) test_x[numeric_cols] = \ standard_scaler.transform(test_x[numeric_cols])Code language: Python (python) Classification for Network Security Analysis By applying the default or initial best guess parameters to the algorithm, we can quickly get initial classification results for Network Security. While these results may not be close to the accuracy of our goal, they will usually give us a rough indication of the potential of the algorithm. from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import confusion_matrix, zero_one_loss classifier = DecisionTreeClassifier(random_state=0) classifier.fit(train_x, train_Y) pred_y = classifier.predict(test_x) results = confusion_matrix(test_Y, pred_y) error = zero_one_loss(test_Y, pred_y)Code language: Python (python) [[9365 56 289 1 0] [1541 5998 97 0 0] [ 675 220 1528 0 0] [2278 1 14 277 4] [ 179 0 5 5 11]] 0.238245209368 Also, Read – Machine Learning Algorithms and it’s Types. With only a few lines of code and no adjustment at all, a classification accuracy of 76.2% (1 – error rate) in a five-class classification problem is not too bad. This way we can use the Network Security Analysis to improve the security of our networks. I hope you liked this article on Network Security Analysis with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Also, Read – Machine Learning in Finance. Follow Us: Facebook Instagram;Network Security with Machine Learning
2020-08-13 13:18:12;Sentiment analysis is the process by which all of the content can be quantified to represent the ideas, beliefs, and opinions of entire sectors of the audience. The implications of sentiment analysis are hard to underestimate to increase the productivity of the business. Sentiment Analysis is one of those common NLP tasks that every Data Scientist need to perform.For example, you are a student in an online course and you have a problem. You post it on the class forum. The sentiment analysis would be able to not only identify the topic you are struggling with, but also how frustrated or discouraged you are, and tailor their comments to that sentiment. This is already happening because the technology is already there.;https://thecleverprogrammer.com/2020/08/13/sentiment-analysis/;['keras', 'sklearn'];1.0;['NLP'];['ML', 'Sentiment Analysis', 'NLP', 'Random Forest', 'Classification'];['sentiment analysis', 'predict', 'fit', 'model', 'machine learning', 'random forest', 'classif', 'natural language processing', 'training data', 'train', 'label', 'test data'];"Sentiment analysis is the process by which all of the content can be quantified to represent the ideas, beliefs, and opinions of entire sectors of the audience. The implications of sentiment analysis are hard to underestimate to increase the productivity of the business. Sentiment Analysis is one of those common NLP tasks that every Data Scientist need to perform. For example, you are a student in an online course and you have a problem. You post it on the class forum. The sentiment analysis would be able to not only identify the topic you are struggling with, but also how frustrated or discouraged you are, and tailor their comments to that sentiment. This is already happening because the technology is already there. Sentiment Analysis with Machine Learning Hope you understood what sentiment analysis means. Now I’m going to introduce you to a very easy way to analyze sentiments with machine learning. The data I’ll be using includes 27,481 tagged tweets in the training set and 3,534 tweets in the test set. You can easily download the data from here. Now let’s start with this task by looking at the data using pandas: import pandas as pd training = pd.read_csv(""train.csv"") test = pd.read_csv(""test.csv"") print(""Training data: \n"",training.head()) print(""Test Data: \n"",test.head())Code language: Python (python) Training data: textID ... sentiment 0 cb774db0d1 ... neutral 1 549e992a42 ... negative 2 088c60f138 ... negative 3 9642c003ef ... negative 4 358bd9e861 ... negative [5 rows x 4 columns] Test Data: textID text sentiment 0 f87dea47db Last session of the day http://twitpic.com/67ezh neutral 1 96d74cb729 Shanghai is also really exciting (precisely -... positive 2 eee518ae67 Recession hit Veronique Branquinho, she has to... negative 3 01082688c6 happy bday! positive 4 33987a8ee5 http://twitpic.com/4w75p - I like it!! positive Data processing For the sake of simplicity, we don’t want to go overboard on the data cleaning side, but there are a few simple things we can do to help our machine learning model identify the sentiments. The data cleaning process is as follows: Remove all hyperlinks from tweetsReplace common contractionsRemove punctuation As a process of data preparation, we can create a function to map the labels of sentiments to integers and return them from the function: import re contractions_dict = {""can`t"": ""can not"", ""won`t"": ""will not"", ""don`t"": ""do not"", ""aren`t"": ""are not"", ""i`d"": ""i would"", ""couldn`t"": ""could not"", ""shouldn`t"": ""should not"", ""wouldn`t"": ""would not"", ""isn`t"": ""is not"", ""it`s"": ""it is"", ""didn`t"": ""did not"", ""weren`t"": ""were not"", ""mustn`t"": ""must not"", } def prepare_data(df: pd.DataFrame) -&gt; pd.DataFrame: df[""text""] = df[""text""] \ .apply(lambda x: re.split('http:\/\/.*', str(x))[0]) \ .str.lower() \ .apply(lambda x: replace_words(x, contractions_dict)) df[""label""] = df[""sentiment""].map( {""neutral"": 1, ""negative"": 0, ""positive"": 2} ) return df.text.values, df.label.values def replace_words(string: str, dictionary: dict): for k, v in dictionary.items(): string = string.replace(k, v) return string train_tweets, train_labels = prepare_data(training) test_tweets, test_labels = prepare_data(test)Code language: Python (python) Tokenization Now we need to tokenize each tweet into a single fixed-length vector – specifically a TFIDF integration. To do this we can use Tokenizer() built into Keras, suitable for training data: from keras.preprocessing.text import Tokenizer tokenizer = Tokenizer() tokenizer.fit_on_texts(train_tweets) train_tokenized = tokenizer.texts_to_matrix(train_tweets,mode='tfidf') test_tokenized = tokenizer.texts_to_matrix(test_tweets,mode='tfidf')Code language: Python (python) Machine Learning Model for Sentiment Analysis Now, I will train our model for sentiment analysis using the Random Forest Classification algorithm provided by Scikit-Learn: from sklearn.ensemble import RandomForestClassifier forest = RandomForestClassifier(n_estimators=500, min_samples_leaf=2,oob_score=True,n_jobs=-1,) forest.fit(train_tokenized,train_labels) print(f""Train score: {forest.score(train_tokenized,train_labels)}"") print(f""OOB score: {forest.oob_score_}"")Code language: Python (python) Train score: 0.7672573778246788 OOB score: 0.6842545758887959 Evaluating the Model on Test Set Scikit-Learn makes it easy to use both the classifier and the test data to produce a confusion matrix algorithm showing performance on the test set as follows: print(""Test score: "",forest.score(test_tokenized,test_labels))Code language: Python (python) Test score: 0.687889077532541 Also, Read – Data Science VS. Data Engineering. The accuracy rate is not that great because most of our mistakes happen when predicting the difference between positive and neutral and negative and neutral feelings, which in the grand scheme of errors is not the worst thing to have. Although fortunately, we rarely confuse positive with a negative feeling and vice versa. I hope you liked this article on Sentiment Analysis, feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Also, Read – Natural Language Processing Tutorial. Follow Us: Facebook Instagram";Sentiment Analysis
2020-08-16 11:34:00;Text to speech is the generation of speech synthesized from the text. The technology is used to communicate with users when reading a screen is not possible or impractical. This not only opens up apps and information to use in new ways but can also make the world more accessible to people who cannot read text on a screen. In this article, I will take you through building your TTS with Python.The technology behind the TTS has evolved over the past few decades. Using natural language processing, it is now possible to produce very natural speech that includes changes in pitch, speed, pronunciation and inflexion. ;https://thecleverprogrammer.com/2020/08/16/text-to-speech-with-python/;['nltk'];1.0;[];['ML', 'Recommender', 'NLP'];['recommend', 'natural language processing', 'machine learning'];"Text to speech is the generation of speech synthesized from the text. The technology is used to communicate with users when reading a screen is not possible or impractical. This not only opens up apps and information to use in new ways but can also make the world more accessible to people who cannot read text on a screen. In this article, I will take you through building your TTS with Python. The technology behind the TTS has evolved over the past few decades. Using natural language processing, it is now possible to produce very natural speech that includes changes in pitch, speed, pronunciation and inflexion. Text to Speech with Python I will simply start with importing all the necessary libraries that we need for this task to create a program that takes the text of an article online and converts it into speech: #Import the libraries from newspaper import Article import nltk from gtts import gTTS import osCode language: Python (python) Now, if you have face any error in importing the libraries, then you must have not installed any of these libraries. You can easily install any library in python, by writing a very simple command in your terminal – pip install package name. Now after installing and importing the libraries, we need to get an article from online sources so that we can create a program to convert text to speech from that article: #Get the article article = Article('https://hackernoon.com/future-of-python-language-bright-or-dull-uv41u3xwx')Code language: Python (python) Now, let’s download and parse the article: article.download() article.parse()​x article.download()article.parse() Now you need to download the “punkt” package if you have already downloaded it before you can skip downloading it if you will still download it again, it will give you a reminder that it is already available in your system which will not harm anything. So now, I will download the punkt package and apply Natural Language processing on it: nltk.download('punkt') article.nlp()Code language: Python (python) Now, I will define a variable to store the article: #Get the articles text mytext = article.textCode language: Python (python) Now we have to choose the language of speech. Note “en” means English. You can also use “pt-br” for Portuguese and there are others: language = 'en' #EnglishCode language: Python (python) Now we need to pass the text and language to the engine to convert the text to speech and store it in a variable. Mark slow as False to tell the plug-in that the converted audio should be at high speed: myobj = gTTS(text=mytext, lang=language, slow=False)Code language: Python (python) Running Text to Speech Program Now, we have converted the article for text-to-speech, so now the next step is to save this speech to mp3 file: myobj.save(""read_article.mp3"")Code language: Python (python) Now let’s play the converted audio file from text to speech in Windows, using the Windows command “start” followed by the name of the mp3 file: os.system(""start read_article.mp3"")Code language: Python (python) Output Also, Read – Fashion Recommendation System with Machine Learning. I hope you liked this article on converting text to speech using python. Feel free to ask your valuable questions in the comments section below. You can also follow me on my Medium Publication to learn every topic of Machine Learning. Also, Read – The Future of Machine Learning. Follow Us: Facebook Instagram";Text To Speech with Python
2020-08-19 16:54:44;The term hate speech is understood as any type of verbal, written or behavioural communication that attacks or uses derogatory or discriminatory language against a person or group based on what they are, in other words, based on their religion, ethnicity, nationality, race, colour, ancestry, sex or another identity factor. In this article, I will take you through a hate speech detection model with Machine Learning and Python.Hate Speech Detection is generally a task of sentiment classification. So for training, a model that can classify hate speech from a certain piece of text can be achieved by training it on a data that is generally used to classify sentiments. So for the task of hate speech detection model, I will use the Twitter data.Also, Read – Linear Regression with PyTorch.;https://thecleverprogrammer.com/2020/08/19/hate-speech-detection-model/;['sklearn'];1.0;[];['ML', 'Linear Regression', 'Classification', 'Regression'];['detect', 'regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'label'];"The term hate speech is understood as any type of verbal, written or behavioural communication that attacks or uses derogatory or discriminatory language against a person or group based on what they are, in other words, based on their religion, ethnicity, nationality, race, colour, ancestry, sex or another identity factor. In this article, I will take you through a hate speech detection model with Machine Learning and Python. Hate Speech Detection is generally a task of sentiment classification. So for training, a model that can classify hate speech from a certain piece of text can be achieved by training it on a data that is generally used to classify sentiments. So for the task of hate speech detection model, I will use the Twitter data. Also, Read – Linear Regression with PyTorch. Hate Speech Detection Model The data set I will use for the hate speech detection model consists of a test and train set. The training package includes a list of 31,962 tweets, a corresponding ID and a tag 0 or 1 for each tweet. The particular sentiment we need to detect in this dataset is whether or not the tweet is based on hate speech. You can download the dataset from here. So, let’s get started with the task of building a hate speech detection model. I will simply start with reading the datasets by using the pandas package in python: import pandas as pd train = pd.read_csv('train.csv') print(""Training Set:""% train.columns, train.shape, len(train)) test = pd.read_csv('test.csv') print(""Test Set:""% test.columns, test.shape, len(test)) view raw hate speech hosted with ❤ by GitHub View this gist on GitHub Training Set: (31962, 3) 31962 Test Set: (17197, 2) 17197 Data Cleaning Data cleaning is the process of preparing incorrectly formated data for analysis by deleting or modifying the incorrectly formatted data which is generally not necessary or useful for data analysis, as it can hinder the process or provide inaccurate results. Now I will perform the process of data cleaning by using the re library in Python: import re def clean_text(df, text_field): df[text_field] = df[text_field].str.lower() df[text_field] = df[text_field].apply(lambda elem: re.sub(r""(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?"", """", elem)) return df test_clean = clean_text(test, ""tweet"") train_clean = clean_text(train, ""tweet"") view raw hate speech hosted with ❤ by GitHub View this gist on GitHub Handling Imbalanced data for Hate Speech Detection Model If you will deeply analyse the task we are working on with context to the data we are using, you will find that the tweets regarding hate speeches are comparatively lesser than others, so this is a situation of an unbalanced data. If we will fit this data to train our hate speech detection model, then the model will not generalize any hate speech because the data with context to the hate speech is very less than the positive ones. So in this situation, we need to prepare the data to fit properly in our model. There are a number of methods you can use to deal with this. One approach is to use either oversampling or downsampling. In the case of oversampling, we use a function that repeatedly samples, with replacement, from the minority class until the class is the same size as the majority. Let’s see how we can handle this: from sklearn.utils import resample train_majority = train_clean[train_clean.label==0] train_minority = train_clean[train_clean.label==1] train_minority_upsampled = resample(train_minority, replace=True, n_samples=len(train_majority), random_state=123) train_upsampled = pd.concat([train_minority_upsampled, train_majority]) train_upsampled['label'].value_counts() view raw hate speech hosted with ❤ by GitHub View this gist on GitHub 1 29720 0 29720 Name: label, dtype: int64 Creating a Pipeline For simplicity and reproducibility of the hate speech detection model, I will use the Scikit-Learn’s pipeline with an SGDClassifier, before training our model: from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer from sklearn.linear_model import SGDClassifier pipeline_sgd = Pipeline([ ('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('nb', SGDClassifier()),]) view raw hate speech hosted with ❤ by GitHub View this gist on GitHub Training the Hate Speech Detection Model Now, before training the model, let’s split the data into a training set and a test set: from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(train_upsampled['tweet'], train_upsampled['label'],random_state = 0) view raw hate speech hosted with ❤ by GitHub View this gist on GitHub Now let’s train the model and predict the results on the test set using the F1 score method: model = pipeline_sgd.fit(X_train, y_train) y_predict = model.predict(X_test) from sklearn.metrics import f1_score f1_score(y_test, y_predict) view raw hate speech hosted with ❤ by GitHub View this gist on GitHub 0.9696 Also, Read – Telegram Bot with Python. So we got an F1 score of 0.96 per cent which is generally appreciatable. This model can now be deployed and used in production. I hope you liked this article on Hate Speech Detection Model with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Hate Speech Detection Model
2020-08-20 22:55:11;Contact tracing is a process used by public health ministries to help stop the spread of infectious disease, such as COVID-19, within a community. In this article, I will take you through the task of contact tracing with Machine Learning.;https://thecleverprogrammer.com/2020/08/20/contact-tracing-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Clustering'];['clustering', 'detect', 'fit', 'model', 'machine learning', 'filter', 'rank', 'label'];"Contact tracing is a process used by public health ministries to help stop the spread of infectious disease, such as COVID-19, within a community. In this article, I will take you through the task of contact tracing with Machine Learning. How Contact Tracing Works? Once a person is positive for coronavirus, it is very important to identify other people who may have been infected by the patients diagnosed. To identify infected people, the authorities follow the activity of patients diagnosed in the last 14 days. This process is called contact tracking. Depending on the country and the local authority, the search for contacts is carried out either by manual methods or by numerical methods. In this article, I will be proposing a digital contact tracing algorithm that relies on GPS data, which can be used in contact tracing with machine learning. Contact Tracing with Machine Learning DBSCAN is a density-based data clustering algorithm that groups data points in a given space. The DBSCAN algorithm groups data points close to each other and marks outlier data points as noise. I will use the DBSCAN algorithm for the task of contact tracing with Machine Learning. Also, Read – What is Competitive Programming? The dataset that I will use in this task is a JSON data which can be easily downloaded from here. Now let’s import all the libraries that we need for this task and get started with reading the dataset and exploring some insights from the data: import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import datetime as dt from sklearn.cluster import DBSCAN df = pd.read_json(‘livedata.json’) df.head() view raw contact tracing hosted with ❤ by GitHub View this gist on GitHub id timestamp latitude longitude 0 David 2020-07-04 15:35:30 13.148953 77.593651 1 David 2020-07-04 16:35:30 13.222397 77.652828 2 Frank 2020-07-04 14:35:30 13.236507 77.693792 3 Carol 2020-07-04 21:35:30 13.163716 77.562842 4 Ivan 2020-07-04 22:35:30 13.232095 77.580273Code language: CSS (css) Now, let’s analyze the dataset using the scatter plot showing the ids with their latitudes and longitudes on the x-axis and the Y-axis respectively: plt.figure(figsize=(8,6)) sns.scatterplot(x=‘latitude’, y=‘longitude’, data=df, hue=‘id’) plt.legend(bbox_to_anchor= [1, 0.8]) plt.show() Creating a Model for Contact Tracing with Machine Learning Now let’s create a model for contact tracing using the DBSCAN model. The function below will help us to create the DBSCAN model, using this model we will generate clusters, which will help identify infections by filtering the data in the clusters: def get_infected_names(input_name): epsilon = 0.0018288 # a radial distance of 6 feet in kilometers model = DBSCAN(eps=epsilon, min_samples=2, metric='haversine').fit(df[['latitude', 'longitude']]) df['cluster'] = model.labels_.tolist() input_name_clusters = [] for i in range(len(df)): if df['id'][i] == input_name: if df['cluster'][i] in input_name_clusters: pass else: input_name_clusters.append(df['cluster'][i]) infected_names = [] for cluster in input_name_clusters: if cluster != -1: ids_in_cluster = df.loc[df['cluster'] == cluster, 'id'] for i in range(len(ids_in_cluster)): member_id = ids_in_cluster.iloc[i] if (member_id not in infected_names) and (member_id != input_name): infected_names.append(member_id) else: pass return infected_names view raw contact tracing hosted with ❤ by GitHub View this gist on GitHub Now, let’s generate clusters using our model: abels = model.labels_ fig = plt.figure(figsize=(12,10)) sns.scatterplot(df['latitude'], df['longitude'], hue = ['cluster-{}'.format(x) for x in labels]) plt.legend(bbox_to_anchor = [1, 1]) plt.show()Code language: JavaScript (javascript) Tracing Infected People To find people who may be infected by the patient, we’ll just call the get_infected_names function and enter a name from the dataset as a parameter: print(get_infected_names(""Erin"")Code language: PHP (php) [‘Ivan’] Also, Read – Hate Speech Detection Model. From the above results, we can say that a clustering algorithm like DBSCAN can perform data point clustering without prior knowledge of the datasets. I hope you liked this article on Contact Tracing with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Contact Tracing with Machine Learning
2020-08-22 21:17:10;In Machine Learning, a decision tree is a decision support tool that uses a graphical or tree model of decisions and their possible consequences, including the results of random events, resource costs, and utility. This is a way of displaying an algorithm that contains only conditional control statements. In this article, I will take you through how we can visualize a decision tree using Python.Visualizing a Decision tree is very much different from the visualization of data where we have used a decision tree algorithm. So, If you are not very much familiar with the decision tree algorithm then I will recommend you to first go through the decision tree algorithm from here.Also, Read – Visualize Real-Time Stock Prices with Python.;https://thecleverprogrammer.com/2020/08/22/visualize-a-decision-tree-in-machine-learning/;['sklearn'];1.0;['ML'];['Recommender', 'ML', 'Decision Tree', 'Chatbot', 'Classification'];['predict', 'fit', 'model', 'machine learning', 'classif', 'chatbot', 'train', 'recommend', 'decision tree'];In Machine Learning, a decision tree is a decision support tool that uses a graphical or tree model of decisions and their possible consequences, including the results of random events, resource costs, and utility. This is a way of displaying an algorithm that contains only conditional control statements. In this article, I will take you through how we can visualize a decision tree using Python. Visualizing a Decision tree is very much different from the visualization of data where we have used a decision tree algorithm. So, If you are not very much familiar with the decision tree algorithm then I will recommend you to first go through the decision tree algorithm from here. Also, Read – Visualize Real-Time Stock Prices with Python. How to Visualize a Decision Tree? If you are a practitioner in machine learning or you have applied the decision tree algorithm before in a lot of classification tasks then you must be confused about why I am stressing to visualize a decision tree. Just look at the picture down below. In the right side, we have a visualization of the output we get when we use a decision tree algorithm on data to predict the possibilities. In the left side, we have the structure that a decision tree algorithm follows to make predictions by making trees. So, I hope now you know what’s the difference between visualizing the decision tree algorithm on the data, and to visualize the structure of a decision tree algorithm. Now let’s see how we can visualize a decision tree. Visualize a Decision Tree To explain you the process of how we can visualize a decision tree, I will use the iris dataset which is a set of 3 different types of iris species (Setosa, Versicolour, and Virginica) petal and sepal length, which is stored in a NumPy array dimension of 150×4. Now, let’s import the necessary libraries to get started with the task of visualizing a decision tree: import pandas as pd import numpy as np from sklearn.datasets import load_iris, load_boston from sklearn import treeCode language: JavaScript (javascript) Now, let’s load the iris dataset and have a quick look at the first 5 rows of the data by using the pandas.head() method: iris = load_iris() df_iris = pd.DataFrame(iris['data'], columns=iris['feature_names']) df_iris['target'] = iris['target'] df_iris.head()Code language: JavaScript (javascript) Train a Decision Tree For visualizing a decision tree, the first step is to train it on the data, because the visualization of a decision tree is nothing but the structure that it will use to make predictions. So, to visualize the structure of the predictions made by a decision tree, we first need to train it on the data: clf = tree.DecisionTreeClassifier() clf = clf.fit(iris.data, iris.target) Now, we can visualize the structure of the decision tree. For this, we need to use a package known as graphviz, which can be easily installed by using the pip command – pip install graphviz. Now, if you have installed this package successfully, let’s move forward for the task of visualizing the decision tree: !pip install graphviz import graphviz dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = graphviz.Source(dot_data) graph Code language: PHP (php) In the output, we can see the structure of the decision tree that is used in making predictions on the data. But these are numerical values which means a lot in machine learning, but to make this task interesting let’s visualize the graphical representation of each step involved in the structure of the decision tree. Graphical Visualization of Each Step For this task, we need to install another package known as dtreeviz, which can be easily installed by using the pip command – pip install dtreeviz. Now, if you have installed this package successfully let’s see how we can visualize the graphical representation of each step involved in making predictions: !pip install dtreeviz from dtreeviz.trees import dtreeviz viz = dtreeviz(clf, iris['data'], iris['target'], target_name='', feature_names=np.array(iris['feature_names']), class_names={0:'setosa',1:'versicolor',2:'virginica'}) vizCode language: JavaScript (javascript) Also, Read – Build and Deploy a Chatbot with HTML, CSS and Python. In the output above, we can see the distribution for each class at each node, you can also see where is the decision boundary for each split, and can see the sample size at each leaf as the size of the circle. I hope you liked this article on how we can visualize the structure of a decision tree. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram;Visualize a Decision Tree in Machine Learning
2020-08-23 08:35:17;In Machine Learning, the Stemming process is widely used in tagging, indexing, SEO, web search results and information search. For example, the search for fish on Google will also result in fish, fishing as fish is the root of the two words. In this article, I will take you through Stemming in Machine Learning with Python.;https://thecleverprogrammer.com/2020/08/23/stemming-in-machine-learning/;['nltk'];1.0;['ML', 'AI'];['ML', 'NLP', 'Decision Tree', 'AI'];['natural language processing', 'decision tree', 'machine learning'];"In Machine Learning, the Stemming process is widely used in tagging, indexing, SEO, web search results and information search. For example, the search for fish on Google will also result in fish, fishing as fish is the root of the two words. In this article, I will take you through Stemming in Machine Learning with Python. What is Stemming? Stemming is the task of reducing the inflexion of words to their root form, such as mapping a group of words to the same root even though the root itself is not a valid word in the language. Also, Read – Visualize a Decision Tree in Machine Learning. Here I will introduce you to the process of stemming words and sentences with Machine Learning using natural language processing. Let’s import all the necessary libraries we need to stem words and sentences to get started with the task: import nltk from nltk.stem import PorterStemmer from nltk.stem import LancasterStemmer from nltk.tokenize import sent_tokenize, word_tokenize nltk.download('punkt') porter = PorterStemmer() lancaster=LancasterStemmer()Code language: JavaScript (javascript) Now, I will create 2 lists of words and I will define a variable with a sentence that I will use for the process of stemming words and sentences: l_words1 = ['cats', 'trouble', 'troubling', 'troubled'] l_words2 = ['dogs', 'programming', 'programs', 'programmed', 'cakes', 'indices', 'matrices'] print(l_words1) print(l_words2) sentence = 'Hi, I am Aman Kharwal. I am a programmer from India, and I am here to guide you with Machine Learning for free. I hope you will learn a lot in your journey towards ML and AI with me.' sentenceCode language: PHP (php) ['cats', 'trouble', 'troubling', 'troubled'] ['dogs', 'programming', 'programs', 'programmed', 'cakes', 'indices', 'matrices'] 'Hi, I am Aman Kharwal. I am a programmer from India, and I am here to guide you with Machine Learning for free. I hope you will learn a lot in your journey towards ML and AI with me.' In the output above you can see we have two lists of words and one variable “sentence” that I will use in the further process to explain you Stemming. It has some popular methods, Now I will take you some popular methods of stemming by using our variable and lists that I have defined above. Stemming Words with Python In Machine learning, we have two popular methods to stem words, Porter Method and Lancaster Method. Now let’s go through both these methods. Porter Method The Porter Method only keeps the prefix for each word and leaves non-English words such as troubl. It might not be useful to see non-English words for further analysis, but it is simple and effective. Now let’s go through this method for stemming words by using our defined lists: for word in l_words1: print(f'{word} \t -&gt; {porter.stem(word)}'.expandtabs(15))Code language: PHP (php) cats -> cat trouble -> troubl troubling -> troubl troubled -> troubl for word in l_words2: print(f'{word} \t -&gt; {porter.stem(word)}'.expandtabs(15))Code language: PHP (php) dogs -> dog programming -> program programs -> program programmed -> program cakes -> cake indices -> indic matrices -> matric Lancaster Method Lancaster Method is a rule-based derivation method which is based on the last letter of words. It is heavier in calculus than the Porter method. Now, let’s go through this method for stemming words by using our defined lists: for word in l_words1: print(f'{word} \t -&gt; {lancaster.stem(word)}'.expandtabs(15))Code language: PHP (php) cats -> cat trouble -> troubl troubling -> troubl troubled -> troubl for word in l_words2: print(f'{word} \t -&gt; {lancaster.stem(word)}'.expandtabs(15))Code language: PHP (php) dogs -> dog programming -> program programs -> program programmed -> program cakes -> cak indices -> ind matrices -> mat I hope you have now understood how we can stem words in Machine Learning by using natural language processing. Now, I will proceed with the task to stem sentences in Machine Learning. Stemming Sentences with Python To stem sentences we have the same methods but the application is a little bit different. First, we need to tokenize our sentence so that we can easily use our sentence for the process of stemming. Now let’s see how we can do this. Tokenization Tokenization is the process of dividing a text or a word into a list of tokens. We can think of the token as coins because a word is a token in a sentence, a sentence is a token in a paragraph. Now, let’s tokenize the sentence that we defined above and get started with the task to stem a sentence using python: tokenized_words=word_tokenize(sentence) print(tokenized_words)Code language: PHP (php) ['Hi', ',', 'I', 'am', 'Aman', 'Kharwal', '.', 'I', 'am', 'a', 'programmer', 'from', 'India', ',', 'and', 'I', 'am', 'here', 'to', 'guide', 'you', 'with', 'Machine', 'Learning', 'for', 'free', '.', 'I', 'hope', 'you', 'will', 'learn', 'a', 'lot', 'in', 'your', 'journey', 'towards', 'ML', 'and', 'AI', 'with', 'me', '.'] Now, you can see we have tokenized our sentence properly, now the next step is stemming the sentence. I will simply use both the methods that I introduced to you above. I will first use the Porter method: tokenized_sentence = [] for word in tokenized_words: tokenized_sentence.append(porter.stem(word)) tokenized_sentence = "" "".join(tokenized_sentence) tokenized_sentenceCode language: JavaScript (javascript) Hi , I am aman kharwal . I am a programm from india , and I am here to guid you with machin learn for free . I hope you will learn a lot in your journey toward ML and AI with me . The porter method gave us a good result, now let’s do this on the Lancaster method: tokenized_sentence = [] for word in tokenized_words: tokenized_sentence.append(lancaster.stem(word)) tokenized_sentence = "" "".join(tokenized_sentence) tokenized_sentenceCode language: JavaScript (javascript) hi , i am am kharw . i am a program from ind , and i am her to guid you with machin learn for fre . i hop you wil learn a lot in yo journey toward ml and ai with me . Also, Read – Visualize Real-Time Stock Prices with Python. So we can clearly see that the output of both the methods is different from one another and both are accurate. I hope you liked this article on Stemming words and sentences in Machine Learning using Python. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning and Python. Follow Us: Facebook Instagram";Stemming in Machine Learning
2020-08-24 08:08:43;Text Summarization involves condensing a piece of text into a shorter version, reducing the size of the original text while preserving key information and the meaning of the content. Since manual text synthesis is a long and generally laborious task, task automation is gaining in popularity and therefore a strong motivation for academic research. In this article, I will take you through the task of Natural Language Processing to summarize text with Machine Learning.In Machine Learning, there are important applications for text summarization in various Natural Language Processing related tasks such as text classification, answering questions, legal text synthesis, news synthesis, and headline generation which can be achieved with Machine Learning. The intention to summarize a text is to create an accurate and fluid summary containing only the main points described in the document.Also, Read – Scraping YouTube with Python.;https://thecleverprogrammer.com/2020/08/24/summarize-text-with-machine-learning/;['sklearn', 'nltk'];1.0;[];['ML', 'Supervised Learning', 'NLP', 'Text Classification', 'Classification', 'Unsupervised learning'];['text classification', 'supervised learning', 'machine learning', 'classif', 'layer', 'natural language processing', 'rank', 'unsupervised learning'];"Text Summarization involves condensing a piece of text into a shorter version, reducing the size of the original text while preserving key information and the meaning of the content. Since manual text synthesis is a long and generally laborious task, task automation is gaining in popularity and therefore a strong motivation for academic research. In this article, I will take you through the task of Natural Language Processing to summarize text with Machine Learning. In Machine Learning, there are important applications for text summarization in various Natural Language Processing related tasks such as text classification, answering questions, legal text synthesis, news synthesis, and headline generation which can be achieved with Machine Learning. The intention to summarize a text is to create an accurate and fluid summary containing only the main points described in the document. Also, Read – Scraping YouTube with Python. Types of Approaches to Summarize Text Before I dive into showing you how we can summarize text using machine learning and python, it is important to understand what are the types of text summarization to understand how the process works, so that we can use logic while using machine learning techniques to summarize the text. Generally, Text Summarization is classified into two main types: Extraction Approach and Abstraction Approach. Now let’s go through both these approaches before we dive into the coding part. The Extractive Approach The Extractive approach takes sentences directly from the document according to a scoring function to form a cohesive summary. This method works by identifying the important sections of the text cropping and assembling parts of the content to produce a condensed version. The Abstractive Approach The Abstraction approach aims to produce a summary by interpreting the text using advanced natural language techniques to generate a new, shorter text – parts of which may not appear in the original document, which conveys the most information. In this article, I will be using the extractive approach to summarize text using Machine Learning and Python. I will use the TextRank algorithm which is an extractive and unsupervised machine learning algorithm for text summarization. Summarize Text with Machine Learning So now, I hope you know what text summarization is and how it works. Now, without wasting any time let’s see how we can summarize text using machine learning. The dataset that I will use in this task can be downloaded from here. Now, let’s import the necessary packages that we need to get started with the task: import pandas as pd import numpy as np import nltk nltk.download('punkt') import re from nltk.corpus import stopwordsCode language: JavaScript (javascript) Now, as I have imported the necessary packages, the next step is to look at the data to get some idea of what we are going to work with: from google.colab import files uploaded = files.upload() df = pd.read_csv(""tennis.csv"") df.head()Code language: JavaScript (javascript) df['article_text'][1]Code language: CSS (css) ""BASEL, Switzerland (AP), Roger Federer advanced to the 14th Swiss Indoors final of his career by beating seventh-seeded Daniil Medvedev 6-1, 6-4 on Saturday. Seeking a ninth title at his hometown event, and a 99th overall, Federer will play 93th-ranked Marius Copil on Sunday. Federer dominated the 20th-ranked Medvedev and had his first match-point chance to break serve again at 5-1. He then dropped his serve to love, and let another match point slip in Medvedev's next service game by netting a backhand. He clinched on his fourth chance when Medvedev netted from the baseline. Copil upset expectations of a Federer final against Alexander Zverev in a 6-3, 6-7 (6), 6-4 win over the fifth-ranked German in the earlier semifinal. The Romanian aims for a first title after arriving at Basel without a career win over a top-10 opponent. Copil has two after also beating No. 6 Marin Cilic in the second round. Copil fired 26 aces past Zverev and never dropped serve, clinching after 2 1/2 hours with a forehand volley winner to break Zverev for the second time in the semifinal. He came through two rounds of qualifying last weekend to reach the Basel main draw, including beating Zverev's older brother, Mischa. Federer had an easier time than in his only previous match against Medvedev, a three-setter at Shanghai two weeks ago."" Now, I will split the sequences into the data by tokenizing them using a list: from nltk.tokenize import sent_tokenize sentences = [] for s in df['article_text']: sentences.append(sent_tokenize(s)) sentences = [y for x in sentences for y in x]Code language: JavaScript (javascript) Now I am going to use the Glove method for word representation, it is an unsupervised learning algorithm developed by Stanford University to generate word integrations by aggregating the global word-to-word co-occurrence matrix from a corpus. To implement this method you have to download a file from here and store it into the same directory where your python file is: word_embeddings = {} f = open('glove.6B.100d.txt', encoding='utf-8') for line in f: values = line.split() word = values[0] coefs = np.asarray(values[1:], dtype='float32') word_embeddings[word] = coefs f.close() clean_sentences = pd.Series(sentences).str.replace(""[^a-zA-Z]"", "" "") clean_sentences = [s.lower() for s in clean_sentences] stop_words = stopwords.words('english') def remove_stopwords(sen): sen_new = "" "".join([i for i in sen if i not in stop_words]) return sen_new clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]Code language: JavaScript (javascript) Now, I will create vectors for the sentences: sentence_vectors = [] for i in clean_sentences: if len(i) != 0: v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001) else: v = np.zeros((100,)) sentence_vectors.append(v)Code language: PHP (php) Finding Similarities to Summarize Text The next step is to find similarities between the sentences, and I will use the cosine similarity approach for this task. Let’s create an empty similarity matrix for this task and fill it with cosine similarities of sentences: sim_mat = np.zeros([len(sentences), len(sentences)]) from sklearn.metrics.pairwise import cosine_similarity for i in range(len(sentences)): for j in range(len(sentences)): if i != j: sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]Code language: JavaScript (javascript) Now I am going to convert the sim_mat similarity matrix into the graph, the nodes in this graph will represent the sentences and the edges will represent the similarity scores between the sentences: import networkx as nx nx_graph = nx.from_numpy_array(sim_mat) scores = nx.pagerank(nx_graph)Code language: JavaScript (javascript) Now, let’s summarize text: ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True) for i in range(5): print(""ARTICLE:"") print(df['article_text'][i]) print('\n') print(""SUMMARY:"") print(ranked_sentences[i][1]) print('\n')Code language: PHP (php) Output: ARTICLE: Maria Sharapova has basically no friends as tennis players on the WTA Tour. The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much. I think everyone knows this is my job here. When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. I'm a pretty competitive girl. I say my hellos, but I'm not sending any players flowers as well. Uhm, I'm not really friendly or close to many players. I have not a lot of friends away from the courts.' When she said she is not really close to a lot of players, is that something strategic that she is doing? Is it different on the men's tour than the women's tour? 'No, not at all. I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized, you're a tennis player, so you're going to get along with tennis players. I think every person has different interests. I have friends that have completely different jobs and interests, and I've met them in very different parts of my life. I think everyone just thinks because we're tennis players we should be the greatest of friends. But ultimately tennis is just a very small part of what we do. There are so many other things that we're interested in, that we do.' SUMMARY: When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. So, I will congratulate you as you have made a text summarization system using machine learning which will reduce your effort to find important and the most relevant information from any topic. Also, Read – What is Cloud Computing in Machine Learning. I hope you liked this article on summarize text using machine learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Summarize Text with Machine Learning
2020-08-25 10:58:04;In this article, I’m going to take you through an in-depth review of BERT in Machine Learning for word embeddings produced by Google for Machine Learning. Here I’ll show you how to get started with BERT in Machine Learning by producing your word embeddings.;https://thecleverprogrammer.com/2020/08/25/bert-in-machine-learning/;['pytorch', 'vocabulary'];1.0;['NLP', 'NN', 'CNN'];['LSTM', 'NN', 'ML', 'NLP', 'CNN', 'Classification'];['recogn', 'predict', 'model', 'machine learning', 'classif', 'natural language processing', 'train', 'lstm'];"In this article, I’m going to take you through an in-depth review of BERT in Machine Learning for word embeddings produced by Google for Machine Learning. Here I’ll show you how to get started with BERT in Machine Learning by producing your word embeddings. What is BERT in Machine Learning? BERT stands for Bidirectional Encoder Representations from Transformers, BERT in Machine Learning are models for pre-trained language representations that can be used to create models for the tasks of Natural Language Processing. Also, Read – 5 Python Projects for Beginners You can either use these models to extract high-quality language functionality from your text data, or you can refine these models on specific tasks such as classification, feature recognition, answering questions, etc. with your data to produce a state of artistic predictions. Why BERT Embeddings for NLP? First, the BERT embeddings are very useful for keyword expansion, semantic search, and other information retrievals. For example, if you want to match customer questions or research to previously answered questions or well-researched research, these representations will help you accurately retrieve results that match customer intent and contextual meaning, even in the absence of overlapping keywords or phrases. Secondly, and perhaps the most important reason is that these vectors can be used as high-quality features inputs in the downstream models. NLP models such as LSTMs or CNNs require inputs in the form of digital vectors, which typically means translating features such as vocabulary and parts of speech into digital representations. Implementing BERT Algorithm For the implementation of the BERT algorithm in machine learning, you must install the PyTorch package. I selected PyTorch because it strikes a good balance between high-level APIs and TensorFlow code. Now, let’s implement the necessary packages to get started with the task: !pip install torch import torch !pip install pytorch_pretrained_bert from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM import matplotlib.pyplot as plt %matplotlib inline tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')Code language: JavaScript (javascript) Input Formatting Since BERT is a pre-trained model that expects input data in a specific format, we will need: A special token, [SEP], to mark the end of a sentence or the separation between two sentencesA special token, [CLS], at the start of our text. This token is used for classification tasks, but BERT expects it regardless of your application. text = ""This is the sample sentence for BERT word embeddings"" marked_text = ""[CLS] "" + text + "" [SEP]"" print (marked_text)Code language: PHP (php) Output: [CLS] This is the sample sentence for BERT word embeddings [SEP] Tokenization The BERT model provides its tokenizer, which we imported above. Let’s see how it handles the sample text below: tokenized_text = tokenizer.tokenize(marked_text) print (tokenized_text)Code language: PHP (php) Output: ['[CLS]', 'this', 'is', 'the', 'sample', 'sentence', 'for', 'bert', 'word', 'em', '##bed', '##ding', '##s', '[SEP]'] The original text has been split into smaller subwords and characters. The two hash signs that precede some of these subwords are just how our tokenizer indicates that this subword or character is part of a larger word and is preceded by another subword. Converting Tokens to ID To tokenize a word under this template, the tokenizer first checks whether the entire word is in the vocabulary. Otherwise, it tries to break the word down into the largest possible sub-words contained in the vocabulary, and as a last resort will break the word down into individual characters. Note that because of this, we can still represent a word as, at the very least, all of its characters: indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) for tup in zip(tokenized_text, indexed_tokens): print(tup)Code language: PHP (php) Output: ('[CLS]', 101) ('this', 2023) ('is', 2003) ('the', 1996) ('sample', 7099) ('sentence', 6251) ('for', 2005) ('bert', 14324) ('word', 2773) ('em', 7861) ('##bed', 8270) ('##ding', 4667) ('##s', 2015) ('[SEP]', 102) Also, Read – Best IDEs for Machine Learning. In this way, you can prepare word embeddings using the BERT model for any task of NLP. I hope you liked this article. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";BERT in Machine Learning
2020-08-25 15:57:33;In this article, I will take you through a very famous case study for machine learning practitioners which is to predict titanic survival with Machine Learning. I will first introduce you to this case study and then I will show you how we can build a predictive model to predict survival with Machine Learning.;https://thecleverprogrammer.com/2020/08/25/titanic-survival-with-machine-learning/;['sklearn'];1.0;['CV'];['ML', 'Logistic Regression', 'CV', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'layer', 'training data', 'train', 'logistic regression', 'test data'];In this article, I will take you through a very famous case study for machine learning practitioners which is to predict titanic survival with Machine Learning. I will first introduce you to this case study and then I will show you how we can build a predictive model to predict survival with Machine Learning. Machine Learning Case Study: Titanic Survival Analysis The sinking of the Titanic is one of the most infamous wrecks in history. On April 15, 1912, during her maiden voyage, the RMS Titanic, widely considered “unsinkable”, sank after hitting an iceberg. Also, Read – Google’s BERT Algorithm in Machine Learning. Unfortunately, there were not enough lifeboats for everyone on board, resulting in the deaths of 1,502 out of 2,224 passengers and crew. While there was an element of luck in survival, it appears that certain groups of people were more likely to survive than others. Here, your challenge is to build a predictive model that can give a solution to the question, “What types of people were more likely to survive?” using passenger data (i.e. name, age, sex, socio-economic class, etc.). Predict Titanic Survival with Machine Learning Now, as a solution to the above case study for predicting titanic survival with machine learning, I’m using a now-classic dataset, which relates to passenger survival rates on the Titanic, which sank in 1912. I’ll start this task by loading the test and training dataset using pandas: import pandas as pd train = pd.read_csv('train.csv') test = pd.read_csv('test.csv') train[:4]Code language: JavaScript (javascript) Scikit-learn’s algorithms generally cannot be powered by missing data, so I’ll be looking at the columns to see if there are any that contain missing data: train.isnull().sum()Code language: CSS (css) PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 test.isnull().sum()Code language: CSS (css) PassengerId 0 Pclass 0 Name 0 Sex 0 Age 86 SibSp 0 Parch 0 Ticket 0 Fare 1 Cabin 327 Embarked 0 dtype: int64 Data Preparation In statistics and machine learning examples like this, a typical task is to predict whether a passenger will survive based on the characteristics of the data. A model is fitted to a training data set and then evaluated on an out-of-sample test data set. I would like to use Age as a predictor, but data is missing. There are several ways to do missing data imputation, but I’ll make a simple one and use the median of the training dataset to fill in the null values ​​in both tables: impute_value = train['Age'].median() train['Age'] = train['Age'].fillna(impute_value) test['Age'] = test['Age'].fillna(impute_value)Code language: JavaScript (javascript) We now need to specify our models. I’ll add an IsFemale column as the encoded version of the ‘Sex’ column: train['IsFemale'] = (train['Sex'] == 'female').astype(int) test['IsFemale'] = (test['Sex'] == 'female').astype(int)Code language: JavaScript (javascript) Next, we decide on some model variables and create NumPy arrays: predictors = ['Pclass', 'IsFemale', 'Age'] X_train = train[predictors].values X_test = test[predictors].values y_train = train['Survived'].values X_train[:5]Code language: JavaScript (javascript) array([[ 3., 0., 22.], [ 1., 1., 38.], [ 3., 1., 26.], [ 1., 1., 35.], [ 3., 0., 35.]]) Machine Learning Model to Predict Titanic Survival Now I’m going to use the LogisticRegression model from scikit-learn and create a model instance: from sklearn.linear_model import LogisticRegression model = LogisticRegression()Code language: JavaScript (javascript) Now we can fit this model to the training data using the scikit-learn’s fit method: model.fit(X_train, y_train)Code language: CSS (css) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) Now, we can make predictions on the test dataset using model.predict: y_predict = model.predict(X_test) y_predict[:10] array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0]) In practice, there are often many additional layers of complexity in training the models. Many models have parameters that can be adjusted, and there are techniques such as cross-validation that can be used for parameter tuning to prevent overfitting of training data. This can often improve predictive performance or the robustness of new data. Implementing Cross-Validation Cross-validation works by splitting training data to simulate out-of-sample prediction. Based on a model accuracy score such as the root mean square error, one can perform a grid search on the model parameters. Some models, like logistic regression, have classes of estimators with built-in cross-validation. For example, the LogisticRegressionCV class can be used with a parameter indicating the degree of precision of a grid search to be performed on the model regularization parameter C: from sklearn.linear_model import LogisticRegressionCV model_cv = LogisticRegressionCV(10) model_cv.fit(X_train, y_train)Code language: JavaScript (javascript) LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False, fit_intercept=True, intercept_scaling=1.0, l1_ratios=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0) To perform cross-validation by hand, you can use the cross_val_score helper function, which handles the process of splitting data. For example, to validate our model with four non-overlapping divisions of training data, we can do: from sklearn.model_selection import cross_val_score model = LogisticRegression(C=10) scores = cross_val_score(model, X_train, y_train, cv=4) scoresCode language: JavaScript (javascript) array([0.77578475, 0.79820628, 0.77578475, 0.78828829]) The default rating metric depends on the model, but it is possible to choose an explicit rating function. Cross-validated models take longer to train, but can often improve model performance. Also, Read – Five Python Projects for Beginners I hope you like this article on my work on the case study to predict titanic survival with machine learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram;Titanic Survival with Machine Learning
2020-08-26 16:17:31;Humans take no effort to distinguish a dog, cat, or flying saucer. But this process is quite difficult for a computer to emulate: it only looks easy because God designs our brains incredibly well to recognize images. One common example of image recognition with machine learning is optical character recognition. In this article, I will take you through building an Image Recognition model with Machine Learning using PyTorch.;https://thecleverprogrammer.com/2020/08/26/image-recognition-with-machine-learning-using-pytorch/;['pattern', 'Pillow'];1.0;[];['ResNet', 'CV', 'NN', 'DL', 'ML', 'CNN', 'VGG', 'AlexNet'];['detect', 'recogn', 'computer vision', 'predict', 'model', 'machine learning', 'neural network', 'layer', 'alexnet', 'deep learning', 'resnet', 'convolutional neural network', 'train', 'label', 'neuron', 'vgg'];"Humans take no effort to distinguish a dog, cat, or flying saucer. But this process is quite difficult for a computer to emulate: it only looks easy because God designs our brains incredibly well to recognize images. One common example of image recognition with machine learning is optical character recognition. In this article, I will take you through building an Image Recognition model with Machine Learning using PyTorch. What is PyTorch? Before diving into this task let’s first understand what is PyTorch. PyTorch is a library for Python programs that make it easy to create deep learning models. Like Python does for programming, PyTorch provides a great introduction to deep learning. At the same time, PyTorch has proven to be fully qualified for use in professional contexts for high-level real-world work. Also, Read – Data Science Skills: Every Data Scientist Should Know. Image Recognition with Machine Learning For the image recognition task, in this article, I will be using the TorchVision package which contains some of the best performing neural network architectures for computer vision, such as AlexNet. It also provides easy access to datasets like ImageNet and other utilities to learn about computer vision applications in PyTorch. The predefined models can be found in torchvision.models: from torchvision import models dir(models)Code language: JavaScript (javascript) ['AlexNet', 'DenseNet', 'GoogLeNet', 'GoogLeNetOutputs', 'Inception3', 'InceptionOutputs', 'MNASNet', 'MobileNetV2', 'ResNet', 'ShuffleNetV2', 'SqueezeNet', 'VGG', '_GoogLeNetOutputs', '_InceptionOutputs', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_utils', 'alexnet', 'densenet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'detection', 'googlenet', 'inception', 'inception_v3', 'mnasnet', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet', 'mobilenet_v2', 'quantization', 'resnet', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'segmentation', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'shufflenetv2', 'squeezenet', 'squeezenet1_0', 'squeezenet1_1', 'utils', 'vgg', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'video', 'wide_resnet101_2', 'wide_resnet50_2'] The uppercase names refer to Python classes that implement several popular models. Lowercase names are handy functions that return patterns instantiated from these classes, sometimes with different sets of parameters. AlexNet To run the AlexNet architecture on an input image, we can create an instance of the AlexNet class. Here’s how to do it: alexnet = models.AlexNet() At this stage, alexnet is an object that runs the AlexNet architecture. It is not essential for us to understand the details of this architecture at this time. At the moment, AlexNet is just an opaque object that can be called as a function. By providing alexnet with precisely sized input data, we will perform a direct transfer across the network. In other words, the input will go through the first set of neurons, the outputs of which will be transmitted to the next set of neurons, until the final output. ResNet By using the resnet101 method, we can now instantiate a 101-layer convolutional neural network. Now let’s create an instance of the network. We’re going to pass an argument that will ask the function to download the resnet101 weights formed on the ImageNet dataset, with 1.2 million images and 1000 categories: resnet = models.resnet101(pretrained=True) resnetCode language: PHP (php) Now, the resnet variable can be called as a function. Before we can do that, however, we need to preprocess the input images so that they are the correct size and their values ​​(colours) are roughly in the same numeric range. To do this, we need to use the torchvision module which provides transformations, which will allow us to quickly define pipelines of basic preprocessing functions: from torchvision import transforms preprocess = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] )])Code language: JavaScript (javascript) In this case, we have defined a preprocessing function that will scale the input image to 256 × 256, crop the image to 224 × 224 around the centre, turn it into a tensor, and normalize its RGB components. (red, green, blue) so that they have defined means and standard deviations. Image Recognition Now we can use an image for the image recognition task using our model. I took a picture of a dog. We can start by loading an image from the local filesystem using Pillow, an image manipulation module for Python: from google.colab import files uploaded = files.upload() from PIL import Image img = Image.open(""dog.png"") imgCode language: JavaScript (javascript) Next, we need to pass the image through our preprocessing pipeline for image recognition: img_t = preprocess(img) Now we can reshape, crop, and normalize the input tensor in the way the network expects: import torch batch_t = torch.unsqueeze(img_t, 0) resnet.eval() out = resnet(batch_t) outCode language: JavaScript (javascript) Run The Image Recognition Model The process of running a trained model on new data is called inference in deep learning circles. In order to make inferences for this image recognition model, we need to put the network into evaluation mode. Now let’s load the file containing the 1,000 labels for the ImageNet dataset classes: with open('imagenet_classes.txt') as f: labels = [line.strip() for line in f.readlines()] _, index = torch.max(out, 1) percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100 labels[index[0]], percentage[index[0]].item()Code language: JavaScript (javascript) (‘golden retriever’, 96.29334259033203) This gives us something that roughly resembles the confidence the model has in its prediction. In this case, the model is 96% certain that he knows what he is looking at is a golden retriever. Also, Read – Uses of Data Science. I hope you liked this article on Image Recognition with Machine Learning using PyTorch. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Image Recognition with Machine Learning using PyTorch
2020-08-27 09:03:00;A very common confusion among those new to machine learning is the difference between a machine learning algorithm and a Model in machine learning. The two terms are often used interchangeably, which makes it even more confusing. So in this article, I will tell you what is the difference between algorithm and model in machine learning.;https://thecleverprogrammer.com/2020/08/27/difference-between-algorithm-and-model-in-machine-learning/;['pattern'];1.0;[];['Regression', 'ML', 'Linear Regression', 'Decision Tree', 'Logistic Regression', 'Clustering', 'K-Means', 'Classification'];['clustering', 'recogn', 'k-means', 'regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'logistic regression', 'decision tree'];A very common confusion among those new to machine learning is the difference between a machine learning algorithm and a Model in machine learning. The two terms are often used interchangeably, which makes it even more confusing. So in this article, I will tell you what is the difference between algorithm and model in machine learning. Is There a Difference Between Algorithm and Model? My answer will be Yes, as a machine learning algorithm is like a procedure executed on data to find patterns and rules that are stored and used to create a machine learning model that is like a program that can be used to make predictions. Also, Read – Machine Learning Skills You Must Know. What is an Algorithm? A machine learning algorithm is essentially a procedure that is used to find patterns within data and learn from the data. It is commonly said to be fit on a dataset which means it is applied on the dataset. There are many types of algorithms with many different functions and purposes. The three main ones are: Regression: Used to make predictions where the output is a continuous value, such as logistic regression.Classification: are those algorithms that are used to classify between the categorical values. Clustering: Used to group similar items or clustered data points, such as K-Means. What is a Model? A machine learning model represents the output of the algorithm. It represents what has been learned from “learning” the algorithm on the data and contains a specific set of functionality of the algorithm. A linear regression model stores the vector of coefficients and constants that best fit the data.A decision tree template stores the set of if-then statements corresponding to individual branches. The model can be saved for later and acts as a program, using the previously stored functionality of the algorithm to make new predictions. If the model is trained efficiently and sufficiently, it can be used to make many more predictions on similar data with a certain level of precision and confidence. The Difference Between Algorithm and Model Now as we know what an algorithm and a model in machine learning are, so it’s easier to see how they relate. As mentioned earlier, an algorithm is run on data to create a model. This model includes both data and a procedure for using the data to predict new data. The process is just like an algorithm. However, not all models store a prediction algorithm. Some, like the k nearest neighbours, store the dataset that serves as a prediction algorithm. This is all based on your model’s goal, however. We want a machine learning model and don’t care as much about the algorithm behind it. However, it is important to know which algorithm to apply to your model for the best results. But once you know that, there are only a few lines of code and a few levels of interaction left before you have a perfectly working model. Also, Read – Image Recognition with Machine Learning using PyTorch. I hope you liked this article, feel free to ask your valuable questions in the comments section. Don’t forget to subscribe for the daily newsletters below if you want to get notified in your inbox. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram;Difference Between Algorithm and Model in Machine Learning
2020-08-27 23:52:36;In machine learning, one hot encoding is a method of quantifying categorical data. Briefly, this method produces a vector of length equal to the number of categories in the dataset. In this article, I will introduce you to the One Hot Encoding Algorithm in Machine Learning.To learn what One Hot Encoding is we first need to go through what Encoding is to understand what One Hot Encoding is. And yes it is one of the most important concepts in Machine Learning. So let’s get started with this task.Also, Read – Machine Learning Books You Need to Read.;https://thecleverprogrammer.com/2020/08/27/one-hot-encoding-in-machine-learning/;['sklearn'];1.0;[];['ML', 'Classification'];['fit', 'model', 'machine learning', 'classif', 'label'];"In machine learning, one hot encoding is a method of quantifying categorical data. Briefly, this method produces a vector of length equal to the number of categories in the dataset. In this article, I will introduce you to the One Hot Encoding Algorithm in Machine Learning. To learn what One Hot Encoding is we first need to go through what Encoding is to understand what One Hot Encoding is. And yes it is one of the most important concepts in Machine Learning. So let’s get started with this task. Also, Read – Machine Learning Books You Need to Read. Encoding Class Labels Many machine learning libraries require class labels to be coded as integer values. Although most classification estimators in scikit-learn convert class labels to integers internally, it is considered a good practice to provide class labels in the form of integer arrays to avoid technical problems. To encode class labels, we must remember that class labels are not ordinal, and no matter what integer we assign to a particular string label. So we can just list the class labels, starting from 0: import pandas as pd df = pd.DataFrame([['green', 'M', 10.1, 'class1'], ['red', 'L', 13.5, 'class2'], ['blue', 'XL', 15.3, 'class1']]) df.columns = ['color', 'size', 'price', 'classlabel'] import numpy as np class_mapping = {label:idx for idx,label in enumerate(np.unique(df['classlabel']))} class_mappingCode language: JavaScript (javascript) {'class1': 0, 'class2': 1} Then we can use the mapping dictionary to turn the class labels into integers: df['classlabel'] = df['classlabel'].map(class_mapping) dfCode language: JavaScript (javascript) We can now reverse the pairs in the mapping dictionary as follows to map the converted labels to represent the original string: inv_class_mapping = {v: k for k, v in class_mapping.items()} df['classlabel'] = df['classlabel'].map(inv_class_mapping) dfCode language: JavaScript (javascript) Alternatively, there is a handy LabelEncoder class directly implemented in scikit-learn to achieve this: from sklearn.preprocessing import LabelEncoder class_le = LabelEncoder() y = class_le.fit_transform(df['classlabel'].values) yCode language: JavaScript (javascript) array([0, 1, 0]) The fit_transform method is only a shortcut to call the fit and transform separately, and we can also use the inverse_transform method to transform the set of class labels into their original string representation: class_le.inverse_transform(y)Code language: CSS (css) array(['class1', 'class2', 'class1'], dtype=object) Performing One Hot encoding In the section above, we used a simple dictionary mapping approach to convert the ordinal size function to integers. Since scikit-learn estimators for classification treat class labels as categorical data that does not imply any (nominal) ordering, we used the LabelEncoder practice to encode the string labels as integers. We could use a similar approach to transform the nominal color column of our data set, like so: X = df[['color', 'size', 'price']].values color_le = LabelEncoder() X[:, 0] = color_le.fit_transform(X[:, 0]) XCode language: JavaScript (javascript) array([[1, 'M', 10.1], [2, 'L', 13.5], [0, 'XL', 15.3]], dtype=object) After running the previous code, the first column of the NumPy X table now contains the new color values, which are coded as follows: blue = 0 green = 1 red = 2 If we stop at this here and feed this data to our classification model, we will end up by making one of the most common mistakes in the processing of categorical data. Can you spot this problem? Although color values do not come in a particular order, a learning algorithm will now assume that Green is larger than blue and red is larger than green. As this assumption is incorrect, but the algorithm can still produce useful results. However, these results would not be optimal. A common workaround for this problem is to use a technique called one hot coding. The idea behind this approach is to create a new dummy entity for each unique value in the nominal characteristic column. Here we would convert the color feature to three new features: blue, green, and red. Then the Binary figures can ​be used to represent the particular color of each sample; for example, a blue sample can be encoded as Blue = 1, Green = 0, Red = 0. To perform this transformation, we can use the One Hot Encoding implemented with the scikit-learn.preprocessing module: from sklearn.preprocessing import OneHotEncoder ohe = OneHotEncoder(categorical_features=[0]) ohe.fit_transform(X).toarray() Code language: JavaScript (javascript) array([[ 0. , 1. , 0. , 1. , 10.1], [ 0. , 0. , 1. , 2. , 13.5], [ 1. , 0. , 0. , 3. , 15.3]]) One more efficient way to create dummy features via one hot encoding is by using the get_dummies method which is implemented in the pandas package: pd.get_dummies(df[['price', 'color', 'size']])Code language: CSS (css) If we use the get_dummies function, we can drop the first column by passing a True argument to the drop_first parameter, as shown below: pd.get_dummies(df[['price', 'color', 'size']], drop_first=True) Code language: PHP (php) Also, Read – Daily Births Forecasting with Machine Learning. I hope you liked this article on One Hot Encoding algorithm in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";One Hot Encoding in Machine Learning
2020-08-28 17:35:18;The first galaxy was observed by a Persian astronomer Abd al-Rahman over 1,000 years ago, and it was first believed to be an unknown extended structure. which is now known as Messier-31 or the infamous Andromeda Galaxy. From that point on, these unknown structures are more frequently observed and recorded, but it took more than 9 centuries for astronomers to manifest on an agreement that they were not just astronomical objects, but entire galaxies. In this article, I will introduce you to the Galaxy Classification Model with Machine Learning.As the discoveries and classification of galaxies increased, several astronomers observed the divergent morphologies. Then, they started grouping previously reported galaxies and newly discovered galaxies based on morphological features which then formed a meaningful classification scheme.Also, Read – My Journey From Commerce to Machine Learning.;https://thecleverprogrammer.com/2020/08/28/galaxy-classification-with-machine-learning/;['keras', 'sklearn', 'tensorflow'];1.0;['AI'];['NN', 'AI', 'ML', 'ReLu', 'Classification'];['epoch', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'relu', 'train', 'label', 'neuron'];"The first galaxy was observed by a Persian astronomer Abd al-Rahman over 1,000 years ago, and it was first believed to be an unknown extended structure. which is now known as Messier-31 or the infamous Andromeda Galaxy. From that point on, these unknown structures are more frequently observed and recorded, but it took more than 9 centuries for astronomers to manifest on an agreement that they were not just astronomical objects, but entire galaxies. In this article, I will introduce you to the Galaxy Classification Model with Machine Learning. As the discoveries and classification of galaxies increased, several astronomers observed the divergent morphologies. Then, they started grouping previously reported galaxies and newly discovered galaxies based on morphological features which then formed a meaningful classification scheme. Also, Read – My Journey From Commerce to Machine Learning. Galaxy Classification Model Astronomy in this contemporary era has evolved massively in parallel with advances in computing over the years. Sophisticated computational techniques such as machine learning models are much more efficient now due to the dramatically increased efficiency in computer performance and huge data available to us today. Long Centuries ago, the galaxy classification was done by hand with a massive group of experienced people, who used to evaluate the results by using cross-validation algorithm. With this inspiration here I will introduce you to a Galaxy Classification Model with Machine Learning. The dataset that I am using is very large, so you need to show patience while downloading it. The dataset can be downloaded from here. Exploring The Data Now, let’s start this task of creating a Galaxy Classification Model by importing all the necessary packages: Now, as you can see, I have imported all the packages, now let’s start reading the data and exploring it to have a quick look at what we are going to work with: import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import cufflinks as cf cf.go_offline() %matplotlib inline #Reading the data from google.colab import files uploaded = files.upload() zoo = pd.read_csv('GalaxyZoo1_DR_table2.csv') zoo.head()Code language: PHP (php) The first column is a unique identifier which cannot be a feature for our model, and the second and third columns are the absolute positions of galaxies which do not correlate with our classes/targets, so we can remove them all: data = zoo.drop(['OBJID','RA','DEC'],axis=1)Code language: JavaScript (javascript) As this is a Galaxy classification model, so we have to check the class imbalance, in a dataset where we perform classification task even though its class binary imbalance may have a major effect in the phase training, and ultimately on precision. To plot the value_counts for three-class columns, we can do it like the code below: plt.figure(figsize=(10,7)) plt.title('Count plot for Galaxy types ') countplt = data[['SPIRAL','ELLIPTICAL','UNCERTAIN']] sns.countplot(x=""variable"",hue='value', data=pd.melt(countplt)) plt.xlabel('Classes') plt.show()Code language: JavaScript (javascript) Splitting The Data For any machine learning model that learns from data, this is a conventional method of dividing the original data into training sets and test sets, where the allocation percentages are 80% d training set and 20% test set. and the data set at least should have 1000 data points to avoid overfitting and to simply increase the training period of any model. So now let’s split the data into training and test sets: X = data.drop(['SPIRAL','ELLIPTICAL','UNCERTAIN'],axis=1).values y = data[['SPIRAL','ELLIPTICAL','UNCERTAIN']].values from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=101) # normalising the data from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)Code language: PHP (php) Building Neural Networks for Galaxy Classification Model Sequential, in Keras, allows us to build the Multilayered Perceptron model from scratch. We can add each layer with a unit number as a parameter of the Dense function where each unit number implies that many densely connected neurons. Now let’s build neural networks using TensorFlow and Keras: from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense model = Sequential() model.add(Dense(10,activation='relu')) model.add(Dense(5,activation='relu')) model.add(Dense(3, activation = 'softmax')) model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy']) start = time.perf_counter() Code language: JavaScript (javascript) Now let’s fit the data into our neural network. It will take some time to run as the data is itself very large and neural network models take time to run: model.fit(x=X_train,y=y_train,epochs=20) print('\nTIME ELAPSED {}Seconds'.format(time.perf_counter() - start))Code language: PHP (php) Epoch 1/20 16699/16699 [==============================] - 9s 551us/step - loss: 0.2877 - accuracy: 0.8750 Epoch 2/20 16699/16699 [==============================] - 9s 538us/step - loss: 0.2618 - accuracy: 0.8881 Epoch 3/20 16699/16699 [==============================] - 9s 551us/step - loss: 0.2595 - accuracy: 0.8891 Epoch 4/20 16699/16699 [==============================] - 9s 539us/step - loss: 0.2549 - accuracy: 0.8898 Epoch 5/20 16699/16699 [==============================] - 9s 537us/step - loss: 0.2470 - accuracy: 0.8916 Epoch 6/20 16699/16699 [==============================] - 9s 540us/step - loss: 0.2422 - accuracy: 0.8920 Epoch 7/20 16699/16699 [==============================] - 9s 541us/step - loss: 0.2387 - accuracy: 0.8929 Epoch 8/20 16699/16699 [==============================] - 9s 540us/step - loss: 0.2332 - accuracy: 0.8943 Epoch 9/20 16699/16699 [==============================] - 9s 540us/step - loss: 0.2297 - accuracy: 0.8952 Epoch 10/20 16699/16699 [==============================] - 9s 545us/step - loss: 0.2256 - accuracy: 0.8977 Epoch 11/20 16699/16699 [==============================] - 9s 546us/step - loss: 0.2235 - accuracy: 0.8986 Epoch 12/20 16699/16699 [==============================] - 11s 688us/step - loss: 0.2222 - accuracy: 0.8990 Epoch 13/20 16699/16699 [==============================] - 11s 644us/step - loss: 0.2217 - accuracy: 0.8994 Epoch 14/20 16699/16699 [==============================] - 9s 542us/step - loss: 0.2210 - accuracy: 0.8994 Epoch 15/20 16699/16699 [==============================] - 10s 571us/step - loss: 0.2208 - accuracy: 0.8995 Epoch 16/20 16699/16699 [==============================] - 10s 608us/step - loss: 0.2203 - accuracy: 0.8996 Epoch 17/20 16699/16699 [==============================] - 9s 565us/step - loss: 0.2201 - accuracy: 0.8993 Epoch 18/20 16699/16699 [==============================] - 9s 561us/step - loss: 0.2196 - accuracy: 0.8995 Epoch 19/20 16699/16699 [==============================] - 10s 602us/step - loss: 0.2192 - accuracy: 0.8998 Epoch 20/20 16699/16699 [==============================] - 10s 591us/step - loss: 0.2189 - accuracy: 0.8999 TIME ELAPSED 189.8537580230004Seconds Now let’s plot the accuracy to have a look at the accuracy of the neural networks at each epoch: mod_history = pd.DataFrame(model.history.history) plt.figure(figsize=(10,7)) plt.style.use('seaborn-whitegrid') plt.title('Model History') plt.ylabel('Accuracy') plt.xlabel('Epoch') plt.plot(mod_history['accuracy'],color='orange',lw=2)Code language: JavaScript (javascript) From this precision graph, we can deduce that after a certain epoch, i.e. approximately from the 6th epoch, the precision remained constant for all other epochs. Now let’s take our model through the confusion matrix algorithm and print a classification report: y_pred = model.predict_classes(X_test) from sklearn.metrics import confusion_matrix,classification_report confusion_matrix(y_test.argmax(axis=1),y_pred) print(classification_report(y_test.argmax(axis=1),y_pred))Code language: JavaScript (javascript) precision recall f1-score support 0 0.84 0.93 0.88 38281 1 0.90 0.77 0.83 12554 2 0.93 0.90 0.92 82754 accuracy 0.90 133589 macro avg 0.89 0.87 0.87 133589 weighted avg 0.90 0.90 0.90 133589 Well, this is very basic astronomical data with features that I can’t even begin to interpret. But still, we got very good results. If I had an astronomy background to study, organize and add more features, this model will be sure to work well better than what he did. Also, Read – Binary Search Algorithm with Python. I hope you liked this article on Galaxy Classification model with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Galaxy Classification with Machine Learning
2020-08-29 15:05:05;"Machine learning algorithms often get the majority of attention when people discuss machine learning; however, success depends on good data. There are mainly two types of data, structured data and unstructured data. In this article, I’ll walk you through how to identify your data.Understanding your data is critical to your success. If you build a model based on bad data, your predictions will be inaccurate. You should also think about what data to include in your machine learning application.Also, Read – Time Series with LSTM Model.";https://thecleverprogrammer.com/2020/08/29/structured-data-and-unstructured-data-in-machine-learning/;['pattern'];1.0;[];['ML', 'LSTM'];['predict', 'lstm', 'model', 'machine learning'];"Machine learning algorithms often get the majority of attention when people discuss machine learning; however, success depends on good data. There are mainly two types of data, structured data and unstructured data. In this article, I’ll walk you through how to identify your data. Understanding your data is critical to your success. If you build a model based on bad data, your predictions will be inaccurate. You should also think about what data to include in your machine learning application. Also, Read – Time Series with LSTM Model. Identify Relevant Data: Structured Data and Unstructured Data Business decisions must be made based on constantly changing data from various sources. Your data sources can include both traditional systems of record data (such as customer, product, transactional, and financial data) and external data (for example, social media, news, weather data, image data or geospatial data). Also, many data structures are essential for analyzing information, including structured data and unstructured data. Structured Data Sources Structured data is generally stored in traditional relational databases and refers to data that has defined a certain length and a format. Most organizations have a large amount of structured data in their on-premises data centres. Here are some examples of structured data: Sensor data: Examples include radio frequency identification (RFID) tags, smart meters, medical devices, and global positioning system (GPS) data.Blog data: when servers, applications, networks, etc. work, they capture all kinds of data about their activity.Point of Sale Data: When the cashier swipes the barcode of any product you purchase, all data associated with the product is generated.Financial data: Many financial systems are now programmatic; they operate according to predefined rules that automate the processes.Weather data: Sensors to collect weather data are deployed in towns, cities and regions to collect data on things like temperature, wind, barometric pressure and precipitation. This data can help meteorologists create hyperlocal forecasts.Click Flow Data: Data is generated every time you click a link on a website. This data can be analyzed to determine customer behaviour and purchasing patterns. Unstructured Data Sources Although unstructured data has an implicit structure, it does not follow a specified format. Unstructured data is still vastly underutilized by businesses and offers a great opportunity for monetization. Cloud, mobile and social media have contributed to a huge increase in unstructured data. Here are examples of unstructured data: Internal text of the company: Think about all the text in documents, journals, survey results and emails. Corporate information today represents a significant percentage of textual information in the world.Social media data: As the name suggests this data is generated from social media platforms, such as Facebook, Twitter, YouTube, LinkedIn, etc.Mobile data: This includes text messages, notes, calendar entries, images, videos, and data entered into third-party mobile apps.Satellite imagery: This includes weather data or data that the government captures in its satellite surveillance imagery.Photographs and video: this includes security, surveillance and traffic data.Radar or Sonar Data: This includes vehicle, weather and oceanographic data. Also, Read – Machine Learning Interview Questions. I hope now you understood what are the types of data Machine Learning Experts use, and what’s the difference between structured data and unstructured data. I hope you liked this article on structured and unstructured data in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Structured Data and Unstructured Data in Machine Learning
2020-08-29 12:55:15;Neural networks can be a difficult concept to understand. I think it’s mainly because they can be used for so many different things like classification, identification or just regression. In this article, I will walk you through how to set up a simple way to forecast time series with LSTM model.Before getting started with the coding part to forecast time series with LSTM first let’s go through some of the major concepts involved for all the beginners who are reading this article.Also, Read – How to Practice Machine Learning?;https://thecleverprogrammer.com/2020/08/29/time-series-with-lstm-in-machine-learning/;['keras', 'sklearn'];1.0;[];['LSTM', 'RNN', 'NN', 'Regression', 'ML', 'Classification'];['epoch', 'regression', 'recurrent neural network', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'train', 'lstm'];Neural networks can be a difficult concept to understand. I think it’s mainly because they can be used for so many different things like classification, identification or just regression. In this article, I will walk you through how to set up a simple way to forecast time series with LSTM model. Before getting started with the coding part to forecast time series with LSTM first let’s go through some of the major concepts involved for all the beginners who are reading this article. Also, Read – How to Practice Machine Learning? What is Time Series Forecasting? Time series forecasting is a technique for predicting events through a time sequence. The technique is used in many fields of study, from geology to behaviour to economics. Techniques predict future events by analyzing trends from the past, assuming that future trends will hold similar to historical trends. What is LSTM? LSTM stands for Short Term Long Term Memory. It is a model or an architecture that extends the memory of recurrent neural networks. Typically, recurrent neural networks have “short-term memory” in that they use persistent past information for use in the current neural network. Essentially, the previous information is used in the current task. This means that we do not have a list of all of the previous information available for the neural node. Forecast Time Series with LSTM I hope you have understood what time series forecasting means and what are LSTM models. Now I will be heading towards creating a machine learning model to forecast time series with LSTM in Machine Learning. For this task to forecast time series with LSTM, I will start by importing all the necessary packages we need: import numpy import matplotlib.pyplot as plt import pandas import math from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_squared_error # fix random seed for reproducibility numpy.random.seed(7)Code language: CSS (css) Now let’s load the data, and prepare the data so that we can use it on the LSTM model, you can download the dataset I am using in this task from here: # load the dataset dataframe = pandas.read_csv('airline-passengers.csv', usecols=[1], engine='python') dataset = dataframe.values dataset = dataset.astype('float32') # normalize the dataset scaler = MinMaxScaler(feature_range=(0, 1)) dataset = scaler.fit_transform(dataset)Code language: PHP (php) Now, I will split the data into training sets and test sets: # split into train and test sets train_size = int(len(dataset) * 0.67) test_size = len(dataset) - train_size train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:] print(len(train), len(test))Code language: PHP (php) 96 48 Time Series with LSTM Now before training the data on the LSTM model, we need to prepare the data so that we can fit it on the model, for this task I will define a helper function: # convert an array of values into a dataset matrix def create_dataset(dataset, look_back=1): dataX, dataY = [], [] for i in range(len(dataset)-look_back-1): a = dataset[i:(i+look_back), 0] dataX.append(a) dataY.append(dataset[i + look_back, 0]) return numpy.array(dataX), numpy.array(dataY)Code language: PHP (php) Now, we need to reshape the data before applying it into the LSTM model: # reshape into X=t and Y=t+1 look_back = 1 trainX, trainY = create_dataset(train, look_back) testX, testY = create_dataset(test, look_back) # reshape input to be [samples, time steps, features] trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1])) testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))Code language: PHP (php) Now as all the tasks are completed concerning data preparation to fit into the LSTM model, it time to fit the data on the model and let’s train the model: # create and fit the LSTM network model = Sequential() model.add(LSTM(4, input_shape=(1, look_back))) model.add(Dense(1)) model.compile(loss='mean_squared_error', optimizer='adam') model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)Code language: PHP (php) Now, let’s make predictions and visualize the time series trends by using the matplotlib package in python: # make predictions trainPredict = model.predict(trainX) testPredict = model.predict(testX) # invert predictions trainPredict = scaler.inverse_transform(trainPredict) trainY = scaler.inverse_transform([trainY]) testPredict = scaler.inverse_transform(testPredict) testY = scaler.inverse_transform([testY]) # calculate root mean squared error trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0])) testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0])) # shift train predictions for plotting trainPredictPlot = numpy.empty_like(dataset) trainPredictPlot[:, :] = numpy.nan trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict # shift test predictions for plotting testPredictPlot = numpy.empty_like(dataset) testPredictPlot[:, :] = numpy.nan testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict # plot baseline and predictions plt.plot(scaler.inverse_transform(dataset)) plt.plot(trainPredictPlot) plt.plot(testPredictPlot) plt.show()Code language: PHP (php) Also, Read – Machine Learning Interview Questions. I hope you liked this article on forecasting time series with LSTM model. Feel free to ask you valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram;Time Series with LSTM in Machine Learning
2020-08-30 10:37:46;In this article, I am going to explain how we can train a neural network model for the task of image classification with TensorFlow. For those new to TensorFlow, TensorFlow is an end-to-end open-source platform for machine learning. It has a comprehensive and flexible ecosystem of tools, libraries, and community resources that allow researchers to push cutting-edge advancements in ML, and developers to easily build and deploy machine learning-based applications.;https://thecleverprogrammer.com/2020/08/30/image-classification-with-tensorflow-in-machine-learning/;['keras', 'pattern', 'tensorflow'];1.0;['ML'];['CV', 'NN', 'DL', 'ML', 'ReLu', 'Classification'];['recogn', 'epoch', 'computer vision', 'predict', 'fit', 'model', 'image classification', 'loss', 'machine learning', 'classif', 'layer', 'neural network', 'relu', 'deep learning', 'train', 'label'];"In this article, I am going to explain how we can train a neural network model for the task of image classification with TensorFlow. For those new to TensorFlow, TensorFlow is an end-to-end open-source platform for machine learning. It has a comprehensive and flexible ecosystem of tools, libraries, and community resources that allow researchers to push cutting-edge advancements in ML, and developers to easily build and deploy machine learning-based applications. What is Image Classification? Image classification is the process of categorizing and labelling groups of pixels or vectors in an image according to specific rules. The categorization law can be designed using one or more spectral or textural characteristics. Also, Read – Why Python is Better than R? Image Classification with TensorFlow Now, Image Classification can also be done by using less complex models provided by Scikit-Learn, so why TensorFlow. By using TensorFlow we can build a neural network for the task of Image Classification. By building a neural network we can discover more hidden patterns than just classification. Now let’s get started with the task of Image Classification with TensorFlow by importing some necessary packages: # TensorFlow and tf.keras import tensorflow as tf from tensorflow import keras # Helper libraries import numpy as np import matplotlib.pyplot as plt Code language: PHP (php) Import the Fashion MNIST dataset Fashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the “Hello, World” of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the images of clothing that I will use for the task of image classification with TensorFlow. The Fashion MNIST Dataset is an advanced version of the traditional MNIST dataset which is very much used as the “Hello, World” of machine learning. The MNIST dataset contains images of handwritten numbers (0, 1, 2, etc.) in the same format as the clothing images I will be using for the image classification task with TensorFlow. Now let’s import the Fashion MNIST dataset to get started with the task: fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() In the dataset, each image is mapped into a single label. Since the class names are not defined in the dataset, we need to store them here so that we can use them later when viewing the images: class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']Code language: JavaScript (javascript) Preprocess The Data For this task of Image Classification with TensorFlow, the data must be preprocessed before training the neural network. If you inspect the first frame of the training set, you will find that the pixel values ​​are between 0 and 255: plt.figure() plt.imshow(train_images[0]) plt.colorbar() plt.grid(False) plt.show()Code language: CSS (css) Now I’m going to scale these values ​​to a range of 0 to 1 before passing them to the neural network model. To do this, we need to divide the values ​​by 255. The training set and the test set should be preprocessed in the same way: train_images = train_images / 255.0 test_images = test_images / 255.0 To verify that the data is in the correct format and to verify that we are ready to create and train the neural network for image classification with TensorFlow, let’s display the first 25 images of the training set and display the name of the class under each image: plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]]) plt.show()Code language: PHP (php) Image Classification with TensorFlow: Building Model Now to Build the neural network for the task of Image Classification with TensorFlow, we first need to configure the model layers and then move forward with compiling the model. Setting Up Layers The basic building block of neural networks is its layers. Layers work by extracting the representations from data fed into them. Most of the deep learning, Models involves doing simple layers together. Now, let’s create the layers of our neural network: model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation='relu'), keras.layers.Dense(10) ])Code language: JavaScript (javascript) Compiling The Model Now, let’s move forward with compiling our model: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])Code language: PHP (php) Image Classification with TensorFlow: Training Model Now, let’s train the Neural Network for the task of Image Classification with TensorFlow, and make predictions on it: #Fitting the Model model.fit(train_images, train_labels, epochs=10) #Evaluating Accuracy test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print('\nTest accuracy:', test_acc)Code language: PHP (php) Test accuracy: 0.8817999958992004 #Make Predictions probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()]) predictions = probability_model.predict(test_images) predictions[0]Code language: PHP (php) array([1.1349098e-09, 1.0395625e-09, 3.4154518e-10, 8.3033120e-12, 6.5739442e-10, 5.9645530e-03, 9.4151291e-09, 1.1747092e-02, 8.7000714e-08, 9.8228824e-01], dtype=float32) A prediction is an array of 10 numbers. They represent the “confidence” of the model that the image matches each of the 10 different garments. Let’s see which label has the highest confidence value: np.argmax(predictions[0])Code language: CSS (css) Output: 9 Thus, the model is most convinced that this image is an ankle boot, or class_names [9]. Examination of the test label shows that this classification is correct: test_labels[0]Code language: CSS (css) 9 Now, I will create a helper function to plot our predictions: def plot_image(i, predictions_array, true_label, img): true_label, img = true_label[i], img[i] plt.grid(False) plt.xticks([]) plt.yticks([]) plt.imshow(img, cmap=plt.cm.binary) predicted_label = np.argmax(predictions_array) if predicted_label == true_label: color = 'blue' else: color = 'red' plt.xlabel(""{} {:2.0f}% ({})"".format(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_label]), color=color) def plot_value_array(i, predictions_array, true_label): true_label = true_label[i] plt.grid(False) plt.xticks(range(10)) plt.yticks([]) thisplot = plt.bar(range(10), predictions_array, color=""#777777"") plt.ylim([0, 1]) predicted_label = np.argmax(predictions_array) thisplot[predicted_label].set_color('red') thisplot[true_label].set_color('blue')Code language: PHP (php) Verify Predictions Let’s look at the 0th frame of the predictions and the prediction table. The correct prediction labels are blue and the incorrect prediction labels are red: i = 0 plt.figure(figsize=(6,3)) plt.subplot(1,2,1) plot_image(i, predictions[i], test_labels, test_images) plt.subplot(1,2,2) plot_value_array(i, predictions[i], test_labels) plt.show() # Plot the first X test images, their predicted labels, and the true labels. # Color correct predictions in blue and incorrect predictions in red. num_rows = 5 num_cols = 3 num_images = num_rows*num_cols plt.figure(figsize=(2*2*num_cols, 2*num_rows)) for i in range(num_images): plt.subplot(num_rows, 2*num_cols, 2*i+1) plot_image(i, predictions[i], test_labels, test_images) plt.subplot(num_rows, 2*num_cols, 2*i+2) plot_value_array(i, predictions[i], test_labels) plt.tight_layout() plt.show()Code language: PHP (php) Also, Read – Structured and Unstructured Data in Machine Learning. The output looks great, only the boots are recognized wrong as sandals. I hope you liked this article on Image Classification with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Image Classification with TensorFlow in Machine Learning
2020-08-30 16:05:53;In this article, I will train a model to predict weather with machine learning. We will act as if we do not have access to the weather forecast. We have access to a century of historical averages of global temperatures, including global maximum temperatures, global minimum temperatures, and global land and ocean temperatures. Having all this, we know that this is a supervised regression machine learning problem.;https://thecleverprogrammer.com/2020/08/30/predict-weather-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Random Forest', 'Classification', 'Regression'];['regression', 'predict', 'fit', 'model', 'image classification', 'machine learning', 'random forest', 'classif', 'train'];"In this article, I will train a model to predict weather with machine learning. We will act as if we do not have access to the weather forecast. We have access to a century of historical averages of global temperatures, including global maximum temperatures, global minimum temperatures, and global land and ocean temperatures. Having all this, we know that this is a supervised regression machine learning problem. Weather Dataset to Predict Weather First of all, we need some data, the data I am using to predict weather with machine learning was created from one of the most prestigious research universities in the world, we will assume that the data in the dataset is true. You can easily download this data from here. Also, Read – Image Classification with TensorFlow. Now, let’s get started with reading the dataset: import pandas as pd global_temp = pd.read_csv(""GlobalTemperatures.csv"") print(global_temp.shape) print(global_temp.columns) print(global_temp.info()) print(global_temp.isnull().sum())Code language: PHP (php) Data Preparation Unfortunately, we’re not quite at the point where we can just feed the raw data into a model and have it send back a response. We will need to make some minor edits to put our data in a machine learning model. The exact steps in data preparation will depend on the model used and the data collected, but some amount of data manipulation will be required. First, I’ll create a function called wrangle () in which I’ll call our dataframe: #Data Preparation def wrangle(df): df = df.copy() df = df.drop(columns=[""LandAverageTemperatureUncertainty"", ""LandMaxTemperatureUncertainty"", ""LandMinTemperatureUncertainty"", ""LandAndOceanAverageTemperatureUncertainty""], axis=1) Code language: PHP (php) We want to make a copy of the dataframe so as not to corrupt the original. After that, we are going to remove the columns that have high cardinality. High cardinality refers to columns whose values ​​are very rare or unique. Given the frequency of high cardinality data in most time-series datasets, we will solve this problem directly by completely removing these high cardinality columns from our dataset so as not to confuse our model in the future. Now, I will create a function to convert temperature, and to convert the columns into DateTime object: def converttemp(x): x = (x * 1.8) + 32 return float(x) df[""LandAverageTemperature""] = df[""LandAverageTemperature""].apply(converttemp) df[""LandMaxTemperature""] = df[""LandMaxTemperature""].apply(converttemp) df[""LandMinTemperature""] = df[""LandMinTemperature""].apply(converttemp) df[""LandAndOceanAverageTemperature""] = df[""LandAndOceanAverageTemperature""].apply(converttemp) df[""dt""] = pd.to_datetime(df[""dt""]) df[""Month""] = df[""dt""].dt.month df[""Year""] = df[""dt""].dt.year df = df.drop(""dt"", axis=1) df = df.drop(""Month"", axis=1) df = df[df.Year &gt;= 1850] df = df.set_index([""Year""]) df = df.dropna() return df global_temp = wrangle(global_temp) print(global_temp.head()) Code language: PHP (php) After calling our wrangle function to our global_temp dataframe, we can now see a new cleaned-up version of our global_temp dataframe with no missing values. Visualization Now, before moving forward with training a model to predict weather with machine learning, let’s visualize this data to find correlations between the data: import seaborn as sns import matplotlib.pyplot as plt corrMatrix = global_temp.corr() sns.heatmap(corrMatrix, annot=True) plt.show()Code language: JavaScript (javascript) As we can see, and as some of you have probably guessed, the columns that we have chosen to keep moving forward are highly correlated with each other. Separating Our Target to Predict Weather Now we need to separate the data into features and targets. The target, also called Y, is the value we want to predict, in this case, the actual average land and ocean temperature and features are all the columns the model uses to make a prediction: target = ""LandAndOceanAverageTemperature"" y = global_temp[target] x = global_temp[[""LandAverageTemperature"", ""LandMaxTemperature"", ""LandMinTemperature""]]Code language: JavaScript (javascript) Train Test Split Now, to create a model to predict weather with machine learning we need to split the data by using the train_test_split method provided by scikit-learn: from sklearn.model_selection import train_test_split xtrain, xval, ytrain, yval = train_test_split(x, y, test_size=0.25, random_state=42) print(xtrain.shape) print(xval.shape) print(ytrain.shape) print(yval.shape)Code language: PHP (php) (1494, 3) (498, 3) (1494,) (498,) Baseline Mean Absolute Error Before we can make and evaluate any predictions on our machine learning model to predict weather, we need to establish a baseline, a sane metric that we hope to beat with our model. If our model cannot improve from the baseline then it will fail and we should try a different model or admit that machine learning is not suitable for our problem: from sklearn.metrics import mean_squared_error ypred = [ytrain.mean()] * len(ytrain) print(""Baseline MAE: "", round(mean_squared_error(ytrain, ypred), 5))Code language: JavaScript (javascript) Training Model To Predict Weather Now to predict weather with Machine Learning I will train a Random Forest algorithm which is capable of performing both the tasks of Classification as well as Regression: from sklearn.feature_selection import SelectKBest from sklearn.ensemble import RandomForestRegressor forest = make_pipeline( SelectKBest(k=""all""), StandardScaler(), RandomForestRegressor( n_estimators=100, max_depth=50, random_state=77, n_jobs=-1 ) ) forest.fit(xtrain, ytrain)Code language: JavaScript (javascript) Model Evaluation of Machine Learning model to Predict Weather To put our predictions in perspective, we can calculate a precision using the average percentage error subtracted from 100%: import numpy as np errors = abs(ypred - yval) mape = 100 * (errors/ytrain) accuracy = 100 - np.mean(mape) print(""Random Forest Model: "", round(accuracy, 2), ""%"")Code language: JavaScript (javascript) Random Forest Model: 99.52 % Also, Read – Why Python is Better than R. Our model has learned to predict weather conditions with machine learning for next year with 99% accuracy. I hope you liked this article on how to build a model to predict weather with machine learning. Feel free to ask you valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Predict Weather with Machine Learning
2020-08-31 20:16:45;In this article, I will introduce you to a text classification model with TensorFlow on movie reviews as positive or negative using the text of the reviews. This is a binary classification problem, which is an important and widely applicable type of machine learning problem.;https://thecleverprogrammer.com/2020/08/31/text-classification-with-tensorflow-in-machine-learning/;['keras', 'tensorflow'];1.0;['AI'];['CV', 'AI', 'ML', 'ReLu', 'Text Classification', 'Classification'];['epoch', 'text classification', 'computer vision', 'fit', 'model', 'loss', 'machine learning', 'relu', 'classif', 'layer', 'train', 'label'];"In this article, I will introduce you to a text classification model with TensorFlow on movie reviews as positive or negative using the text of the reviews. This is a binary classification problem, which is an important and widely applicable type of machine learning problem. Text Classification with TensorFlow I’ll walk you through the basic application of transfer learning with TensorFlow Hub and Keras. I will be using the IMDB dataset which contains the text of 50,000 movie reviews from the internet movie database. These are divided into 25,000 assessments for training and 25,000 assessments for testing. The training and test sets are balanced in a way that they contain an equal number of positive and negative reviews. Also, Read – Data Science Project on Diamonds Analysis with Python. Now, let’s get started with this task of text classification with TensorFlow by importing some necessary libraries: import numpy as np import tensorflow as tf !pip install tensorflow-hub !pip install tensorflow-datasets import tensorflow_hub as hub import tensorflow_datasets as tfds print(""Version: "", tf.__version__) print(""Eager mode: "", tf.executing_eagerly()) print(""Hub version: "", hub.__version__) print(""GPU is"", ""available"" if tf.config.experimental.list_physical_devices(""GPU"") else ""NOT AVAILABLE"")Code language: JavaScript (javascript) Although the dataset I am using here is available online to download, but I will simply load the data using TensorFlow. It means you don’t need to download the dataset from any external sources. Now, I will simply load the data and split it into training and test sets: # Split the training set into 60% and 40%, so we'll end up with 15,000 examples # for training, 10,000 examples for validation and 25,000 examples for testing. train_data, validation_data, test_data = tfds.load( name=""imdb_reviews"", split=('train[:60%]', 'train[60%:]', 'test'), as_supervised=True)Code language: PHP (php) Data Exploration Let’s have a look at the data to figure out what we are going to work with. I will simply print the first 10 samples from the dataset: train_examples_batch, train_labels_batch = next(iter(train_data.batch(10))) train_examples_batch Now, let’s print the first 10 labels from the data set: train_labels_batch Output: <tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0])> Building Text Classification Model To build a model for the task of Text Classification with TensorFlow, I will use a pre-trained model provided by TensorFlow which is known by the name TensorFlow Hub. Let’s first create a Keras layer that uses a TensorFlow Hub model to the embed sentences, and try it out on some sample input: embedding = ""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1"" hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True) hub_layer(train_examples_batch[:3])Code language: PHP (php) Now build the model on the complete dataset: model = tf.keras.Sequential() model.add(hub_layer) model.add(tf.keras.layers.Dense(16, activation='relu')) model.add(tf.keras.layers.Dense(1)) model.summary()Code language: JavaScript (javascript) Model: ""sequential"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer (KerasLayer) (None, 20) 400020 _________________________________________________________________ dense (Dense) (None, 16) 336 _________________________________________________________________ dense_1 (Dense) (None, 1) 17 ================================================================= Total params: 400,373 Trainable params: 400,373 Non-trainable params: 0 _________________________________________________________________ Compile The Model Now, I will compile the model by using the loss function and the adam optimizer: model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])Code language: PHP (php) Trani The Text Classification Model Train the model for 20 epochs in mini-sets of 512 samples. These are 20 iterations on all the samples of the tensors x_train and y_train. During training, monitor model loss and accuracy on the 10,000 samples in the validation set: history = model.fit(train_data.shuffle(10000).batch(512), epochs=20, validation_data=validation_data.batch(512), verbose=1) Evaluating The Model And let’s see how the text classification model works. Two values ​​will be returned. Loss and accuracy rate: results = model.evaluate(test_data.batch(512), verbose=2) for name, value in zip(model.metrics_names, results): print(""%s: %.3f"" % (name, value))Code language: PHP (php) 49/49 - 3s - loss: 0.3217 - accuracy: 0.8553 loss: 0.322 accuracy: 0.855 Also, Read – Computer Vision Tutorial with Python. So our Text Classification Model achieved an accuracy rate of 85 per cent which is generally appreciated. I hope you liked this article on Text Classification Model with TensorFlow. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Text Classification with TensorFlow in Machine Learning
2020-09-01 13:07:00;Image segmentation is one of the key processes in machine vision applications to partition a digital image into a group of pixels. There are many great ways to segment an image. In this article, I will take you through the task of Image Segmentation with Python.;https://thecleverprogrammer.com/2020/09/01/image-segmentation-with-python/;['skimage'];1.0;[];['ML', 'Text Classification', 'Image Segmentation', 'Classification'];['text classification', 'image segmentation', 'fit', 'machine learning', 'classif', 'filter', 'label'];"Image segmentation is one of the key processes in machine vision applications to partition a digital image into a group of pixels. There are many great ways to segment an image. In this article, I will take you through the task of Image Segmentation with Python. Image Segmentation with Python Take a look at the image below of candies placed in a particular order to form a word. And, if a robot with vision was a task to count the number of candies by colour, it would be important for him to understand the boundaries between the candies. Also, Read – Spacy in Machine Learning. I will use the image above for the task of Image Segmentation with Python. Now, let’s load the necessary packages and load the image from Unsplash to get started with this task: import matplotlib.pyplot as plt from skimage.io import imread from skimage import color import numpy as np cimage = imread('https://images.unsplash.com/photo-1580015915218-685fd3cbfa97?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1267&amp;q=80') fig, ax = plt.subplots(figsize=(20,20)) ax.imshow(cimage) ax.axis('off')Code language: JavaScript (javascript) To segment this image we need to find the boundary of all the candies and then we will see what colour they are consisting off, for this I will plot a scatter plot to visualize all the colours of the candies with proper segmentation to understand all the colours inside the image. It will be a long code but it will be easy to run and learn: # convert the image from RGB to LAB lab_img = color.rgb2lab(cimage) x,y,z = lab_img.shape # to plot the colors we will use the RGB values from the # image directly for colors. to_plot = cimage.reshape(x*y, 3) colors_map = to_plot.astype(np.float)/256 # create dataset for scatter plot scatter_x = [] scatter_y = [] for xi in range(x): for yi in range(y): L_val = lab_img[xi,yi][0] A_val = lab_img[xi,yi][1] B_val = lab_img[xi,yi][2] scatter_x.append(A_val) scatter_y.append(B_val) plt.figure(figsize=(20,20)) plt.xlabel(""a* from green to red"") plt.ylabel(""b* from blue to yellow"") plt.scatter(scatter_x,scatter_y, c=colors_map) view raw image segment hosted with ❤ by GitHub View this gist on GitHub As we are done with the scatter plot to segment the image according to the colours of the candies, now we can use this idea to segment the candies inside the image properly according to their colours: def filter_color(L_val_min, A_val_min, A_val_max, B_val_min, B_val_max): filtered_image = np.copy(cimage) for xi in range(x): for yi in range(y): L_val = lab_img[xi,yi][0] A_val = lab_img[xi,yi][1] B_val = lab_img[xi,yi][2] if L_val > L_val_min and A_val > A_val_min and A_val < A_val_max and B_val > B_val_min and B_val < B_val_max: pass else: filtered_image[xi, yi] = [255,255,255] return filtered_image lab_img = color.rgb2lab(cimage) yellow = filter_color(70, -50, 0, 30, 100) red = filter_color(30, 25, 100, 0, 100) green = filter_color(50, -128, -20, 0, 50) blue = filter_color(50,-40, 30, -128, -20) white = filter_color(93, -25, 25, -25, 25) pink = filter_color(50, 20,128,-50,0) fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(20,20)) ax[0][0].imshow(pink) ax[0][0].set_title(""pink Candies"") ax[0][0].axis('off') ax[0][1].imshow(yellow) ax[0][1].set_title(""yellow Candies"") ax[0][1].axis('off') ax[1][0].imshow(red) ax[1][0].set_title(""red Candies"") ax[1][0].axis('off') ax[1][1].imshow(green) ax[1][1].set_title(""green Candies"") ax[1][1].axis('off') ax[2][0].imshow(white) ax[2][0].set_title(""white Candies"") ax[2][0].axis('off') ax[2][1].imshow(blue) ax[2][1].set_title(""blue Candies"") ax[2][1].axis('off') view raw image segment hosted with ❤ by GitHub View this gist on GitHub Also, Read – Text Classification with TensorFlow. This looks amazing right. I hope you now know how to perform a task of Image segmentation with Python. I hope you liked this article on Image Segmentation with Python. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Image Segmentation with Python
2020-09-01 10:44:50;In Machine Learning, spaCy is a very useful open-source library for advanced natural language processing (NLP) tasks for Python. If you work with a lot of text, you might want to learn more about it. For example, what is it? What do the words mean in context? Who does what to whom? Which companies and which products are mentioned? Which texts are similar to each other? In this article, I will take you through, spaCy in Machine Learning.;https://thecleverprogrammer.com/2020/09/01/spacy-in-machine-learning/;['spacy'];1.0;['NER', 'NLP'];['DL', 'ML', 'NLP', 'NER', 'Text Classification', 'Classification'];['recogn', 'text classification', 'model', 'machine learning', 'classif', 'deep learning', 'named entity recognition', 'natural language processing', 'label'];"In Machine Learning, spaCy is a very useful open-source library for advanced natural language processing (NLP) tasks for Python. If you work with a lot of text, you might want to learn more about it. For example, what is it? What do the words mean in context? Who does what to whom? Which companies and which products are mentioned? Which texts are similar to each other? In this article, I will take you through, spaCy in Machine Learning. Why spaCY? spaCy is specially designed for production use and helps you create applications that process and “understand” large volumes of text. It can be used to create systems for extracting information or understanding natural language, or for preprocessing text for deep learning. Also, Read – Text Classification with TensorFlow. Installing spaCy in your systems is a very easy task like installing all other packages in Python. You can easily install it by using the pip command in your terminal – pip install spacy. You will also need to access at least one of the spaCy language models. spaCy can be used to analyze texts from different languages ​​including English, German, Spanish and French, each with its models. We’re going to be working with English text for this simple analysis, so go ahead and take spaCy’s little English language template, again via the command line: python -m spacy download en_core_web_sm. Tokenization The task of Text processing now comes down to loading your language model and passing strings directly to it. Now let’s see what it does with a sample review: import spacy nlp = spacy.load(""en_core_web_sm"") review = ""I'am so happy I went to this awesome Vegas buffet!"" doc = nlp(review)Code language: JavaScript (javascript) To see the resulting output, we need to loop over the above NLP document: for token in doc: print(token.text, token.pos_, token.lemma_, token.is_stop)Code language: CSS (css) I'am PROPN I'am False so ADV so True happy ADJ happy False I PRON -PRON- True went VERB go False to ADP to True this DET this True awesome ADJ awesome False Vegas PROPN Vegas False buffet NOUN buffet False ! PUNCT ! False spaCy does not explicitly divide the original text into a list, but tokens are accessible by the index range: print(doc[:5])Code language: CSS (css) Output: I’am so happy I went Spacy Dependencies NLP consists of a lot of unique challenges, certainly with syntactic and semantic issues. spaCy identifies all the dependencies of each token as the text passes through the language model, let’s check the dependencies in our Text review: for token in doc: print(token.text, token.dep_)Code language: CSS (css) I'am ROOT so advmod happy amod I nsubj went ccomp to prep this det awesome amod Vegas compound buffet pobj ! punct It looks somewhat interesting, but visualizing these relationships reveals an even fuller story. Start by loading a submodule called displaCy to help with visualization: from spacy import displacy displacy.serve(doc)Code language: JavaScript (javascript) Then we need to render the dependency tree from the document: Named Entity Recognition with Spacy Machine learning practitioners often seek to identify key elements and individuals in unstructured text. This task, called Named Entity Recognition (NER), runs automatically as the text passes through the language model. To see which tokens it identifies as named entities in our restaurant review, simply browse doc.ents: for ent in doc.ents: print(ent.text, ent.label_)Code language: CSS (css) Vegas GPE It recognizes “Vegas” as a named entity, but what does the label “GPE” mean? If you don’t know what any of the abbreviations mean, just ask spaCy to explain it to you: spacy.explain(""GPE"")Code language: JavaScript (javascript) Countries, cities, states Additionally, the displacement method of displaCy can highlight named entities if the style argument is specified: displacy.serve(doc, style='ent')Code language: JavaScript (javascript) The coloured texts represent named entities by type. Consider this more complicated example with four different types of entities: document = nlp(""One year ago, I visited the Eiffel Tower with Jeff in Paris, France"") displacy.serve(document, style='ent')Code language: JavaScript (javascript) Also, Read – Data Science Project on Diamonds Analysis with Python. I hope you liked this article on Spacy in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Don’t forget to subscribe for the daily newsletters below to get our notifications in your inbox. Follow Us: Facebook Instagram";Spacy in Machine Learning
2020-09-02 11:09:28;In these types of machine learning problems to predict fuel efficiency, we aim to predict the output of a continuous value, such as a price or a probability. In this article, I will take you through how we can predict Fuel Efficiency with Machine Learning.;https://thecleverprogrammer.com/2020/09/02/predict-fuel-efficiency-with-machine-learning/;['keras', 'tensorflow'];1.0;[];['ML', 'ReLu', 'Recommender'];['hidden layer', 'epoch', 'output layer', 'predict', 'fit', 'model', 'loss', 'machine learning', 'recommend', 'layer', 'relu', 'train', 'label', 'test data'];"In these types of machine learning problems to predict fuel efficiency, we aim to predict the output of a continuous value, such as a price or a probability. In this article, I will take you through how we can predict Fuel Efficiency with Machine Learning. Predict Fuel Efficiency Here I will use one of the famous datasets among machine learning practitioners, Auto MPG dataset to create a model to predict fuel efficiency of vehicles in the late 1970s and early 1980s. To do this, we will provide the model with a description of many automobiles from this period. This description includes attributes such as cylinders, displacement, horsepower and weight. Also, Read – Introduction to Robots with Python. Let’s import the necessary libraries to get started with this task: import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import tensorflow as tf from tensorflow import keras from tensorflow.keras import layersCode language: JavaScript (javascript) Now, the next thing to do is to download the dataset. You can easily download the dataset from here. Now, let’s import the data using the pandas package: column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin'] dataset = pd.read_csv(""auto.csv"", names=column_names, na_values = ""?"", comment='\t', sep="" "", skipinitialspace=True)Code language: PHP (php) The “origin” column in the dataset is categorical, so to move forward we need to use some one-hot encoding on it: origin = dataset.pop('Origin') dataset['USA'] = (origin == 1)*1.0 dataset['Europe'] = (origin == 2)*1.0 dataset['Japan'] = (origin == 3)*1.0 Code language: JavaScript (javascript) Now, let’s split the data into training and test sets: train_dataset = dataset.sample(frac=0.8,random_state=0) test_dataset = dataset.drop(train_dataset.index) Before training and test to predict fuel efficiency with machine learning, let’s visualize the data by using the seaborn’s pair plot method: sns.pairplot(train_dataset[[""MPG"", ""Cylinders"", ""Displacement"", ""Weight""]], diag_kind=""kde"") Code language: JavaScript (javascript) Now, I will separate the target values from the features in the dataset. This label is that feature that I will use to train the model to predict fuel efficiency: train_labels = train_dataset.pop('MPG') test_labels = test_dataset.pop('MPG')Code language: JavaScript (javascript) Normalize The Data It is recommended that you standardize features that use different scales and ranges. Although the model can converge without standardization of features, this makes learning more difficult and makes the resulting model dependent on the choice of units used in the input. We need to do this to project the test dataset into the same distribution the model was trained on: def norm(x): return (x - train_stats['mean']) / train_stats['std'] normed_train_data = norm(train_dataset) normed_test_data = norm(test_dataset)Code language: JavaScript (javascript) Note: The statistics used to normalize the inputs here (mean and standard deviation) should be applied to all other data provided to the model, with the one-hot encoding we did earlier. This includes the test set as well as live data when the model is used in production. Build The Model Let’s build our model. Here, I will use the sequential API with two hidden layers and one output layer that will return a single value. The steps to build the model are encapsulated in a function, build_model, since we will be creating a second model later: def build_model(): model = keras.Sequential([ layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]), layers.Dense(64, activation=tf.nn.relu), layers.Dense(1) ]) optimizer = tf.keras.optimizers.RMSprop(0.001) model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_absolute_error', 'mean_squared_error']) return model model = build_model() model.summary() Code language: JavaScript (javascript) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 64) 640 _________________________________________________________________ dense_1 (Dense) (None, 64) 4160 _________________________________________________________________ dense_2 (Dense) (None, 1) 65 ================================================================= Total params: 4,865 Trainable params: 4,865 Non-trainable params: 0 _________________________________________________________________ Now, before training the model to predict fuel efficiency let’s tray this model in the first 10 samples: example_batch = normed_train_data[:10] example_result = model.predict(example_batch) example_result array([[-0.12670723], [-0.03443428], [ 0.3062502 ], [ 0.3065169 ], [ 0.36841604], [ 0.02191051], [ 0.3674207 ], [ 0.10561748], [ 0.00638346], [-0.00226454]], dtype=float32) Training Model To Predict Fuel Efficiency Now, let’s train the model to predict fuel efficiency: class PrintDot(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs): if epoch % 100 == 0: print('') print('.', end='') EPOCHS = 1000 history = model.fit( normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()]) Now, let’s visualize the model training: def plot_history(history): hist = pd.DataFrame(history.history) hist['epoch'] = history.epoch plt.figure() plt.xlabel('Epoch') plt.ylabel('Mean Abs Error [MPG]') plt.plot(hist['epoch'], hist['mean_absolute_error'], label='Train Error') plt.plot(hist['epoch'], hist['val_mean_absolute_error'], label = 'Val Error') plt.ylim([0,5]) plt.legend() plt.figure() plt.xlabel('Epoch') plt.ylabel('Mean Square Error [$MPG^2$]') plt.plot(hist['epoch'], hist['mean_squared_error'], label='Train Error') plt.plot(hist['epoch'], hist['val_mean_squared_error'], label = 'Val Error') plt.ylim([0,20]) plt.legend() plt.show() plot_history(history) Code language: JavaScript (javascript) This graph below represents a little improvement or even degradation in validation error after about 100 epochs. Now, let’s update the model.fit method to stop training when the validation score does not improve. We’ll be using an EarlyStopping callback that tests a training condition for each epoch. If a set number of epochs pass without showing improvement, automatically stop training: model = build_model() # The patience parameter is the amount of epochs to check for improvement early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10) history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()]) plot_history(history)Code language: PHP (php) The graph shows that over the validation set, the average error is typically around +/- 2 MPG. Is it good? We will leave that decision to you. Let’s see how the model generalizes using the test set, which we didn’t use when training the model. This shows how well it is expected that model to predict when we use it in the real world: loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=0) print(""Testing set Mean Abs Error: {:5.2f} MPG"".format(mae)) Code language: PHP (php) Testing set Mean Abs Error: 1.97 MPG Now, let’s make predictions on the model to predict fuel efficiency: test_predictions = model.predict(normed_test_data).flatten() plt.scatter(test_labels, test_predictions) plt.xlabel('True Values [MPG]') plt.ylabel('Predictions [MPG]') plt.axis('equal') plt.axis('square') plt.xlim([0,plt.xlim()[1]]) plt.ylim([0,plt.ylim()[1]]) _ = plt.plot([-100, 100], [-100, 100])Code language: JavaScript (javascript) Also, Read – The Process of Data Science. It looks like that the model predicted well. I hope you liked this article, to Predict Fuel Efficiency with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Predict Fuel Efficiency with Machine Learning
2020-09-03 13:51:52;ABC analysis assumes that income-generating items in an inventory follow a Pareto distribution, where a very small percentage of items generate the most income. In this article, I’ll walk you through how we can perform ABC analysis with Machine Learning.;https://thecleverprogrammer.com/2020/09/03/abc-analysis-with-machine-learning/;['sklearn'];1.0;['AI'];['AI', 'ML', 'Clustering', 'K-Means', 'Classification'];['clustering', 'k-means', 'fit', 'model', 'machine learning', 'classif', 'rank', 'train', 'label'];"ABC analysis assumes that income-generating items in an inventory follow a Pareto distribution, where a very small percentage of items generate the most income. In this article, I’ll walk you through how we can perform ABC analysis with Machine Learning. Conventions of ABC Analysis Using the conventions of ABC analysis, an inventory item is assigned a letter based on its importance: Also, Read – Moving Averages with Python. A articles represent 20% of articles, but contribute 70% of revenueB-articles represent 30% of articles, but contribute 25% of revenueC articles represent 50% of articles, but contribute 5% of revenue Keep in mind that these numbers are approximate and will vary widely depending on the actual distribution of sales. The main takeaway is that A items make up a small percentage of inventory but contribute the most to income, C items make up a large percentage of inventory but contribute the least to income and B items are somewhere around leaves in the middle. Importance of ABC Analysis Inventory planning and warehousing strategies of an organization rely on ABC analysis to make any key decisions. For example, a warehouse manager typically wants A items closest to the shipping docks to reduce the time it takes to pick them up. This increases productivity and reduces labour costs. ABC Analysis with Machine Learning The data used in this project comes from a popular online retailer dataset. The dataset only includes online sales of clothing throughout the summer. More importantly, it shows the number of units sold and the price sold, which will generate the revenue per item. The dataset can be easily downloaded from here. The goal of this project is to sort all the elements of the dataset into an ABC categorization based on their importance. When viewing the results, there should be relatively few A items that generate the majority of income and a large number of C items that do not generate much income. Data Preparation Now, let’s get started with this task with data preparation. I will start this off by importing the necessary packages and reading the dataset: # Import libraries import pandas as pd import numpy as np # read the data to a dataframe df = pd.read_csv(""Summer_Sales.csv"")Code language: PHP (php) I will add a new column to the data for revenue by simply multiplying the number of units sold by the price. It is possible that the price has changed over time, especially when flash sales have taken place, but without additional data to analyze, it is assumed that all items sold at a single, stable price: df[""revenue""] = df[""units_sold""] * df[""price""]Code language: JavaScript (javascript) Now, lets visualize the revenue by using the seaborn package in python: import seaborn as sns sns.distplot(df[""revenue""])Code language: JavaScript (javascript) The graph above shows the Pareto distribution found in the data. The vast majority of articles generate less than € 200,000 in sales. At the same time, it shows that some of the items sell for between € 400,000 and € 800,000, which is contributing in the majority of the revenue. Now, I’m going to define a function to categorize the amount of income generated by an item into bins, and then I’ll apply it to the data: def bins(x): for bar in range(20000, 820000, 20000): if x &lt;= bar: return bar # Create new column to apply the bin function df[""rev_dist""] = df[""revenue""].apply(lambda x: bins(x))Code language: PHP (php) Now I’m going to create a pivot table to list the number of items that fall into each category: df[""count""] = 1 # Create a pivot table of the revenue distributions pivot_table = pd.pivot_table(df, index = [""rev_dist""], values = [""count""], aggfunc = np.sum)Code language: PHP (php) Applying Machine Learning Algorithm To properly train the model, it is not enough to just look at the income generated by each item. He must also know how income is distributed. This pivot table provides a very manageable data set that the model can train on. I will use the K-Means Clustering algorithm for this task of ABC Analysis: # import model from SKLearn from sklearn.cluster import KMeans # K -clusters is equal to 3 because things will be sorted into A, B, and C kmeans = KMeans(n_clusters=3) kmeans.fit(pivot_table)Code language: PHP (php) I will now add a new column to the pivot table giving the classification of the model. It should be noted that by default, scikit-learn’s K-means algorithm will rank items on a numeric scale instead of the alphabetical scale used in the ABC analysis. Therefore, each row will be labelled as zero, one, or two: pivot_table[""category""] = kmeans.labels_Code language: JavaScript (javascript) Now, I will define a new dictionary to classify each row for the task of ABC analysis: ABC_dict = { 0: ""A"", 1: ""C"", 2: ""B"" } pivot_table[""ABC""] = pivot_table[""category""].apply(lambda x: ABC_dict[x])Code language: JavaScript (javascript) Now, remember that the model was trained on a pivot table. The elements have not yet been assigned an ABC classification. Instead, it was assigned an income classification: df = pd.merge(df, pivot_table, on = ""rev_dist"", how =""left"")Code language: JavaScript (javascript) This means that while we don’t immediately know which items fall into Category A, we do know that some income classifications are classified as A Items. As a result, we can just merge the main data frame and the PivotTable to give each item its ABC classification. When analyzing the final distribution of the elements, it was found that: A-items represent 11.4% of articles, but 61.7% of turnoverB-items represent 20.5% of items, but 30.7% of turnoverC articles represent 68.1% of articles, but 7.6% of turnover Also, Read – Edge AI in Machine Learning. I hope you liked this article on ABC analysis with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";ABC Analysis with Machine Learning
2020-09-04 12:40:12;By the end of this article, you will understand the concepts of overfitting and underfitting in machine learning, and you can also apply these concepts to train your machine learning models more accurately.;https://thecleverprogrammer.com/2020/09/04/overfitting-and-underfitting-in-machine-learning/;['sklearn'];1.0;[];['ML', 'NN'];['predict', 'fit', 'model', 'machine learning', 'neural network', 'filter', 'training data', 'train', 'validation data'];"By the end of this article, you will understand the concepts of overfitting and underfitting in machine learning, and you can also apply these concepts to train your machine learning models more accurately. What are Overfitting and Underfitting in Machine Learning? Overfitting: Let’s say you are visiting a foreign country and the taxi driver scams you. You might be tempted to say that all the taxi drivers in this country are thieves. Overgeneralization is something we humans do too often, and unfortunately, machines can fall into the same trap if we’re not careful. In machine learning, this is called overfitting: it means that the model works well on the training data, but it does not generalize well. Also, Read – The Best Laptop for Machine Learning. Overfitting occurs when the model is too complex for the amount and noise of the training data. Here are the possible solutions: Simplify the model by selecting a model with fewer parameters (for example, a linear model rather than a high degree polynomial model), reducing the number of attributes in the training data, or constraining the model.Collect more training data.Reduce noise in training data (for example, correct data errors and remove outliers). Underfitting: As you can guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. Here are the main options for solving the problem of underfitting: Select a more powerful model, with more parameters.Bring better features to the learning algorithm (feature engineering).Reduce the constraints on the model (for example, reduce the regularization hyperparameter). While training a Machine Learning model we care more about the accuracy of the performance of our trained model on new data, which we can estimate from the validation set, the idea is to strike a balance between overfitting and underfitting. Handling Overfitting and Underfitting I will consider a case study to take you through how we can practically handle Overfitting and Underfitting with Machine Learning. Case Study: There are very fewer alternatives for controlling the depth of the tree, and many allow certain routes through the tree to have greater depth than other routes. But the max_leaf_nodes argument provides a very good way to control overfitting versus underfitting. The more you will allow the model to make predictions, the more you will go from the area of ​​underfitting in the above diagram to the area of ​​overfitting. Now let’s see how we can solve this problem of overfitting and underfitting with machine learning code. I’ll be using a utility function to help compare MAE scores of different values ​​for max_leaf_nodes: from sklearn.metrics import mean_absolute_error from sklearn.tree import DecisionTreeRegressor def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y): model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0) model.fit(train_X, train_y) preds_val = model.predict(val_X) mae = mean_absolute_error(val_y, preds_val) return(mae)Code language: JavaScript (javascript) The dataset I am using here can be easily downloaded from here. Now I will load the data into train_X, val_X, train_y and val_y: import pandas as pd # Load data melbourne_data = pd.read_csv(""melb_data"") # Filter rows with missing values filtered_melbourne_data = melbourne_data.dropna(axis=0) # Choose target and features y = filtered_melbourne_data.Price melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude'] X = filtered_melbourne_data[melbourne_features] from sklearn.model_selection import train_test_split # split data into training and validation data, for both features and target train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)Code language: PHP (php) We can now use a for loop to compare the precision or accuracy rate of models built with different values ​​for max_leaf_nodes: # compare MAE with differing values of max_leaf_nodes for max_leaf_nodes in [5, 50, 500, 5000]: my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y) print(""Max leaf nodes: %d \t\t Mean Absolute Error: %d"" %(max_leaf_nodes, my_mae))Code language: PHP (php) Max leaf nodes: 5 Mean Absolute Error: 347380 Max leaf nodes: 50 Mean Absolute Error: 258171 Max leaf nodes: 500 Mean Absolute Error: 243495 Max leaf nodes: 5000 Mean Absolute Error: 254983 Also, Read – 8 Neural Networks Projects for Machine Learning. I hope you liked this article on the concepts of Overfitting and Underfitting in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Overfitting and Underfitting in Machine Learning
2020-09-04 17:38:57;XGBoost or Gradient Boosting is a machine learning algorithm that goes through cycles to iteratively add models to a set. In this article, I will take you through the XGBoost algorithm in Machine Learning.The cycle of the XGBoost algorithm begins by initializing the whole with a unique model, the predictions of which can be quite naive.Also, Read – Overfitting and Underfitting in Machine Learning.The Process of XGBoost Algorithm:;https://thecleverprogrammer.com/2020/09/04/xgboost-in-machine-learning/;['sklearn', 'xgboost'];1.0;[];['ML', 'Gradient Boosting'];['predict', 'fit', 'model', 'loss', 'machine learning', 'training data', 'train', 'validation data', 'gradient boosting', 'test data'];"XGBoost or Gradient Boosting is a machine learning algorithm that goes through cycles to iteratively add models to a set. In this article, I will take you through the XGBoost algorithm in Machine Learning. The cycle of the XGBoost algorithm begins by initializing the whole with a unique model, the predictions of which can be quite naive. Also, Read – Overfitting and Underfitting in Machine Learning. The Process of XGBoost Algorithm: First, we use the current set to generate predictions for each observation in the dataset. To make a prediction, we add the predictions of all the models in the set.These predictions are used to calculate a loss function.Then we use the loss function to fit a new model which will be added to the set. Specifically, we determine the parameters of the model so that adding this new model to the set reduces the loss.Finally, we add the new model to the set, and …then repeat! XGBoost Algorithm in Action I’ll start by loading the training and validation data into X_train, X_valid, y_train and y_valid. The dataset, I am using here can be easily downloaded from here. import pandas as pd from sklearn.model_selection import train_test_split # Read the data data = pd.read_csv('melb_data.csv') # Select subset of predictors cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt'] X = data[cols_to_use] # Select target y = data.Price # Separate data into training and validation sets X_train, X_valid, y_train, y_valid = train_test_split(X, y)Code language: PHP (php) Now, here you will learn how to use the XGBoost algorithm. Here we need to import the scikit-learn API for XGBoost (xgboost.XGBRegressor). This allows us to create and adjust a model like we would in scikit-learn. As you will see in the output, the XGBRegressor class has many adjustable parameters: from xgboost import XGBRegressor my_model = XGBRegressor() my_model.fit(X_train, y_train)Code language: JavaScript (javascript) XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type='gain', learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) Now, we need to make predictions and evaluate our model: from sklearn.metrics import mean_absolute_error predictions = my_model.predict(X_valid) print(""Mean Absolute Error: "" + str(mean_absolute_error(predictions, y_valid)))Code language: JavaScript (javascript) Mean Absolute Error: 280355.04334039026 Parameter Tuning XGBoost has a few features that can drastically affect the accuracy and speed of training. The first feature you need to understand are: n_estimators n_estimators specifies the number of times to skip the modelling cycle described above. It is equal to the number of models we include in the set. Too low a value results in an underfitting, leading to inaccurate predictions on training data and test data.Too high a value results in overfitting, resulting in accurate predictions on training data, but inaccurate predictions on test data (which is important to us). Typical the values ​​lie between 100 to 1000, although it all depends a lot on the learning_rate parameter described below. Here is the code to set the number of models in the set: my_model = XGBRegressor(n_estimators=500) my_model.fit(X_train, y_train) XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type='gain', learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=500, n_jobs=1, nthread=None, objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) early_stopping_rounds early_stopping_rounds provides a way to automatically find the ideal value for n_estimators. Stopping early causes the iteration of the model to stop when the validation score stops improving, even though we are not stopping hard for n_estimators. It’s a good idea to set n_estimators high and then use early_stopping_rounds to find the optimal time to stop the iteration. Since random chance sometimes causes a single round where validation scores do not improve, you must specify a number for the number of direct deterioration turns to allow before stopping. Setting early_stopping_rounds = 5 is a reasonable choice. In this case, we stop after 5 consecutive rounds of deterioration of validation scores. Now let’s see how we can use early_stopping: my_model = XGBRegressor(n_estimators=500) my_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)Code language: PHP (php) XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type='gain', learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=500, n_jobs=1, nthread=None, objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) learning_rate Instead of getting predictions by simply adding up the predictions of each component model, we can multiply the predictions of each model by a small number before adding them. This means that every tree we add to the set helps us less. So we can set a high value for the n_estimators without overfitting. If we use early shutdown, the appropriate number of trees will be determined automatically. Now, let’s see how we can use learning_rate in XGBoost algorithm: my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05) my_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)Code language: PHP (php) XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type='gain', learning_rate=0.05, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1, nthread=None, objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) n_jobs On larger datasets where execution is a consideration, you can use parallelism to build your models faster. It is common to set the n_jobs parameter equal to the number of cores on your machine. On smaller data sets, this won’t help. The resulting model will not be better, so micro-optimizing the timing of the fit is usually just a distraction. But it’s very useful in large datasets where you would spend a lot of time waiting for the fit command. Now, let’s see how to use this parameter in the XGBoost algorithm: my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4) my_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)Code language: PHP (php) XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type='gain', learning_rate=0.05, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=1000, n_jobs=4, nthread=None, objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) Also, Read – The Best Laptop for Machine Learning. XGBoost is a leading software library for working with standard tabular data with fine-tuning of parameters, you can train very precise models. I hope you liked this article on the XGBoost algorithm in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";XGBoost in Machine Learning
2020-09-05 14:55:55;In this article, you will learn what data leakage is and how to avoid it. If you don’t know how to prevent it, leaks will frequently occur and ruin your models in subtle and dangerous ways. It is, therefore, one of the most important concepts for all machine learning practitioners.;https://thecleverprogrammer.com/2020/09/05/data-leakage-in-machine-learning/;['sklearn'];1.0;[];['ML', 'Classification'];['detect', 'predict', 'model', 'machine learning', 'classif', 'training data', 'train', 'validation data'];"In this article, you will learn what data leakage is and how to avoid it. If you don’t know how to prevent it, leaks will frequently occur and ruin your models in subtle and dangerous ways. It is, therefore, one of the most important concepts for all machine learning practitioners. What is Data Leakage? Data leakage generally occurs when our training data is fed with the information about the target, but similar data is available when the model is used in predictions. This leads to high performance on the drive assembly, but the model will perform poorly in production. In simple words, data leakage makes a machine learning model look very precise until you start making predictions with the model and then the model becomes very inaccurate. Data Leakage is of two types: target leakage and train-test contamination. Target leakage A target leak occurs when your predictors include data that will not be available at the time you make the predictions. It’s important to think of the target leak in terms of the timing or chronological order of data availability, and not just whether a feature makes good predictions. Train-Test Contamination A different type of leak occurs when you are not careful to distinguish training data from validation data. Validation is meant to be a measure of how well the model performs on data it has not previously considered. You can subtly corrupt this process if the validation data affects preprocessing behaviour. This is referred to as train-test contamination. Data Leakage in Action Here you will learn one way to detect and remove target leaks. I will use credit card apps dataset and ignore the master data setup code. The result is that the information about each credit card application is stored in an X DataFrame. I will use it to predict which applications have been accepted in a y series. You can download the dataset from here: import pandas as pd # Read the data data = pd.read_csv('AER_credit_card_data.csv', true_values = ['yes'], false_values = ['no']) # Select target y = data.card # Select predictors X = data.drop(['card'], axis=1) print(""Number of rows in the dataset:"", X.shape[0]) X.head()Code language: PHP (php) Number of rows in the dataset: 1319 reports	age	income	share	expenditure	owner	selfemp	dependents	months	majorcards	active 0	0	37.66667	4.5200	0.033270	124.983300	True	False	3	54	1	12 1	0	33.25000	2.4200	0.005217	9.854167	False	False	3	34	1	13 2	0	33.66667	4.5000	0.004156	15.000000	True	False	4	58	1	5 3	0	30.50000	2.5400	0.065214	137.869200	False	False	0	25	1	7 4	0	32.16667	9.7867	0.067051	546.503300	True	False	2	64	1	5 Since this is a small dataset, I will use cross-validation to ensure accurate measures of model quality: from sklearn.pipeline import make_pipeline from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score # Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!) my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100)) cv_scores = cross_val_score(my_pipeline, X, y, cv=5, scoring='accuracy') print(""Cross-validation accuracy: %f"" % cv_scores.mean())Code language: PHP (php) Cross-validation accuracy: 0.979525 With experience, you will find that it is very rare to find accurate models for 98% of the time. It does happen, but it’s quite rare that we have to inspect the data more closely to detect any target leaks. Here is a summary of the data, that you will observe: card: 1 if credit card application accepted, 0 if not reports: Number of major derogatory reports age: Age n years plus twelfths of a year income: Yearly income (divided by 10,000) share: Ratio of monthly credit card expenditure to yearly income expenditure: Average monthly credit card expenditure owner: 1 if owns home, 0 if rents selfempl: 1 if self-employed, 0 if not dependents: 1 + number of dependents months: Months living at current address majorcards: Number of major credit cards held active: Number of active credit accounts Some variables seem suspicious. For example, does an expense mean an expense on this card or cards used before the application? At this point, baseline data comparisons can be very helpful: expenditures_cardholders = X.expenditure[y] expenditures_noncardholders = X.expenditure[~y] print('Fraction of those who did not receive a card and had no expenditures: %.2f' %((expenditures_noncardholders == 0).mean())) print('Fraction of those who received a card and had no expenditures: %.2f' %(( expenditures_cardholders == 0).mean()))Code language: PHP (php) Fraction of those who did not receive a card and had no expenditures: 1.00 Fraction of those who received a card and had no expenditures: 0.02 As noted above, all of those who did not receive a card had no spending, while only 2% of those who received a card had no spending. It is not surprising that our model appears to have high accuracy. But it also appears to be a case of goal leakage, where spending likely means spending on the card they requested. Since the share is partly determined by expenditure, it should also be excluded. The active and major variables are a little less clear, but from the description, they look worrisome. In most of the situations, it’s better to play safe than sorry if you can’t track down the people who created the data to find out more. I will run a model with no target leak as follows: # Drop leaky predictors from dataset potential_leaks = ['expenditure', 'share', 'active', 'majorcards'] X2 = X.drop(potential_leaks, axis=1) # Evaluate the model with leaky predictors removed cv_scores = cross_val_score(my_pipeline, X2, y, cv=5, scoring='accuracy') print(""Cross-val accuracy: %f"" % cv_scores.mean())Code language: PHP (php) Cross-val accuracy: 0.830924 This accuracy is a bit lower, which can be disappointing. However, we can expect it to be correct about 80% of the time when used on new applications when the leaky model would likely do a lot worse than that. Also, Read – XGBoost Algorithm in Machine Learning. Data Leakage can be a million-dollar mistake in many Machine Learning tasks. I hope you liked this article on how to handle data leakage in machine learning tasks. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Data Leakage in Machine Learning
2020-09-06 13:44:59;In this article, I will introduce you to the application of Machine Learning in healthcare. I will show you how we can work on the lung segmentation task with machine learning using python.;https://thecleverprogrammer.com/2020/09/06/lung-segmentation-with-machine-learning/;['skimage'];1.0;['NN', 'CNN'];['ML', 'Image Segmentation', 'NN', 'CNN'];['detect', 'label', 'machine learning', 'image segmentation'];"In this article, I will introduce you to the application of Machine Learning in healthcare. I will show you how we can work on the lung segmentation task with machine learning using python. Lung Segmentation with Machine Learning Lung segmentation is one of the most useful tasks of machine learning in healthcare. Lung CT image segmentation is an initial step necessary for lung image analysis, it is a preliminary step to provide accurate lung CT image analysis such as detection of lung cancer. Also, Read – Cross-Validation in Machine Learning. Now let’s see how we can use machine learning for the lung segmentation task. Before we start, I’ll import a few packages and determine the available patients: import numpy as np import pandas as pd import dicom import os import scipy.ndimage import matplotlib.pyplot as plt from skimage import measure, morphology from mpl_toolkits.mplot3d.art3d import Poly3DCollection # Some constants INPUT_FOLDER = 'path to sample_images' patients = os.listdir(INPUT_FOLDER) patients.sort()Code language: PHP (php) Loading Data for Lung Segmentation Dicom is the de-facto repository in medical imaging. This is my first time working with it, but it seems pretty straightforward. These files contain a lot of metadata. This analysis pixel size/coarseness differs from analysis to analysis, which can adversely affect the performance of CNN approaches. Below is the code to load an analysis, which consists of multiple slices, which we simply save in a Python list. Each record in the dataset is an analysis. A metadata field is missing, the size of the pixels in the Z direction, which is the thickness of the slice. Fortunately, we can infer it and add it to the metadata: def load_scan(path): slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)] slices.sort(key = lambda x: float(x.ImagePositionPatient[2])) try: slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2]) except: slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation) for s in slices: s.SliceThickness = slice_thickness return slicesCode language: PHP (php) The unit of measure for CT scans is the Hounsfield Unit (HU), which is a measure of radiodensity. CT scanners are carefully calibrated to measure this accurately. From Wikipedia: Source – Wikipedia By default, the returned values ​​are not in this unit. We first need to fix this. Some scanners have cylindrical scan limits, but the output image is square. Pixels that go outside these limits get the fixed value -2000. The first step is to set these values ​​to 0, which is currently air. Then back to HU units, multiplying by the rescaling slope and adding the intercept: def get_pixels_hu(slices): image = np.stack([s.pixel_array for s in slices]) # Convert to int16 (from sometimes int16), # should be possible as values should always be low enough (&amp;lt;32k) image = image.astype(np.int16) # Set outside-of-scan pixels to 0 # The intercept is usually -1024, so air is approximately 0 image[image == -2000] = 0 # Convert to Hounsfield units (HU) for slice_number in range(len(slices)): intercept = slices[slice_number].RescaleIntercept slope = slices[slice_number].RescaleSlope if slope != 1: image[slice_number] = slope * image[slice_number].astype(np.float64) image[slice_number] = image[slice_number].astype(np.int16) image[slice_number] += np.int16(intercept) return np.array(image, dtype=np.int16)Code language: PHP (php) Now let’s take a look at one of the patients: first_patient = load_scan(INPUT_FOLDER + patients[0]) first_patient_pixels = get_pixels_hu(first_patient) plt.hist(first_patient_pixels.flatten(), bins=80, color='c') plt.xlabel(""Hounsfield Units (HU)"") plt.ylabel(""Frequency"") plt.show() # Show some slice in the middle plt.imshow(first_patient_pixels[80], cmap=plt.cm.gray) plt.show()Code language: PHP (php) By looking at the information of Lung CT measurements from Wikipedia and the histogram above, we can see which pixels are air and which are tissue. We will use this for the lung segmentation task later. Resampling A CT scan normally has a pixel spacing of [2.5, 0.5, 0.5], which means that the distance between the slices is 2.5 millimetres. For a different analysis, this can be [1,5, 0,725, 0,725], it can be problematic for an automatic analysis (eg using ConvNets). A common method of solving this problem is to resample the entire data set to a certain isotropic resolution: def resample(image, scan, new_spacing=[1,1,1]): # Determine current pixel spacing spacing = np.array([scan[0].SliceThickness] + scan[0].PixelSpacing, dtype=np.float32) resize_factor = spacing / new_spacing new_real_shape = image.shape * resize_factor new_shape = np.round(new_real_shape) real_resize_factor = new_shape / image.shape new_spacing = spacing / real_resize_factor image = scipy.ndimage.interpolation.zoom(image, real_resize_factor, mode='nearest') return image, new_spacingCode language: PHP (php) When you apply this, to the whole dataset due to rounding, this may be slightly off from the desired spacing. We need to resample our patient’s pixels to an isomorphic resolution of 1 by 1 by 1 mm: pix_resampled, spacing = resample(first_patient_pixels, first_patient, [1,1,1]) print(""Shape before resampling\t"", first_patient_pixels.shape) print(""Shape after resampling\t"", pix_resampled.shape)Code language: PHP (php) Shape before resampling (134, 512, 512) Shape after resampling (335, 306, 306) 3D Plotting of Lungs For visualization, it is useful to be able to display a 3D image of the scan. So we’re going to use walking cubes to create a rough mesh for our 3D object, and the plot that with matplotlib: def plot_3d(image, threshold=-300): # Position the scan upright, # so the head of the patient would be at the top facing the camera p = image.transpose(2,1,0) verts, faces = measure.marching_cubes(p, threshold) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection='3d') # Fancy indexing: `verts[faces]` to generate a collection of triangles mesh = Poly3DCollection(verts[faces], alpha=0.70) face_color = [0.45, 0.45, 0.75] mesh.set_facecolor(face_color) ax.add_collection3d(mesh) ax.set_xlim(0, p.shape[0]) ax.set_ylim(0, p.shape[1]) ax.set_zlim(0, p.shape[2]) plt.show() Code language: PHP (php) Our plot function takes an argument which we can use to plot structures, such as all tissue or only bone. 400 is a good threshold to show only bones: plot_3d(pix_resampled, 400) Lung segmentation In order to reduce the problem space, we can segment the lungs (and usually certain tissues around it). The method that my fellow students and I developed was quite effective. It involves several smart steps. It consists of a series of regional growth applications and morphological operations. In this case, we will only use the analysis of connected components. The steps: Image Threshold (-320 HU is a good threshold, but whatever for this approach)Make connected components, determine the label of the air around the person, fill it with 1s in the binary imageOptional: For each axial section of the scan, determine the largest connected solid component (the body + air around the person) and set the others to 0. This fills the lung structures of the mask.Keep only the largest air pocket (the human body has other air pockets here and there). def largest_label_volume(im, bg=-1): vals, counts = np.unique(im, return_counts=True) counts = counts[vals != bg] vals = vals[vals != bg] if len(counts) &amp;gt; 0: return vals[np.argmax(counts)] else: return None def segment_lung_mask(image, fill_lung_structures=True): # not actually binary, but 1 and 2. # 0 is treated as background, which we do not want binary_image = np.array(image &amp;gt; -320, dtype=np.int8)+1 labels = measure.label(binary_image) # Pick the pixel in the very corner to determine which label is air. # Improvement: Pick multiple background labels from around the patient # More resistant to ""trays"" on which the patient lays cutting the air # around the person in half background_label = labels[0,0,0] #Fill the air around the person binary_image[background_label == labels] = 2 # Method of filling the lung structures (that is superior to something like # morphological closing) if fill_lung_structures: # For every slice we determine the largest solid structure for i, axial_slice in enumerate(binary_image): axial_slice = axial_slice - 1 labeling = measure.label(axial_slice) l_max = largest_label_volume(labeling, bg=0) if l_max is not None: #This slice contains some lung binary_image[i][labeling != l_max] = 1 binary_image -= 1 #Make the image actual binary binary_image = 1-binary_image # Invert it, lungs are now 1 # Remove other air pockets insided body labels = measure.label(binary_image, background=0) l_max = largest_label_volume(labels, bg=0) if l_max is not None: # There are air pockets binary_image[labels != l_max] = 0 return binary_imageCode language: PHP (php) But there is one thing we can fix, it’s probably a good idea to include structures in the lungs (like the nodules are solid), we don’t just want to ventilate in the lungs: segmented_lungs = segment_lung_mask(pix_resampled, False) segmented_lungs_fill = segment_lung_mask(pix_resampled, True) plot_3d(segmented_lungs, 0) Code language: PHP (php) It’s better. Let’s also visualize the difference between the two: plot_3d(segmented_lungs_fill - segmented_lungs, 0) Also, Read – Data Leakage in Machine Learning. I hope you liked this article on the Lung Segmentation as an application of Machine Learning on Healthcare. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Lung Segmentation with Machine Learning
2020-09-08 14:27:09;In this article, I will take you through a real-world task of Machine Learning task to predict the migration of humans between countries. Human migration is a type of human mobility, where a journey involves a person moving to change their domicile. Predicting human migration as accurately as possible is important in city planning applications, international trade, the spread of infectious diseases, conservation planning, and public policymaking.Also, Read – Build a Genetic Algorithm with Python.;https://thecleverprogrammer.com/2020/09/08/predict-migration-with-machine-learning/;['sklearn'];1.0;[];['ML'];['predict', 'fit', 'machine learning', 'train', 'label'];"In this article, I will take you through a real-world task of Machine Learning task to predict the migration of humans between countries. Human migration is a type of human mobility, where a journey involves a person moving to change their domicile. Predicting human migration as accurately as possible is important in city planning applications, international trade, the spread of infectious diseases, conservation planning, and public policymaking. Also, Read – Build a Genetic Algorithm with Python. Predict Migration with Machine Learning I will start this task to predict migration by importing all the necessary libraries: import pandas as pd from sklearn.cross_validation import train_test_split from sklearn import svm import seaborn as sns import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error import numpy as np from sklearn.naive_bayes import GaussianNBCode language: JavaScript (javascript) The dataset, I am using in this task to predict migration can be easily downloaded from here. Let’s see what the data looks like: I’d like to turn your attention to the “Measure”, “Country” and “CitizenShip” column. If we want to get a prediction result, we need to convert all of these string values ​​to an integer: data = pd.read_csv('migration_nz.csv') data.head(10)Code language: JavaScript (javascript) But first, let’s see the unique values ​​we have in the “Measure” column: data['Measure'].unique()Code language: CSS (css) array(['Arrivals', 'Departures', 'Net'], dtype=object) Now we need to give each unique string value its unique integer value: in case there are not that many values, it is possible to use the “replace” function: data['Measure'].replace(""Arrivals"",0,inplace=True) data['Measure'].replace(""Departures"",1,inplace=True) data['Measure'].replace(""Net"",2,inplace=True)Code language: PHP (php) Now let’s check if everything has been correctly assigned: data['Measure'].unique()Code language: CSS (css) array([0, 1, 2]) In this case, we have about 250 unique countries: data['Country'].unique()Code language: CSS (css) array(['Oceania', 'Antarctica', 'American Samoa', 'Australia', 'Cocos Islands', 'Cook Islands', 'Christmas Island', 'Fiji', 'Micronesia', 'Guam', 'Kiribati', 'Marshall Islands', 'Northern Mariana Islands', 'New Caledonia', 'Norfolk Island', 'Nauru', 'Niue', 'New Zealand', 'French Polynesia', 'Papua New Guinea', 'Pitcairn Island', 'Palau', 'Solomon Islands', 'French Southern Territories', 'Tokelau', 'Tonga', 'Tuvalu', 'Vanuatu', 'Wallis and Futuna', 'Samoa', 'Asia', 'Afghanistan', 'Armenia', 'Azerbaijan', 'Bangladesh', 'Brunei Darussalam', 'Bhutan', 'China', 'Georgia', 'Hong Kong', 'Indonesia', 'India', 'Japan', 'Kyrgyzstan', 'Cambodia', 'North Korea', 'South Korea', 'Kazakhstan', 'Laos', 'Sri Lanka', 'Myanmar', 'Mongolia', 'Macau', 'Maldives', 'Malaysia', 'Nepal', 'Philippines', 'Pakistan', 'Singapore', 'Thailand', 'Tajikistan', 'Timor-Leste', 'Turkmenistan', 'Taiwan', 'Uzbekistan', 'Vietnam', 'Europe', 'Andorra', 'Albania', 'Austria', 'Bosnia and Herzegovina', 'Belgium', 'Bulgaria', 'Belarus', 'Switzerland', 'Czechoslovakia', 'Cyprus', 'Czechia', 'East Germany', 'Germany', 'Denmark', 'Estonia', 'Spain', 'Finland', 'Faeroe Islands', 'France', 'UK', 'Gibraltar', 'Greenland', 'Greece', 'Croatia', 'Hungary', 'Ireland', 'Iceland', 'Italy', 'Kosovo', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Latvia', 'Monaco', 'Moldova', 'Montenegro', 'Macedonia', 'Malta', 'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania', 'Serbia', 'Russia', 'Sweden', 'Slovenia', 'Slovakia', 'San Marino', 'USSR', 'Ukraine', 'Vatican City', 'Yugoslavia/Serbia and Montenegro', 'Americas', 'Antigua and Barbuda', 'Anguilla', 'Netherlands Antilles', 'Argentina', 'Aruba', 'Barbados', 'Bermuda', 'Bolivia', 'Brazil', 'Bahamas', 'Belize', 'Canada', 'Chile', 'Colombia', 'Costa Rica', 'Cuba', 'Curacao', 'Dominica', 'Dominican Republic', 'Ecuador', 'Falkland Islands', 'Grenada', 'French Guiana', 'Guadeloupe', 'South Georgia and the South Sandwich Islands', 'Guatemala', 'Guyana', 'Honduras', 'Haiti', 'Jamaica', 'St Kitts and Nevis', 'Cayman Islands', 'St Lucia', 'Martinique', 'Montserrat', 'Mexico', 'Nicaragua', 'Panama', 'Peru', 'St Pierre and Miquelon', 'Puerto Rico', 'Paraguay', 'Suriname', 'El Salvador', 'St Maarten', 'Turks and Caicos', 'Trinidad and Tobago', 'US Minor Outlying Islands', 'USA', 'Uruguay', 'St Vincent and the Grenadines', 'Venezuela', 'British Virgin Islands', 'US Virgin Islands', 'Africa and the Middle East', 'UAE', 'Angola', 'Burkina Faso', 'Bahrain', 'Burundi', 'Benin', 'Botswana', 'Democratic Republic of the Congo', 'Central African Republic', 'Congo', ""Cote d'Ivoire"", 'Cameroon', 'Cape Verde', 'Djibouti', 'Algeria', 'Egypt', 'Western Sahara', 'Eritrea', 'Ethiopia', 'Gabon', 'Ghana', 'Gambia', 'Guinea', 'Equatorial Guinea', 'Guinea-Bissau', 'Israel', 'British Indian Ocean Territory', 'Iraq', 'Iran', 'Jordan', 'Kenya', 'Comoros', 'Kuwait', 'Lebanon', 'Liberia', 'Lesotho', 'Libya', 'Morocco', 'Madagascar', 'Mali', 'Mauritania', 'Mauritius', 'Malawi', 'Mozambique', 'Namibia', 'Niger', 'Nigeria', 'Oman', 'Palestine', 'Qatar', 'Reunion', 'Rwanda', 'Saudi Arabia', 'Seychelles', 'Sudan', 'St Helena', 'Sierra Leone', 'Senegal', 'Somalia', 'South Sudan', 'Sao Tome and Principe', 'Syria', 'Swaziland', 'Chad', 'Togo', 'Tunisia', 'Turkey', 'Tanzania', 'Uganda', 'South Yemen', 'Yemen', 'Mayotte', 'South Africa', 'Zambia', 'Zimbabwe', 'Not stated', 'All countries'], dtype=object) Now we need to assign each unique string value its unique integer value: data['CountryID'] = pd.factorize(data.Country)[0] data['CitID'] = pd.factorize(data.Citizenship)[0]Code language: JavaScript (javascript) Now, let’s see if everything is okay: data['CountryID'].unique()Code language: CSS (css) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]) Another problem is that we have some missing values, let’s see how many and where exactly they are: data.isnull().sum()Code language: CSS (css) Measure 0 Country 0 Citizenship 0 Year 0 Value 72 CountryID 0 CitID 0 dtype: int64 Now, I will simply fill these missing values with the median values: data[""Value""].fillna(data[""Value""].median(),inplace=True)Code language: PHP (php) Now, let’s see if everything is fine so far: data.isnull().sum()Code language: CSS (css) Measure 0 Country 0 Citizenship 0 Year 0 Value 0 CountryID 0 CitID 0 dtype: int64 Split The Data into Train and Test sets Now, I will split the data into 70 per cent training and 30 per cent test set: data.drop('Country', axis=1, inplace=True) data.drop('Citizenship', axis=1, inplace=True) from sklearn.cross_validation import train_test_split X= data[['CountryID','Measure','Year','CitID']].as_matrix() Y= data['Value'].as_matrix() X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.3, random_state=9)Code language: JavaScript (javascript) Predict Migration Now, let’s predict migration using our Machine Learning algorithm and visualize the results: from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(n_estimators=70,max_features = 3,max_depth=5,n_jobs=-1) rf.fit(X_train ,y_train) rf.score(X_test, y_test)Code language: JavaScript (javascript) 0.73654599831394985 X = data[['CountryID','Measure','Year','CitID']] Y = data['Value'] X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.3, random_state=9) grouped = data.groupby(['Year']).aggregate({'Value' : 'sum'}) #Growth of migration to New-Zeland by year grouped.plot(kind='line');plt.axhline(0, color='g') sns.plt.show()Code language: PHP (php) grouped.plot(kind='bar');plt.axhline(0, color='g') sns.plt.show()Code language: JavaScript (javascript) import seaborn as sns corr = data.corr() sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values) sns.plt.show()Code language: JavaScript (javascript) Also, Read – What is BigQuery in Data Science? I hope you liked this article of a simple real-world task based on how to predict the migration of humans between countries. I hope you liked this article on predicting migrations with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Predict Migration with Machine Learning
2020-09-09 14:04:57;Natural Language Processing (NLP) is a great task in Machine Learning to work with languages. However, you must have seen everyone working with only in the English language while working on a task of NLP. So what about other languages that we have. In this article, I will take you through NLP for other Languages with Machine Learning.Everyone knows India is a very diverse country and a hotbed of many languages, but did you know India speaks 780 languages. It’s time to move beyond English when it comes to NLP. This article is intended for those who know a little about NLP and want to start using NLP for other languages.Also, Read – Analyze Healthcare Data with Python.;https://thecleverprogrammer.com/2020/09/09/nlp-for-other-languages-with-machine-learning/;['nltk'];1.0;['NLP'];['ML', 'NLP', 'Chatbot', 'Text Classification', 'Classification', 'Machine Translation'];['text classification', 'predict', 'fit', 'model', 'machine learning', 'classif', 'chatbot', 'natural language processing', 'machine translation'];"Natural Language Processing (NLP) is a great task in Machine Learning to work with languages. However, you must have seen everyone working with only in the English language while working on a task of NLP. So what about other languages that we have. In this article, I will take you through NLP for other Languages with Machine Learning. Everyone knows India is a very diverse country and a hotbed of many languages, but did you know India speaks 780 languages. It’s time to move beyond English when it comes to NLP. This article is intended for those who know a little about NLP and want to start using NLP for other languages. Also, Read – Analyze Healthcare Data with Python. NLP for Other Languages Before we get into the task of NLP for other languages, let’s take a look at some essential concepts and recent achievements in NLP. NLP helps computers understand human language. Text classification, information extraction, semantic analysis, question answering, text synthesis, machine translation and chatbots are some applications of NLP. For computers to understand human language, we must first represent words in digital form. The digitally represented words can then be used by machine learning models to perform any NLP task. Traditionally, methods like One Hot Encoding, TF-IDF Representation have been used to describe the text as numbers. But traditional methods have resulted in sparse representation by not grasping the meaning of the word. Neural Word Embeddings then came to the rescue by solving the problems in traditional ways. Word2Vec and GloVe are the two most commonly used word embedding elements. These methods have resulted in dense representations where words with similar meanings will have similar representations. A significant weakness of this method is that the words are considered to have only one meaning. But we know that a word can have many meanings depending on the context in which it is used. NLP has leapt forward in the modern family of language models. The incorporation of words is no longer independent of the context. The same word can have multiple digital representations depending on the context in which it is used. BERT, Elmo, ULMFit, GPT-2 are currently popular language models. The last generation is so good and some people see it as dangerous. The information written by these linguistic models was even deemed as credible as the New York Times by readers. NLP for Other Languages in Action I will now get into the task of NLP for other languages ​​by getting the integration of words for Indian languages. The digital representation of words plays a role in any NLP task. We are going to use the iNLTK (Natural Language Toolkit for Indic Languages) library. You can easily install the iNLTK library by using the pip command: pip install inltk. The Languages provided by inltk library are given below: Using iNLTK we can quickly get the embedding vectors for the sentences written in Indian languages. Below is an example that shows how to get the integration vectors for a sentence written in Hindi. The given sentence will be divided into tokens, and each token will be represented using a vector. A token can be a word or a subword. Since tokens can be subwords, we can also get meaningful vector representation for rare words. Let’s see how to use inltk library for NLP for other languages: from inltk.inltk import setup from inltk.inltk import tokenize from inltk.inltk import get_embedding_vectors setup('hi') example_sent = ""बहुत समय से मिले नहीं"" # Tokenize the sentence example_sent_tokens = tokenize(example_sent,'hi') # Get the embedding vector for each token example_sent_vectors = get_embedding_vectors(example_sent, 'hi') print(""Tokens:"", example_sent_tokens) print(""Number of vectors:"", len(example_sent_vectors)) print(""Shape of each vector:"", len(example_sent_vectors[0]))Code language: PHP (php) Output: Tokens: ['▁बहुत', '▁समय', '▁से', '▁मिले', '▁नहीं '] Number of vectors: 5 Shape of each vector: 400 We have got the word embeddings in the output above. Next is NLP for Indian languages. Multiple NLP Tasks for Indic Languages Numerically represented natural language can be used by machine learning models to perform many NLP tasks. Apart from that, we can immediately use iNLTK for many NLP tasks. In the example below, we will use iNLTK to predict the next n-words and get similar sentences. For an entry like “It’s been a while since we last met” in Tamil, we get a prediction for the next word like “And, because of this”. And the results for a similar sentence task are also impressive: from inltk.inltk import setup from inltk.inltk import predict_next_words from inltk.inltk import get_similar_sentences setup('ta') example_sent = ""உங்களைப் பார்த்து நிறைய நாட்கள் ஆகிவிட்டது"" # Predict next 'n' tokens n = 5 pred_sent = predict_next_words(example_sent, n, 'ta') # Get 'n' similar sentence n = 2 simi_sent = get_similar_sentences(example_sent, n, 'ta') print(""Predicted Words:"", pred_sent) print(""Similar Sentences:"", simi_sent)Code language: PHP (php) Output: Predicted Words: உங்களைப் பார்த்து நிறைய நாட்கள் ஆகிவிட்டது. மேலும், இதற்கு காரணமாக Similar Sentences: ['உங்களைத் பார்த்து நாட்கள் ஆகிவிட்டது ', 'உங்களைப் பார்த்து ஏராளமான நாட்கள் ஆகிவிட்டது '] It is time to move beyond English and use the real power of NLP for other languages ​​to serve all. Much recent research has focused on multilingual NLP. Also, Read – Should You Choose Python as your First Programming Language? So this is how we can use NLP for other languages with Machine Learning. I hope you liked this article on NLP for other languages with Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";NLP for Other Languages with Machine Learning
2020-09-11 18:53:37;Great, you’ve got approval to launch a machine learning model. Now you need to prepare your solution for production (e.g. tweaking the code, writing documentation and testing, etc.). Then you can launch your machine learning model in your production environment.;https://thecleverprogrammer.com/2020/09/11/launch-a-machine-learning-model/;['sklearn'];1.0;['ML', 'AI'];['ML', 'Recommender', 'Classification', 'AI'];['predict', 'model', 'machine learning', 'classif', 'train', 'recommend'];Great, you’ve got approval to launch a machine learning model. Now you need to prepare your solution for production (e.g. tweaking the code, writing documentation and testing, etc.). Then you can launch your machine learning model in your production environment. How To Launch a Machine Learning Model? One way to launch a machine learning model is to save the trained model (for example, using joblib), including the entire preprocessing and prediction pipeline, and then load that trained model into your production environment. Also, Read – Word Embeddings in Machine Learning. To save a model using Joblib: from sklearn.externals import joblib joblib.dump(model,'model_name')Code language: JavaScript (javascript) And then to load our saved model: from sklearn.externals import joblib joblib.load('model_name')Code language: JavaScript (javascript) Another popular strategy is to launch your machine learning model in the cloud, for example on Google Cloud AI Platform (formerly known as Google Cloud ML Engine): simply save your model using joblib and download- le on Google Cloud Storage (GCS), then head to on Google Cloud AI Platform and create a new model version by pointing it to the GCS file. That’s all! This gives you a simple web service that does the load balancing and scaling for you. It takes JSON requests containing the input data (for example, from a district) and returns JSON responses containing the predictions. You can then use this web service on your website (or whatever production environment you use). Once you launch a machine learning model, it is not the end of the story. You should also write a monitoring code to check the live performance of your system at regular intervals and trigger alerts in the event of a drop. It can be a big drop, possibly due to a broken component in your infrastructure, but be aware that it can also be a slight degradation that could easily go unnoticed for a long time. This is quite common because machine learning models tend to “rot” with time, as the world changes, so if the model was trained on the data of last year then it may not be suitable for data of this year. So you need to keep updating your model. If you will launch a machine learning model trained to classify images of cats and dogs, it may also need to be recycled regularly, not because cats and dogs will mutate overnight, but because cameras constantly change, as well as aspect ratios, sharpness, brightness and aspect ratios. Plus, people may like different breeds next year or decide to dress their pets in tiny hats – who knows? So you need to monitor the live performance of your model. But how do you do that? It depends. In some cases, model performance can be inferred from downstream metrics. For example, if your model is part of a recommendation system and suggests products that are likely to be of interest to users, then it’s easy to monitor the number of recommended products sold each day. If that number decreases, the prime suspect is the model. This could be because the data pipeline is interrupted, or maybe the model needs to be recycled to new data. However, it is not always possible to determine the performance of the model without any human analysis. Either way, you need to have a monitoring system in place, along with all the relevant processes to define what to do in the event of an outage and how to prepare for it. Unfortunately, it can take a lot of work. It is often a lot more work than building and training a model. Also, Read – Rainfall Prediction with Machine Learning. If the data continues to evolve, you will need to update your data sets and recycle your model regularly. I hope you liked this article on how to launch a machine learning model. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram;Launch a Machine Learning Model
2020-09-11 12:42:17;Rainfall Prediction is one of the difficult and uncertain tasks that have a significant impact on human society. Timely and accurate forecasting can proactively help reduce human and financial loss. This study presents a set of experiments that involve the use of common machine learning techniques to create models that can predict whether it will rain tomorrow or not based on the weather data for that day in major cities in Australia.I’ve always liked knowing the parameters meteorologists take into account before making a weather forecast, so I found the dataset interesting. From an expert’s point of view, however, this dataset is fairly straightforward. At the end of this article, you will learn:Also, Read – Linear Search Algorithm with Python.Let’s start this task of rainfall prediction by importing the data, you can download the dataset I am using in this task from here:;https://thecleverprogrammer.com/2020/09/11/rainfall-prediction-with-machine-learning/;['sklearn', 'catboost', 'xgboost', 'lightgbm'];1.0;['ML', 'GBM'];['NN', 'Regression', 'ML', 'Decision Tree', 'GBM', 'Logistic Regression', 'Random Forest', 'Classification'];['detect', 'regression', 'predict', 'fit', 'model', 'loss', 'logistic regression', 'machine learning', 'classif', 'layer', 'filter', 'neural network', 'random forest', 'training data', 'train', 'label', 'test data', 'decision tree'];"Rainfall Prediction is one of the difficult and uncertain tasks that have a significant impact on human society. Timely and accurate forecasting can proactively help reduce human and financial loss. This study presents a set of experiments that involve the use of common machine learning techniques to create models that can predict whether it will rain tomorrow or not based on the weather data for that day in major cities in Australia. I’ve always liked knowing the parameters meteorologists take into account before making a weather forecast, so I found the dataset interesting. From an expert’s point of view, however, this dataset is fairly straightforward. At the end of this article, you will learn: Also, Read – Linear Search Algorithm with Python. How is balancing done for an unbalanced datasetHow Label Coding Is Done for Categorical VariablesHow sophisticated imputation like MICE is usedHow outliers can be detected and excluded from the dataHow the filter method and wrapper methods are used for feature selectionHow to compare speed and performance for different popular modelsWhich metric can be the best to judge the performance on an unbalanced data set: precision and F1 score. Let’s start this task of rainfall prediction by importing the data, you can download the dataset I am using in this task from here: import pandas as pd from google.colab import files uploaded = files.upload() full_data = pd.read_csv('weatherAUS.csv') full_data.head()Code language: JavaScript (javascript) Data Exploration We will first check the number of rows and columns. Next, we’ll check the size of the dataset to decide if it needs size compression. full_data.shapeCode language: CSS (css) (142193, 24) full_data.info()Code language: CSS (css) “RainToday” and “RainTomorrow” are objects (Yes / No). I will convert them to binary (1/0) for our convenience. full_data['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True) full_data['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)Code language: PHP (php) Next, we will check if the dataset is unbalanced or balanced. If the data set is unbalanced, we need to either downsample the majority or oversample the minority to balance it. import matplotlib.pyplot as plt fig = plt.figure(figsize = (8,5)) full_data.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0) plt.title('RainTomorrow Indicator No(0) and Yes(1) in the Imbalanced Dataset') plt.show()Code language: JavaScript (javascript) We can observe that the presence of “0” and “1” is almost in the 78:22 ratio. So there is a class imbalance and we have to deal with it. To fight against the class imbalance, we will use here the oversampling of the minority class. Since the size of the dataset is quite small, majority class subsampling wouldn’t make much sense here. Handling Class Imbalance For Rainfall Prediction from sklearn.utils import resample no = full_data[full_data.RainTomorrow == 0] yes = full_data[full_data.RainTomorrow == 1] yes_oversampled = resample(yes, replace=True, n_samples=len(no), random_state=123) oversampled = pd.concat([no, yes_oversampled]) fig = plt.figure(figsize = (8,5)) oversampled.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0) plt.title('RainTomorrow Indicator No(0) and Yes(1) after Oversampling (Balanced Dataset)') plt.show()Code language: JavaScript (javascript) Now, I will now check the missing data model in the dataset: # Missing Data Pattern in Training Data import seaborn as sns sns.heatmap(oversampled.isnull(), cbar=False, cmap='PuBu')Code language: PHP (php) Obviously, “Evaporation”, “Sunshine”, “Cloud9am”, “Cloud3pm” are the features with a high missing percentage. So we will check the details of the missing data for these 4 features. total = oversampled.isnull().sum().sort_values(ascending=False) percent = (oversampled.isnull().sum()/oversampled.isnull().count()).sort_values(ascending=False) missing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) missing.head(4)Code language: PHP (php) Total	Percent Sunshine	104831	0.475140 Evaporation	95411	0.432444 Cloud3pm	85614	0.388040 Cloud9am	81339	0.368664 We observe that the 4 features have less than 50 per cent missing data. So instead of rejecting them completely, we’ll consider them in our model with proper imputation. Imputation and Transformation We will impute the categorical columns with mode, and then we will use the label encoder to convert them to numeric numbers. Once all the columns in the full data frame are converted to numeric columns, we will impute the missing values ​​using the Multiple Imputation by Chained Equations (MICE) package. Then we will detect outliers using the interquartile range and remove them to get the final working dataset. Finally, we will check the correlation between the different variables, and if we find a pair of highly correlated variables, we will discard one while keeping the other. oversampled.select_dtypes(include=['object']).columnsCode language: PHP (php) Index(['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'], dtype='object') # Impute categorical var with Mode oversampled['Date'] = oversampled['Date'].fillna(oversampled['Date'].mode()[0]) oversampled['Location'] = oversampled['Location'].fillna(oversampled['Location'].mode()[0]) oversampled['WindGustDir'] = oversampled['WindGustDir'].fillna(oversampled['WindGustDir'].mode()[0]) oversampled['WindDir9am'] = oversampled['WindDir9am'].fillna(oversampled['WindDir9am'].mode()[0]) oversampled['WindDir3pm'] = oversampled['WindDir3pm'].fillna(oversampled['WindDir3pm'].mode()[0])Code language: PHP (php) # Convert categorical features to continuous features with Label Encoding from sklearn.preprocessing import LabelEncoder lencoders = {} for col in oversampled.select_dtypes(include=['object']).columns: lencoders[col] = LabelEncoder() oversampled[col] = lencoders[col].fit_transform(oversampled[col])Code language: PHP (php) import warnings warnings.filterwarnings(""ignore"") # Multiple Imputation by Chained Equations from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer MiceImputed = oversampled.copy(deep=True) mice_imputer = IterativeImputer() MiceImputed.iloc[:, :] = mice_imputer.fit_transform(oversampled)Code language: PHP (php) Thus, the dataframe has no “NaN” value. We will now detect and eliminate outliers from the inter-quartile interval-based data set. # Detecting outliers with IQR Q1 = MiceImputed.quantile(0.25) Q3 = MiceImputed.quantile(0.75) IQR = Q3 - Q1 print(IQR)Code language: PHP (php) Date 1535.000000 Location 25.000000 MinTemp 9.300000 MaxTemp 10.200000 Rainfall 2.400000 Evaporation 4.119679 Sunshine 5.947404 WindGustDir 9.000000 WindGustSpeed 19.000000 WindDir9am 8.000000 WindDir3pm 8.000000 WindSpeed9am 13.000000 WindSpeed3pm 11.000000 Humidity9am 26.000000 Humidity3pm 30.000000 Pressure9am 8.800000 Pressure3pm 8.800000 Cloud9am 4.000000 Cloud3pm 3.681346 Temp9am 9.300000 Temp3pm 9.800000 RainToday 1.000000 RISK_MM 5.200000 RainTomorrow 1.000000 dtype: float64 # Removing outliers from dataset MiceImputed = MiceImputed[~((MiceImputed &lt; (Q1 - 1.5 * IQR)) |(MiceImputed &gt; (Q3 + 1.5 * IQR))).any(axis=1)] MiceImputed.shapeCode language: HTML, XML (xml) (156852, 24) We observe that the original dataset had the form (87927, 24). After running a code snippet for removing outliers, the dataset now has the form (86065, 24). As a result, the dataset is now free of 1862 outliers. We are now going to check multicollinearity, that is to say if a character is strongly correlated with another. # Correlation Heatmap import numpy as np import matplotlib.pyplot as plt import seaborn as sns corr = MiceImputed.corr() mask = np.triu(np.ones_like(corr, dtype=np.bool)) f, ax = plt.subplots(figsize=(20, 20)) cmap = sns.diverging_palette(250, 25, as_cmap=True) sns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5, cbar_kws={""shrink"": .9})Code language: PHP (php) The following feature pairs have a strong correlation with each other: MaxTemp and MinTempPressure9h and pressure3hTemp9am and Temp3pmEvaporation and MaxTempMaxTemp and Temp3pm But in no case is the correlation value equal to a perfect “1”. We are therefore not removing any functionality However, we can delve deeper into the pairwise correlation between these highly correlated characteristics by examining the following pair diagram. Each of the paired plots shows very clearly distinct clusters of RainTomorrow’s “yes” and “no” clusters. There is very minimal overlap between them. sns.pairplot( data=MiceImputed, vars=('MaxTemp','MinTemp','Pressure9am','Pressure3pm', 'Temp9am', 'Temp3pm', 'Evaporation'), hue='RainTomorrow' ) Code language: JavaScript (javascript) Feature Selection for Rainfall Prediction I will use both the filter method and the wrapper method for feature selection to train our rainfall prediction model. Selecting features by filtering method (chi-square value): before doing this, we must first normalize our data. We use MinMaxScaler instead of StandardScaler in order to avoid negative values. # Standardizing data from sklearn import preprocessing r_scaler = preprocessing.MinMaxScaler() r_scaler.fit(MiceImputed) modified_data = pd.DataFrame(r_scaler.transform(MiceImputed), index=MiceImputed.index, columns=MiceImputed.columns) Code language: PHP (php) # Feature Importance using Filter Method (Chi-Square) from sklearn.feature_selection import SelectKBest, chi2 X = modified_data.loc[:,modified_data.columns!='RainTomorrow'] y = modified_data[['RainTomorrow']] selector = SelectKBest(chi2, k=10) selector.fit(X, y) X_new = selector.transform(X) print(X.columns[selector.get_support(indices=True)])Code language: PHP (php) Index(['Sunshine', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday', 'RISK_MM'], dtype='object') We can observe that “Sunshine”, “Humidity9am”, “Humidity3pm”, “Pressure9am”, “Pressure3pm” have higher importance compared to other features. Selection of features by wrapping method (random forest): from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier as rf X = MiceImputed.drop('RainTomorrow', axis=1) y = MiceImputed['RainTomorrow'] selector = SelectFromModel(rf(n_estimators=100, random_state=0)) selector.fit(X, y) support = selector.get_support() features = X.loc[:,support].columns.tolist() print(features) print(rf(n_estimators=100, random_state=0).fit(X,y).feature_importances_)Code language: JavaScript (javascript) ['Sunshine', 'Cloud3pm', 'RISK_MM'] [0.00205993 0.00215407 0.00259089 0.00367568 0.0102656 0.00252838 0.05894157 0.00143001 0.00797518 0.00177178 0.00167654 0.0014278 0.00187743 0.00760691 0.03091966 0.00830365 0.01193018 0.02113544 0.04962418 0.00270103 0.00513723 0.00352198 0.76074491] Training Rainfall Prediction Model with Different Models We will divide the dataset into training (75%) and test (25%) sets respectively to train the rainfall prediction model. For best results, we will standardize our X_train and X_test data: features = MiceImputed[['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']] target = MiceImputed['RainTomorrow'] # Split into test and train from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=12345) # Normalize Features from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.fit_transform(X_test)Code language: PHP (php) def plot_roc_cur(fper, tper): plt.plot(fper, tper, color='orange', label='ROC') plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver Operating Characteristic (ROC) Curve') plt.legend() plt.show()Code language: JavaScript (javascript) import time from sklearn.metrics import accuracy_score, roc_auc_score, cohen_kappa_score, plot_confusion_matrix, roc_curve, classification_report def run_model(model, X_train, y_train, X_test, y_test, verbose=True): t0=time.time() if verbose == False: model.fit(X_train,y_train, verbose=0) else: model.fit(X_train,y_train) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) roc_auc = roc_auc_score(y_test, y_pred) coh_kap = cohen_kappa_score(y_test, y_pred) time_taken = time.time()-t0 print(""Accuracy = {}"".format(accuracy)) print(""ROC Area under Curve = {}"".format(roc_auc)) print(""Cohen's Kappa = {}"".format(coh_kap)) print(""Time taken = {}"".format(time_taken)) print(classification_report(y_test,y_pred,digits=5)) probs = model.predict_proba(X_test) probs = probs[:, 1] fper, tper, thresholds = roc_curve(y_test, probs) plot_roc_cur(fper, tper) plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues, normalize = 'all') return model, accuracy, roc_auc, coh_kap, time_takenCode language: PHP (php) # Logistic Regression from sklearn.linear_model import LogisticRegression params_lr = {'penalty': 'l1', 'solver':'liblinear'} model_lr = LogisticRegression(**params_lr) model_lr, accuracy_lr, roc_auc_lr, coh_kap_lr, tt_lr = run_model(model_lr, X_train, y_train, X_test, y_test) # Decision Tree from sklearn.tree import DecisionTreeClassifier params_dt = {'max_depth': 16, 'max_features': ""sqrt""} model_dt = DecisionTreeClassifier(**params_dt) model_dt, accuracy_dt, roc_auc_dt, coh_kap_dt, tt_dt = run_model(model_dt, X_train, y_train, X_test, y_test) # Neural Network from sklearn.neural_network import MLPClassifier params_nn = {'hidden_layer_sizes': (30,30,30), 'activation': 'logistic', 'solver': 'lbfgs', 'max_iter': 500} model_nn = MLPClassifier(**params_nn) model_nn, accuracy_nn, roc_auc_nn, coh_kap_nn, tt_nn = run_model(model_nn, X_train, y_train, X_test, y_test) # Random Forest from sklearn.ensemble import RandomForestClassifier params_rf = {'max_depth': 16, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 12345} model_rf = RandomForestClassifier(**params_rf) model_rf, accuracy_rf, roc_auc_rf, coh_kap_rf, tt_rf = run_model(model_rf, X_train, y_train, X_test, y_test) # Light GBM import lightgbm as lgb params_lgb ={'colsample_bytree': 0.95, 'max_depth': 16, 'min_split_gain': 0.1, 'n_estimators': 200, 'num_leaves': 50, 'reg_alpha': 1.2, 'reg_lambda': 1.2, 'subsample': 0.95, 'subsample_freq': 20} model_lgb = lgb.LGBMClassifier(**params_lgb) model_lgb, accuracy_lgb, roc_auc_lgb, coh_kap_lgb, tt_lgb = run_model(model_lgb, X_train, y_train, X_test, y_test) # Catboost !pip install catboost import catboost as cb params_cb ={'iterations': 50, 'max_depth': 16} model_cb = cb.CatBoostClassifier(**params_cb) model_cb, accuracy_cb, roc_auc_cb, coh_kap_cb, tt_cb = run_model(model_cb, X_train, y_train, X_test, y_test, verbose=False) # XGBoost import xgboost as xgb params_xgb ={'n_estimators': 500, 'max_depth': 16} model_xgb = xgb.XGBClassifier(**params_xgb) model_xgb, accuracy_xgb, roc_auc_xgb, coh_kap_xgb, tt_xgb = run_model(model_xgb, X_train, y_train, X_test, y_test) view raw 7 Models hosted with ❤ by GitHub View this gist on GitHub Plotting Decision Region for all Models import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import itertools from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.neural_network import MLPClassifier from sklearn.ensemble import RandomForestClassifier import lightgbm as lgb import catboost as cb import xgboost as xgb from mlxtend.classifier import EnsembleVoteClassifier from mlxtend.plotting import plot_decision_regions value = 1.80 width = 0.90 clf1 = LogisticRegression(random_state=12345) clf2 = DecisionTreeClassifier(random_state=12345) clf3 = MLPClassifier(random_state=12345, verbose = 0) clf4 = RandomForestClassifier(random_state=12345) clf5 = lgb.LGBMClassifier(random_state=12345, verbose = 0) clf6 = cb.CatBoostClassifier(random_state=12345, verbose = 0) clf7 = xgb.XGBClassifier(random_state=12345) eclf = EnsembleVoteClassifier(clfs=[clf4, clf5, clf6, clf7], weights=[1, 1, 1, 1], voting='soft') X_list = MiceImputed[[""Sunshine"", ""Humidity9am"", ""Cloud3pm""]] #took only really important features X = np.asarray(X_list, dtype=np.float32) y_list = MiceImputed[""RainTomorrow""] y = np.asarray(y_list, dtype=np.int32) # Plotting Decision Regions gs = gridspec.GridSpec(3,3) fig = plt.figure(figsize=(18, 14)) labels = ['Logistic Regression', 'Decision Tree', 'Neural Network', 'Random Forest', 'LightGBM', 'CatBoost', 'XGBoost', 'Ensemble'] for clf, lab, grd in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf7, eclf], labels, itertools.product([0, 1, 2], repeat=2)): clf.fit(X, y) ax = plt.subplot(gs[grd[0], grd[1]]) fig = plot_decision_regions(X=X, y=y, clf=clf, filler_feature_values={2: value}, filler_feature_ranges={2: width}, legend=2) plt.title(lab) plt.show() view raw Plotting Models hosted with ❤ by GitHub View this gist on GitHub We can observe the difference in the class limits for different models, including the set one (the plot is done considering only the training data). CatBoost has the distinct regional border compared to all other models. However, the XGBoost and Random Forest models also have a much lower number of misclassified data points compared to other models. Rainfall Prediction Model Comparison Now we need to decide which model performed best based on Precision Score, ROC_AUC, Cohen’s Kappa and Total Run Time. One point to mention here is: we could have considered F1-Score as a better metric for judging model performance instead of accuracy, but we have already converted the unbalanced dataset to a balanced one, so consider accuracy as a metric for deciding the best model is justified in this case. For a better decision, we chose “Cohen’s Kappa” which is actually an ideal choice as a metric to decide on the best model in case of unbalanced datasets. Let’s check which model worked well on which front: accuracy_scores = [accuracy_lr, accuracy_dt, accuracy_nn, accuracy_rf, accuracy_lgb, accuracy_cb, accuracy_xgb] roc_auc_scores = [roc_auc_lr, roc_auc_dt, roc_auc_nn, roc_auc_rf, roc_auc_lgb, roc_auc_cb, roc_auc_xgb] coh_kap_scores = [coh_kap_lr, coh_kap_dt, coh_kap_nn, coh_kap_rf, coh_kap_lgb, coh_kap_cb, coh_kap_xgb] tt = [tt_lr, tt_dt, tt_nn, tt_rf, tt_lgb, tt_cb, tt_xgb] model_data = {'Model': ['Logistic Regression','Decision Tree','Neural Network','Random Forest','LightGBM','Catboost','XGBoost'], 'Accuracy': accuracy_scores, 'ROC_AUC': roc_auc_scores, 'Cohen_Kappa': coh_kap_scores, 'Time taken': tt} data = pd.DataFrame(model_data) fig, ax1 = plt.subplots(figsize=(12,10)) ax1.set_title('Model Comparison: Accuracy and Time taken for execution', fontsize=13) color = 'tab:green' ax1.set_xlabel('Model', fontsize=13) ax1.set_ylabel('Time taken', fontsize=13, color=color) ax2 = sns.barplot(x='Model', y='Time taken', data = data, palette='summer') ax1.tick_params(axis='y') ax2 = ax1.twinx() color = 'tab:red' ax2.set_ylabel('Accuracy', fontsize=13, color=color) ax2 = sns.lineplot(x='Model', y='Accuracy', data = data, sort=False, color=color) ax2.tick_params(axis='y', color=color)Code language: PHP (php) fig, ax3 = plt.subplots(figsize=(12,10)) ax3.set_title('Model Comparison: Area under ROC and Cohens Kappa', fontsize=13) color = 'tab:blue' ax3.set_xlabel('Model', fontsize=13) ax3.set_ylabel('ROC_AUC', fontsize=13, color=color) ax4 = sns.barplot(x='Model', y='ROC_AUC', data = data, palette='winter') ax3.tick_params(axis='y') ax4 = ax3.twinx() color = 'tab:red' ax4.set_ylabel('Cohen_Kappa', fontsize=13, color=color) ax4 = sns.lineplot(x='Model', y='Cohen_Kappa', data = data, sort=False, color=color) ax4.tick_params(axis='y', color=color) plt.show()Code language: PHP (php) We can observe that XGBoost, CatBoost and Random Forest performed better compared to other models. However, if speed is an important thing to consider, we can stick with Random Forest instead of XGBoost or CatBoost. Also, Read – Proximity Analysis with Python. I hope you liked this article on how we can create and compare different Rainfall prediction models. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Rainfall Prediction with Machine Learning
2020-09-11 15:00:04;Word embeddings or word vectors represent each word numerically so that the vector matches how that word is used or what it means. Vector encodings are learned by considering the context in which the words appear.Words that appear in similar contexts will have similar vectors. For example, the vectors for “leopard”, “lion” and “tiger” will be close to each other, while they will be far from “planet” and “castle”.Also, Read – Machine Learning Project on Rainfall Prediction Model.;https://thecleverprogrammer.com/2020/09/11/word-embeddings-in-machine-learning/;['spacy', 'sklearn', 'xgboost'];1.0;[];['ML', 'Classification', 'Support Vector Machines'];['predict', 'fit', 'model', 'machine learning', 'classif', 'support vector machines', 'train', 'label'];"Word embeddings or word vectors represent each word numerically so that the vector matches how that word is used or what it means. Vector encodings are learned by considering the context in which the words appear. Words that appear in similar contexts will have similar vectors. For example, the vectors for “leopard”, “lion” and “tiger” will be close to each other, while they will be far from “planet” and “castle”. Also, Read – Machine Learning Project on Rainfall Prediction Model. Word Embeddings in Action Even cooler, the relationships between words can be examined with math operations. Subtracting the vectors for “male” and “female” will return another vector. If you add that to the vector for “king”, the result is close to the vector for “queen”. These vectors can be used as features for machine learning models. Word embeddings will generally improve the performance of your models above encoding a bag of words. spaCy provides incorporations learned from a template called Word2Vec. You can access it by loading a large language model like en_core_web_lg. Then they will be available on the tokens of the vector attribute. import numpy as np import spacy # Need to load the large model to get the vectors nlp = spacy.load('en_core_web_lg')Code language: PHP (php) These are vectors of 300 dimensions, with a vector for each word. However, we only have document-level tags and our templates will not be able to use word-level embeds. So you need a vector representation for the whole document. # Disabling other pipes because we don't need them and it'll speed up this part a bit text = ""These vectors can be used as features for machine learning models."" with nlp.disable_pipes(): vectors = np.array([token.vector for token in nlp(text)]) vectors.shapeCode language: PHP (php) (12, 300) There are many ways to combine all the word embeddings into a single document vector that we can use for training the model. A simple and surprisingly efficient approach is to simply average the vectors for each word in the document. Then you can use these document vectors for modelling. spaCy calculates the average document vector you can get with doc.vector. Here is an example of loading spam data and converting it to document vectors. The dataset I am using here can be downloaded from here. import pandas as pd # Loading the spam data # ham is the label for non-spam messages spam = pd.read_csv('spam.csv') with nlp.disable_pipes(): doc_vectors = np.array([nlp(text).vector for text in spam.text]) doc_vectors.shapeCode language: PHP (php) (5572, 300) Classification Models for Word Embeddings With document vectors, you can train scikit-learn models, xgboost models, or any other standard approach to modelling. from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label, test_size=0.1, random_state=1)Code language: JavaScript (javascript) Here is an example using Support Vector Machines (SVM). Scikit-learn provides an SVM LinearSVC classifier. It works the same as other scikit-learn models. from sklearn.svm import LinearSVC # Set dual=False to speed up training, and it's not needed svc = LinearSVC(random_state=1, dual=False, max_iter=10000) svc.fit(X_train, y_train) print(f""Accuracy: {svc.score(X_test, y_test) * 100:.3f}%"", )Code language: PHP (php) Accuracy: 97.312% Document Similarity Documents with similar content usually have similar vectors. So you can find similar documents by measuring the similarity between vectors. A common metric for this is cosine similarity which measures the angle between two vectors, a and b. def cosine_similarity(a, b): return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b)) a = nlp(""REPLY NOW FOR FREE TEA"").vector b = nlp(""According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water."").vector cosine_similarity(a, b)Code language: JavaScript (javascript) 0.7030031 Also, Read – Linear Search Algorithm with Python. I hope you liked this article on Word Embeddings in Machine Learning. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";Word Embeddings in Machine Learning
2020-09-12 11:17:36;In Machine Learning, a seasonal autoregressive integrated moving average (SARIMA) model is a different step from an ARIMA model based on the concept of seasonal trends. In this article, I will introduce you to the SARIMA model in machine learning.;https://thecleverprogrammer.com/2020/09/12/sarima-in-machine-learning/;['statsmodels'];1.0;['AI'];['ML', 'AI'];['predict', 'fit', 'model', 'machine learning', 'label'];"In Machine Learning, a seasonal autoregressive integrated moving average (SARIMA) model is a different step from an ARIMA model based on the concept of seasonal trends. In this article, I will introduce you to the SARIMA model in machine learning. What is the SARIMA Model? Seasonal variations of the time series can take into account periodic models, allowing more accurate predictions. Seasonal ARIMA (SARIMA) defines both a seasonal and a non-seasonal component of the ARIMA model, allowing periodic characteristics to be captured. Also, Read – How To Launch Your Machine Learning Model? By choosing an appropriate forecasting model, always visualize your data to identify trends, seasons and cycles. If seasonality is a strong feature of the series, consider models with seasonal adjustments such as the SARIMA model. SARIMA Model in Action In this article, I will use the number of tourist arrivals in Italy. The data is taken from European statistics: annual data on tourism industries. You can download this dataset from here. First, we import the data set for foreign tourist arrivals in Italy from 2012 to October 2019 and then convert it to a time series. Let’s get started with the task by importing and reading the data: import pandas as pd df = pd.read_csv('IT_tourists_arrivals.csv') df['date'] = pd.to_datetime(df['date']) df = df[df['date'] &gt; '2012-01-01'] df.set_index('date', inplace=True)Code language: JavaScript (javascript) Preliminary analysis The preliminary analysis is a visual analysis of the time series, to understand its trend and behaviour. First, we create the time series and store it in the variable ts. ts = df['value'] import matplotlib.pylab as plt plt.plot(ts) plt.ylabel('Total Number of Tourists Arrivals') plt.grid() plt.tight_layout() plt.savefig('plots/IT_tourists_arrivals.png') plt.show()Code language: JavaScript (javascript) Now, let’s tune the parameters: from statsmodels.tsa.stattools import adfuller def test_stationarity(timeseries): dftest = adfuller(timeseries, autolag='AIC') dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used']) for key,value in dftest[4].items(): dfoutput['Critical Value (%s)'%key] = value critical_value = dftest[4]['5%'] test_statistic = dftest[0] alpha = 1e-3 pvalue = dftest[1] if pvalue &lt; alpha and test_statistic &lt; critical_value: # null hypothesis: x is non stationary print(""X is stationary"") return True else: print(""X is not stationary"") return FalseCode language: PHP (php) We now need to transform the time series via the diff () function as many times as the time series becomes stationary: ts_diff = pd.Series(ts) d = 0 while test_stationarity(ts_diff) is False: ts_diff = ts_diff.diff().dropna() d = d + 1Code language: PHP (php) Now, let’s plot the correlations between the parameters: from statsmodels.graphics.tsaplots import plot_acf, plot_pacf plot_acf(ts_trend, lags =12) plt.savefig('plots/acf.png') plt.show()Code language: JavaScript (javascript) Building the SARIMA Model: Now, let’s build our model by using the SARIMAX method provided by the statsmodels library: from statsmodels.tsa.statespace.sarimax import SARIMAX p = 9 q = 1 model = SARIMAX(ts, order=(p,d,q)) model_fit = model.fit(disp=1,solver='powell') fcast = model_fit.get_prediction(start=1, end=len(ts)) ts_p = fcast.predicted_mean ts_ci = fcast.conf_int()Code language: JavaScript (javascript) Now, we need to plot the results of our model: plt.plot(ts_p,label='prediction') plt.plot(ts,color='red',label='actual') plt.fill_between(ts_ci.index[1:], ts_ci.iloc[1:, 0], ts_ci.iloc[1:, 1], color='k', alpha=.2) plt.ylabel('Total Number of Tourists Arrivals') plt.legend() plt.tight_layout() plt.grid() plt.savefig('plots/IT_trend_prediction.png') plt.show()Code language: JavaScript (javascript) Also, Read – Word Embeddings in Machine Learning. I hope you liked this article on how we can build the SARIMA Model for seasonality effects. Feel free to ask your valuable questions in the comments section below. You can also follow me on Medium to learn every topic of Machine Learning. Follow Us: Facebook Instagram";SARIMA in Machine Learning
2020-09-15 11:04:48;Automated machine learning, also known as AutoML, is an emerging field in which the process of building machine learning models to model data is automated. AutoML can make modelling easier and more accessible for everyone. In this article, I’ll walk you through which Python AutoML libraries you should start practising to stay ahead of the competition.;https://thecleverprogrammer.com/2020/09/15/python-automl-libraries/;['sklearn'];1.0;['ML', 'AutoML'];['Recommender', 'NN', 'DL', 'ML', 'Bayesian', 'AutoML'];['recogn', 'model', 'bayesian', 'machine learning', 'neural network', 'deep learning', 'train', 'recommend', 'automated machine learning'];Automated machine learning, also known as AutoML, is an emerging field in which the process of building machine learning models to model data is automated. AutoML can make modelling easier and more accessible for everyone. In this article, I’ll walk you through which Python AutoML libraries you should start practising to stay ahead of the competition. Python AutoML Libraries You Should Know Here are the four Python AutoML Libraries that every machine Learning practitioner should know: Also, Read – Will AutoML Replace Data Science Jobs? auto-sklearn auto-sklearn is an automated machine learning toolkit that integrates seamlessly with the standard sklearn interface that many in the community are familiar with. With the use of recent methods like Bayesian optimization, the library is designed to navigate the space of possible models and learn to deduce whether a specific configuration will perform well on a given task. Like we start Machine Learning by working on the algorithms provided by Scikit-Learn, likewise starting auto-sklearn for AutoML from all the python AutoML libraries is not a bad choice. In addition to learning about data preparation and model selections for a dataset, it learns from models that perform well on similar datasets. The best performing models are grouped into a set. TPOT TPOT is also one of the AutoML Python libraries that automates the modelling pipeline, with a greater emphasis on data preparation as well as modelling algorithms and model hyperparameters. It automates the selection, preprocessing and construction of functionalities thanks to a scalable tree structure. TPOT pipeline optimizers can take a few hours to produce great results, as do many AutoML algorithms unless the dataset is small. You can easily run these long programs in Kaggle commits or Google Colab. HyperOpt HyperOpt is also one of the best Python AutoML libraries for Bayesian optimization, developed by James Bergstra. It is designed for the task of optimizing models with hundreds of parameters, HyperOpt is explicitly used to optimize machine learning pipelines, with additional options to scale the process across multiple cores and machines. However, HyperOpt is difficult to use directly, as it is very technical and requires carefully specified optimization procedures and parameters. Instead, it is recommended to use HyperOpt-sklearn, a wrapper around HyperOpt that integrates the sklearn library. AutoKeras Neural networks and deep learning are significantly more powerful, and therefore more difficult to automate than standard machine learning libraries. Using AutoKeras, you can create a model with complex elements such as spatial incorporations and reductions that would otherwise be less accessible to those who are still learning deep learning. When AutoKeras creates models for you, much of preprocessing, like vectorizing or cleaning text data, is done and optimized for you. It takes two lines to initiate and train a search. AutoKeras boasts a Keras-like interface, so it’s not hard to remember and use at all. Also, Read – Colour Recognition with Python. So these were the four Python AutoML libraries that you should learn to stay ahead of the competition. I hope you liked this article on the most important Python AutoML Libraries. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram;Python AutoML Libraries
2020-09-19 14:37:52;Electronically stored medical imaging data is plentiful and Machine Learning algorithms can be fed with this type of dataset, to detect and uncover patterns and anomalies. In this article, I will introduce you to five machine learning projects for healthcare.Machines and algorithms can interpret imaging data just as a highly trained radiologist could identify suspicious spots on the skin, lesions, tumours and bleeding in the brain. The use of machine learning tools and platforms to help radiologists is therefore poised to grow exponentially.Also, Read – Analyze Call Records with Machine Learning using Google Cloud Platform.;https://thecleverprogrammer.com/2020/09/19/machine-learning-for-healthcare/;['pattern'];1.0;['NN', 'CNN'];['NN', 'ML', 'CNN', 'Image Segmentation', 'Classification'];['detect', 'image segmentation', 'predict', 'model', 'machine learning', 'classif', 'train'];Electronically stored medical imaging data is plentiful and Machine Learning algorithms can be fed with this type of dataset, to detect and uncover patterns and anomalies. In this article, I will introduce you to five machine learning projects for healthcare. Machines and algorithms can interpret imaging data just as a highly trained radiologist could identify suspicious spots on the skin, lesions, tumours and bleeding in the brain. The use of machine learning tools and platforms to help radiologists is therefore poised to grow exponentially. Also, Read – Analyze Call Records with Machine Learning using Google Cloud Platform. Machine Learning Projects for Healthcare Machine learning is used in many spheres around the world. The healthcare industry is no exception. Machine learning can play a critical role in predicting the presence/absence of locomotor disorders, heart disease, cancer, lungs disease and more. Such information, if predicted well in advance, can provide important information to physicians who can then tailor their diagnosis and treatment per patient. Now let’s take a look at some machine learning projects for healthcare. Heart Disease Prediction: Here you will learn about the emerging possibilities of heart disease. The results of this machine learning model will provide the odds of heart disease occurring in percentage terms. See this machine learning based healthcare project here. Skin Cancer Classification: Skin cancer is one of the most common types of disease in the United States. Up to 4 million cases were reported dead due to skin cancer in the United States during the year. Here you will learn how to create a skin cancer classification model with machine learning. That’s a huge number, really 4 million, people have died from just skin cancer in one country. Like all of these people were dying, but half of these cases or maybe, even more, did not go to the doctor in the early stages of the disease when it could have been prevented. See this machine learning based healthcare project here. Lung Segmentation: Lung segmentation is one of the most useful tasks of machine learning in healthcare. Lung CT image segmentation is an initial step necessary for lung image analysis, it is a preliminary step to provide accurate lung CT image analysis such as detection of lung cancer. Dicom is the de-facto repository in medical imaging. These files contain a lot of metadata. This analysis pixel size/coarseness differs from analysis to analysis, which can adversely affect the performance of CNN approaches. See this machine learning based healthcare project here. Predict Diabetes: About one in seven American adults currently has diabetes, according to the Centers for Disease Control and Prevention. But by 2050, that rate could skyrocket to one in three. With that in mind, here’s what we’ll learn here: Learn to use Machine Learning to help us predict diabetes. See this machine learning based healthcare project here. Contact Tracing: Contact tracing is a process used by public health ministries to help stop the spread of infectious disease, such as COVID-19, within a community. Once a person is positive for coronavirus, it is very important to identify other people who may have been infected by the patients diagnosed. To identify infected people, the authorities follow the activity of patients diagnosed in the last 14 days. This process is called contact tracking. Depending on the country and the local authority, the search for contacts is carried out either by manual methods or by numerical methods. See this machine learning project on healthcare from here. Also, Read – Python List Tutorial. Follow Us: Facebook Instagram;Machine Learning for Healthcare
2020-09-20 19:27:16;Machine learning is about designing algorithms that automatically extract valuable information from data. The emphasis here is on ‘automatic’, that is, machine learning is about general-purpose methodologies that can be applied to many sets of data while producing something meaningful. In this article, I will take you through why you need to learn some maths for machine learning and some important topics of maths that you need to learn.;https://thecleverprogrammer.com/2020/09/20/maths-for-machine-learning/;['pattern'];1.0;[];['ML', 'Regression'];['regression', 'model', 'machine learning'];Machine learning is about designing algorithms that automatically extract valuable information from data. The emphasis here is on ‘automatic’, that is, machine learning is about general-purpose methodologies that can be applied to many sets of data while producing something meaningful. In this article, I will take you through why you need to learn some maths for machine learning and some important topics of maths that you need to learn. Why Maths for Machine Learning? Because machine learning is inherently data-driven, data acts as the heart of machine learning algorithms. The goal of machine learning is to design general-purpose methodologies to extract valuable models from data, ideally without a lot of domain-specific expertise. To achieve this goal, we design models that are generally related to the process that generates data, similar to the model of the dataset provided to us. Also, Read – Model Validation in Machine Learning. A model learns from data if its performance on a given task improves after taking the data into account. The goal is to find good models that generalize well to still invisible data, which we might care about in the future. Learning can be understood as a means of learning to automatically find patterns and structure in data by optimizing model parameters. To stay in the field of machine learning for a longer time, I believe that the maths for machine learning is very important for understanding the fundamentals on which more complex machine learning systems are built. There are many more reasons why maths for machine learning is important, some of the reasons are: Selecting the right algorithm which includes considerations on the accuracy, learning time, model complexity, number of parameters and number of featuresChoice of parameter settings and validation strategies.Estimation of the correct confidence interval and uncertainty. The Maths You Need For Machine Learning The main question when trying to understand an interdisciplinary field such as machine learning is how much math is needed and what level of math is needed to understand these techniques. My answer to this question is that I think you need some minimum level of maths to be an expert in machine learning and you should be familiar with the importance of each mathematical concept. The important topics of maths are mentioned below that you need for Machine Learning. Linear Algebra: Linear algebra is the study of vectors and linear functions. In broad terms, vectors are things you can add and linear functions are functions of vectors that respect vector addition. The goal of Linear Algebra is to teach you to organize information about vector spaces in a way that makes problems involving linear functions of many variables easy. Probability Theory and Statistics: Probability is the study of random events. It is used to analyze genetics, weather forecasts, and a myriad of other everyday events. Statistics is the mathematics we use to collect, organize and interpret numerical data. It is used to describe and analyze sets of test results, election results, and buyer preferences for particular products. Multivariate Calculus: Multivariate calculus is used in regression analysis to derive formulas for estimating relationships between various sets of empirical data. Multivariate calculus is used in many fields of the natural and social sciences and engineering to model and study high-dimensional systems that exhibit deterministic behaviour. Algorithms and Complex Optimizations: This is important to understand the computational efficiency and scalability of our machine learning algorithm and to exploit the scarcity of our datasets. Knowledge of data structures, dynamic programming, random and sublinear algorithm, graphics, gradient, and Primal-Dual methods are required. Others: This includes other maths topics for machine learning that are not covered in the four main areas described above. They include real and complex analysis, information theory (entropy, information gain), functional spaces and collectors. So, hope you liked this article on maths for machine learning and all the necessary topics you need. For beginners, you don’t need a lot of maths to start machine learning. As you begin to develop in this industry, you will need knowledge of all of the above topics to build more complex machine learning systems. You can download this book to learn maths for machine learning. Please feel free to ask your valuable questions in the comments section below. Also, Read – Machine Learning Projects for Healthcare. Follow Us: Facebook Instagram;Maths for Machine Learning
2020-09-21 12:11:52;In this article, I will introduce you to a very important concept for machine learning practitioners: when do we need machine learning. It’s one of those basic issues that every computer science student faces when moving from basic computing practices to machine learning.If you are one of those people who does not know when we should use programming and when do we use machine learning algorithms, I hope by the end of this article you will understand all about when do we use machine learning.Also, Read – Predict Car Prices with Machine Learning.;https://thecleverprogrammer.com/2020/09/21/when-do-we-need-machine-learning/;['pattern'];1.0;['ML'];['ML', 'Speech Recognition', 'AI'];['detect', 'artificial intelligence', 'recogn', 'predict', 'fit', 'model', 'machine learning', 'train', 'speech recognition'];"In this article, I will introduce you to a very important concept for machine learning practitioners: when do we need machine learning. It’s one of those basic issues that every computer science student faces when moving from basic computing practices to machine learning. If you are one of those people who does not know when we should use programming and when do we use machine learning algorithms, I hope by the end of this article you will understand all about when do we use machine learning. Also, Read – Predict Car Prices with Machine Learning. So What is Machine Learning? Machine learning has become one of the most important topics within development organizations looking for innovative ways to leverage data assets to help the business gain a new level of understanding. Why add it to the mix? With the right ML models, companies can continuously predict changes in the business so they can better predict what’s next. With data constantly being added, ML models ensure that the solution is constantly updated. The value is simple: If you use the most appropriate and constantly evolving data sources in the context of ML, you have the power to predict the future. ML is a form of artificial intelligence that allows a system to learn from data rather than through explicit programming. However, using ML algorithms is not a simple process. When Do We Need Machine Learning? When do we need machine learning rather than directly programming our computers to do the job at hand? Two aspects of any given problem may require the use of programs that learn and improve based on their “experience”: the complexity of the problem and the need for adaptability. Tasks That Are Too Complex to Program: Tasks Done by Animals / Humans: There are many tasks that we humans perform regularly, but our introspection into how we do them is not elaborate enough to extract a well-defined agenda. Examples of such tasks include driving, speech recognition, and picture understanding. In all of these tasks, advanced ML programs, programs that learn from experience, achieve quite satisfactory results when exposed to enough training examples. Tasks beyond human capacities: Another set of tasks that gets a great benefit from ML algorithms is related to the analysis of a very large and complex data such as astronomical data, the transformation of medical records into medical knowledge, forecasting weather, genomic data analysis, web search engines and e-commerce. With more and more digitally recorded data available, it is becoming evident that there are treasures of meaningful information buried in data archives that are far too large and complex for humans to understand. ML can easily extract meaningful patterns in large and complex data sets with very much promising results. Adaptivity A limiting characteristic of programmed tools is their rigidity – once the program has been written and installed, it remains unchanged. However, many tasks change over time or from user to user. ML tools – programs whose behaviour adapts to their input data – offer a solution to these problems; they are by nature adaptive to changes in the environment with which they interact. Some very successful applications of ML regarding such problems include applications that decode handwritten text, where a fixed program can easily adapt to different variations in handwriting from different users; spam detection programs, automatically adapting to changes in the nature of spam e-mails; and voice recognition programs. I hope you now know the difference when we should do programming and when do we need to use Machine Learning. I hope you liked this article on when do we need machine learning. Feel free to ask your valuable questions in the comments section below. Also, Read – Maths for Machine Learning. Follow Us: Facebook Instagram";When Do We Need Machine Learning?
2020-09-22 16:26:51;In Machine Learning, StandardScaler is used to resize the distribution of values ​​so that the mean of the observed values ​​is 0 and the standard deviation is 1. In this article, I will walk you through how to use StandardScaler in Machine Learning.StandardScaler is an important technique that is mainly performed as a preprocessing step before many machine learning models, in order to standardize the range of functionality of the input dataset.Also, Read – Why Python is the best language for Machine Learning.Some machine learning practitioners tend to standardize their data blindly before each machine learning model without making the effort to understand why it should be used, or even whether it is needed or not. So you need to understand when you should use the StandardScaler to scale your data.;https://thecleverprogrammer.com/2020/09/22/standardscaler-in-machine-learning/;['sklearn'];1.0;[];['ML'];['predict', 'fit', 'model', 'machine learning'];In Machine Learning, StandardScaler is used to resize the distribution of values ​​so that the mean of the observed values ​​is 0 and the standard deviation is 1. In this article, I will walk you through how to use StandardScaler in Machine Learning. StandardScaler is an important technique that is mainly performed as a preprocessing step before many machine learning models, in order to standardize the range of functionality of the input dataset. Also, Read – Why Python is the best language for Machine Learning. Some machine learning practitioners tend to standardize their data blindly before each machine learning model without making the effort to understand why it should be used, or even whether it is needed or not. So you need to understand when you should use the StandardScaler to scale your data. When and How To Use StandardScaler? StandardScaler comes into play when the characteristics of the input dataset differ greatly between their ranges, or simply when they are measured in different units of measure. StandardScaler removes the mean and scales the data to the unit variance. However, outliers have an influence when calculating the empirical mean and standard deviation, which narrows the range of characteristic values. These differences in the initial features can cause problems for many machine learning models. For example, for models based on the calculation of distance, if one of the features has a wide range of values, the distance will be governed by that particular characteristic. The idea behind the StandardScaler is that variables that are measured at different scales do not contribute equally to the fit of the model and the learning function of the model and could end up creating a bias. So, to deal with this potential problem, we need to standardize the data (μ = 0, σ = 1) that is typically used before we integrate it into the machine learning model. Now, let’s see how to use StandardScaler using Scikit-learn: from sklearn.preprocessing import StandardScaler import numpy as np # 4 samples/observations and 2 variables/features X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]]) # the scaler object (model) scaler = StandardScaler() # fit and transform the data scaled_data = scaler.fit_transform(X) print(X)Code language: PHP (php) [[0, 0], [1, 0], [0, 1], [1, 1]]) print(scaled_data)Code language: PHP (php) [[-1. -1.] [ 1. -1.] [-1. 1.] [ 1. 1.]] To verify the mean of features is 0: scaled_data.mean(axis = 0) array([0., 0.]) I hope you liked this article on the StandardScaler in Machine Learning. Feel free to ask your valuable questions in the comments section below. Also, Read – How to predict IPL winner with Machine Learning. Follow Us: Facebook Instagram;StandardScaler in Machine Learning
2020-09-24 16:42:42;You must have used Instagram once in your life, have you noticed that you see a variety of Instagram filters when uploading an image. These filters are designed to improve the quality of the image. These filters are an example of complex machine learning algorithms that are deployed in production to serve Instagram. So if these filters are created by machine learning algorithms, it means that we can also create Instagram filters with Python.In this article, I’ll walk you through how to create beautiful Instagram filters using machine learning. The best part of this task is that we are using Python. The biggest advantage of using such a popular language like Python is that you will get packages for almost every task.Also, Read – Machine Learning Full Course For Free.;https://thecleverprogrammer.com/2020/09/24/instagram-filters-with-python/;['opencv-python'];1.0;['CV'];['ML', 'CV', 'NN'];['computer vision', 'model', 'machine learning', 'neural network', 'filter'];"You must have used Instagram once in your life, have you noticed that you see a variety of Instagram filters when uploading an image. These filters are designed to improve the quality of the image. These filters are an example of complex machine learning algorithms that are deployed in production to serve Instagram. So if these filters are created by machine learning algorithms, it means that we can also create Instagram filters with Python. In this article, I’ll walk you through how to create beautiful Instagram filters using machine learning. The best part of this task is that we are using Python. The biggest advantage of using such a popular language like Python is that you will get packages for almost every task. Also, Read – Machine Learning Full Course For Free. Creating Instagram Filters with Python While building a neural network to create Instagram filters is a complex task. In this article, I will use the Instafilter library in Python which will help us to use Instagram filters with Python. If you want to how we can create such amazing filters with Python without using the Instafilter package just mention in the comments section below. Now let’s see how we can use the Instafilter library in Python to create Instagram Filters with Python. You can easily install this library by using the pip command – pip install instafilter. I hope you will not get any errors while installing this library. The other library we need for creating Instagram filters is the Open Source Computer Vision Library of Python which is OpenCV. If you have never worked with OpenCV you can easily install it by using a pip command – pip install opencv-python. To use OpenCV we do not import it by the same name, we import it by the name of cv2 (import cv2). Let’s create Instagram filters with Python: from instafilter import Instafilter model = Instafilter(""Lo-fi"") new_image = model(""image.jpg"") # To save the image, use cv2 import cv2 cv2.imwrite(""modified_image.jpg"", new_image)Code language: PHP (php) Your saved images will look like: I hope you liked this article on how we can create Instagram filters. Feel Free to ask your valuable questions in the comments section below. Also, Read – Learn how to prepare your data for Machine Learning Models. Follow Us: Facebook Instagram";Instagram Filters with Python
2020-09-26 11:44:34;The discovery of the TikTok Algorithm is a very popular and powerful recommendation system. Similar to Netflix and YouTube, the TikTok algorithm works out of you. It can only provide you with profiled recommendations if you use the app by interacting with it in some way. In this article, I’ll walk you through how to build a TikTok algorithm with Machine Learning.Just think about how long a person can watch a video, which videos he watches, which ones he skips, which ones he likes and on what videos he comments on, etc. These are features that we have to take into account to feed the algorithm.Also, Read – Machine Learning Full Course for free.TikTok has graciously shared the conceptual workings of their algorithm on its website. It focuses on the concepts of the algorithm, rather than the code, so here I’m going to apply those concepts to the code and share a simple guide to creating this type of popular algorithm.;https://thecleverprogrammer.com/2020/09/26/tiktok-algorithm-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Recommender', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'recommend', 'filter', 'train', 'label'];"The discovery of the TikTok Algorithm is a very popular and powerful recommendation system. Similar to Netflix and YouTube, the TikTok algorithm works out of you. It can only provide you with profiled recommendations if you use the app by interacting with it in some way. In this article, I’ll walk you through how to build a TikTok algorithm with Machine Learning. Just think about how long a person can watch a video, which videos he watches, which ones he skips, which ones he likes and on what videos he comments on, etc. These are features that we have to take into account to feed the algorithm. Also, Read – Machine Learning Full Course for free. TikTok has graciously shared the conceptual workings of their algorithm on its website. It focuses on the concepts of the algorithm, rather than the code, so here I’m going to apply those concepts to the code and share a simple guide to creating this type of popular algorithm. How the TikTok Algorithm Works? The TikTok algorithm splits into two types of recommendation algorithms which can ultimately be formed into a single ensemble-type algorithm, namely collaborative filtering and content-based filtering. Collaborative Filtering Collaborative filtering works by comparing you to others. Lets’s say you are new to this app and you start liking a video, you will start to see the recommended videos. These videos are the videos that other people like, who also liked the same past videos of you. This type of recommendation is technically called alternating least squares and matrix factorization. Content-Based Filtering Content-based filtering works by comparing video attributes to suggest similar videos sharing those same attributes. The basic idea of this recommendation is the cosine similarity in this case. Think about attributes of the video like length, sound, and text. These features are used to recommend videos that carry similar attribute values. Let’s Code The TikTok Algorithm Now, how do you implement the concepts of Collaborative-Filtering and Content-Based filtering to create a TikTok algorithm? Let’s see how we can code the TikTok algorithm with Machine Learning. Collaborative-Filtering: You need to start by importing your essential libraries from PySpark first. Next, you need to load your data and create a pandas dataframe. Then you divide your data into a train and a test. Then, you will adapt your ALS model (alternation of least squares). Next, you need to make predictions and evaluate the model against a designated metric; in this case, it was RMSE. Finally, you can print the video recommendations based on the columns or features that you have chosen to include in your user interaction dataset: from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml.recommendation import ALS from pyspark.sql import Row lines = spark.read.text(""TEXT_FILE.txt"").rdd parts = lines.map(lambda row: row.value.split(""::"")) liked_data = parts.map(lambda p: Row(COLUMN_1, COLUMN_2, like=like) liked_df = spark.createDataFrame(liked_data) (training, test) = liked_df.randomSplit([0.75, 0.25]) als = ALS(maxIter=4, regParam=0.05, users=""COLUMN_1"", items=""COLUMN_2"", ratingCol=""like"", coldStartStrategy=""drop"") model = als.fit(training) predictions = model.transform(test) evaluator = RegressionEvaluator(metricName=""rmse"", labelCol=""like"", predictionCol=""prediction"") rmse = evaluator.evaluate(predictions) print(""RMSE = "" + str(rmse)) videoRecs = model.recommendForUsers(20) view raw TitTok Algorithm hosted with ❤ by GitHub View this gist on GitHub Content-Based Filtering: This code is relatively simple, but you can also expand it. Essentially, you will import your libraries and use the consine_similarity library which will calculate the videos from the past, to suggest similar videos as recommendations: from sklearn.metrics.pairwise import cosine_similarity import pandas as pd data = pd.read_csv(""DATA.csv"") # computing the cosine similarity alg = cosine_similarity(data) view raw TitTok Algorithm hosted with ❤ by GitHub View this gist on GitHub I hope you like this article on the TikTok Algorithm with Machine Learning. These algorithms can be used not only for TikTok recommendations but also for other similar platforms. Feel free to ask your valuable questions in the comments section below. Also, Read – How To Improve Your Programming Skills? Follow Us: Facebook Instagram";TikTok Algorithm with Machine Learning
2020-09-27 22:18:04;In this article, I will take you through how we can classify the nationalities of people by using their names. You will be thinking about how we can classify nationalities by using just names. There is a lot about how we can play with names.;https://thecleverprogrammer.com/2020/09/27/classify-nationalities-with-machine-learning/;['keras', 'sklearn', 'tensorflow'];1.0;[];['ML', 'Classification', 'LSTM', 'NN'];['epoch', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'train', 'lstm', 'label'];"In this article, I will take you through how we can classify the nationalities of people by using their names. You will be thinking about how we can classify nationalities by using just names. There is a lot about how we can play with names. Classify Nationalities Let’s get started with this machine learning task to classify nationalities by importing the necessary packages. I will classify nationalities based on names as Indian or Non-Indian. So, let’s import some packages and get started with the task: Also, Read – Machine Learning Full Course for free. from tensorflow import keras import tensorflow as tf import pandas as pd import os import reCode language: JavaScript (javascript) Now, let’s import the datasets. The datasets I am using here in this article can be easily downloaded from here. Now after importing the datasets I will prepare two helper functions for data cleaning and data processing: male_data = pd.read_csv(male.csv) female_data = pd.read_csv(femaile.csv) repl_list = ['s/o','d/o','w/o','/','&',',','-'] def clean_data(name): name = str(name).lower() name = (''.join(i for i in name if ord(i)<128)).strip() for repl in repl_list: name = name.replace(repl,"" "") if '@' in name: pos = name.find('@') name = name[:pos].strip() name = name.split("" "") name = "" "".join([each.strip() for each in name]) return name def remove_records(merged_data): merged_data['delete'] = 0 merged_data.loc[merged_data['name'].str.find('with') != -1,'delete'] = 1 merged_data.loc[merged_data['count_words']>=5,'delete']=1 merged_data.loc[merged_data['count_words']==0,'delete']=1 merged_data.loc[merged_data['name'].str.contains(r'\d') == True,'delete']=1 cleaned_data = merged_data[merged_data.delete==0] return cleaned_data merged_data = pd.concat((male_data,female_data),axis=0) merged_data['name'] = merged_data['name'].apply(clean_data) merged_data['count_words'] = merged_data['name'].str.split().apply(len) cleaned_data = remove_records(merged_data) indian_cleaned_data = cleaned_data[['name','count_words']].drop_duplicates(subset='name',keep='first') indian_cleaned_data['label'] = 'indian' len(indian_cleaned_data) view raw classify nationalities hosted with ❤ by GitHub View this gist on GitHub 13754 After loading and removing the wrong entries in the data, we got a few records around 13,000. For non-Indian names, there is a nifty package called Faker. This generates names from different regions: from faker import Faker fake = Faker(‘en_US’) fake.name()Code language: JavaScript (javascript) ‘Brian Evans’ We have generated approximately the same number of names as we have in the Indian data set. We then removed samples longer than 5 words. The Indian data set contained a lot of names with just first names. So we need to make the overall non-Indian distribution also similar. non_indian_data.head()Code language: CSS (css) namecount_words0sara gulbrandsen21kathryn villarreal22jennifer mccormick23james eaton24melissa bond2 We end up with about 14,000 non-Indian names and 13,000 Indian names. Now let’s build a neural network to classify nationalities using names: from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.preprocessing import sequence from keras.preprocessing.text import Tokenizer from keras.layers.embeddings import Embedding from keras.utils import to_categorical import numpy as np from sklearn.preprocessing import LabelEncoder from keras.callbacks import Callback np.random.seed(42) def char_encoded_representation(data,tokenizer,vocab_size,max_len): char_index_sentences = tokenizer.texts_to_sequences(data) sequences = [to_categorical(x, num_classes=vocab_size) for x in char_index_sentences] X = sequence.pad_sequences(sequences, maxlen=max_len) return X def build_model(hidden_units,max_len,vocab_size): model = Sequential() model.add(LSTM(hidden_units,input_shape=(max_len,vocab_size))) model.add(Dense(1, activation=’sigmoid’)) model.compile(loss=’binary_crossentropy’, optimizer=’adam’, metrics=[‘accuracy’]) print(model.summary()) return model model = build_model(100,max_len,vocab_size) model.fit(X_train, y_train, epochs=20, batch_size=64,callbacks=myCallback(X_test,y_test)) view raw classify nationalities hosted with ❤ by GitHub View this gist on GitHub namespredictions_lstm_char0lalithaindian1tysonnon_indian2shailajaindian3shyamalaindian4vishwanathanindian5ramanujamindian6conannon_indian7kryslovskynon_indian8ratnaniindian9diegonon_indian10kakoliindian11shreyasindian12braydennon_indian13shanonnon_indian So this is how we can easily classify nationalities with machine learning. I did not include the full code and exploration here, you can have a look at the full code from here. Feel free to ask your valuable questions in the comments section below. Also, Read – How to Save Machine Learning Models? Follow Us: Facebook Instagram";Classify Nationalities with Machine Learning
2020-09-27 18:59:19;In this article, I’ll walk you through how to save machine learning models. Model progress can be recorded during and after training. This means that a model can pick up where it left off and avoid long periods of training. You should know how to save complex machine learning models so that you can share your model and your team members can recreate your work.You should know how to save machine learning models because in the professional world you will be working with as complex machine learning models which will need you to work within teams and will require days. So to share your work within the teams and continue where you left will always require you to know how you can save machine learning models.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/09/27/save-machine-learning-models/;['keras', 'tensorflow'];1.0;[];['ML', 'ReLu'];['epoch', 'fit', 'model', 'loss', 'machine learning', 'relu', 'layer', 'train', 'label'];"In this article, I’ll walk you through how to save machine learning models. Model progress can be recorded during and after training. This means that a model can pick up where it left off and avoid long periods of training. You should know how to save complex machine learning models so that you can share your model and your team members can recreate your work. You should know how to save machine learning models because in the professional world you will be working with as complex machine learning models which will need you to work within teams and will require days. So to share your work within the teams and continue where you left will always require you to know how you can save machine learning models. Also, Read – Machine Learning Full Course for free. How to Save Machine Learning Models? Now to demonstrate to you how to save machine learning models, I will first train a TensorFlow model because these models are large and complex enough that they need to be saved and worked within teams. So I will start by importing all the necessary packages we need for this task: import os import tensorflow as tf from tensorflow import kerasCode language: JavaScript (javascript) Now, we need a dataset to train our model. I will use the MNIST dataset: (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data() train_labels = train_labels[:1000] test_labels = test_labels[:1000] train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0 test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0 Building a Model Now to move forward with this task I will build a simple sequential model: # Define a simple sequential model def create_model(): model = tf.keras.models.Sequential([ keras.layers.Dense(512, activation='relu', input_shape=(784,)), keras.layers.Dropout(0.2), keras.layers.Dense(10) ]) model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[tf.metrics.SparseCategoricalAccuracy()]) return model # Create a basic model instance model = create_model() # Display the model's architecture model.summary()Code language: PHP (php) Model: ""sequential"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 512) 401920 _________________________________________________________________ dropout (Dropout) (None, 512) 0 _________________________________________________________________ dense_1 (Dense) (None, 10) 5130 ================================================================= Total params: 407,050 Trainable params: 407,050 Non-trainable params: 0 To Save Checkpoints While Training You can use a trained model without having to retrain it or resume training where you left off, in case the training process is interrupted. The ModelCheckpoint callback method allows you to permanently save the model during and at the end of the training: checkpoint_path = ""training_1/cp.ckpt"" checkpoint_dir = os.path.dirname(checkpoint_path) # Create a callback that saves the model's weights cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1) # Train the model with the new callback model.fit(train_images, train_labels, epochs=10, validation_data=(test_images,test_labels), callbacks=[cp_callback]) # Pass callback to training Code language: PHP (php) Create a new untrained model. When you are restoring a model from weights only, you must have another model with the same architecture as the trained model. Because this is the same model architecture, you can share weights even if it is a different instance of the model. Now rebuild a new untrained model and rate it on the test set. An untrained model will run at random levels (about 10% accuracy): # Create a basic model instance model = create_model() # Evaluate the model loss, acc = model.evaluate(test_images, test_labels, verbose=2) print(""Untrained model, accuracy: {:5.2f}%"".format(100*acc))Code language: PHP (php) 32/32 - 0s - loss: 2.3319 - sparse_categorical_accuracy: 0.1410 Untrained model, accuracy: 14.10% Now load the weights and reevaluate: # Loads the weights model.load_weights(checkpoint_path) # Re-evaluate the model loss,acc = model.evaluate(test_images, test_labels, verbose=2) print(""Restored model, accuracy: {:5.2f}%"".format(100*acc))Code language: PHP (php) 32/32 - 0s - loss: 0.4239 - sparse_categorical_accuracy: 0.8680 Restored model, accuracy: 86.80% Callback Checkpoint Train a new model with unique checkpoints: # Include the epoch in the file name (uses `str.format`) checkpoint_path = ""training_2/cp-{epoch:04d}.ckpt"" checkpoint_dir = os.path.dirname(checkpoint_path) # Create a callback that saves the model's weights every 5 epochs cp_callback = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_path, verbose=1, save_weights_only=True, period=5) # Create a new model instance model = create_model() # Save the weights using the `checkpoint_path` format model.save_weights(checkpoint_path.format(epoch=0)) # Train the model with the new callback model.fit(train_images, train_labels, epochs=50, callbacks=[cp_callback], validation_data=(test_images,test_labels), verbose=0)Code language: PHP (php) Now, reset the model and load the latest checkpoint: # Create a new model instance model = create_model() # Load the previously saved weights model.load_weights(latest) # Re-evaluate the model loss, acc = model.evaluate(test_images, test_labels, verbose=2) print(""Restored model, accuracy: {:5.2f}%"".format(100*acc))Code language: PHP (php) Manually save weights Now, let’s see how we can manually save weights of the model: # Save the weights model.save_weights('./checkpoints/my_checkpoint') # Create a new model instance model = create_model() # Restore the weights model.load_weights('./checkpoints/my_checkpoint') # Evaluate the model loss,acc = model.evaluate(test_images, test_labels, verbose=2) print(""Restored model, accuracy: {:5.2f}%"".format(100*acc))Code language: PHP (php) 32/32 - 0s - loss: 0.5075 - sparse_categorical_accuracy: 0.8730 Restored model, accuracy: 87.30% To Save Entire Machine Learning Model Call model.save to save the architecture, weights, and training configuration of a model in a single file/folder. This will allow you to export a model so that it can be used without having the code that is used in training of the model. Since the optimizer state is recovered, you can resume training exactly where you left off. # Create and train a new model instance. model = create_model() model.fit(train_images, train_labels, epochs=5) # Save the entire model as a SavedModel. !mkdir -p saved_model model.save('saved_model/my_model') Code language: PHP (php) This will save your model in the directory. I hope you liked this article on how to save machine learning models. Feel free to ask your valuable questions in the comments section below. Also, Read – Best Data Science Certifications Available Online. Follow Us: Facebook Instagram";Save Machine Learning Models
2020-09-29 15:58:41;Fake Currency Detection is a real problem for both individuals and businesses. Counterfeiters are constantly finding new methods and techniques to produce counterfeit banknotes, which are essentially indistinguishable from real money. At least for the human eye. In this article, I will introduce you to Fake Currency Detection with Machine Learning.Fake Currency Detection is a task of binary classification in machine learning. If we have enough data on real and fake banknotes, we can use that data to train a model that can classify the new banknotes as real or fake.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/09/29/fake-currency-detection-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Logistic Regression', 'Classification', 'Regression'];['detect', 'regression', 'predict', 'fit', 'model', 'machine learning', 'classif', 'train', 'logistic regression'];"Fake Currency Detection is a real problem for both individuals and businesses. Counterfeiters are constantly finding new methods and techniques to produce counterfeit banknotes, which are essentially indistinguishable from real money. At least for the human eye. In this article, I will introduce you to Fake Currency Detection with Machine Learning. Fake Currency Detection is a task of binary classification in machine learning. If we have enough data on real and fake banknotes, we can use that data to train a model that can classify the new banknotes as real or fake. Also, Read – Machine Learning Full Course for free. Fake Currency Detection The dataset I will use in this task for fake currency detection can be downloaded from here. The dataset contains these four input characteristics: The variance of the image transformed into waveletsThe asymmetry of the image transformed into waveletsKurtosis of the image transformed into waveletsImage entropy The target value is simply 0 for real banknotes and 1 for fake banknotes. Now let’s get started with this task of Fake Currency Detection with Machine Learning. I will start this task by importing the necessary packages: import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix view raw fake currency hosted with ❤ by GitHub View this gist on GitHub Now, let’s have a look at the dataset, the data does not have headings so I will also assign headings in the process and then I will print the first 5 rows from the data: data = pd.read_csv('data_banknote_authentication.txt', header=None) data.columns = ['var', 'skew', 'curt', 'entr', 'auth'] print(data.head()) view raw fake currency hosted with ❤ by GitHub View this gist on GitHub var skew curt entr auth 0 3.62160 8.6661 -2.8073 -0.44699 0 1 4.54590 8.1674 -2.4586 -1.46210 0 2 3.86600 -2.6383 1.9242 0.10645 0 3 3.45660 9.5228 -4.0112 -3.59440 0 4 0.32924 -4.4552 4.5718 -0.98880 0 Data Exploration Now let’s start exploring the dataset. First, I’ll check the data types and if there are any missing values ​​in the data: print(data.info)Code language: CSS (css) We, therefore, have no missing values ​​in the data. We can now draw a pair diagram to get an overview of the relationship between all the entities. I will also colour the observations: blue for genuine banknotes and orange for counterfeit banknotes: sns.pairplot(data, hue='auth') plt.show() Code language: JavaScript (javascript) From this pair plot we can make several interesting observations: The distribution of both variance and skewness appears to be quite different for the two target characteristics, while kurtosis and entropy appear to be more similar.There are clear linear and nonlinear trends in the input features.Some characteristics seem to be correlated.Some features seem to separate genuine and fake banknotes quite well. Now let’s check if our data is balanced against the target values: plt.figure(figsize=(8,6)) plt.title('Distribution of Target', size=18) sns.countplot(x=data['auth']) target_count = data.auth.value_counts() plt.annotate(s=target_count[0], xy=(-0.04,10+target_count[0]), size=14) plt.annotate(s=target_count[1], xy=(0.96,10+target_count[1]), size=14) plt.ylim(0,900) plt.show() view raw fake currency hosted with ❤ by GitHub View this gist on GitHub The dataset is fairly balanced, but for the binary classification task, we need to balance it perfectly. So let’s start preprocessing the data by doing just that. Data Processing Now we need to balance our data, the easiest way to do this is to randomly drop a number of instances of the overrepresented target function. This is called random undersampling. Otherwise, we could also create new synthetic data for the under-represented target class. This is called oversampling. For now, let’s start by randomly deleting 152 observations of actual banknotes: nb_to_delete = target_count[0] - target_count[1] data = data.sample(frac=1, random_state=42).sort_values(by='auth') data = data[nb_to_delete:] print(data['auth'].value_counts()) view raw fake currency hosted with ❤ by GitHub View this gist on GitHub 1 610 0 610 Name: auth, dtype: int64 Now we have a perfectly balanced dataset. Next, we need to divide the data into training and test sets: x = data.loc[:, data.columns != 'auth'] y = data.loc[:, data.columns == 'auth'] x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42) view raw fake currency hosted with ❤ by GitHub View this gist on GitHub Now I will standardize the data by using the StandardScalar method provided by Scikit-learn: scalar = StandardScaler() scalar.fit(x_train) x_train = scalar.transform(x_train) x_test = scalar.transform(x_test) view raw fake currency hosted with ❤ by GitHub View this gist on GitHub Logistic Regression for Fake Currency Detection Now, I will train and test our model for fake currency detection by using the Logistic Regressing Algorithm. Let’s first fit the data on the Logistic Regression model to train the model: clf = LogisticRegression(solver='lbfgs', random_state=42, multi_class='auto') clf.fit(x_train, y_train.values.ravel())Code language: JavaScript (javascript) Now let’s test the accuracy of our model: y_pred = np.array(clf.predict(x_test)) conf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=[""Pred.Negative"", ""Pred.Positive""], index=['Act.Negative', ""Act.Positive""]) tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel() accuracy = round((tn+tp)/(tn+fp+fn+tp), 4) print(conf_mat) print(f'\n Accuracy = {round(100*accuracy, 2)}%') view raw fake currency hosted with ❤ by GitHub View this gist on GitHub Pred.Negative Pred.Positive Act.Negative 187 6 Act.Positive 0 173 Accuracy = 98.36% The logistic regression model achieved an accuracy of 98.36%. And not only that, when our fake currency detection model predicted that a banknote was real, it was correct 100% of the time. Now let’s simulate the prediction of a single banknote. All we need to do is extract the features, scale them, and integrate them into our pre-trained model. We can also inspect the banknote probabilities of belonging to each target class: new_banknote = np.array([4.5, -8.1, 2.4, 1.4], ndmin=2) new_banknote = scalar.transform(new_banknote) print(f'Prediction: Class{clf.predict(new_banknote)[0]}') print(f'Probability [0/1]: {clf.predict_proba(new_banknote)[0]}') view raw fake currency hosted with ❤ by GitHub View this gist on GitHub Prediction: Class0Probability [0/1]: [0.61112576 0.38887424] Our model predicts that this banknote is real. I hope you liked this article on fake currency detection with machine learning. Feel free to ask your valuable questions in the comments section below. Also, Read – Python Coding Interview Tips. Follow Us: Facebook Instagram";Fake Currency Detection with Machine Learning
2020-09-30 13:02:37;In this article, I’ll walk you through how to convert an image to a pencil sketch with Python in less than 20 lines of code. Python is a general-purpose programming language and with the growing popularity of Python, it can be used in any task today.;https://thecleverprogrammer.com/2020/09/30/pencil-sketch-with-python/;['opencv-python'];1.0;['CV'];['ML', 'CV'];['machine learning'];"In this article, I’ll walk you through how to convert an image to a pencil sketch with Python in less than 20 lines of code. Python is a general-purpose programming language and with the growing popularity of Python, it can be used in any task today. Image to Pencil Sketch with Python Before we write any code, let’s go over some of the steps that will be used and try to understand them a bit. First, find an image that you want to convert to a pencil sketch with Python. I will be using the image of a puppy as you can see below. Also, Read – Machine Learning Full Course for free. Source – https://www.wallpapertip.com/ Next, we need to read the image in RBG format and then convert it to a grayscale image. This will turn an image into a classic black and white photo. Then the next thing to do is invert the grayscale image also called negative image, this will be our inverted grayscale image. Inversion can be used to enhance details. Then we can finally create the pencil sketch by mixing the grayscale image with the inverted blurry image. This can be done by dividing the grayscale image by the inverted blurry image. Since images are just arrays, we can easily do this programmatically using the divide function from the cv2 library in Python. Let’s Code The only library we need for converting an image into a pencil sketch with Python is an OpenCV library in Python. It can be used by using the pip command; pip install opencv-python. But it is not imported by the same name. Let’s import it to get started with the task: import cv2Code language: JavaScript (javascript) I will not display the image at every step, if you want to display the image at every step to see the changes in the image then you need to use two commands; cv2.imshow(“Title You want to give”, Image) and then simply write cv2.waitKey(0). This will display the image. Now the next thing to do is to read the image: image = cv2.imread(""dog.jpg"") cv2.imshow(""Dog"", image) cv2.waitKey(0)Code language: JavaScript (javascript) Now after reading the image, we will create a new image by converting the original image to greyscale: gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) cv2.imshow(""New Dog"", gray_image) cv2.waitKey(0)Code language: JavaScript (javascript) Now the next step is to invert the new grayscale image: inverted_image = 255 - gray_image cv2.imshow(""Inverted"", inverted_image) cv2.waitKey() Code language: JavaScript (javascript) Now the next step in the process is to blur the image by using the Gaussian Function in OpenCV: blurred = cv2.GaussianBlur(inverted_image, (21, 21), 0)Code language: Python (python) Then the final step is to invert the blurred image, then we can easily convert the image into a pencil sketch: inverted_blurred = 255 - blurred pencil_sketch = cv2.divide(gray_image, inverted_blurred, scale=256.0) cv2.imshow(""Sketch"", pencil_sketch) cv2.waitKey(0) Code language: JavaScript (javascript) And finally, if you want to have a look at both the original image and the pencil sketch then you can use the following commands: cv2.imshow(""original image"", image) cv2.imshow(""pencil sketch"", pencil_sketch) cv2.waitKey(0)Code language: CSS (css) So this is how we can convert an image into a pencil sketch with Python. I hope you liked this article on how to convert an image into a pencil sketch with Python. Feel free to ask your valuable questions in the comments section below. Also, Read – Google Search Algorithm with Python. Follow Us: Facebook Instagram";Pencil Sketch with Python
2020-10-01 20:36:14;In this article, I will take you through how we can predict the US presidential elections with Python. Here, I will not train any machine learning model. I will analyze the sentiments of people for the candidates and then at the end, I will conclude based on the most number of positive and negative tweets against the candidates.The datasets that I am using in this task to predict the US Elections are collected from twitter by the official twitter handles of Donald Trump and Joe Biden. You can download the datasets that I am using from here.Also, Read – Machine Learning Full Course For Free.;https://thecleverprogrammer.com/2020/10/01/predict-us-elections-with-python/;['textblob'];1.0;[];['ML', 'Text Classification', 'Sentiment Analysis', 'Classification'];['text classification', 'sentiment analysis', 'predict', 'model', 'machine learning', 'classif', 'train', 'label'];"In this article, I will take you through how we can predict the US presidential elections with Python. Here, I will not train any machine learning model. I will analyze the sentiments of people for the candidates and then at the end, I will conclude based on the most number of positive and negative tweets against the candidates. The datasets that I am using in this task to predict the US Elections are collected from twitter by the official twitter handles of Donald Trump and Joe Biden. You can download the datasets that I am using from here. Also, Read – Machine Learning Full Course For Free. Predict US Elections with Python Now without wasting any time let’s get started with this task to predict the US Elections with Python by importing the necessary libraries and the datasets: import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from textblob import TextBlob from wordcloud import WordCloud import plotly.graph_objects as go import plotly.express as px trump_reviews = pd.read_csv(""Trumpall2.csv"") biden_reviews = pd.read_csv(""Bidenall2.csv"") view raw US Elections hosted with ❤ by GitHub View this gist on GitHub Now let’s have a quick look at the first 5 rows from both the datasets: print(trump_reviews.head()) print(biden_reviews.head())Code language: CSS (css) user text 0 manny_rosen @sanofi please tell us how many shares the Cr... 1 osi_abdul https://t.co/atM98CpqF7 Like, comment, RT #P... 2 Patsyrw Your AG Barr is as useless & corrupt as y... 3 seyedebrahimi_m Mr. Trump! Wake Up! Most of the comments bel... 4 James09254677 After 4 years you think you would have figure... user text 0 MarkHodder3 @JoeBiden And we’ll find out who won in 2026... 1 K87327961G @JoeBiden Your Democratic Nazi Party cannot be... 2 OldlaceA @JoeBiden So did Lying Barr 3 penblogger @JoeBiden It's clear you didnt compose this tw... 4 Aquarian0264 @JoeBiden I will vote in person thank you. Sentiment Analysis Now, I will get started with sentiment analysis. I will do it by using the Textblob package in Python. Here I will use this package to perform simple text classification in either positive or negative on the basis of sentiment analysis: textblob1 = TextBlob(trump_reviews[""text""][10]) print(""Trump :"",textblob1.sentiment) textblob2 = TextBlob(biden_reviews[""text""][500]) print(""Biden :"",textblob2.sentiment) view raw US Elections hosted with ❤ by GitHub View this gist on GitHub Trump : Sentiment(polarity=0.15, subjectivity=0.3125)Biden : Sentiment(polarity=0.6, subjectivity=0.9) def find_pol(review): return TextBlob(review).sentiment.polarity trump_reviews[""Sentiment Polarity""] = trump_reviews[""text""].apply(find_pol) print(trump_reviews.tail()) biden_reviews[""Sentiment Polarity""] = biden_reviews[""text""].apply(find_pol) print(biden_reviews.tail()) view raw US Elections hosted with ❤ by GitHub View this gist on GitHub user ... Sentiment Polarity 2783 4diva63 ... 0.000 2784 hidge826 ... 0.000 2785 SpencerRossy ... 0.225 2786 ScoobyMcpherson ... 0.000 2787 bjklinz ... -0.500 user ... Sentiment Polarity 2535 meryn1977 ... 0.15 2536 BSNelson114 ... 0.00 2537 KenCapel ... 0.00 2538 LeslyeHale ... 0.10 2539 rerickre ... 0.20 Now before moving forward let’s understand what is Polarity. Polarity ranges from -1 to +1(negative to positive) and tells whether the text has negative sentiments or positive sentiments. Polarity tells about factual information. Sentiment Polarity on Both the candidates: Now I will add a new attribute in both the datasets by the name of “Expression Label”: trump_reviews[""Expression Label""] = np.where(trump_reviews[""Sentiment Polarity""]>0, ""positive"", ""negative"") trump_reviews[""Expression Label""][trump_reviews[""Sentiment Polarity""]==0]=""Neutral"" print(trump_reviews.tail()) biden_reviews[""Expression Label""] = np.where(biden_reviews[""Sentiment Polarity""]>0, ""positive"", ""negative"") biden_reviews[""Expression Label""][trump_reviews[""Sentiment Polarity""]==0]=""Neutral"" print(biden_reviews.tail()) view raw US Elections hosted with ❤ by GitHub View this gist on GitHub Now I will drop all the tweets with neutral polarity from both the datasets to balance the data equally. I will also perform some data cleaning operations so that at the can we can easily predict the US Elections: reviews1 = trump_reviews[trump_reviews['Sentiment Polarity'] == 0.0000] print(reviews1.shape) cond1=trump_reviews['Sentiment Polarity'].isin(reviews1['Sentiment Polarity']) trump_reviews.drop(trump_reviews[cond1].index, inplace = True) print(trump_reviews.shape) reviews2 = biden_reviews[biden_reviews['Sentiment Polarity'] == 0.0000] print(reviews2.shape) cond2=biden_reviews['Sentiment Polarity'].isin(reviews1['Sentiment Polarity']) biden_reviews.drop(biden_reviews[cond2].index, inplace = True) print(biden_reviews.shape) view raw US Elections hosted with ❤ by GitHub View this gist on GitHub Now, before moving forward we need to balance both the datasets: # Donald Trump np.random.seed(10) remove_n =324 drop_indices = np.random.choice(trump_reviews.index, remove_n, replace=False) df_subset_trump = trump_reviews.drop(drop_indices) print(df_subset_trump.shape) # Joe Biden np.random.seed(10) remove_n =31 drop_indices = np.random.choice(biden_reviews.index, remove_n, replace=False) df_subset_biden = biden_reviews.drop(drop_indices) print(df_subset_biden.shape) view raw US Elections hosted with ❤ by GitHub View this gist on GitHub Now let’s analyze the data to predict the US Elections, by analyzing the number of positive and negative sentiments in both the accounts: count_1 = df_subset_trump.groupby('Expression Label').count() print(count_1) negative_per1 = (count_1['Sentiment Polarity'][0]/1000)*10 positive_per1 = (count_1['Sentiment Polarity'][1]/1000)*100 count_2 = df_subset_biden.groupby('Expression Label').count() print(count_2) negative_per2 = (count_2['Sentiment Polarity'][0]/1000)*100 positive_per2 = (count_2['Sentiment Polarity'][1]/1000)*100 Politicians = ['Joe Biden', 'Donald Trump'] lis_pos = [positive_per1, positive_per2] lis_neg = [negative_per1, negative_per2] fig = go.Figure(data=[ go.Bar(name='Positive', x=Politicians, y=lis_pos), go.Bar(name='Negative', x=Politicians, y=lis_neg) ]) # Change the bar mode fig.update_layout(barmode='group') fig.show() view raw US Elections hosted with ❤ by GitHub View this gist on GitHub From the above figure, it is very clear that Joe Biden is getting more Positive tweets and less negative tweets as compared to Donald Trump. So it will not be wrong to conclude that Joe Bined is more prefered by the people to win the US Presidential Elections than Donald Trump. I hope you liked this article on how to predict the US Elections winner. The analysis is totally based on the twitter data. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Predict US Elections with Python
2020-10-03 10:12:38;In this article, I will take you through Deepfake Detection with Python and Machine Learning. I recently wrote an article on what is Deepfake and how it is dangerous. If you don’t know what is Deepfake then I will suggest you have a quick look at my previous article here, before getting your hands dirty with the task of Deepfake detection with Python and Machine Learning.;https://thecleverprogrammer.com/2020/10/03/deepfake-detection-with-python/;['skimage'];1.0;['NN', 'CNN'];['ML', 'NN', 'CNN'];['detect', 'machine learning'];"In this article, I will take you through Deepfake Detection with Python and Machine Learning. I recently wrote an article on what is Deepfake and how it is dangerous. If you don’t know what is Deepfake then I will suggest you have a quick look at my previous article here, before getting your hands dirty with the task of Deepfake detection with Python and Machine Learning. Deepfake Detection with Python There have been many reports of fake videos of popular celebrities or politicians. These fake videos are difficult to detect with the naked eye and are becoming a major problem in society. Also, Read – Machine Learning Full Course for free. It has been experienced so far that the Deepfake videos go easily viral at platforms like Facebook, twitter, youtube, etc. As these platforms work to fix this issue, Facebook is making a big investment ($ 10 million) to fix this issue, and other platforms like Twitter and Google are also working on fixing this issue. Deepfake detection is therefore not an easy task. In this article, we will see how to identify the fakes from the real ones. It includes decomposing videos into a frame, detecting faces from real and fake videos, cropping faces and analyzing them. Deepfake Detection in Action Now let’s see how we can detection Deepfake content by using Python and Machine Learning. I will get started with this task by importing the necessary libraries: import numpy as np import matplotlib.pyplot as plt import cv2 import pandas as pd import glob2 import os, fnmatch from pathlib import Path # import mtcnn from mtcnn.mtcnn import MTCNN view raw Deepfake hosted with ❤ by GitHub View this gist on GitHub Now, let’s say we have two types of videos one real and one fake we want to detect which is the fake one amongst the two, probably this is what we need to do in the task of Deepfake detection. Now, I will create a function to process both the videos: def extract_multiple_videos(intput_filenames, image_path_infile): """"""Extract video files into sequence of images."""""" i = 1 # Counter of first video # Iterate file names: cap = cv2.VideoCapture('your_video_file_path.avi' or intput_filenames) if (cap.isOpened()== False): print(""Error opening file"") # Keep iterating break while True: ret, frame = cap.read() # Read frame from first video if ret: cv2.imwrite(os.path.join(image_path_infile , str(i) + '.jpg'), frame) # Write frame to JPEG file (1.jpg, 2.jpg, ...) # you can uncomment this line if you want to view them. # cv2.imshow('frame', frame) # Display frame for testing i += 1 # Advance file counter else: # Break the interal loop when res status is False. break cv2.waitKey(50) #Wait 50msec cap.release() view raw Deepfake hosted with ❤ by GitHub View this gist on GitHub Calling The function: extract_multiple_videos(fake_video_name, fake_image_path_for_frame) extract_multiple_videos(real_video_name, real_image_path_for_frame)Code language: Python (python) Now after running the function we will be able the read the videos and process them for our task of Deepfake detection. Now let’s see how we can identify the Deepfakes by comparing both the videos: from skimage import measure def mse(imageA, imageB): # the 'Mean Squared Error' between the two images is the # sum of the squared difference between the two images; # NOTE: the two images must have the same dimension err = np.sum((imageA.astype(""float"") - imageB.astype(""float"")) ** 2) err /= float(imageA.shape[0] * imageA.shape[1]) # return the MSE, the lower the error, the more ""similar"" # the two images are return err def compare_images(imageA, imageB, title): # compute the mean squared error and structural similarity # index for the images m = mse(imageA, imageB) s = measure.compare_ssim(imageA, imageB) # setup the figure fig = plt.figure(title) plt.suptitle(""MSE: %.2f, SSIM: %.2f"" % (m, s)) # show first image ax = fig.add_subplot(1, 2, 1) plt.imshow(imageA, cmap = plt.cm.gray) plt.axis(""off"") # show the second image ax = fig.add_subplot(1, 2, 2) plt.imshow(imageB, cmap = plt.cm.gray) plt.axis(""off"") # show the images plt.show() view raw Deepfake hosted with ❤ by GitHub View this gist on GitHub In the code above, we comparing the extracted images from the original video and the corresponding image from fake videos. In the last section of the code, I checked if both the two images have any differences. Output: So this is how we can detect the Deepfakes by comparing the original and fake files. I hope you liked this article on Deepfake Detection with Python and Machine Learning. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Deepfake Detection with Python
2020-10-04 14:40:26;Have you ever thought about how the autocorrect features works in the keyboard of a smartphone? Almost every smartphone brand irrespective of its price provides an autocorrect feature in their keyboards today. So let’s understand how the autocorrect features works. In this article, I will take you through how to build autocorrect with Python.;https://thecleverprogrammer.com/2020/10/04/autocorrect-with-python/;['vocabulary'];1.0;[];['ML', 'NLP'];['natural language processing', 'machine learning'];"Have you ever thought about how the autocorrect features works in the keyboard of a smartphone? Almost every smartphone brand irrespective of its price provides an autocorrect feature in their keyboards today. So let’s understand how the autocorrect features works. In this article, I will take you through how to build autocorrect with Python. Autocorrect with Python: How It Works? With the context of machine learning, autocorrect is based on natural language processing. As the name suggests it is programmed to correct spellings and errors while typing. So how it works? Also, Read – Machine Learning Full Course for free. Before I get into the coding stuff let’s understand how autocorrect works. Let’s say you typed a word in your keyboard if the word will exist in the vocabulary of our smartphone then it will assume that you have written the right word. Now it doesn’t matter whether you write a name, a noun or any word on the planet. If the word exists in the history of the smartphone, it will generalize the word as a correct word. What if the word doesn’t exist? If the word that you typed is a non-existing word in the history of our smartphone then the autocorrect is programmed to find the most similar words in the history of our smartphone. Build an Autocorrect with Python I hope you now know what autocorrect is and how it works. Now let’s see how we can build an autocorrect feature with Python. Like our smartphone uses history to match the type words whether it’s correct or not. So here we also need to use some words to put the functionality in our autocorrect. So I will use the text from a book which you can easily download from here. Now let’s get started with the task to build an autocorrect with Python. For this task, we need some libraries. The libraries that I am going to use are very general as a machine learning practitioner. So you must be having all the libraries installed in your system already except one. You need to install a library known as textdistance, which can be easily installed by using the pip command; pip install textdistance. Now let’s get started with this task by importing all the necessary packages and by reading our text file: import pandas as pd import numpy as np import textdistance import re from collections import Counter words = [] with open('moby.txt', 'r') as f: file_name_data = f.read() file_name_data=file_name_data.lower() words = re.findall('\w+',file_name_data) # This is our vocabulary V = set(words) print(f""The first ten words in the text are: \n{words[0:10]}"") print(f""There are {len(V)} unique words in the vocabulary."") view raw autocorrect hosted with ❤ by GitHub View this gist on GitHub The first ten words in the text are: ['moby', 'dick', 'by', 'herman', 'melville', '1851', 'etymology', 'supplied', 'by', 'a'] There are 17140 unique words in the vocabulary. In the above code, we made a list of words, and now we need to build the frequency of those words, which can be easily done by using the counter function in Python: word_freq_dict = {} word_freq_dict = Counter(words) print(word_freq_dict.most_common()[0:10]) view raw autocorrect hosted with ❤ by GitHub View this gist on GitHub [('the', 14431), ('of', 6609), ('and', 6430), ('a', 4736), ('to', 4625), ('in', 4172), ('that', 3085), ('his', 2530), ('it', 2522), ('i', 2127)] Relative Frequency of words Now we want to get the probability of occurrence of each word, this equals the relative frequencies of the words: probs = {} Total = sum(word_freq_dict.values()) for k in word_freq_dict.keys(): probs[k] = word_freq_dict[k]/Total view raw autocorrect hosted with ❤ by GitHub View this gist on GitHub Finding Similar Words Now we will sort similar words according to the Jaccard distance by calculating the 2 grams Q of the words. Next, we will return the 5 most similar words ordered by similarity and probability: def my_autocorrect(input_word): input_word = input_word.lower() if input_word in V: return('Your word seems to be correct') else: similarities = [1-(textdistance.Jaccard(qval=2).distance(v,input_word)) for v in word_freq_dict.keys()] df = pd.DataFrame.from_dict(probs, orient='index').reset_index() df = df.rename(columns={'index':'Word', 0:'Prob'}) df['Similarity'] = similarities output = df.sort_values(['Similarity', 'Prob'], ascending=False).head() return(output) view raw autocorrect hosted with ❤ by GitHub View this gist on GitHub Now, let’s find the similar words by using our autocorrect function: my_autocorrect('neverteless')Code language: JavaScript (javascript) WordProbSimilarity2209nevertheless0.0002290.75000013300boneless0.0000140.41666712309elevates0.0000050.416667718never0.0009420.4000006815level0.0001100.400000 As we took words from a book the same way their are some words already present in the vocabulary of the smartphone and some words it records while the user starts using the keyboard. I hope you liked this article on how to build an autocorrect feature with Python. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Autocorrect with Python
2020-10-05 09:50:04;In this article, I will use the YouTube trending videos dataset and the Python programming language to train a model of text generation language using machine learning, which will be used for the task of title generator for youtube videos or even for your blogs.Title generator is a natural language processing task and is a central issue for several machine learning, including text synthesis, speech to text, and conversational systems. Also, Read – Machine Learning Full Course for free.To build a model for the task of title generator or a text generator, the model should be trained to learn the probability of a word occurring, using words that have already appeared in the sequence as context.;https://thecleverprogrammer.com/2020/10/05/title-generator-with-machine-learning/;['keras', 'tensorflow'];1.0;['RNN', 'NN'];['LSTM', 'RNN', 'NN', 'ML', 'NLP'];['hidden layer', 'epoch', 'input layer', 'output layer', 'predict', 'fit', 'recurrent neural network', 'model', 'loss', 'machine learning', 'neural network', 'layer', 'natural language processing', 'train', 'lstm', 'label', 'neuron'];"In this article, I will use the YouTube trending videos dataset and the Python programming language to train a model of text generation language using machine learning, which will be used for the task of title generator for youtube videos or even for your blogs. Title generator is a natural language processing task and is a central issue for several machine learning, including text synthesis, speech to text, and conversational systems. Also, Read – Machine Learning Full Course for free. To build a model for the task of title generator or a text generator, the model should be trained to learn the probability of a word occurring, using words that have already appeared in the sequence as context. Title Generator with Machine Learning I will start this task to build a title generator with Python and machine learning by importing the libraries and by reading the datasets. The datasets that I am using in this task can be downloaded from here: import pandas as pd import string import numpy as np import json from keras.preprocessing.sequence import pad_sequences from keras.layers import Embedding, LSTM, Dense, Dropout from keras.preprocessing.text import Tokenizer from keras.callbacks import EarlyStopping from keras.models import Sequential import keras.utils as ku import tensorflow as tf tf.random.set_seed(2) from numpy.random import seed seed(1) #load all the datasets df1 = pd.read_csv('USvideos.csv') df2 = pd.read_csv('CAvideos.csv') df3 = pd.read_csv('GBvideos.csv') #load the datasets containing the category names data1 = json.load(open('US_category_id.json')) data2 = json.load(open('CA_category_id.json')) data3 = json.load(open('GB_category_id.json')) view raw Title Generator hosted with ❤ by GitHub View this gist on GitHub Now we need to process our data so that we can use this data to train our machine learning model for the task of title generator. Here are all the data cleaning and processing steps that we need to follow: def category_extractor(data): i_d = [data['items'][i]['id'] for i in range(len(data['items']))] title = [data['items'][i]['snippet'][""title""] for i in range(len(data['items']))] i_d = list(map(int, i_d)) category = zip(i_d, title) category = dict(category) return category #create a new category column by mapping the category names to their id df1['category_title'] = df1['category_id'].map(category_extractor(data1)) df2['category_title'] = df2['category_id'].map(category_extractor(data2)) df3['category_title'] = df3['category_id'].map(category_extractor(data3)) #join the dataframes df = pd.concat([df1, df2, df3], ignore_index=True) #drop rows based on duplicate videos df = df.drop_duplicates('video_id') #collect only titles of entertainment videos #feel free to use any category of video that you want entertainment = df[df['category_title'] == 'Entertainment']['title'] entertainment = entertainment.tolist() #remove punctuations and convert text to lowercase def clean_text(text): text = ''.join(e for e in text if e not in string.punctuation).lower() text = text.encode('utf8').decode('ascii', 'ignore') return text corpus = [clean_text(e) for e in entertainment] view raw Title Generator hosted with ❤ by GitHub View this gist on GitHub Generating Sequences Natural language processing tasks require input data in the form of a sequence of tokens. The first step after data cleansing is to generate a sequence of n-gram tokens. An n-gram is an adjacent sequence of n elements of a given sample of text or vocal corpus. Elements can be words, syllables, phonemes, letters, or base pairs. In this case, the n-grams are a sequence of words in a corpus of titles. Tokenization is the process of extracting tokens from the corpus: tokenizer = Tokenizer() def get_sequence_of_tokens(corpus): #get tokens tokenizer.fit_on_texts(corpus) total_words = len(tokenizer.word_index) + 1 #convert to sequence of tokens input_sequences = [] for line in corpus: token_list = tokenizer.texts_to_sequences([line])[0] for i in range(1, len(token_list)): n_gram_sequence = token_list[:i+1] input_sequences.append(n_gram_sequence) return input_sequences, total_words inp_sequences, total_words = get_sequence_of_tokens(corpus) view raw Title Generator hosted with ❤ by GitHub View this gist on GitHub Padding the sequences Since the sequences can be of variable length, the sequence lengths must be equal. When using neural networks, we usually feed an input into the network while waiting for output. In practice, it is more efficient to process data in batches rather than one at a time. This is done by using matrices [batch size x sequence length], where the length of the sequence corresponds to the longest sequence. In this case, we fill the sequences with a token (usually 0) to fit the size of the matrix. This process of filling sequences with tokens is called filling. To enter the data into a training model, I need to create predictors and labels. I will create sequences of n-gram as predictors and the following word of n-gram as label: def generate_padded_sequences(input_sequences): max_sequence_len = max([len(x) for x in input_sequences]) input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding=’pre’)) predictors, label = input_sequences[:,:-1], input_sequences[:, -1] label = ku.to_categorical(label, num_classes = total_words) return predictors, label, max_sequence_len predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences) view raw Title Generator hosted with ❤ by GitHub View this gist on GitHub LSTM Model In recurrent neural networks, the activation outputs are propagated in both directions, i.e. from input to output and outputs to inputs, unlike direct-acting neural networks where outputs d activation are propagated in only one direction. This creates loops in the architecture of the neural network which acts as a “memory state” of neurons. As a result, the RNN preserves a state through the stages of time or “remembers” what has been learned over time. The state of memory has its advantages, but it also has its disadvantages. The gradient that disappears is one of them. In this problem, while learning with a large number of layers, it becomes really difficult for the network to learn and adjust the parameters of the previous layers. To solve this problem, a new type of RNN has been developed; LSTM (long-term memory). Title Generator with LSTM Model The LSTM model contains an additional state (the state of the cell) which essentially allows the network to learn what to store in the long term state, what to delete and what to read. . The LSTM of this model contains three layers: Input layer: takes the sequence of words as inputLSTM Layer: Calculates the output using LSTM units.Dropout layer: a regularization layer to avoid overfittingOutput layer: calculates the probability of the next possible word on output Now I will use the LSTM Model to build a model for the task of Title Generator with Machine Learning: def create_model(max_sequence_len, total_words): input_len = max_sequence_len — 1 model = Sequential() # Add Input Embedding Layer model.add(Embedding(total_words, 10, input_length=input_len)) # Add Hidden Layer 1 — LSTM Layer model.add(LSTM(100)) model.add(Dropout(0.1)) # Add Output Layer model.add(Dense(total_words, activation=’softmax’)) model.compile(loss=’categorical_crossentropy’, optimizer=’adam’) return model model = create_model(max_sequence_len, total_words) model.fit(predictors, label, epochs=20, verbose=5) view raw Title Generator hosted with ❤ by GitHub View this gist on GitHub Title Generator with Machine Learning: Testing The Model Now that our machine learning model for title generator is ready and has been trained using the data, it’s time to predict a headline based on the input word. The input word is first tokenized, the sequence is then completed before being passed into the trained model to return the predicted sequence: def generate_text(seed_text, next_words, model, max_sequence_len): for _ in range(next_words): token_list = tokenizer.texts_to_sequences([seed_text])[0] token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding=’pre’) predicted = model.predict_classes(token_list, verbose=0) output_word = “” for word,index in tokenizer.word_index.items(): if index == predicted: output_word = word break seed_text += “ “+output_word return seed_text.title() view raw Title Generator hosted with ❤ by GitHub View this gist on GitHub Generating Titles: Now as we have created a function to generate titles let’s test our title generator model: print(generate_text(“spiderman”, 5, model, max_sequence_len))Code language: PHP (php) Output: Spiderman The Voice 2018 Blind Audition I hope you liked this article on how to build a title generator model with machine learning and Python programming language. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Title Generator with Machine Learning
2020-10-08 17:44:49;In this article, I’ll walk you through how to create a language classification model to classify text into language categories. This language classification model can be used on data analyzed from online sources to categorize text into languages and filter the desired language before running an analysis such as sentiment analysis.The language classification is the grouping of associated languages in the same category. Languages are grouped diachronically into language families. In other words, languages are grouped according to their development and evolution throughout history, with languages that descend from a common ancestor being grouped in the same language family.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/10/08/language-classification-with-python/;['sklearn'];1.0;[];['ML', 'Classification', 'Sentiment Analysis', 'Decision Tree'];['sentiment analysis', 'predict', 'fit', 'model', 'loss', 'machine learning', 'classif', 'filter', 'rank', 'train', 'label', 'test data', 'decision tree'];"In this article, I’ll walk you through how to create a language classification model to classify text into language categories. This language classification model can be used on data analyzed from online sources to categorize text into languages and filter the desired language before running an analysis such as sentiment analysis. The language classification is the grouping of associated languages in the same category. Languages are grouped diachronically into language families. In other words, languages are grouped according to their development and evolution throughout history, with languages that descend from a common ancestor being grouped in the same language family. Also, Read – Machine Learning Full Course for free. Language Classification with Machine Learning Using Python First I will need to import some of the common Python packages and modules used to manage data, metrics and machine learning models needed to build and evaluate our predictive models, as well as modules to visualize our data. So let’s start with the task of language classification with Machine Learning using the python programming language by importing all the modules and packages needed for this task: import numpy as np # For arithmetics and arrays import math # For inbuilt math functions import pandas as pd # For handling data frames import collections # used for dictionaries and counters from itertools import permutations # used to find permutations from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier from sklearn.model_selection import train_test_split # Import train_test_split function to easily split data into training and testing samples from sklearn.decomposition import PCA # Principal component analysis used to reduce the number of features in a model from sklearn.preprocessing import StandardScaler # used to scale data to be used in the model from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation from sklearn.metrics import confusion_matrix from sklearn.metrics import classification_report from sklearn.metrics import roc_auc_score from sklearn.metrics import roc_curve from sklearn.metrics import accuracy_score from sklearn.metrics import log_loss import pickle # To save the trained model and then read it import seaborn as sns # Create plots sns.set(style=""ticks"") import matplotlib.pyplot as plt view raw language classification hosted with ❤ by GitHub View this gist on GitHub Now, let’s import and clean our data. You can download the dataset that I am using in this task from here: df = pd.read_csv('lang_data.csv') # Read raw data df = df.dropna() # remove null values for the ""text"" column df['text'] = df['text'].astype(str) # Convert the column ""text"" from object to a string in order to operate on it df['language'] = df['language'].astype(str) view raw language classification hosted with ❤ by GitHub View this gist on GitHub Language Classification: Feature Creation I will now create a new set of features that will be used in the language classification model to classify text into three languages: English, Afrikaans, and Dutch. As mentioned, different features may be more effective in classifying other languages: # Define a list of commonly found punctuations punc = ('!', "","" ,""\'"" ,"";"" ,""\"""", ""."", ""-"" ,""?"") vowels=['a','e','i','o','u'] # Define a list of double consecutive vowels which are typically found in Dutch and Afrikaans languages same_consecutive_vowels = ['aa','ee', 'ii', 'oo', 'uu'] consecutive_vowels = [''.join(p) for p in permutations(vowels,2)] dutch_combos = ['ij'] # Create a pre-defined set of features based on the ""text"" column in order to allow us to characterize the string df['word_count'] = df['text'].apply(lambda x : len(x.split())) df['character_count'] = df['text'].apply(lambda x : len(x.replace("" "",""""))) df['word_density'] = df['word_count'] / (df['character_count'] + 1) df['punc_count'] = df['text'].apply(lambda x : len([a for a in x if a in punc])) df['v_char_count'] = df['text'].apply(lambda x : len([a for a in x if a.casefold() == 'v'])) df['w_char_count'] = df['text'].apply(lambda x : len([a for a in x if a.casefold() == 'w'])) df['ij_char_count'] = df['text'].apply(lambda x : sum([any(d_c in a for d_c in dutch_combos) for a in x.split()])) df['num_double_consec_vowels'] = df['text'].apply(lambda x : sum([any(c_v in a for c_v in same_consecutive_vowels) for a in x.split()])) df['num_consec_vowels'] = df['text'].apply(lambda x : sum([any(c_v in a for c_v in consecutive_vowels) for a in x.split()])) df['num_vowels'] = df['text'].apply(lambda x : sum([any(v in a for v in vowels) for a in x.split()])) df['vowel_density'] = df['num_vowels']/df['word_count'] df['capitals'] = df['text'].apply(lambda comment: sum(1 for c in comment if c.isupper())) df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['character_count']),axis=1) df['num_exclamation_marks'] =df['text'].apply(lambda x: x.count('!')) df['num_question_marks'] = df['text'].apply(lambda x: x.count('?')) df['num_punctuation'] = df['text'].apply(lambda x: sum(x.count(w) for w in punc)) df['num_unique_words'] = df['text'].apply(lambda x: len(set(w for w in x.split()))) df['num_repeated_words'] = df['text'].apply(lambda x: len([w for w in collections.Counter(x.split()).values() if w > 1])) df['words_vs_unique'] = df['num_unique_words'] / df['word_count'] df['encode_ascii'] = np.nan for i in range(len(df)): try: df['text'].iloc[i].encode(encoding='utf-8').decode('ascii') except UnicodeDecodeError: df['encode_ascii'].iloc[i] = 0 else: df['encode_ascii'].iloc[i] = 1 view raw language classification hosted with ❤ by GitHub View this gist on GitHub Language Classification: Summarizing Features After building the above feature set, we can calculate averages of these features by language to check if there are any obvious significant differences. To do this, simply run the command below: df.groupby('language').mean().TCode language: JavaScript (javascript) Looking at the first feature, for example, word_count, we can notice that Afrikaans sentences are likely to be made up of more words than English and Dutch. Language Classification: Correlation Next, we need to look at the degree of correlation between the characteristics we have created. The idea behind correlation with the context of our task it that if two or more characteristics are strongly correlated with each other, then it is likely that they will have very similar explanatory power when classifying languages. As such, we can only keep one of these features and get the same predictive power from our model. To calculate the correlation matrix, we can run the following command: df.corr(method ='pearson')Code language: JavaScript (javascript) We can also visualize the pairwise correlation matrix using the following command: sns.pairplot(df) Code language: CSS (css) We can notice how several of the variables are strongly positively correlated. For example, word_count and character_count have a correlation of around 96%, which means they tell us roughly the same thing in terms of the length of a text for each language considered. Language Classification: Splitting The Data Before going any further in building our linguistic classification model, we need to divide the dataset into training and test sets: I divided the dataset into 80% training and 20% testing. Other percentage splits can be used, but these values are generally used.The training set will be used to fit the models and store the parameters, the robustness of which will be tested on the test set. #split dataset into features and target variable feature_cols = list(df.columns)[2:] X = df[feature_cols] # Features y = df[['language']] # Target variable # Split dataset into training set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 80% train and 20% test view raw language classification hosted with ❤ by GitHub View this gist on GitHub Reducing Correlation We should aim to use only the most unique characteristics in our classification models, as the correlated variables do not add much to the predictive power of the models. One method used in machine learning to reduce the correlation between features is called principal component analysis or PCA: # Standardize the data scaler = StandardScaler() # Fit on training set only. scaler.fit(X_train) # Transform both the training set and the test set. X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) # Make an instance of the model to retain 95% of the variance within the old features. pca = PCA(.95) pca.fit(X_train) print('Number of Principal Components = '+str(pca.n_components_)) # Number of Principal Components = 13 X_train = pca.transform(X_train) X_test = pca.transform(X_test) view raw language classification hosted with ❤ by GitHub View this gist on GitHub After running the code above, you will notice that the PCA reduced the number of features from 20 to 13 by turning the original features into a new set of components that keep 95% of the variance of the information in the original set. Using Decision Tree Algorithm A decision tree model learns by dividing the training set into subsets based on an attribute value test, and this process is repeated over recursive partitions until the subset at a node has the same value as the target parameter, or when additional splitting does not improve. the predictive capacity of the model. I will adapt the decision tree classifier to the training set and save the model parameters to a pickle file, which can be imported for future use. We then use the model to predict or rank the texts in the languages using the test set. dt_clf = DecisionTreeClassifier() # Create Decision Tree classifer object dt_clf = dt_clf.fit(X_train,y_train) # Fit/Train Decision Tree Classifer on training set # Save model to file in the current working directory so that it can be imported and used. # I use the pickle library to save the parameters of the trained model pkl_file = ""decision_tree_model.pkl"" with open(pkl_file, 'wb') as file: pickle.dump(dt_clf, file) # Load previously trained model from pickle file with open(pkl_file, 'rb') as file: dt_clf = pickle.load(file) dt_clf # parameters of the Decision Tree model are shown below and can be further optimized to improve model performance y_pred = dt_clf.predict(X_test) #Predict the response for test dataset view raw language classification hosted with ❤ by GitHub View this gist on GitHub Now let’s have a look at the accuracy of our language classification model: accuracy_score_dt = accuracy_score(y_test, y_pred) The decision tree algorithm gave an accuracy of almost 90%. Now let’s have a look at the confusion matrix to visualize the classified languages with their accuracy: labels = [‘English’, ‘Afrikaans’, ‘Nederlands’] # Confusion Matrix cm_Model_dt = confusion_matrix(y_test, y_pred, labels) fig = plt.figure(figsize=(9,9)) ax = fig.add_subplot(111) sns.heatmap(cm_Model_dt, annot=True, fmt="".3f"", linewidths=.5, square = True, cmap = 'Blues_r') plt.ylabel('Actual') plt.xlabel('Predicted') ax.set_xticklabels(labels) ax.set_yticklabels(labels) title = 'Decision Tree Model Accuracy Score = '+ str(round(accuracy_score_dt*100,2)) +""%"" plt.title(title, size = 15) view raw language classification hosted with ❤ by GitHub View this gist on GitHub The graph above shows how many texts were categorized correctly in each of the languages, with the y-axis representing actual or actual output and the x-axis representing expected output. This tells us that the model does well at predicting English texts, in addition to Afrikaans texts. Hope you liked this article on the task of classifying languages ​​with machine learning using the programming language Python. Please feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Language Classification with Python
2020-10-09 18:25:31;In this article, I will show you how to create a simple face detector using Python. Building a program that detects faces is a great way to get started with machine learning computer vision tasks. So I will introduce you here a very easy way of face detection with Python.As the title suggests, I will write a program for the task of face detection with Python. When I say a program you can understand it like teaching a machine what to do. I like to use teaching rather than programming because that’s actually what I’m doing here.Also, Read – Machine Learning Full Course for free.The best way to learn about everything is to teach, so while teaching a machine to detect faces, we are also learning. So, before I start this face detection task with Python, I want to share the difference between face detection and face recognition.;https://thecleverprogrammer.com/2020/10/09/face-detection-with-python/;['opencv-python'];1.0;['ML', 'CV'];['ML', 'CV', 'Classification'];['detect', 'recogn', 'computer vision', 'model', 'machine learning', 'classif', 'train'];"In this article, I will show you how to create a simple face detector using Python. Building a program that detects faces is a great way to get started with machine learning computer vision tasks. So I will introduce you here a very easy way of face detection with Python. As the title suggests, I will write a program for the task of face detection with Python. When I say a program you can understand it like teaching a machine what to do. I like to use teaching rather than programming because that’s actually what I’m doing here. Also, Read – Machine Learning Full Course for free. The best way to learn about everything is to teach, so while teaching a machine to detect faces, we are also learning. So, before I start this face detection task with Python, I want to share the difference between face detection and face recognition. Face Detection Vs Face Recognition Face detection and face recognition may look very similar, but in reality, they are not the same. Let’s understand the difference so as not to miss the point. Face detection is the process of detecting faces, from an image or a video does not matter. The program does nothing more than finding the faces. But on the other hand in the task of face recognition, the program finds faces and can also tell which face belongs to which. So it’s more informative than just detecting them. There is more programming, in other words, more training in the process. Let’s say you are looking at the street and the cars are passing. Face detection is like saying the passing object is a car. And facial recognition is like being able to tell the model of the passing car. Here is a nice picture showing the difference in practice. In this article, I will continue with the task of detecting faces. If you want to learn facial recognition, you can mention me in the comments section below. Face Detection with Python I will use the OpenCV library in Python which is used as the primary tool for the tasks of computer vision in Python. If you are new to OpenCV then this task is the best to start with it. If you want to learn more about the OpenCV you can watch a complete tutorial on OpenCV from here. Now let’s get started with the task of Face Detection with Python. First you need to install the OpenCV library in Python which can be easily installed by using the pip command; pip install opencv-python. After installing this library you need to simply import this by using the command below: import cv2 Code language: Python (python) OpenCV library in python is blessed with many pre-trained classifiers for face, eyes, smile, etc. These XML files are stored in a folder. We will use the face detection model. You can download the pre-trained face detection model from here. After downloading and saving the file in your directory, let’s load it into the face detection program: face_cascade = cv2.CascadeClassifier('face_detector.xml')Code language: Python (python) The next step is to choose an image on which you want to test your code. Make sure there is at least one face in the image so that the face detection program can find at least one face. After choosing an image, let’s define it in our program. Make sure the image is in the same directory you are working in: img = cv2.imread('image.jpg')Code language: Python (python) Detect Faces You will be amazed at how short the face detection code is. Thanks to the people who contribute to OpenCV. Here is the code that detects faces in an image: faces = face_cascade.detectMultiScale(img, 1.1, 4) Code language: Python (python) Now the last step is to draw rectangles around the detected faces, which can be easily done with the following code: for (x, y, w, h) in faces: cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2) cv2.imwrite(""face_detected.png"", img) print('Successfully saved') Code language: Python (python) So this is how we can easily detect a face or as many as faces in the image. I hope you liked this article on face detection with Python. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Face Detection with Python
2020-10-09 01:28:27;In this article, I will take you through the Ridge and Lasso Regression in Machine Learning and how to implement it by using the Python Programming Language.The Ridge and Lasso regression models are regularized linear models which are a good way to reduce overfitting and to regularize the model: the less degrees of freedom it has, the harder it will be to overfit the data. A simple way to regularize a polynomial model is to reduce the number of polynomial degrees.Also, Read – Machine Learning Full Course for free.For a linear model, regularization is usually done by constraining the model weights. We will now look at the Ridge regression and lasso regression, which implement the different ways of constraining weights.;https://thecleverprogrammer.com/2020/10/09/ridge-and-lasso-regression-with-python/;['sklearn'];1.0;['CV'];['ML', 'CV', 'Linear Regression', 'Regression'];['regression', 'linear regression', 'fit', 'model', 'machine learning', 'train', 'label'];"In this article, I will take you through the Ridge and Lasso Regression in Machine Learning and how to implement it by using the Python Programming Language. The Ridge and Lasso regression models are regularized linear models which are a good way to reduce overfitting and to regularize the model: the less degrees of freedom it has, the harder it will be to overfit the data. A simple way to regularize a polynomial model is to reduce the number of polynomial degrees. Also, Read – Machine Learning Full Course for free. For a linear model, regularization is usually done by constraining the model weights. We will now look at the Ridge regression and lasso regression, which implement the different ways of constraining weights. Ridge Regression Ridge regression is a regularized version of linear regression. This forces the training algorithm not only to fit the data but also to keep the model weights as small as possible. Note that the accrual term should only be added to the cost function during training. After you train the model, you want to use the unregulated performance measure to evaluate the performance of the model. Lasso Regression Least absolute shrinkage and selection operator regression (usually just called lasso regression) is another regularized version of linear regression: just like peak regression, it adds a regularization term to the cost function. , but it uses the ℓ1 norm of the weight vector instead of half the square of the ℓ2 norm. Ridge and Lasso Regression with Python Like other tasks, in this task to show the implementation of Ridge and Lasso Regression with Python, I will start with importing the required Python packages and modules: import pandas as pd import numpy as np import matplotlib.pyplot as pltCode language: Python (python) Now let’s import the data and do some data cleaning and have a look at how the data looks we are going to work with. You can download the dataset that I am using in this task from here: data = pd.read_csv(""Advertising.csv"") print(data.head())Code language: Python (python) Unnamed: 0 TV Radio Newspaper Sales 0 1 230.1 37.8 69.2 22.1 1 2 44.5 39.3 45.1 10.4 2 3 17.2 45.9 69.3 9.3 3 4 151.5 41.3 58.5 18.5 4 5 180.8 10.8 58.4 12.9 Now I will remove the unnamed column: data.drop([""Unnamed: 0""], axis=1, inplace=True)Code language: Python (python) Now we only have three advertising media and sales are our target variable. Let’s see how each variable affects sales by creating a scatter plot. First, we build a helper function to create a scatter plot: def scatter_plot(feature, target): plt.figure(figsize=(16, 18)) plt.scatter(data[feature], data[target], c='black' ) plt.xlabel(""Money Spent on {} ads ($)"".format(feature)) plt.ylabel(""Sales ($k)"") plt.show() scatter_plot(""TV"", ""Sales"") scatter_plot(""Radio"", ""Sales"") scatter_plot(""Newspaper"", ""Sales"")Code language: Python (python) Multiple Linear Regression Algorithm As Ridge and Lasso Regression models are a way of regularizing the linear models so we first need to prepare a linear model. So now, let’s code for preparing a multiple linear regression model: from sklearn.model_selection import cross_val_score from sklearn.linear_model import LinearRegression xs = data.drop([""Sales""], axis=1) y = data[""Sales""].values.reshape(-1,1) linreg = LinearRegression() MSE = cross_val_score(linreg, xs, y, scoring=""neg_mean_squared_error"", cv=5) mean_MSE = np.mean(MSE) print(mean_MSE)Code language: Python (python) Now, we need to see what’s better Ridge Regression or Lasso Regression. Ridge Regression For the ridge regression algorithm, I will use GridSearchCV model provided by Scikit-learn, which will allow us to automatically perform the 5-fold cross-validation to find the optimal value of alpha. This is how the code looks like for the Ridge Regression algorithm: # Ridge Regression from sklearn.model_selection import GridSearchCV from sklearn.linear_model import Ridge ridge = Ridge() parameters = {""alpha"":[1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]} ridge_regression = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5) ridge_regression.fit(xs, y) Code language: Python (python) And then we can easily find the best parameter and the best MSE by using the following commands: print(ridge_regression.best_params_) print(ridge_regression.best_score_)Code language: Python (python) {‘alpha’: 20}-3.0726713383411424 Lasso Regression For the Lasso Regression also we need to follow the same process as we din in the Ridge Regression. This is how the code looks like: from sklearn.linear_model import Lasso lasso = Lasso() parameters = {""alpha"":[1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]} lasso_regression = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=5) lasso_regression.fit(xs, y) print(lasso_regression.best_params_) print(lasso_regression.best_score_)Code language: Python (python) {‘alpha’: 1}-3.041405896751369 Hope you now know how to implement Ridge and Lasso regression in machine learning with the Python programming language. In this case, the lasso is the best method of adjustment, with a regularization value of 1. I hope you liked this article on how to implement the Ridge and Lasso algorithms in Machine Learning by using Python Programming Language. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Ridge and Lasso Regression with Python
2020-10-10 13:58:59;As part of any machine learning task, data visualization plays an important role in learning more about the available data and in identifying any major patterns. In this article, I’ll walk you through the most important techniques of data visualization for machine learning that you need to know when working in a professional environment.Here I will cover some important techniques that could help us meet the challenges of data visualization for machine learning, such as parallel coordinate plots, summary data tables, drawing ANN charts and many more.Also, Read – Machine Learning Full Course For free.;https://thecleverprogrammer.com/2020/10/10/data-visualization-for-machine-learning/;['keras', 'sklearn', 'pattern'];1.0;['NN', 'ANN'];['NN', 'Regression', 'DL', 'ML', 'Decision Tree', 'ANN', 'ReLu', 'Logistic Regression', 'Random Forest', 'Naive Bayes', 'Classification'];['artificial neural network', 'regression', 'predict', 'fit', 'model', 'machine learning', 'logistic regression', 'neural network', 'classif', 'layer', 'random forest', 'relu', 'deep learning', 'naive bayes', 'rank', 'train', 'label', 'decision tree'];"As part of any machine learning task, data visualization plays an important role in learning more about the available data and in identifying any major patterns. In this article, I’ll walk you through the most important techniques of data visualization for machine learning that you need to know when working in a professional environment. Here I will cover some important techniques that could help us meet the challenges of data visualization for machine learning, such as parallel coordinate plots, summary data tables, drawing ANN charts and many more. Also, Read – Machine Learning Full Course For free. Data Preparation Before getting into the task of data visualization for machine learning let’s prepare our data for this task. I will start by importing some necessary libraries that we will need in the process: import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt import matplotlib as mpl import os from sklearn_pandas import CategoricalImputer from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report,confusion_matrix from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score view raw data visualization hosted with ❤ by GitHub View this gist on GitHub Now, I will prepare the data by identifying and removing the missing values, and then I will create a new data frame so that we can easily continue with the task of data visualization for machine learning. You can download the data that I am using in this task from here: df = pd.read_csv('vgsales.csv') percent_missing = df.isnull().sum() * 100 / len(df) missing_values = pd.DataFrame({'column_name': df.columns, 'percent_missing': percent_missing}) labels = df['Genre'] imputer = CategoricalImputer() df['Year'] = imputer.fit_transform(df['Year'].values) df['Publisher'] = imputer.fit_transform(df['Publisher'].values) percent_missing = df.isnull().sum() * 100 / len(df) missing_values = pd.DataFrame({'column_name': df.columns, 'percent_missing': percent_missing}) df = df.drop(['Rank', 'Year'], axis=1) df = df.apply(preprocessing.LabelEncoder().fit_transform) enc_labels = df['Genre'] df = pd.get_dummies(df) plt.rcParams['legend.fontsize'] = '16' df2 = df.drop(['Genre'], axis=1) df2['Genre'] = labels #df.plot(figsize=(10,10), fontsize=24) fig = plt.figure(figsize=(10,7)) ax = fig.add_subplot(111) ax.set_title(""Parallel Coordinates Example"", fontsize=20) ax.tick_params(axis='x', rotation=30) ax.tick_params(axis='both', labelsize=20) parallel_coordinates(df2, class_column='Genre', ax=ax) view raw data visualization hosted with ❤ by GitHub View this gist on GitHub Now our data is ready to be used in the task of techniques of data visualization for machine learning. Techniques of Data Visualization for Machine Learning Hyperparameter Optimization: One of the most common activities in machine learning is hyperparameter optimization. Tuning machine learning models is one type of optimization problem. We have a set of hyperparameters and we are looking to find the right combination of their values ​​that can help us find the minimum or the maximum of a function. One of the best solutions for this type of task is to use a parallel coordinate plot. By using this type of graph, we can easily compare different variables together to discover possible relationships. In the case of hyperparameter optimization, this can be used as a simple tool to inspect which combination of parameters can give us the highest test accuracy. Another possible use of parallel coordinate plots in data analysis is to inspect relationships in values ​​between different entities in a data frame: import plotly.express as px fig = px.parallel_coordinates(df2, color=""mean_test_score"", labels=dict(zip(list(df2.columns), list(['_'.join(i.split('_')[1:]) for i in df2.columns]))), color_continuous_scale=px.colors.diverging.Tealrose, color_continuous_midpoint=27) fig.show() view raw data visualization hosted with ❤ by GitHub View this gist on GitHub Plotly Prediction Table: When working with time-series data in machine learning, sometimes it can be really handy to be able to quickly understand which data points our model is performing poorly, to try and understand the limitations it might face. One possible approach is to create a summary table with actual and predicted values and some form of metric summarizing how well a data point has been predicted. Using Plotly, this can be easily done by creating a plot function: import chart_studio.plotly as py import plotly.graph_objs as go from plotly.offline import init_notebook_mode, iplot init_notebook_mode(connected=True) import plotly def predreport(y_pred, Y_Test): diff = y_pred.flatten() - Y_Test.flatten() perc = (abs(diff)/y_pred.flatten())*100 priority = [] for i in perc: if i > 0.4: priority.append(3) elif i> 0.1: priority.append(2) else: priority.append(1) print(""Error Importance 1 reported in "", priority.count(1), ""cases\n"") print(""Error Importance 2 reported in"", priority.count(2), ""cases\n"") print(""Error Importance 3 reported in "", priority.count(3), ""cases\n"") colors = ['rgb(102, 153, 255)','rgb(0, 255, 0)', 'rgb(255, 153, 51)', 'rgb(255, 51, 0)'] fig = go.Figure(data=[go.Table(header= dict( values=['Actual Values', 'Predictions', '% Difference', ""Error Importance""], line_color=[np.array(colors)[0]], fill_color=[np.array(colors)[0]], align='left'), cells=dict( values=[y_pred.flatten(),Y_Test.flatten(), perc, priority], line_color=[np.array(colors)[priority]], fill_color=[np.array(colors)[priority]], align='left'))]) init_notebook_mode(connected=False) py.plot(fig, filename = 'Predictions_Table', auto_open=True) fig.show() view raw data visualization hosted with ❤ by GitHub View this gist on GitHub Decision Trees: Decision trees are one of the most easily explained types of machine learning models. Thanks to their basilar structure, it is easily possible to examine how the algorithm decides to make its decision by looking at the conditions on the different branches of the tree. Additionally, decision trees can also be used as a feature selection technique, as the algorithm places features at higher levels of the tree that are most useful for our desired classification/regression tasks. In this way, the features at the bottom of the tree could be ignored because they contain less information: from dtreeviz.trees import * viz = dtreeviz(clf, X_train, y_train.values, target_name='Genre', feature_names=list(X.columns), class_names=list(labels.unique()), histtype='bar', orientation ='TD') viz view raw data visualization hosted with ❤ by GitHub View this gist on GitHub In the figure above, the different classes are represented by a different colour. The entity distributions of all the different classes are represented in the starting node of the tree. As we move down each branch, the algorithm then tries to better separate the different distributions using the function described under each of the node graphs. The circles generated next to the distributions represent the number of elements correctly classified after following a certain node, the greater the number of elements, the larger the size of the circle. Decision Boundaries: Decision limits are one of the simplest approaches to graphically understanding how a machine learning model makes its predictions. One of the easiest ways to plot decision boundaries in Python is to use Mlxtend. This library can, in fact, be used to trace the decision boundaries of machine learning and deep learning models. Let’s see how to draw a decision boundary: from mlxtend.plotting import plot_decision_regions import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import itertools gs = gridspec.GridSpec(2, 2) fig = plt.figure(figsize=(10,8)) clf1 = LogisticRegression(random_state=1, solver='newton-cg', multi_class='multinomial') clf2 = RandomForestClassifier(random_state=1, n_estimators=100) clf3 = GaussianNB() clf4 = SVC(gamma='auto') labels = ['Logistic Regression','Random Forest','Naive Bayes','SVM'] for clf, lab, grd in zip([clf1, clf2, clf3, clf4], labels, itertools.product([0, 1], repeat=2)): clf.fit(X_Train, Y_Train) ax = plt.subplot(gs[grd[0], grd[1]]) fig = plot_decision_regions(X_Train, Y_Train, clf=clf, legend=2) plt.title(lab) plt.show() view raw data visualization hosted with ❤ by GitHub View this gist on GitHub Artificial Neural Networks: Another technique of data visualization for machine learning that can be very useful when creating new neural network architectures is to visualize their structure. This can be easily done using the ANN Visualiser: from keras.models import Sequential from keras.layers import Dense from ann_visualizer.visualize import ann_viz model = Sequential() model.add(Dense(units=4,activation='relu', input_dim=7)) model.add(Dense(units=4,activation='sigmoid')) model.add(Dense(units=2,activation='relu')) ann_viz(model, view=True, filename=""example"", title=""Example ANN"") view raw data visualization hosted with ❤ by GitHub View this gist on GitHub So these were the most important techniques of data visualization for machine learning. I hope you liked this article on the most important techniques of data visualization for machine learning that you need to know while working in a professional environment. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Data Visualization for Machine Learning
2020-10-18 17:31:47;Feature Selection means figuring out which signals you can use to identify patterns, and then integrate them into your training and scoring pipeline. In this article, I’ll walk you through what feature selection is and how it affects the formation of our machine learning models.;https://thecleverprogrammer.com/2020/10/18/feature-selection-in-machine-learning/;['pattern'];1.0;[];['Regression', 'ML', 'Decision Tree', 'Logistic Regression', 'Classification'];['regression', 'fit', 'model', 'loss', 'machine learning', 'classif', 'training data', 'train', 'logistic regression', 'decision tree'];"Feature Selection means figuring out which signals you can use to identify patterns, and then integrate them into your training and scoring pipeline. In this article, I’ll walk you through what feature selection is and how it affects the formation of our machine learning models. What is Feature Selection? If you have an efficient machine learning infrastructure, then most of your time and energy will be wasted on selecting the best features. To get the most out of your efforts, you only want to use features that offer high discriminating power; adding each feature should significantly improve your model. Also, Read – Machine Learning Full Course for free. In addition to requiring additional effort for construction and maintenance, redundant features can adversely affect the quality of your model. If the number of features is greater than the number of data points, your model will be overfitted: there are enough model parameters to draw a curve through all of the training data. Additionally, highly correlated characteristics can cause instability in model decisions. For example, if you have a feature that is “number of connections yesterday” and another that is “number of connections in the last two days”, the information you are trying to collect will be split between the two features in a way essentially arbitrary, and the model might not learn that either of these characteristics is important. How To Select Features? You can solve the feature selection problem by calculating covariance matrices between your features and combining highly correlated features. There are several techniques to resolve the feature selection issue: Logistic regression, SVMs, and decision trees/forests have methods for determining the relative importance of features; you can run them and keep only the most important features. You can use L1 regularization for the selection of characteristics in logistic regression and SVM classifiers. If the number n of entities is reasonably small (say, n <100), you can use a “construction” approach: build n single-feature models and determine which is best on your validation set; then build n – 1 two-feature model, and so on, until the gain from adding additional functionality is below a certain threshold. Likewise, you can use a “leave out” approach: build a model on n features, then n models on n – 1 feature and keep the best ones, and so on until the loss of deletion of d. ‘extra functionality is too important. I hope you liked this article on what is feature selection in machine learning. You can learn about its practical implementation from here. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Feature Selection in Machine Learning
2020-10-23 14:04:44;In this article, I will walk you through how to make a barcode and Qr code reader with Python and Machine Learning. This is a great machine learning task to get started with computer vision.Barcodes and QR codes are very cool and interesting because they store information in a different format. The fun part about them is that we can’t tell what they are storing until we analyze them. It’s like playing a puzzle game. And another thing I love about them is that they can be part of the physical world and still connect us to the internet world.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/10/23/barcode-and-qr-code-reader-with-python/;['Pillow'];1.0;['CV'];['ML', 'Recommender', 'CV', 'AI'];['detect', 'artificial intelligence', 'recogn', 'computer vision', 'machine learning', 'recommend'];"In this article, I will walk you through how to make a barcode and Qr code reader with Python and Machine Learning. This is a great machine learning task to get started with computer vision. Barcodes and QR codes are very cool and interesting because they store information in a different format. The fun part about them is that we can’t tell what they are storing until we analyze them. It’s like playing a puzzle game. And another thing I love about them is that they can be part of the physical world and still connect us to the internet world. Also, Read – Machine Learning Full Course for free. How does the barcode and QR code reader work? If you are wondering how barcode and QR code readers work, let’s do a little practical exercise. Turn on your phone camera and view the featured image from this item. You will see a link appear, it is very easy to use. Today you will learn how to create your barcode and QRcode reader with Python and Machine Learning, without wasting time, let’s get started. I will start by installing the libraries we will need for this project and then I will start with the coding part. For this task of creating a barcode and QRcode reader with Python, I recommend using a regular code editor like VScode or Pycharm. Getting Started The first step is to install the following three libraries: Pillow, OpenCV and Pyzbar. Pillow is the extension of PIL, which stands for Python Image Library. OpenCV is a well-known library, especially when working with computer vision tasks. And the last library we need is Pyzbar, a python library that will help us read barcode and QR codes. You can easily install all the libraries using the pip command. Building Barcode and QR code Reader with Python and Machine Learning Now the next step is to write the decode function, where most of the cool stuff will happen. The decode function will mainly do three things and can be listed as follows: Recognize and decode the barcode / QR code that we are going to show to the camera.Added information stored as text on recognized barcode / QR code.And finally, export the stored information as a text document. Let’s import the libraries we installed before writing to the function: import cv2 from pyzbar import pyzbarCode language: Python (python) Now let’s define the decoding function: def read_barcodes(frame): barcodes = pyzbar.decode(frame) for barcode in barcodes: x, y , w, h = barcode.rect barcode_info = barcode.data.decode('utf-8') cv2.rectangle(frame, (x, y),(x+w, y+h), (0, 255, 0), 2) font = cv2.FONT_HERSHEY_DUPLEX cv2.putText(frame, barcode_info, (x + 6, y - 6), font, 2.0, (255, 255, 255), 1) with open(""barcode_result.txt"", mode ='w') as file: file.write(""Recognized Barcode:"" + barcode_info) return frame view raw barcode and qrcode hosted with ❤ by GitHub View this gist on GitHub Now let’s go through the above function to understand what I did: First, we decode the barcode or QR code information. And then draw a rectangle around it. It helps us to see if our machine detected the barcode / Qr code.Second, we add text above the rectangle that has been created. The text will display the decoded information.Third, we export the information to a text document. Now the next step is to write the main function for building a Barcode and QR code reader with Python. Let’s create our main function: def main(): camera = cv2.VideoCapture(0) ret, frame = camera.read() while ret: ret, frame = camera.read() frame = read_barcode(frame) cv2.imshow('Barcode/QR code reader', frame) if cv2.waitKey(1) & 0xFF == 27: break camera.release() cv2.destroyAllWindows() if __name__ == '__main__': main() view raw barcode and qrcode hosted with ❤ by GitHub View this gist on GitHub Now let’s go through the main function above to understand what I did: First of all, we turn on the computer camera using OpenCV. If you have an external camera, you need to change the value 0 to 1 depending on the device.Second, we run a while loop to continue performing the decode function until the “Esc” key is pressed. Otherwise, the loop will not stop and cause problems.Third, we launch the camera that we turned on in the first step. And then we close the application window. OpenCV does all the work, just call the methods.Finally, we call the main function to trigger the program. Now you can easily run the code and scan any barcode and QR code by showing the code to the camera of your laptop. You have now created a program that reads barcodes and QR codes for you. Now you have an idea of how to use computer vision and artificial intelligence in real life. Working on hands-on programming projects like this is the best way to sharpen your coding skills. Hope you liked this article on how to create a barcode and QR code reader with Python and Machine Learning. Please feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Barcode and QR code Reader with Python
2020-10-24 12:30:03;Topic modeling is a type of statistical modeling for discovering abstract “subjects” that appear in a collection of documents. This means creating one topic per document template and words per topic template, modeled as Dirichlet distributions. In this article, I will walk you through the task of Topic Modeling in Machine Learning with Python.The field of Topic modeling has become increasingly important in recent years. Subject modeling is an unsupervised machine learning way to organize text (or image or DNA, etc.) information so that associated pieces of text can be identified.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/10/24/topic-modeling-with-python/;['pattern'];1.0;[];['ML', 'NLP'];['train', 'natural language processing', 'model', 'machine learning'];"Topic modeling is a type of statistical modeling for discovering abstract “subjects” that appear in a collection of documents. This means creating one topic per document template and words per topic template, modeled as Dirichlet distributions. In this article, I will walk you through the task of Topic Modeling in Machine Learning with Python. The field of Topic modeling has become increasingly important in recent years. Subject modeling is an unsupervised machine learning way to organize text (or image or DNA, etc.) information so that associated pieces of text can be identified. Also, Read – Machine Learning Full Course for free. What is Topic Modeling? In machine learning and natural language processing, topic modeling is a type of statistical model for discovering abstract subjects that appear in a collection of documents. Topic modeling is a text mining tool frequently used for discovering hidden semantic structures in body text. Intuitively, since a document is about a particular topic, one would expect that particular words would appear more or less frequently in the document: “dog” and “bone” will appear more often in documents about dogs, “Cat” and “meow” will appear in chat documents, and “the” and “is” will appear roughly equally in both. A document generally concerns several subjects in different proportions; thus, in a 10% cat and 90% dog document, there would probably be about 9 times more dog words than cat words. The “topics” produced by topic modeling techniques are groups of similar words. A topic modeling machine learning model captures this intuition in a mathematical framework, which makes it possible to examine a set of documents and discover, based on the statistics of each person’s words, what the subjects might be and what the balance of the subjects of the subject is. each document. Topic Modeling with Python Now, I will take you through a task of topic modeling with Python programming language by using a real-life example. I will be performing some modeling on research articles. The dataset I will use here is taken from kaggle.com. You can easily download all the files that I am using in this task from here. Now let’s get started with the task of Topic Modeling with Python by importing all the necessary libraries that we need for this task: import pandas as pd import numpy as np import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots view raw topic modeling hosted with ❤ by GitHub View this gist on GitHub Now, the next step is to read all the datasets that I am using in this task: train = pd.read_csv(""Train.csv"") test = pd.read_csv(""Test.csv"") tags = pd.read_csv(""Tags.csv"") sample_sub = pd.read_csv(""SampleSubmission.csv"") view raw topic modeling hosted with ❤ by GitHub View this gist on GitHub Exploratory Data Analysis Exploratory Data Analysis explores the data to find the relationship between measures that tell us they exist, without the cause. They can be used to formulate hypotheses. EDA helps you discover relationships between measures in your data, which do not prove the existence of correlation, as indicated by the expression. Now I will perform some EDA to find some patterns and relationships in the data before getting into topic modeling: print(train.isna().sum)Code language: Python (python) <bound method DataFrame.sum of id ABSTRACT … Superconductivity Systems and Control 0 False False … False False 1 False False … False False 2 False False … False False 3 False False … False False 4 False False … False False … … … … … … 13999 False False … False False 14000 False False … False False 14001 False False … False False 14002 False False … False False 14003 False False … False False [14004 rows x 31 columns]> print(test.isna().sum)Code language: Python (python) <bound method DataFrame.sum of id ABSTRACT Computer Science Mathematics Physics Statistics 0 False False False False False False 1 False False False False False False 2 False False False False False False 3 False False False False False False 4 False False False False False False … … … … … … … 5997 False False False False False False 5998 False False False False False False 5999 False False False False False False 6000 False False False False False False 6001 False False False False False False [6002 rows x 6 columns]> train[""Number of Characters""] = train[""ABSTRACT""].apply(lambda x: len(str(x))) test[""Number of Characters""] = test[""ABSTRACT""].apply(lambda x: len(str(x))) fig = make_subplots(rows=1, cols=2) trace1 = go.Histogram(x = train[""Number of Characters""]) fig.add_trace(trace1, row=1, col=1) trace2 = go.Box(y = train[""Number of Characters""]) fig.add_trace(trace2, row=1, col=2) fig.update_layout(showlegend=False) fig.show() view raw topic modeling hosted with ❤ by GitHub View this gist on GitHub There is great variability in the number of characters in the Abstracts of the Train set. We have a minimum of 54 to a maximum of 4551 characters on the train. The median number of characters is 1065. fig = make_subplots(rows=1, cols=2) trace1 = go.Histogram(x = test[""Number of Characters""]) fig.add_trace(trace1, row=1, col=1) trace2 = go.Box(y = test[""Number of Characters""]) fig.add_trace(trace2, row=1, col=2) fig.update_layout(showlegend=False) fig.show() view raw topic modeling hosted with ❤ by GitHub View this gist on GitHub The test set looks better than the training set as the minimum number of characters in the test set is 46, while the maximum is 2841. So the median number of characters in the test set is 1058, which is very similar to the training set. train['Number of Words'] = train['ABSTRACT'].apply(lambda x: len(str(x).split())) test['Number of Words'] = test['ABSTRACT'].apply(lambda x: len(str(x).split())) fig = make_subplots(rows = 1, cols = 2) trace1 = go.Histogram(x = train['Number of Words']) fig.add_trace(trace1, row = 1, col = 1) trace2 = go.Box(y = train['Number of Words']) fig.add_trace(trace2, row = 1, col = 2) fig.update_layout(showlegend = False) fig.show() view raw topic modeling hosted with ❤ by GitHub View this gist on GitHub The learning set has a similar trend in the number of words as we have seen in the number of characters. Minimum of 8 words and maximum of 665 words. So the median word count is 153. fig = make_subplots(rows = 1, cols = 2) trace1 = go.Histogram(x = test['Number of Words']) fig.add_trace(trace1, row = 1, col = 1) trace2 = go.Box(y = test['Number of Words']) fig.add_trace(trace2, row = 1, col = 2) fig.update_layout(showlegend = False) fig.show() view raw topic modeling hosted with ❤ by GitHub View this gist on GitHub Minimum of 7 words in an abstract and maximum of 452 words in the test set. The median here is exactly the same as that observed in the training set and is equal to 153. Topic Modeling Using Tags There are a lot of methods of topic modeling. I will use the tags in this task, let’s see how to do this by exploring the tags: main_tags = ['Computer Science', 'Mathematics', 'Physics', 'Statistics'] countTagsTrain = pd.DataFrame(train[main_tags].sum(axis = 0) / len(train)) countTagsTest = pd.DataFrame(test[main_tags].sum(axis = 0) / len(test)) trace0 = go.Bar(x = countTagsTrain.index, y = countTagsTrain[0],name = 'Train Set') trace1 = go.Bar(x = countTagsTest.index, y = countTagsTest[0],name = 'Test Set') fig = go.Figure([trace0,trace1]) fig.show() view raw topic modeling hosted with ❤ by GitHub View this gist on GitHub So this is how we can perform the task of topic modeling by using the Python programming language. I hope you liked this article on Topic Modeling in machine learning with Python. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";Topic Modeling with Python
2020-10-24 15:32:24;Unsupervised learning encompasses all types of machine learning where there is no known output, there is no teacher to instruct the learning algorithm. In this article, I’ll introduce you to unsupervised machine learning and its types.;https://thecleverprogrammer.com/2020/10/24/what-is-unsupervised-learning-in-machine-learning/;['pattern'];1.0;[];['ML', 'Clustering', 'Supervised Learning', 'Unsupervised learning'];['clustering', 'supervised learning', 'unsupervised learning', 'machine learning'];"Unsupervised learning encompasses all types of machine learning where there is no known output, there is no teacher to instruct the learning algorithm. In this article, I’ll introduce you to unsupervised machine learning and its types. What is Unsupervised Learning? In unsupervised learning, the learning algorithm is simply shown the input data and prompted to extract knowledge from that data. In unsupervised learning, only the input data is known and no known output data is provided to the algorithm. Although there are many successful applications of these methods, they are generally more difficult to understand and evaluate. Also, Read – Machine Learning Full Course for free. For supervised and unsupervised learning tasks, it is important to have a representation of your input data that is understandable by a computer. It is often useful to think of your data as a table. Every data point that you want to reason about (every email, every customer, every transaction) is a row and every property that describes that data point (for example, a customer’s age or the amount or l ‘location of a transaction) is a column. You can describe users by their age, gender, when they created an account, and how often they purchased from your online store. You can describe the image of a tumour by the grayscale values ​​of each pixel, or perhaps by using the size, shape and colour of the tumour. Types of Unsupervised Learning Now let’s look at the two types of unsupervised machine learning; data set transformations and clustering. Transformations of the Dataset: The Unsupervised transformations of a dataset are algorithms that can create some new representations of the data that will make it easier for the humans to understand or other machine learning algorithms compared to the original representation of the data. A very common application of unsupervised transformation of data includes dimensionality reduction, which takes a high-dimensional data with a lot of features, and finds a new way to represent that data that can summarize the most important features and patterns with fewer features. A common application of dimensionality reduction is a two-dimensional reduction for visualization purposes. Another application for unsupervised transformations is finding the parts that can make up the dataset. One common example is the extraction of topics from the collections of text documents. Here the task is to find the unfamiliar topics mentioned in each document and to learn which topics appear in each document. This can be useful for following the discussion of topics such as elections, gun control or pop stars on social media. Clustering: Clustering algorithms, on the other hand, partition data into separate groups of similar items. Take the example of uploading photos to a social networking site. To help organize your images, the site may want to group images showing the same person. However, the site doesn’t know which images show whom, and it doesn’t know how many different people appear in your photo collection. A better way will be to extract all the faces and split them into groups of faces that look alike. Hope these are from the same person and the pictures can be put together for you. When To Use Unsupervised Learning? Unsupervised machine learning algorithms are often used in an exploratory context when a data scientist wishes to better understand the data, rather than as part of a larger machine system. Another common application of unsupervised machine learning algorithms is a preprocessing step for supervised algorithms. Learning a new representation of data can sometimes improve the accuracy of supervised algorithms, or can lead to reduced memory and time consumption. I hope you liked this article on an introduction to Unsupervised machine learning, its types and when to use it in the process of applying machine learning algorithms. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";What is Unsupervised Learning in Machine Learning?
2020-10-25 11:31:39;In Machine Learning, Cross-validation is a statistical method of evaluating generalization performance that is more stable and thorough than using a division of dataset into a training and test set. In this article, I’ll walk you through what cross-validation is and how to use it for machine learning using the Python programming language.;https://thecleverprogrammer.com/2020/10/25/what-is-cross-validation-in-machine-learning/;['sklearn'];1.0;[];['ML', 'Regression'];['regression', 'fit', 'model', 'machine learning', 'training data', 'ground truth', 'train', 'label', 'rank'];"In Machine Learning, Cross-validation is a statistical method of evaluating generalization performance that is more stable and thorough than using a division of dataset into a training and test set. In this article, I’ll walk you through what cross-validation is and how to use it for machine learning using the Python programming language. What is Cross-Validation? In cross-validation, the data is instead split multiple times and multiple models are trained. The most commonly used version of cross-validation is k-times cross-validation, where k is a user-specified number, usually 5 or 10. Also, Read – Machine Learning Full Course for free. In five-way cross-validation, the data is first partitioned into five parts of (approximately) equal size, called folds. Then, a sequence of models is formed. The first model is trained using the first fold as a test set, and the remaining folds (2–5) are used as a training set. The model is built using data from folds 2 to 5, then the precision is evaluated on fold 1. Then another model is built, this time using fold 2 as the test set and the data from folds 1, 3, 4 and 5 as a training set. This process is repeated using folds 3, 4 and 5 as test sets. For each of these five divisions of the data into training and testing sets, we calculate the precision. In the end, we collected five precision values. Implementation Of Cross-Validation with Python We can easily implement the process of Cross-validation with Python programming language by using the Scikit-learn library in Python. Cross-validation is implemented in scikit-learn using the cross_val_score function of the model_selection module. The parameters of the cross_val_score function are the model we want to evaluate, the training data, and the ground truth labels. Let’s evaluate LogisticRegression on the iris dataset: from sklearn.model_selection import cross_val_score from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression iris = load_iris() logreg = LogisticRegression() scores = cross_val_score(logreg, iris.data, iris.target) print(""Cross-validation scores: {}"".format(scores)) Code language: Python (python) Output: Cross-validation scores: [ 0.961 0.922 0.958] By default, cross_val_score performs triple cross-validation, returning three precision values. We can modify the number of folds used by modifying the cv parameter: scores = cross_val_score(logreg, iris.data, iris.target, cv=5) print(""Cross-validation scores: {}"".format(scores)) Code language: Python (python) Output: Cross-validation scores: [ 1. 0.967 0.933 0.9 1. ] A common way to summarize the precision of cross-validation is to calculate the mean: print(""Average cross-validation score: {:.2f}"".format(scores.mean()))Code language: Python (python) Output: Average cross-validation score: 0.96 Benefits & Drawbacks of Using Cross-Validation There are several advantages to using cross-validation instead of a single division into one training and one set of tests. First of all, remember that train_test_split performs a random division of data. Imagine that we are “lucky” at randomly splitting the data, and all the hard-to-categorize examples end up in the training set. In this case, the test set will only contain “simple” examples, and the accuracy of our test set will be unrealistic. Conversely, if we are “unlucky” we may have randomly placed all of the hard-to-rank examples in the test set and therefore have an unrealistic score. However, when using cross-validation, each example will be in the test set exactly once: each example is in one of the folds, and each fold is the test set once. Therefore, the model must generalize well to all samples in the dataset for all cross-validation scores (and their mean) to be high. Having multiple splits of the data also provides information about the sensitivity of our model to the selection of the training data set. For the iris dataset, we saw accuracies between 90% and 100%. That’s quite a range, and it gives us an idea of ​​how the model might work in the worst-case scenario and the best-case scenario when applied to new data. Another advantage of cross-validation over using a single data division is that we use our data more efficiently. When using train_test_split, we typically use 75% of the data for training and 25% of the data for evaluation. When using five-fold cross-validation, on each iteration we can use four-fifths of the data (80%) to fit the model. When using 10 cross-validations, we can use the nine-tenths of the data (90%) to fit the model. More data will generally result in more accurate models. The main disadvantage is the increase in computational costs. Since we are currently training k models instead of a single model, the cross-validation will be about k times slower than doing a single division of the data. Conclusion Using the mean cross-validation, we can conclude that we expect the model to be around 96% accurate on average. Looking at the five scores produced by the five-fold cross-validation, we can also conclude that there is a relatively high variance in precision between folds, ranging from 100% precision to 90% precision. This could imply that the model is very dependent on the particular folds used for training, but it could also simply be a consequence of the small size of the data set. I hope you liked this article on what is Cross-validation, its implementation using Python and its benefits & drawbacks. Feel free to ask your valuable questions in the comments section below. Follow Us: Facebook Instagram";What is Cross-Validation in Machine Learning?
2020-10-26 11:34:44;One of the simplest but effective and commonly used ways to represent text for machine learning is through the bag of words representation. In this article, I will take you through the implementation of Bag Of Words in Machine Learning with Python programming language.;https://thecleverprogrammer.com/2020/10/26/bag-of-words-in-machine-learning-with-python/;['sklearn', 'vocabulary'];1.0;[];['ML'];['fit', 'model', 'machine learning', 'training data', 'train'];"One of the simplest but effective and commonly used ways to represent text for machine learning is through the bag of words representation. In this article, I will take you through the implementation of Bag Of Words in Machine Learning with Python programming language. Introduction To Bag Of Words in Machine Learning Using the Bag Of Words representation, we remove most of the structure of the input text, such as chapters, paragraphs, sentences, and formatting, and only count how often each word appears in each. text of the corpus. Also, Read – Machine Learning Full Course for free. Ignoring structure and counting only word occurrences leads to the mental image of the text being represented as a “bag.” The calculation of the word bag representation for a corpus of documents consists of the following three steps: Tokenization: Divide each document into words that appear there (called tokens), for example by dividing them into spaces and punctuation marks.Vocabulary building: Collect a vocabulary of all the words that appear in any of the documents and number them (for example, in alphabetical order).Encoding: for each document, count the frequency with which each of the vocabulary words appears in that document. There are a few niceties involved in Steps 1 and 2, which I will discuss below. For now, let’s see how we can apply word processing using scikit-learn. The image below illustrates the process on a string: Process of Bag Of Words The output is a vector of word counts for each document. For each word in the vocabulary, we count the frequency with which it appears in each document. This means that our digital representation has a characteristic for every unique word in the data set. Note that the order of the words in the original string has no relation to the representation of the functions of the bag of words. Implementing Bag Of Words with Python The bag of words representation is implemented in CountVectorizer, which is a transformer. Let’s first apply it to a few sample sentences, made up of two examples, to see it in action: bards_words =[""The fool doth think he is wise,"", ""but the wise man knows himself to be a fool""]Code language: Python (python) Next, we import and instantiate the CountVectorizer and adapt it to our data as follows: from sklearn.feature_extraction.text import CountVectorizer vect = CountVectorizer() vect.fit(bards_words)Code language: Python (python) After the adjustment, the CountVectorizer consists of tokenization of the training data and the construction of the vocabulary, which we can access as a vocabulary_ attribute: print(""Vocabulary size: {}"".format(len(vect.vocabulary_))) print(""Vocabulary content:\n {}"".format(vect.vocabulary_))Code language: Python (python) Vocabulary size: 13 Vocabulary content: {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7, 'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1} The vocabulary consists of 13 words, from “to be” to “wise”. To create the bag of words representation for the training data, we call the transformation method: bag_of_words = vect.transform(bards_words) print(""bag_of_words: {}"".format(repr(bag_of_words)))Code language: Python (python) bag_of_words: <2x13 sparse matrix of type '' with 16 stored elements in Compressed Sparse Row format> The representation of the bag of words is stored in a SciPy fragmented matrix which only stores nonzero entries. The matrix is ​​in the form of 2 × 13, with a row for each of the two data points and a characteristic for each of the vocabulary words. A sparse matrix is ​​used because most documents only contain a small subset of the vocabulary words, which means that most Entity Table entries are 0. Think about how many different words can appear. in a movie review against all the words in the English language (which the vocabulary models). Storing all of these zeros would be prohibitive and a waste of memory. To look at the actual contents of the sparse matrix, we can convert it to a “dense” NumPy array (which also stores all 0 entries) using the toarray method: print(""Dense representation of bag_of_words:\n{}"".format( bag_of_words.toarray()))Code language: Python (python) Dense representation of bag_of_words: [[0 0 1 1 1 0 1 0 0 1 1 0 1] [1 1 0 1 0 1 0 1 1 1 0 1 1]] Conclusion We can see that the word counts for each word are either 0 or 1; neither of the two strings of bards_words contains a word twice. Let’s see how to read these feature vectors. The first string (“The fool thinks he is wise”) is represented as the first line and it contains the first vocabulary word, “to be”, zero times. It also contains the second vocabulary word, “but”, zero times. It contains the third word, “doth”, once, and so on. Looking at the two lines, we can see that the fourth word, “idiot”, the tenth word, “the”, and the thirteenth word, “wise”, appear in both strings. Hope you liked this article on the implementation of Bag Of Words in Machine Learning using the Python programming language. Please feel free to ask your valuable questions in the comments section below.";Bag Of Words in Machine Learning with Python
2020-10-27 10:20:11;I recently shared an article on Bag Of Words, Stopwords in Machine Learning is another way to get rid of uninformative words by rejecting words that are too frequent to be informative. In this article, I will introduce you to the concept of stopwords in machine learning.;https://thecleverprogrammer.com/2020/10/27/what-are-stopwords-in-machine-learning/;['sklearn', 'vocabulary'];1.0;['CV'];['ML', 'CV', 'Regression'];['regression', 'fit', 'model', 'machine learning', 'filter', 'train'];"I recently shared an article on Bag Of Words, Stopwords in Machine Learning is another way to get rid of uninformative words by rejecting words that are too frequent to be informative. In this article, I will introduce you to the concept of stopwords in machine learning. Stopwords in Machine Learning Stop words are commonly used words that are excluded from searches to help index and crawl web pages faster. Some examples of stop words are: “a,” “and” “but” “how”, “or” and “what”. Sometimes certain extremely common words which seem to have little value in helping to select documents corresponding to a user’s need are excluded from the vocabulary entirely. These words are called stopwords. The general strategy for determining a list of stopwords is to sort the terms by collection frequency, then take the most frequent terms, often filtered by hand for their semantic content relative to the domain of the documents to be indexed as a list of stopwords. There are two main approaches: using a language-specific stop word list or removing words that appear too frequently. scikit-learn has a built-in list of stopwords in English in the feature_extraction.text module: from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS print(""Number of stop words: {}"".format(len(ENGLISH_STOP_WORDS))) print(""Every 10th stopword:\n{}"".format(list(ENGLISH_STOP_WORDS)[::10]))​x from sklearn.feature_extraction.text import ENGLISH_STOP_WORDSprint(""Number of stop words: {}"".format(len(ENGLISH_STOP_WORDS)))print(""Every 10th stopword:\n{}"".format(list(ENGLISH_STOP_WORDS)[::10])) Number of stop words: 318 Every 10th stopword: ['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough', 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed', 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the', 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i'] Removing stopwords from the list can only reduce the number of entities by the length of the list – here, 318 – but it can lead to improved performance. Let’s try: # Specifying stop_words=""english"" uses the built-in list. # We could also augment it and pass our own. vect = CountVectorizer(min_df=5, stop_words=""english"").fit(text_train) X_train = vect.transform(text_train) print(""X_train with stop words:\n{}"".format(repr(X_train))) # Specifying stop_words=""english"" uses the built-in list.# We could also augment it and pass our own.vect = CountVectorizer(min_df=5, stop_words=""english"").fit(text_train)X_train = vect.transform(text_train)print(""X_train with stop words:\n{}"".format(repr(X_train))) X_train with stop words: <25000x26966 sparse matrix of type '' with 2149958 stored elements in Compressed Sparse Row format> There are now 305 (27,271–26,966) fewer features in the dataset, meaning that most, but not all, stop words have occurred. Let’s start the search again in the grid: grid = GridSearchCV(LogisticRegression(), param_grid, cv=5) grid.fit(X_train, y_train) print(""Best cross-validation score: {:.2f}"".format(grid.best_score_)) grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)grid.fit(X_train, y_train)print(""Best cross-validation score: {:.2f}"".format(grid.best_score_)) Best cross-validation score: 0.88 The grid search performance decreased slightly using the stopwords—not enough to worry about, but given that excluding 305 features out of over 27,000 is unlikely to change performance or interpretability a lot, it doesn’t seem worth using this list. Fixed lists are most helpful for small datasets, which might not contain enough information for the model to determine which words are stopwords from the data itself. As an exercise, you can try out the other approach, discarding frequently appearing words, by setting the max_df option of CountVectorizer and see how it influences the number of features and the performance. I hope you liked this article on the concept of Stop words in Machine Learning. Feel free to ask your valuable questions in the comments section below.";What are Stopwords in Machine Learning?
2020-10-28 09:23:43;One of the most important ways to resize data in the machine learning process is to use the term frequency inverted document frequency, also known as the tf-idf method. In this article, I will walk you through what the tf-idf method is in Machine Learning and how to implement it using the Python programming language.;https://thecleverprogrammer.com/2020/10/28/what-is-tf-idf-in-machine-learning/;['sklearn'];1.0;[];['ML'];['fit', 'machine learning'];One of the most important ways to resize data in the machine learning process is to use the term frequency inverted document frequency, also known as the tf-idf method. In this article, I will walk you through what the tf-idf method is in Machine Learning and how to implement it using the Python programming language. What is tf-idf? The intuition of the tf-idf method is to give high weight to any term that often appears in a particular document, but not in many documents in the corpus. If a word appears often in a particular document, but not in many documents, it is likely to be very descriptive of the contents of that document. Also, Read – Machine Learning Full Course for free. Scikit-Learn implements the tf -idf method in two classes: TfidfTransformer, which takes in the sparse matrix output produced by CountVectorizer and transforms it, and TfidfVectorizer, which takes in text data and performs both feature extraction of the bag of words and the transformation tf -idf. Why Use Tf-Idf Vectorization? Suppose a search engine has a database with thousands of cat descriptions and a user wants to search for furry cats, then he/she issues the query “furry cat”. A search engine needs to decide which result should be returned from the database. If the search engine has documents that match the exact query, there is no doubt, but what if it needs to decide between partial matches? To simplify, let’s say it has to choose between these two descriptions: “The pretty cat”“A furry kitten” The first description contains 2 of 3 words of the query and the second only matches 1 of 3, then the search engine will choose the first description. How can TF-IDF help it to choose the second description instead of the first? The TF is the same for every word, no difference here. However, one would expect the terms “cat” and “kitten” to appear in many documents (high frequency of documents implies low IDF), while the term “furry” will appear in fewer documents (IDF taller). Thus, the TF-IDF for cat & kitten has a low value while the TF-IDF is larger for “hairy”, that is to say, that in our database the word “hairy” has more power. discriminating as “cat” or “kitten”. If we use the TF-IDF to weight the different words that match the query, “hairy” would be more relevant than “cat” and so we could choose “hairy kitten” as the best match. Implementation with Python Now let’s see how to implement the tf-idf method with Machine Learning using the Python programming language. The example below shows the implementation of tf-idf vectorization using Scikit-learn: from sklearn.feature_extraction.text import TfidfVectorizer corpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?', ] vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(corpus) print(vectorizer.get_feature_names()) print(X.shape) view raw TF-IDF hosted with ❤ by GitHub View this gist on GitHub Output: (4, 9) Keep in mind that tf-idf scaling is intended to find words that distinguish documents, but this is a purely unsupervised technique. Low tf-idf features are those that are either very commonly used in documents or used sparingly and only in very long documents. I hope you liked this article on the TF-IDF vectorization in Machine Learning. Feel free to ask your valuable questions in the comments section below.;What is TF-IDF in Machine Learning?
2020-10-29 10:46:56;In this article, I will take you through the task of Age and Gender Detection with Machine Learning by using the Python programming language. Age and Gender Detection is the task of Computer vision so I will be using the OpenCV library in Python.Before getting started with the task of Age and Gender Detection with Python, I will first take you through what the concept means and how to deal with the problem of age and gender detection. Understanding the concept is important so that in future you can easily perform the task of age and gender detection with not python only but with any programming language.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/10/29/age-and-gender-detection-with-python/;['caffe'];1.0;['CV'];['CV', 'Regression', 'Object Detection', 'ML', 'Classification'];['detect', 'regression', 'computer vision', 'predict', 'model', 'machine learning', 'classif', 'object detection', 'train', 'label'];"In this article, I will take you through the task of Age and Gender Detection with Machine Learning by using the Python programming language. Age and Gender Detection is the task of Computer vision so I will be using the OpenCV library in Python. Before getting started with the task of Age and Gender Detection with Python, I will first take you through what the concept means and how to deal with the problem of age and gender detection. Understanding the concept is important so that in future you can easily perform the task of age and gender detection with not python only but with any programming language. Also, Read – Machine Learning Full Course for free. Introduction to Age and Gender Detection The task of detecting age and gender, however, is an inherently difficult problem, more so than many other computer vision tasks. The main reason for this difficulty gap lies in the data required to train these types of systems. While general object detection tasks can often have access to hundreds of thousands or even millions of images for training, datasets with age and/or gender labels are considerably smaller, usually in the thousands or, at best, tens of thousands. The reason is that to have tags for such images, we need to access the personal information of the subjects in the images. Namely, we would need their date of birth and gender, and in particular date of birth is infrequently published information. Namely, we would need their date of birth and gender, and in particular date of birth is infrequently published information. Therefore, we have to settle for the nature of this problem that we are addressing and adapt network architectures and algorithmic approaches to deal with these limitations. Age and Gender Detection with Python The areas of classification by age and sex have been studied for decades. Various approaches have been taken over the years to tackle this problem, with varying levels of success. Now let’s start with the task of detecting age and gender using the Python programming language. I will present the problem of gender detection as a classification problem and the age detection problem as a regression problem. However, estimating age accurately using regression is difficult. Even humans cannot accurately predict an age by looking at a person. However, we do know if they are in their 30s or 40s. This is also what I’m going to follow using Python. Getting Started: Now let’s get started with the task of Age and Gender detection using the Python programming language. I will first start with writing the code for detecting faces because without face detection we will not be able to move further with the task of age and gender prediction. You can download the necessary OpenCV pre-trained models that you will need in the task of age and gender detection from here. Now after importing the OpenCV module in your python file you can get started with the code below. Python code for Face Detection: def getFaceBox(net, frame, conf_threshold=0.7): frameOpencvDnn = frame.copy() frameHeight = frameOpencvDnn.shape[0] frameWidth = frameOpencvDnn.shape[1] blob = cv.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False) net.setInput(blob) detections = net.forward() bboxes = [] for i in range(detections.shape[2]): confidence = detections[0, 0, i, 2] if confidence > conf_threshold: x1 = int(detections[0, 0, i, 3] * frameWidth) y1 = int(detections[0, 0, i, 4] * frameHeight) x2 = int(detections[0, 0, i, 5] * frameWidth) y2 = int(detections[0, 0, i, 6] * frameHeight) bboxes.append([x1, y1, x2, y2]) cv.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8) return frameOpencvDnn, bboxes view raw age and gender detection hosted with ❤ by GitHub View this gist on GitHub Now the next step is to predict the gender of humans in the image. Here I will load the gender network into memory and transmit the detected face across the network for the gender detection task. Python code for Gender Detection: genderProto = ""gender_deploy.prototxt"" genderModel = ""gender_net.caffemodel"" ageNet = cv.dnn.readNet(ageModel, ageProto) genderList = ['Male', 'Female'] blob = cv.dnn.blobFromImage(face, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False) genderNet.setInput(blob) genderPreds = genderNet.forward() gender = genderList[genderPreds[0].argmax()] print(""Gender Output : {}"".format(genderPreds)) print(""Gender : {}"".format(gender)) view raw age and gender detection hosted with ❤ by GitHub View this gist on GitHub Now the next task is to predict the age of the human in the image. Here I will load the ageing network and use the forward pass to get the output. Since the network architecture is similar to that of the Gender Network, we can make the most of all outputs to get the intended age group for the task to detect age. Python code for age detection: ageProto = ""age_deploy.prototxt"" ageModel = ""age_net.caffemodel"" ageNet = cv.dnn.readNet(ageModel, ageProto) ageList = ['(0 - 2)', '(4 - 6)', '(8 - 12)', '(15 - 20)', '(25 - 32)', '(38 - 43)', '(48 - 53)', '(60 - 100)'] ageNet.setInput(blob) agePreds = ageNet.forward() age = ageList[agePreds[0].argmax()] print(""Gender Output : {}"".format(agePreds)) print(""Gender : {}"".format(age)) view raw age and gender detection hosted with ❤ by GitHub View this gist on GitHub The last code we need to write is to display the output: label = ""{}, {}"".format(gender, age) cv.putText(frameFace, label, (bbox[0], bbox[1]-20), cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 3, cv.LINE_AA) cv.imshow(""Age Gender Demo"", frameFace) view raw age and gender detection hosted with ❤ by GitHub View this gist on GitHub So, as you can see from the output, we are able to predict both gender and age with a high level of accuracy. Hope you liked this article on age and gender classification with the Python programming language. Please feel free to ask your valuable questions in the comments section below.";Age and Gender Detection with Python
2020-11-01 12:01:35;If you are interested in developing a chatbot, you may find that there are many powerful bot development frameworks, tools, and platforms that can be used to implement smart chatbot programs. In this article, I’ll walk you through how to create a Chatbot with Python and Machine Learning.;https://thecleverprogrammer.com/2020/11/01/chatbot-with-machine-learning-and-python/;['keras', 'pattern', 'nltk', 'tensorflow', 'sklearn'];1.0;[];['ML', 'ReLu', 'NN', 'Chatbot'];['epoch', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'layer', 'relu', 'chatbot', 'training data', 'train', 'label'];"If you are interested in developing a chatbot, you may find that there are many powerful bot development frameworks, tools, and platforms that can be used to implement smart chatbot programs. In this article, I’ll walk you through how to create a Chatbot with Python and Machine Learning. How Does a Chatbot Work? Since we will be developing a Chatbot with Python using Machine Learning, we need some data to train our model. But we’re not going to collect or download a large dataset since this is just a chatbot. We can just create our own dataset to train the model. Also, Read – Machine Learning Full Course For free. To create this dataset to create a chatbot with Python, we need to understand what intents we are going to train. An “intention” is the user’s intention to interact with a chatbot or the intention behind every message the chatbot receives from a particular user. Therefore, it is important to understand the good intentions of your chatbot depending on the domain you will be working with. So why does he need to define these intentions? This is a very important point to understand. In order to answer questions asked by the users and perform various other tasks to continue conversations with the users, the chatbot really needs to understand what users are saying or having ‘intention to do. This is why your chatbot must understand the intentions behind users’ messages. How can you get your chatbot to understand the intentions so that users feel like they know what they want and provide accurate answers? The strategy here is to set different intents and create training samples for those intents and train your chatbot model with these sample training data as model training data (X) and intents in as model training categories (Y). Create a Chatbot with Python and Machine Learning To create a chatbot with Python and Machine Learning, you need to install some packages. All the packages you need to install to create a chatbot with Machine Learning using the Python programming language are mentioned below: tensorflow==2.3.1 nltk==3.5 colorama==0.4.3 numpy==1.18.5 scikit_learn==0.23.2 Flask==1.1.2 Defining the Intentions of a Chatbot Now we need to define a few simple intents and a group of messages that match those intents and also map some responses based on each intent category. I’ll create a JSON file named “intents.json” including this data as follows: {""intents"": [ {""tag"": ""greeting"", ""patterns"": [""Hi"", ""Hey"", ""Is anyone there?"", ""Hello"", ""Hay""], ""responses"": [""Hello"", ""Hi"", ""Hi there""] }, {""tag"": ""goodbye"", ""patterns"": [""Bye"", ""See you later"", ""Goodbye""], ""responses"": [""See you later"", ""Have a nice day"", ""Bye! Come back again""] }, {""tag"": ""thanks"", ""patterns"": [""Thanks"", ""Thank you"", ""That's helpful"", ""Thanks for the help""], ""responses"": [""Happy to help!"", ""Any time!"", ""My pleasure"", ""You're most welcome!""] }, {""tag"": ""about"", ""patterns"": [""Who are you?"", ""What are you?"", ""Who you are?"" ], ""responses"": [""I.m Joana, your bot assistant"", ""I'm Joana, an Artificial Intelligent bot""] }, {""tag"": ""name"", ""patterns"": [""what is your name"", ""what should I call you"", ""whats your name?""], ""responses"": [""You can call me Joana."", ""I'm Joana!"", ""Just call me as Joana""] }, {""tag"": ""help"", ""patterns"": [""Could you help me?"", ""give me a hand please"", ""Can you help?"", ""What can you do for me?"", ""I need a support"", ""I need a help"", ""support me please""], ""responses"": [""Tell me how can assist you"", ""Tell me your problem to assist you"", ""Yes Sure, How can I support you""] }, {""tag"": ""createaccount"", ""patterns"": [""I need to create a new account"", ""how to open a new account"", ""I want to create an account"", ""can you create an account for me"", ""how to open a new account""], ""responses"": [""You can just easily create a new account from our web site"", ""Just go to our web site and follow the guidelines to create a new account""] }, {""tag"": ""complaint"", ""patterns"": [""have a complaint"", ""I want to raise a complaint"", ""there is a complaint about a service""], ""responses"": [""Please provide us your complaint in order to assist you"", ""Please mention your complaint, we will reach you and sorry for any inconvenience caused""] } ] } view raw chatbot with Machine Learning hosted with ❤ by GitHub View this gist on GitHub Data preparation: The second step of this task to create a chatbot with Python and Machine Learning is to prepare the data to train our chatbot. I’ll start this step by importing the necessary libraries and packages: import json import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from sklearn.preprocessing import LabelEncoder view raw chatbot with Machine Learning hosted with ❤ by GitHub View this gist on GitHub Now I will read the JSON file and process the required files: with open('intents.json') as file: data = json.load(file) training_sentences = [] training_labels = [] labels = [] responses = [] for intent in data['intents']: for pattern in intent['patterns']: training_sentences.append(pattern) training_labels.append(intent['tag']) responses.append(intent['responses']) if intent['tag'] not in labels: labels.append(intent['tag']) num_classes = len(labels) view raw chatbot with Machine Learning hosted with ❤ by GitHub View this gist on GitHub Now we need to use the label encoder method provided by the Scikit-Learn library in Python: lbl_encoder = LabelEncoder() lbl_encoder.fit(training_labels) training_labels = lbl_encoder.transform(training_labels) view raw chatbot with Machine Learning hosted with ❤ by GitHub View this gist on GitHub Tokenization: Now we need to vectorize the data using the Tokenization method to create a chatbot with Python and Machine Learning: vocab_size = 1000 embedding_dim = 16 max_len = 20 oov_token = ""<OOV>"" tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token) tokenizer.fit_on_texts(training_sentences) word_index = tokenizer.word_index sequences = tokenizer.texts_to_sequences(training_sentences) padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len) view raw chatbot with Machine Learning hosted with ❤ by GitHub View this gist on GitHub Training a Neural Network Now the next and most important step in the process of building a chatbot with Python and Machine Learning is to train a neural network. Now, I will train and create a neural network to train our chatbot: model = Sequential() model.add(Embedding(vocab_size, embedding_dim, input_length=max_len)) model.add(GlobalAveragePooling1D()) model.add(Dense(16, activation='relu')) model.add(Dense(16, activation='relu')) model.add(Dense(num_classes, activation='softmax')) model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.summary() epochs = 500 history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs) view raw chatbot with Machine Learning hosted with ❤ by GitHub View this gist on GitHub Saving The Neural Network: We’ve trained the model, but before we go any further in the process of building a chatbot with Python and Machine Learning, let’s save the model so that we can use this neural network in the future as well: # to save the trained model model.save(""chat_model"") import pickle # to save the fitted tokenizer with open('tokenizer.pickle', 'wb') as handle: pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) # to save the fitted label encoder with open('label_encoder.pickle', 'wb') as ecn_file: pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL) view raw chatbot with Machine Learning hosted with ❤ by GitHub View this gist on GitHub Now let’s Build a Chatbot with Python and our Trained Machine Learning Model Now I am going to implement a chat function to interact with a real user. When the message from the user will be received, the chatbot will compute the similarity between the sequence of the new text and the training data. Taking into account the trust scores obtained for each category, it categorizes the user’s message according to an intention with the highest trust score: import json import numpy as np from tensorflow import keras from sklearn.preprocessing import LabelEncoder import colorama colorama.init() from colorama import Fore, Style, Back import random import pickle with open(""intents.json"") as file: data = json.load(file) def chat(): # load trained model model = keras.models.load_model('chat_model') # load tokenizer object with open('tokenizer.pickle', 'rb') as handle: tokenizer = pickle.load(handle) # load label encoder object with open('label_encoder.pickle', 'rb') as enc: lbl_encoder = pickle.load(enc) # parameters max_len = 20 while True: print(Fore.LIGHTBLUE_EX + ""User: "" + Style.RESET_ALL, end="""") inp = input() if inp.lower() == ""quit"": break result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]), truncating='post', maxlen=max_len)) tag = lbl_encoder.inverse_transform([np.argmax(result)]) for i in data['intents']: if i['tag'] == tag: print(Fore.GREEN + ""ChatBot:"" + Style.RESET_ALL , np.random.choice(i['responses'])) # print(Fore.GREEN + ""ChatBot:"" + Style.RESET_ALL,random.choice(responses)) print(Fore.YELLOW + ""Start messaging with the bot (type quit to stop)!"" + Style.RESET_ALL) chat() view raw chatbot with Machine Learning hosted with ❤ by GitHub View this gist on GitHub This is how we can create a chatbot with Python and Machine Learning. Hope you liked this article on how to create a Chatbot with Python and Machine Learning. Please feel free to ask your valuable questions in the comments section below.";Chatbot with Machine Learning and Python
2020-11-02 13:20:14;Machine learning algorithms are a set of instructions for a computer on how to interact with, manipulate, and transform data. There are so many types of machine learning algorithms. Selecting the right algorithm is both science and art.Two data scientists tasked with solving the same business challenge can choose different algorithms to address the same problem. However, understanding different classes of machine learning algorithms helps data scientists identify the best types of algorithms. In this article, I will introduce you to the main types of machine learning algorithms.Also, Read – Machine Learning Full Course For free.;https://thecleverprogrammer.com/2020/11/02/types-of-machine-learning-algorithms/;['pattern'];1.0;[];['CV', 'NN', 'Regression', 'DL', 'ML', 'Supervised Learning', 'Decision Tree', 'Bayesian', 'Clustering', 'Unsupervised learning'];['regression', 'supervised learning', 'train', 'unsupervised learning', 'label', 'computer vision', 'rank', 'unlabeled', 'hidden layer', 'fit', 'model', 'training data', 'decision tree', 'clustering', 'recogn', 'predict', 'bayesian', 'machine learning', 'neural network', 'layer', 'deep learning'];Machine learning algorithms are a set of instructions for a computer on how to interact with, manipulate, and transform data. There are so many types of machine learning algorithms. Selecting the right algorithm is both science and art. Two data scientists tasked with solving the same business challenge can choose different algorithms to address the same problem. However, understanding different classes of machine learning algorithms helps data scientists identify the best types of algorithms. In this article, I will introduce you to the main types of machine learning algorithms. Also, Read – Machine Learning Full Course For free. Types of Machine Learning Algorithms Bayesian: Bayesian algorithms allow data scientists to encode past beliefs about what models should look like, regardless of the state of the data. With so much focus on the data defining the model, you might wonder why people would be interested in Bayesian algorithms. These algorithms are especially useful when you don’t have huge amounts of data to train a model with confidence. A Bayesian algorithm would make sense, for example, if you have prior knowledge of part of the model and therefore can code it directly. Take the case of a medical imaging diagnostic system that looks for lung disorders. If a study published in a journal estimates the likelihood of different lung disorders based on lifestyle, those probabilities can be encoded into the model. Clustering: Clustering is a fairly easy technique to understand – objects with similar parameters are grouped (in a cluster). All objects in a cluster are more similar to each other than objects in other clusters. Clustering is a type of unsupervised learning because the data is not labelled. The algorithm interprets the parameters that make up each element, then groups them accordingly. Decision tree: Decision tree algorithms use a branching structure to illustrate the results of a decision. Decision trees can be used to map the possible outcomes of a decision. Each node in a decision tree represents a possible outcome. Percentages are assigned to nodes based on the likelihood of the outcome occurring. Decision trees are sometimes used for marketing campaigns. You might want to predict the outcome of sending a 20% coupon to customers and prospects. You can divide customers into four segments: Persuading who will likely buy if they receive awarenessSafe things that will buy no matter whatLost causes that will never buyFragile clients likely to react negatively to an outreach attempt If you are sending out a marketing campaign, you want to avoid sending articles to three of the groups because they won’t respond, but anyway, or respond negatively. Targeting the persuaders will give you the best return on investment (ROI). A decision tree will help you map these four customer groups and organize prospects and customers according to who will respond best to the marketing campaign. Dimensionality Reduction: Dimensionality reduction helps systems remove data that is not useful for analysis. This group of algorithms is used to remove redundant data, outliers, and other unnecessary data. The dimensionality reduction can be useful when analyzing sensor data and other Internet of Things (IoT) use cases. In IoT systems, there can be thousands of data points just telling you that a sensor is activated. Storing and analyzing this “on” data is not useful and will take up significant storage space. Moreover, by removing these redundant data, the performance of a machine learning system will improve. Finally, reducing dimensionality will also help analysts visualize the data. Instance-Based Machine Learning: Instance-based algorithms are used when you want to rank new data points based on similarities to training data. This set of algorithms is sometimes referred to as lazy learners because there is no training phase. Instead, the instance-based algorithms simply match the new data with the training data and rank the new data points based on their similarity to the training data. Instance-based learning is not well suited to datasets with random variations, irrelevant data, or data with missing values. Instance-based algorithms can be very useful in pattern recognition. For example, instance learning is used in chemical and biological structure analysis and spatial analysis. Analysis in the biological, pharmaceutical, chemical and technical fields often uses various instance-based algorithms. Neural Networks: A neural network attempts to mimic the way a human brain approaches problems and uses layers of interconnected units to learn and infer relationships based on observed data. A neural network can have multiple layers connected. When there is more than one hidden layer in a neural network, it is sometimes referred to as deep learning. Models of neural networks can adapt and learn as data changes. Neural networks are often used when data is unlabeled or unstructured. One of the main use cases for neural networks in computer vision. Deep learning is exploited in a variety of applications today. Self-driving cars use deep learning to help the vehicle understand the environment around the car. When cameras capture images of the surrounding environment, deep learning algorithms interpret the unstructured data to help the system make near-real-time decisions. Likewise, deep learning is built into the applications radiologists use to help interpret medical images. Regression: Regression algorithms are commonly used for statistical analysis and are key algorithms for use in machine learning. Regression algorithms help analysts model relationships between data points. Regression algorithms can quantify the strength of the correlation between variables in a data set. Additionally, regression analysis can be useful in predicting future data values ​​based on historical values. However, it is important to remember that regression analysis assumes that correlation is related to causation. Without understanding the context around the data, regression analysis can lead you to inaccurate predictions. Regularization: Regularization is a technique for modifying models to avoid the problem of overfitting. You can apply regularization to any machine learning model. For example, you can regularize a decision tree model. Regularization simplifies models that are too complex and likely to be over-adjusted. If a model is overfitted, it will give inaccurate predictions when exposed to new data sets. Overfitting occurs when a model is created for a specific dataset but will have poor predictive capabilities for a generalized dataset. Rule-Based Machine Learning: Rule-based machine learning algorithms use relational rules to describe data. A rules-based system can be compared to learning systems automatic weaving that creates a pattern that can be generally applied to all incoming data. In the abstract, rule-based systems are very easy to understand: if X data is entered, do Y. However, as systems become operational, a rules-based approach to machine learning can become very complex. For example, a system might have 100 predefined rules. As the system encounters more and more data and is trained, hundreds of rule exemptions could likely emerge. When creating a rules-based approach, it is important to be careful that it does not become so complicated that it loses its transparency. Think about the complexity of creating a rules-based algorithm to enforce the tax code. These were therefore the main types of machine learning algorithms. Hope you liked this article on the types of algorithms in Machine Learning. Please feel free to ask your valuable questions in the comments section below.;Types of Machine Learning Algorithms
2020-11-03 12:56:23;Data visualization is the field of representing data and information in graphical form. Data visualization makes it easier for us to understand data and as a result, finding patterns, trends and correlations in big data becomes much easier. In this article, I’ll introduce you to some important Python libraries for data visualization.Python has some great data visualization libraries for creating interactive charts, charts and everything in between. But not knowing the benefits and basic functionality of these libraries, it would be difficult for someone to choose from these libraries.Also, Read – Machine Learning Full Course for free.Therefore, in this article, I’m going to introduce you to some essential data visualization libraries with their uses so that you can select the best data visualization library for your favourite task.;https://thecleverprogrammer.com/2020/11/03/python-libraries-for-data-visualization/;['pattern'];1.0;['ML'];['ML'];['fit', 'machine learning'];"Data visualization is the field of representing data and information in graphical form. Data visualization makes it easier for us to understand data and as a result, finding patterns, trends and correlations in big data becomes much easier. In this article, I’ll introduce you to some important Python libraries for data visualization. Python has some great data visualization libraries for creating interactive charts, charts and everything in between. But not knowing the benefits and basic functionality of these libraries, it would be difficult for someone to choose from these libraries. Also, Read – Machine Learning Full Course for free. Therefore, in this article, I’m going to introduce you to some essential data visualization libraries with their uses so that you can select the best data visualization library for your favourite task. Essential Python Libraries for Data Visualization Matplotlib: Matplotlib is one of the oldest and most widely used data visualization libraries in Python. It is used to create static, animated and interactive 2D data visualizations in Python and can also be highly customized to create advanced visualizations such as 3D plots. Matplotlib can be used in Python shells, Jupyter notebooks, web applications and is also supported by various Python GUI toolkits. The biggest advantage of Matplotlib is the freedom it offers to customize almost anything and create beautiful, advanced data visualizations, but these visualizations can be easily achieved with other libraries. You can learn more about the practical Matplotlib implementation for data visualization using Python from here. Seaborn: Seaborn is built on Matplotlib and supports pandas and NumPy data structures. By using Seaborn beautiful and high-quality visualizations can be achieved with just a few lines of code. Seaborn can even take data frames and tables that contain the dataset and plot them while handling all semantic mapping and statistical aggregation in the plots. Seabron should be the perfect choice for anyone looking to create beautiful static diagrams. For interactive web visualizations, there are better libraries than Seaborn. You can learn more about Seaborn’s practical implementation for data visualization using Python from here. Plotly: Plotly is built on Plotly.Js and also offers webpage integration. Plotly offers excellent interactivity and over 40 different types of charts. Plotly can be used to create stunning 3D visualizations and also to create standalone HTML visualizations. It takes a little more effort, but the result is a sleek and highly interactive visualization. Plotly will be the choice of those looking to create highly interactive visualizations. You can learn more about the practical Plotly implementation for data visualization with Python from here. How to Choose From Python Libraries for Data Visualization? So all these essential Python libraries for data visualization; Matplotlib, Seaborn and Plotly are unique in their way. So choosing one from them comes down to your personal needs and preferences. To wrap up the uses of these libraries for data visualization, I can just summarize that you can use Matplotlib and Seaborn for static visualizations and Plotly for highly interactive web embedded visualizations. So these were the most essential Python libraries for data visualization that you should know. Please feel free to ask your valuable questions in the comments section below.";Python Libraries for Data Visualization
2020-11-04 12:30:36;Anomaly detection is considered an unsupervised machine learning task because anomalies arise from conflicting or unlikely events with unknown distributions. However, the predictive performance of purely unsupervised anomaly detection often does not match the detection rates required in many tasks, and there is a need for labelled data to guide model generation.In this article, I’ll walk you through what machine learning anomaly detection is. At the end of this article, you will also get some projects based on the problem of anomaly detection to learn its practical implementation.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/11/04/what-is-anomaly-detection-in-machine-learning/;['pattern'];1.0;[];['DL', 'ML', 'Supervised Learning', 'Anomaly Detection', 'Classification', 'Unsupervised learning'];['detect', 'unlabeled', 'predict', 'supervised learning', 'model', 'machine learning', 'classif', 'deep learning', 'anomaly', 'anomaly detection', 'train', 'unsupervised learning', 'label'];Anomaly detection is considered an unsupervised machine learning task because anomalies arise from conflicting or unlikely events with unknown distributions. However, the predictive performance of purely unsupervised anomaly detection often does not match the detection rates required in many tasks, and there is a need for labelled data to guide model generation. In this article, I’ll walk you through what machine learning anomaly detection is. At the end of this article, you will also get some projects based on the problem of anomaly detection to learn its practical implementation. Also, Read – Machine Learning Full Course for free. What is Anomaly Detection? The detection of anomalies involves identifying unlikely and rare events. The classic approach to anomaly detection is to calculate an accurate description of normal data. Each newly arrived instance is compared to the normality model and an anomaly score is calculated. The score describes the deviations of the new instance from the average data instance, and if the deviation exceeds a predefined threshold, the instance is considered an anomaly or outlier and handled appropriately. Identifying data with irregular and suspicious features is crucial in many applications such as medical imaging and network security. In particular, the latter has become a dynamic area of ​​research as computer systems are increasingly exposed to security threats, such as worms, network attacks and malicious code. Network intrusion detection is the detection of previously unknown threats and attacks in network traffic. Conventional security techniques for intrusion detection are based on identifying known patterns of misuse, so-called signatures and therefore, while effective against known attacks, do not fail to protect themselves from new threats. Detection of anomalies is most beneficial in training scenarios where many instances of regular data are given, which allows the machine to come close to the underlying distribution and leads to a concise model of normality. On the other hand, outliers and anomalies are rare and can even come from changes in distributions (for example, new classes of network attacks). Particularly in adversarial contexts, such as network intrusion detection, differences in training and testing distributions are prominent as new threats and tactics are constantly being developed. Is It Supervised or Unsupervised? Detection of anomalies is considered an unsupervised learning task and therefore it is not surprising that there are a large number of applications using unsupervised anomaly detection methods. Fully supervised approaches to anomaly detection typically ignore unlabeled data during the learning phase. For example, finding anomalies in network traffic or program behaviour, reducing noise, or annotating and classifying images and documents. How does Anomaly Detection Work? In the tasks to detect anomalies, we are given n observations x1,. . . , xn ∈ X. The underlying assumption is that most of the data come from the same (unknown) distribution and we call this part of the data normal. A few observations, however, come from different distributions and are considered anomalies. These anomalies can for example be caused by broken sensors or network attacks and cannot be sampled by definition. The end task is to detect these anomalies by finding a concise description of the normal data, so that divergent observations become outliers. Practical Examples: Now, after the above explanation, you should understand that detection of anomalies is not a small concept in machine learning. Future advances in machine learning and deep learning technologies will only add to the reach of anomaly detection techniques and their value to business data. The increasing volume and complexity of data translate into major opportunities to harness that information for business success. Here are some of the practical implementations of anomaly detection using machine learning: Anomaly Detection TutorialAnomaly Detection with ARIMA Model Hope you liked this article on the concept of detection of anomalies or outliers in machine learning and how it works. Please feel free to ask your valuable questions in the comments section below.;What is Anomaly Detection in Machine Learning?
2020-11-06 17:57:45;In this article, I will take you through a complete roadmap for Machine Learning. If you don’t know about me, I was from a commerce background even I followed the same roadmap from learning a programming language to mastering the concepts of Machine Learning.Before moving further with the complete roadmap on Machine Learning if you are not from a science background and still think about how you can master machine learning, what skills you need, what difficulties you may face, and what degree you need? I have already shared my complete journey from commerce to machine learning that you can read here.Now, I will take you through the complete roadmap for machine learning. Before exploring each step in the roadmap let’s have a quick look at the complete roadmap for machine learning below.;https://thecleverprogrammer.com/2020/11/06/roadmap-for-machine-learning/;['pattern'];1.0;[];['NN', 'Regression', 'DL', 'ML', 'Linear Regression', 'Clustering', 'Classification'];['detect', 'clustering', 'regression', 'linear regression', 'model', 'machine learning', 'neural network', 'classif', 'deep learning'];In this article, I will take you through a complete roadmap for Machine Learning. If you don’t know about me, I was from a commerce background even I followed the same roadmap from learning a programming language to mastering the concepts of Machine Learning. Before moving further with the complete roadmap on Machine Learning if you are not from a science background and still think about how you can master machine learning, what skills you need, what difficulties you may face, and what degree you need? I have already shared my complete journey from commerce to machine learning that you can read here. Now, I will take you through the complete roadmap for machine learning. Before exploring each step in the roadmap let’s have a quick look at the complete roadmap for machine learning below. The Complete Roadmap for Machine Learning StepsMachine Learning Roadmap1.Learn A Programming Language (Python)2.Learn Mathematics for Machine Learning3.Learn The Basic Libraries for Mathematics and Data Handling4.Learn Data Visualization5.Learn Machine Learning Algorithms (Classification, Regression, Clustering, Dimensionality Reduction)6.Learn Deep Learning7.Start Working on Projects The above roadmap is all you need to learn machine learning. Now let’s go through each step in the above process to understand what to learn, from where to learn and how to learn. Learn a Programming Language: The first step in the roadmap for Machine Learning is to learn a programming language. Now if you wanted to learn how to develop Operating systems, or learn the art of competitive coding or even if you were looking for an industry-standard language then I would always suggest you learn C++. But you are here for Machine Learning, so I will always suggest you to learn Python for machine learning as the support of libraries for Machine Learning is good with Python. Now if you are new to coding or let’s say you are from a commerce background and want to learn machine learning then I will suggest you learn C++ first and then start learning Python. Now if your end goal is machine learning then you don’t need to learn everything in C++ before learning Python, just learn the concepts of Data Structures and algorithms using C++ and then you can easily shift to Python. Learning C++ will not waste your time if you think that you don’t have anything to do with C++, instead, it will help you to learn Python and machine learning every easily. Learn Mathematics for Machine Learning: The second step in the roadmap for machine learning is to learn mathematics for machine learning. Now you don’t need to learn everything in mathematics, there are some of the topics that you need to learn if you are aiming for machine learning. But why mathematics for machine learning? Without learning mathematics you can learn machine learning but after a certain level, you will feel that you need to grow more in your career. With that being said when you perform machine learning tasks you are using the algorithms that are already made by some popular mathematician like Linear Regression. Now, when you want to grow your career in machine learning the only way is by designing your algorithms instead of using a pre-defined algorithm. So without the proper mathematical knowledge and the intuition behind an algorithm, you will never be able to learn how to design and build your algorithms. The topics of mathematics you need to learn for machine learning can be explored from here. Learn Basic Libraries for Mathematics and Data Handling: The third step in the roadmap for machine learning is to learn the basic Python libraries for Mathematics and Data Handling. You need to spend a lot of time to learn Pandas and NumPy. Pandas and NumPy are the most important libraries in Python. Pandas is for Data handling and data manipulation and NumPy is for numerical Python. As machine learning is all about finding patterns in the data so NumPy and Pandas and really must learn libraries for a beginner. You can learn about the practical implementation of NumPy and Pandas for Machine Learning from below: NumPy TutorialPandas Tutorial Learn Data Visualization: The fourth step in the roadmap for machine learning is to learn the art of data visualization. Data Visualization is one of the most important step in machine learning tasks, as to extract patterns in the data it is very important to visualize the features, past behavior, and patterns in the data. Now data visualization is also done by using libraries in Python. There are mainly three important Python libraries for Data Visualization. Matplotlib and Seaborn for static visualizations and Plotly for interactive visualizations. You can learn about the practical implementation of Matplotlib, Seaborn and Plotly for Data Visualization in Machine Learning from below: Matplotlib TutorialSeaborn TutorialPlotly Tutorial Learn Machine Learning Algorithms: The fifth step in the roadmap for machine learning is to learn about machine learning algorithms. Machine Learning algorithms can be classified into many types that you can explore from here. Without the knowledge of Machine Learning algorithms, you cannot create machine learning models to classify, detect or any other task that you want to do with machine learning. Now there are a lot of machine learning algorithms some are very important and some are one of those that you may hardly ever need to use. So you must know which algorithm you need to learn and which not. You can learn about the implementation of all the essential machine learning algorithms from here. Learn Deep Learning: The sixth step in the roadmap for machine learning is to learn the concepts of deep learning. Deep learning is very different from algorithms that you will learn in the previous step. Deep learning is all about Neural Networks. Neural networks are computational algorithms that have been inspired by the brain of humans. By understanding how a neural network works you can easily learn the concepts of deep learning. Deep learning is even more than just neural networks. But to get started learning Neural networks will be enough. You can learn about neural networks from here. After understanding how a neural network works you can explore more about it by working on projects based on Neural networks that you can easily find here. Start Working on Projects: The above roadmap is like a syllabus that you need to follow to learn all the necessary concepts of Machine Learning. But all the topics will only give you the knowledge of theoretical concepts and practical implementations. No step in the above machine learning roadmap will give you a practical experience. So this is the reason why you need to start working on projects when you will realize that now you know about most of the topics of machine learning and you are ready to get some hands-on experience in Machine Learning. Working on Machine Learning projects is really important. It will give you a practical experience and also it will help you to build a portfolio of your projects that you can show to any employer if you are aiming for a job. You can get more than 100 solved and explained machine learning projects from here. Summary: You can learn all the stuff mentioned above on your own without enrolling yourself into a course also. You just need to improve your research skills. If you need some pdf resources to learn all the topics that you need to cover in the above roadmap for machine learning then you can mention me in the comments section below. I hope you liked this article on the complete roadmap for machine learning. Feel free to ask your valuable questions in the comments section below.;Roadmap for Machine Learning
2020-11-07 14:09:17;In the age of data-driven technologies, it’s very common for you to use terms like data science, artificial intelligence, machine learning, and deep learning. But what these terms mean and how they relate or differ from each other. In this article, I’ll walk you through the difference between data science, artificial intelligence, machine learning, and deep learning.;https://thecleverprogrammer.com/2020/11/07/data-science-artificial-intelligence-machine-learning-and-deep-learning/;['pattern'];1.0;['AI'];['NN', 'Regression', 'DL', 'AI', 'ML', 'Clustering', 'Classification'];['clustering', 'artificial intelligence', 'regression', 'predict', 'fit', 'model', 'machine learning', 'neural network', 'classif', 'deep learning', 'train'];In the age of data-driven technologies, it’s very common for you to use terms like data science, artificial intelligence, machine learning, and deep learning. But what these terms mean and how they relate or differ from each other. In this article, I’ll walk you through the difference between data science, artificial intelligence, machine learning, and deep learning. Difference Between Data Science, Artificial Intelligence, Machine Learning and Deep Learning I’ll start to differentiate between data science, artificial intelligence, machine learning, and deep learning by explaining all of these terms one by one instead of putting them in a tubular comparison as they are somehow completely different from each other and all of these terms are also very dependent on each other. Also, Read – Machine Learning Full Course for free. So it’s better to make a difference between data science, artificial intelligence, machine learning and deep learning one by one instead of putting them in a tabular representation. Data Science: Data science is about the data used to make business decisions. Today, companies are collecting a huge amount of data which has introduced terms like cloud storage and big data. It is now believed that the more data you have, the more patterns you can make, which at the end of the day will eventually help in decision making. Using data science, a business can easily unlock even those models that didn’t even exist before. Such deep models cannot be judged by experienced managers, so this is where the data comes in handy. Today, data science is used from predicting a user’s buying habits to predicting their vote in the next presidential election. Now you might be thinking that it sounds a lot like artificial intelligence. I can only say that you are not completely wrong. Because running predictive machine learning algorithms on huge data sets is also part of data science. Besides, machine learning is used in data science to make predictions and also to discover patterns from the huge amount of data. It also seems like we’re using artificial intelligence, doesn’t it? Now let’s see what artificial intelligence is and how it is different. Artificial Intelligence: Artificial intelligence isn’t new, it’s been around since the 1950s. It sounds like a new concept due to the increased computing power of the hardware we use today. Previously, we didn’t know much about artificial intelligence because we didn’t have the computing power to use it, so it was just kind of a theoretical concept in universities. But now that we’ve discovered some of the most powerful operating systems, processors, and hardware, we can harness the power of artificial intelligence in our systems. Not only powerful computer systems, but we can also now even use the power of artificial intelligence in laptops and smartphones. So what is artificial intelligence? Artificial intelligence is the power of computers that enables machines to understand and learn from patterns hidden in data to make such decisions that are manually impossible for humans. Put simply, artificial intelligence is the collection of those powerful mathematical algorithms that can be used to find relationships in the huge amount of data which can be used to make precise decisions for the future. Machine Learning: So what is Machine Learning? Machine Learning and Artificial Intelligence are strongly related to each other. In the previous section, I explained what is artificial intelligence. So how AI does what it does? By using the mathematical algorithms right? So when we feed those mathematical algorithms to train a model that learns from data then this process is known as Machine Learning. And when we are using the trained model to make predictions or decisions in real-time then this is known as Artificial Intelligence. There are many categories of Machine Learning the popular ones are Supervised and Unsupervised. Also, there are many types of algorithms in Machine Learning, the popular ones are clustering, regression and classification. Machine Learning is not only about training models, as the models are trained by using a huge amount of data and that data first need to go through some processing like the process of feature selection and data preparation to fit into the machine learning algorithms. Deep Learning: Deep Learning is an advanced version of Machine Learning. It’s not that machine learning is failing somewhere, it’s just that sometimes the data is very huge and has too many features. While these features are among those that will affect the accuracy of your model if you remove them, there is no point in using machine learning algorithms. In these cases, we need to go beyond machine learning algorithms such as regression, clustering, and classification. This is where Deep Learning comes in. Deep Learning consists of creating neural networks. The other difference between machine learning and deep learning is that deep learning needs much more powerful hardware than machine learning. Deep Learning requires the use of most powerful hardware systems so mostly GPUs are used for training Deep Learning Models. So, I hope you are now able to differentiate between Data Science, Artificial Intelligence, Machine Learning and Deep Learning. I hope you liked this article on the difference between Data Science, Artificial Intelligence, Machine Learning and Deep Learning. Feel free to ask your valuable questions in the comments section below.;Data Science, Artificial Intelligence, Machine Learning and Deep Learning
2020-11-07 15:48:08;MindsDB is one of the examples of those Machine Learning libraries that are making machine learning easy. By using the MindsDB library we can create a Machine Learning model in under 5 lines of code. In this article, I will take you through the MindsDB library in Machine Learning which helps in creating a Machine Learning model in just 5 lines of code.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/11/07/machine-learning-in-5-lines-with-mindsdb/;['pattern'];1.0;['ML', 'AutoML'];['ML', 'AutoML'];['predict', 'model', 'machine learning', 'train', 'automated machine learning'];MindsDB is one of the examples of those Machine Learning libraries that are making machine learning easy. By using the MindsDB library we can create a Machine Learning model in under 5 lines of code. In this article, I will take you through the MindsDB library in Machine Learning which helps in creating a Machine Learning model in just 5 lines of code. Also, Read – Machine Learning Full Course for free. Introduction to MindsDB MindsDB is an example of Automated Machine Learning or AutoML which helps in to gain some powerful insights from the data. By using this Auto ML library we can easily draw patterns from the data in just 5 lines of code. Installing the MindsDB library is just a matter of one command. Like we install all the packages for Python by using a pip command the same way we can install the MindsDB library in our systems. Yo install the MindsDB library in your systems just write the command below in your command prompt and hit enter: pip3 install mindsdb –-user If you got errors while installing this library then you can have a look at the official documentation here. Machine Learning in 5 Lines of Code with MindsDB: Now I will take you through a quick tutorial for using MindsDB for Machine Learning for creating a model in just 5 lines of code. I will show you this tutorial on real-world data. The data set I am using in this article can be downloaded from here. The data set is about rental home prices and I will use the MindsDB framework to create a Machine Learning model to predict house prices in 5 lines of code. The dataset contains data about the number of rooms, bathrooms, size in square feet, location, market days, neighbourhood, and the price of the rent. I will simply start with importing the MindsDB library. So now let’s get started with this task to create a Machine Learning model in just 5 lines of code: import mindsdb predictor = mindsdb.Predictor(name='home_rental_predictor') view raw MindsDB.py hosted with ❤ by GitHub View this gist on GitHub So we have just done the initialization of the task, now we need to train our machine learning model, which surprisingly takes a one-line command: import mindsdb predictor = mindsdb.Predictor(name='home_rental_predictor') predictor.learn(from_data='home_rentals.csv', to_predict='rental_price') view raw MindsDB.py hosted with ❤ by GitHub View this gist on GitHub That’s it, we just created the model. The best part of the MindsDB library is that it automatically performs all the preprocessing own its own. Now we need to use the trained model to make predictions, let’s see how to do that: import mindsdb predictor = mindsdb.Predictor(name='home_rental_predictor') predictor.learn(from_data='home_rentals.csv', to_predict='rental_price') result = mdb_predict.predict(when={'number_of_rooms': 2,'number_of_bathrooms':1, 'sqft': 1190}) # you can now print the results print('The predicted price is ${price} with {conf} confidence'.format(price=result[0]['rental_price'], conf=result[0]['rental_price_confidence'])) view raw MindsDB.py hosted with ❤ by GitHub View this gist on GitHub The predicted price is $1161.1368408203125 with 0.02491046864422204 confidence So this is how we can create a Machine Learning model in just 5 lines of code by using the MindsDB library. The best thing about MindsDB is that it is not only for the numerical data, it can also be used for images. I hope you liked this article on how to create a Machine earning model in just 5 lines of code by using the MindsDB library in Python. Feel free to ask your valuable questions in the comments section below.;Machine Learning in 5 Lines with MindsDB
2020-11-08 12:10:19;Have you ever looked through your vacation photos and wondered: what is the name of this temple I visited in India? Who created this monument that I saw in California? Landmark Detection can help us detect the names of these places. But how does the detection of landmarks work? In this article, I will introduce you to a machine learning project on landmark detection with Python.;https://thecleverprogrammer.com/2020/11/08/landmark-detection-with-machine-learning/;['keras', 'sklearn'];1.0;[];['ML', 'Classification', 'NN', 'VGG'];['detect', 'epoch', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'training data', 'train', 'label', 'vgg'];"Have you ever looked through your vacation photos and wondered: what is the name of this temple I visited in India? Who created this monument that I saw in California? Landmark Detection can help us detect the names of these places. But how does the detection of landmarks work? In this article, I will introduce you to a machine learning project on landmark detection with Python. What is Landmark Detection? Landmark Detection is a task of detecting popular man-made sculptures, structures, and monuments within an image. We already have a very famous application for such tasks which is popularly known as the Google Landmark Detection, which is used by Google Maps. Also, Read – Machine Learning Full Course for free. At the end of this article, you will learn how google landmark detection works as I will take you through a machine learning project which is based on the functionality of Google Landmark Detection. I will use the Python programming language for building neural networks to detect landmarks within images. Now let’s get started with the task of detecting landmarks within an image. The most challenging task in this project is to find a dataset that includes some images that we can use to train our neural network. Hopefully, after a lot of research, I came across a dataset provided by Google in the Kaggle competitions. You can download the dataset, that I will use to detect landmarks using Machine Learning from here. Google Landmark Detection with Machine Learning Now to get started with this task, I will import all the necessary python libraries that we need to create a Machine Learning model for the task of landmark detection: import numpy as np import pandas as pd import keras import cv2 from matplotlib import pyplot as plt import os import random from PIL import Image view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub So after importing the above libraries the next step in this task is to import the dataset that I will use for detecting landmarks withing images: samples = 20000 df = pd.read_csv(""train.csv"") df = df.loc[:samples,:] num_classes = len(df[""landmark_id""].unique()) num_data = len(df) view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub Now let’s have a look at the size of the training data and the number of unique classes in the training data: print(""Size of training data:"", df.shape) print(""Number of unique classes:"", num_classes) view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub Size of training data: (20001, 2)Number of unique classes: 1020 There are 20,001 training samples, belonging to 1,020 classes, which gives us an average of 19.6 images per class, however, this distribution might not be the case, so let’s look at the distribution of samples by class: data = pd.DataFrame(df['landmark_id'].value_counts()) #index the data frame data.reset_index(inplace=True) data.columns=['landmark_id','count'] print(data.head(10)) print(data.tail(10)) view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub landmark_id count0 1924 9441 27 5042 454 2543 1346 2444 1127 2015 870 1936 2185 1777 1101 1628 389 1409 219 139 landmark_id count1010 499 21011 1942 21012 875 21013 2297 21014 611 21015 1449 21016 1838 21017 604 21018 374 21019 991 2 As we can see, the 10 most frequent landmarks range from 139 data points to 944 data points while the last 10 all have 2 data points. print(data['count'].describe())#statistical data for the distribution plt.hist(data['count'],100,range = (0,944),label = 'test')#Histogram of the distribution plt.xlabel(""Amount of images"") plt.ylabel(""Occurences"") view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub count 1020.000000mean 19.608824std 41.653684min 2.00000025% 5.00000050% 9.00000075% 21.000000max 944.000000Name: count, dtype: float64Text(0, 0.5, 'Occurences') As we can see in the histogram above, the vast majority of classes are not associated with so many images. print(""Amount of classes with five and less datapoints:"", (data['count'].between(0,5)).sum()) print(""Amount of classes with with between five and 10 datapoints:"", (data['count'].between(5,10)).sum()) n = plt.hist(df[""landmark_id""],bins=df[""landmark_id""].unique()) freq_info = n[0] plt.xlim(0,data['landmark_id'].max()) plt.ylim(0,data['count'].max()) plt.xlabel('Landmark ID') plt.ylabel('Number of images') view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub Amount of classes with five and less datapoints: 322Amount of classes with with between five and 10 datapoints: 342Text(0, 0.5, 'Number of images') The graph above shows that over 50% of the 1020 classes have less than 10 images, which can be difficult when training a classifier. There are some “outliers” in terms of the number of images they have, which means we might be biased towards those, as there might have a higher chance of getting a correct “guess” with the highest amount in these classes. Training Model: Now, I will train the Machine Learning model for the task of landmark detection using the Python programming language which will work the same as the Google landmark detection model. from sklearn.preprocessing import LabelEncoder lencoder = LabelEncoder() lencoder.fit(df[""landmark_id""]) def encode_label(lbl): return lencoder.transform(lbl) def decode_label(lbl): return lencoder.inverse_transform(lbl) def get_image_from_number(num): fname, label = df.loc[num,:] fname = fname + "".jpg"" f1 = fname[0] f2 = fname[1] f3 = fname[2] path = os.path.join(f1,f2,f3,fname) im = cv2.imread(os.path.join(base_path,path)) return im, label print(""4 sample images from random classes:"") fig=plt.figure(figsize=(16, 16)) for i in range(1,5): a = random.choices(os.listdir(base_path), k=3) folder = base_path+'/'+a[0]+'/'+a[1]+'/'+a[2] random_img = random.choice(os.listdir(folder)) img = np.array(Image.open(folder+'/'+random_img)) fig.add_subplot(1, 4, i) plt.imshow(img) plt.axis('off') plt.show() view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub from keras.applications import VGG19 from keras.layers import * from keras import Sequential ### Parameters # learning_rate = 0.0001 # decay_speed = 1e-6 # momentum = 0.09 # loss_function = ""sparse_categorical_crossentropy"" source_model = VGG19(weights=None) #new_layer = Dense(num_classes, activation=activations.softmax, name='prediction') drop_layer = Dropout(0.5) drop_layer2 = Dropout(0.5) model = Sequential() for layer in source_model.layers[:-1]: # go through until last layer if layer == source_model.layers[-25]: model.add(BatchNormalization()) model.add(layer) # if layer == source_model.layers[-3]: # model.add(drop_layer) # model.add(drop_layer2) model.add(Dense(num_classes, activation=""softmax"")) model.summary() opt1 = keras.optimizers.RMSprop(learning_rate = 0.0001, momentum = 0.09) opt2 = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07) model.compile(optimizer=opt1, loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""]) #sgd = SGD(lr=learning_rate, decay=decay_speed, momentum=momentum, nesterov=True) # rms = keras.optimizers.RMSprop(lr=learning_rate, momentum=momentum) # model.compile(optimizer=rms, # loss=loss_function, # metrics=[""accuracy""]) # print(""Model compiled! \n"") view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub Model: ""sequential""_________________________________________________________________Layer (type) Output Shape Param # =================================================================batch_normalization (BatchNo (None, 224, 224, 3) 12 _________________________________________________________________block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________block3_conv4 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________block4_conv4 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________block5_conv4 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________flatten (Flatten) (None, 25088) 0 _________________________________________________________________fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________fc2 (Dense) (None, 4096) 16781312 _________________________________________________________________dense (Dense) (None, 1020) 4178940 =================================================================Total params: 143,749,192Trainable params: 143,749,186Non-trainable params: 6 ### Function used for processing the data, fitted into a data generator. def get_image_from_number(num, df): fname, label = df.iloc[num,:] fname = fname + "".jpg"" f1 = fname[0] f2 = fname[1] f3 = fname[2] path = os.path.join(f1,f2,f3,fname) im = cv2.imread(os.path.join(base_path,path)) return im, label def image_reshape(im, target_size): return cv2.resize(im, target_size) def get_batch(dataframe,start, batch_size): image_array = [] label_array = [] end_img = start+batch_size if end_img > len(dataframe): end_img = len(dataframe) for idx in range(start, end_img): n = idx im, label = get_image_from_number(n, dataframe) im = image_reshape(im, (224, 224)) / 255.0 image_array.append(im) label_array.append(label) label_array = encode_label(label_array) return np.array(image_array), np.array(label_array) batch_size = 16 epoch_shuffle = True weight_classes = True epochs = 15 # Split train data up into 80% and 20% validation train, validate = np.split(df.sample(frac=1), [int(.8*len(df))]) print(""Training on:"", len(train), ""samples"") print(""Validation on:"", len(validate), ""samples"") for e in range(epochs): print(""Epoch: "", str(e+1) + ""/"" + str(epochs)) if epoch_shuffle: train = train.sample(frac = 1) for it in range(int(np.ceil(len(train)/batch_size))): X_train, y_train = get_batch(train, it*batch_size, batch_size) model.train_on_batch(X_train, y_train) model.save(""Model.h5"") view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub Training on: 16000 samplesValidation on: 4001 samplesEpoch: 1/15Epoch: 2/15Epoch: 3/15Epoch: 4/15Epoch: 5/15Epoch: 6/15Epoch: 7/15Epoch: 8/15Epoch: 9/15Epoch: 10/15Epoch: 11/15Epoch: 12/15Epoch: 13/15Epoch: 14/15Epoch: 15/15 Now we have trained the model successfully. The next step is to test the model, let’s see how we can test our landmark detection model: ### Test on training set batch_size = 16 errors = 0 good_preds = [] bad_preds = [] for it in range(int(np.ceil(len(validate)/batch_size))): X_train, y_train = get_batch(validate, it*batch_size, batch_size) result = model.predict(X_train) cla = np.argmax(result, axis=1) for idx, res in enumerate(result): print(""Class:"", cla[idx], ""- Confidence:"", np.round(res[cla[idx]],2), ""- GT:"", y_train[idx]) if cla[idx] != y_train[idx]: errors = errors + 1 bad_preds.append([batch_size*it + idx, cla[idx], res[cla[idx]]]) else: good_preds.append([batch_size*it + idx, cla[idx], res[cla[idx]]]) print(""Errors: "", errors, ""Acc:"", np.round(100*(len(validate)-errors)/len(validate),2)) #Good predictions good_preds = np.array(good_preds) good_preds = np.array(sorted(good_preds, key = lambda x: x[2], reverse=True)) fig=plt.figure(figsize=(16, 16)) for i in range(1,6): n = int(good_preds[i,0]) img, lbl = get_image_from_number(n, validate) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) fig.add_subplot(1, 5, i) plt.imshow(img) lbl2 = np.array(int(good_preds[i,1])).reshape(1,1) sample_cnt = list(df.landmark_id).count(lbl) plt.title(""Label: "" + str(lbl) + ""\nClassified as: "" + str(decode_label(lbl2)) + ""\nSamples in class "" + str(lbl) + "": "" + str(sample_cnt)) plt.axis('off') plt.show() view raw landmark_detection.py hosted with ❤ by GitHub View this gist on GitHub As you can see in the above images in the output, they are being classified according to their labels and classes. I hope you liked this article on Machine Learning project on Google landmark detection with Python programming language. Feel free to ask your valuable questions in the comments section below.";Landmark Detection with Machine Learning
2020-11-10 13:42:07;In this article, I will take you through how to train a model for the task of heart disease prediction using Machine Learning. I will use the Logistic Regression algorithm in machine learning to train a model to predict heart disease.;https://thecleverprogrammer.com/2020/11/10/heart-disease-prediction-using-machine-learning/;['sklearn'];1.0;[];['ML', 'Logistic Regression', 'Classification', 'Regression'];['regression', 'predict', 'fit', 'model', 'machine learning', 'logistic regression', 'classif', 'train', 'label'];"In this article, I will take you through how to train a model for the task of heart disease prediction using Machine Learning. I will use the Logistic Regression algorithm in machine learning to train a model to predict heart disease. Introduction to Heart Disease Prediction Predicting and diagnosing heart disease is the biggest challenge in the medical industry and relies on factors such as the physical examination, symptoms and signs of the patient. Also, Read – Machine Learning Full Course for free. Factors that influence heart disease are body cholesterol levels, smoking habit and obesity, family history of illnesses, blood pressure, and work environment. Machine learning algorithms play an essential and precise role in the prediction of heart disease. Advances in technology allow machine language to combine with Big Data tools to manage unstructured and exponentially growing data. Heart disease is seen as the world’s deadliest disease of human life. In particular, in this type of disease, the heart is not able to push the required amount of blood to the remaining organs of the human body to perform regular functions. Heart disease can be predicted based on various symptoms such as age, gender, heart rate, etc. and reduces the death rate of heart patients. Due to the increasing use of technology and data collection, we can now predict heart disease using machine learning algorithms. Now let’s go further with the task of heart disease prediction using machine learning with Python. Heart Disease Prediction Using Machine Learning Now in this section, I will take you through the task of Heart Disease Prediction using machine learning by using the Logistic regression algorithm. As I am going to use the Python programming language for this task of heart disease prediction so let’s start by importing some necessary libraries: import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np %matplotlib inline sns.set_style(""whitegrid"") plt.style.use(""fivethirtyeight"") view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub The dataset that I am using here can be easily downloaded from here. Now let’s import the data and move further: df = pd.read_csv(""heart.csv"") df.head() view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Exploratory Data Analysis Before training the logistic regression we need to observe and analyse the data to see what we are going to work with. The goal here is to learn more about the data and become a topic export on the dataset you are working with. EDA helps us find answers to some important questions such as: What question (s) are you trying to solve? What kind of data do we have and how do we handle the different types? What is missing in the data and how do you deal with it? Where are the outliers and why should you care? How can you add, change, or remove features to get the most out of your data? Now let’s start with exploratory data analysis: pd.set_option(""display.float"", ""{:.2f}"".format) df.describe() view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub df.target.value_counts().plot(kind=""bar"", color=[""salmon"", ""lightblue""]) view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub We have 165 people with heart disease and 138 people without heart disease, so our problem is balanced. # Checking for messing values df.isna().sum() view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub age 0 sex 0 cp 0 trestbps 0 chol 0 fbs 0 restecg 0 thalach 0 exang 0 oldpeak 0 slope 0 ca 0 thal 0 target 0 dtype: int64 This dataset looks perfect to use as we don’t have null values. categorical_val = [] continous_val = [] for column in df.columns: print('==============================') print(f""{column} : {df[column].unique()}"") if len(df[column].unique()) <= 10: categorical_val.append(column) else: continous_val.append(column) view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub ============================== age : [63 37 41 56 57 44 52 54 48 49 64 58 50 66 43 69 59 42 61 40 71 51 65 53 46 45 39 47 62 34 35 29 55 60 67 68 74 76 70 38 77] ============================== sex : [1 0] ============================== cp : [3 2 1 0] ============================== trestbps : [145 130 120 140 172 150 110 135 160 105 125 142 155 104 138 128 108 134 122 115 118 100 124 94 112 102 152 101 132 148 178 129 180 136 126 106 156 170 146 117 200 165 174 192 144 123 154 114 164] ============================== chol : [233 250 204 236 354 192 294 263 199 168 239 275 266 211 283 219 340 226 247 234 243 302 212 175 417 197 198 177 273 213 304 232 269 360 308 245 208 264 321 325 235 257 216 256 231 141 252 201 222 260 182 303 265 309 186 203 183 220 209 258 227 261 221 205 240 318 298 564 277 214 248 255 207 223 288 160 394 315 246 244 270 195 196 254 126 313 262 215 193 271 268 267 210 295 306 178 242 180 228 149 278 253 342 157 286 229 284 224 206 167 230 335 276 353 225 330 290 172 305 188 282 185 326 274 164 307 249 341 407 217 174 281 289 322 299 300 293 184 409 259 200 327 237 218 319 166 311 169 187 176 241 131] ============================== fbs : [1 0] ============================== restecg : [0 1 2] ============================== thalach : [150 187 172 178 163 148 153 173 162 174 160 139 171 144 158 114 151 161 179 137 157 123 152 168 140 188 125 170 165 142 180 143 182 156 115 149 146 175 186 185 159 130 190 132 147 154 202 166 164 184 122 169 138 111 145 194 131 133 155 167 192 121 96 126 105 181 116 108 129 120 112 128 109 113 99 177 141 136 97 127 103 124 88 195 106 95 117 71 118 134 90] ============================== exang : [0 1] ============================== oldpeak : [2.3 3.5 1.4 0.8 0.6 0.4 1.3 0. 0.5 1.6 1.2 0.2 1.8 1. 2.6 1.5 3. 2.4 0.1 1.9 4.2 1.1 2. 0.7 0.3 0.9 3.6 3.1 3.2 2.5 2.2 2.8 3.4 6.2 4. 5.6 2.9 2.1 3.8 4.4] ============================== slope : [0 2 1] ============================== ca : [0 2 1 3 4] ============================== thal : [1 2 3 0] ============================== target : [1 0] plt.figure(figsize=(15, 15)) for i, column in enumerate(categorical_val, 1): plt.subplot(3, 3, i) df[df[""target""] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6) df[df[""target""] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6) plt.legend() plt.xlabel(column) view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Observations from the above plot: cp {Chest pain}: People with cp 1, 2, 3 are more likely to have heart disease than people with cp 0.restecg {resting EKG results}: People with a value of 1 (reporting an abnormal heart rhythm, which can range from mild symptoms to severe problems) are more likely to have heart disease.exang {exercise-induced angina}: people with a value of 0 (No ==> angina induced by exercise) have more heart disease than people with a value of 1 (Yes ==> angina induced by exercise)slope {the slope of the ST segment of peak exercise}: People with a slope value of 2 (Downslopins: signs of an unhealthy heart) are more likely to have heart disease than people with a slope value of 2 slope is 0 (Upsloping: best heart rate with exercise) or 1 (Flatsloping: minimal change (typical healthy heart)).ca {number of major vessels (0-3) stained by fluoroscopy}: the more blood movement the better, so people with ca equal to 0 are more likely to have heart disease.thal {thalium stress result}: People with a thal value of 2 (defect corrected: once was a defect but ok now) are more likely to have heart disease. plt.figure(figsize=(15, 15)) for i, column in enumerate(continous_val, 1): plt.subplot(3, 2, i) df[df[""target""] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6) df[df[""target""] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6) plt.legend() plt.xlabel(column) view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Observations from the above plot: trestbps: resting blood pressure anything above 130-140 is generally of concernchol: greater than 200 is of concern.thalach: People with a maximum of over 140 are more likely to have heart disease.the old peak of exercise-induced ST depression vs. rest looks at heart stress during exercise an unhealthy heart will stress more. # Create another figure plt.figure(figsize=(10, 8)) # Scatter with postivie examples plt.scatter(df.age[df.target==1], df.thalach[df.target==1], c=""salmon"") # Scatter with negative examples plt.scatter(df.age[df.target==0], df.thalach[df.target==0], c=""lightblue"") # Add some helpful info plt.title(""Heart Disease in function of Age and Max Heart Rate"") plt.xlabel(""Age"") plt.ylabel(""Max Heart Rate"") plt.legend([""Disease"", ""No Disease""]); view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Correlation Matrix # Let's make our correlation matrix a little prettier corr_matrix = df.corr() fig, ax = plt.subplots(figsize=(15, 15)) ax = sns.heatmap(corr_matrix, annot=True, linewidths=0.5, fmt="".2f"", cmap=""YlGnBu""); bottom, top = ax.get_ylim() ax.set_ylim(bottom + 0.5, top - 0.5) view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub df.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(12, 8), title=""Correlation with target"") view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Observations from correlation: fbs and chol are the least correlated with the target variable.All other variables have a significant correlation with the target variable. Data Processing After exploring the dataset, we can observe that we need to convert some categorical variables to dummy variables and scale all values before training the machine learning models. So, for this task, I’ll use the get_dummies method to create dummy columns for categorical variables: categorical_val.remove('target') dataset = pd.get_dummies(df, columns = categorical_val) from sklearn.preprocessing import StandardScaler s_sc = StandardScaler() col_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak'] dataset[col_to_scale] = s_sc.fit_transform(dataset[col_to_scale]) view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Applying Logistic Regression Now, I will train a machine learning model for the task of heart disease prediction. I will use the logistic regression algorithm as I mentioned at the beginning of the article. But before training the model I will first define a helper function for printing the classification report of the performance of the machine learning model: from sklearn.metrics import accuracy_score, confusion_matrix, classification_report def print_score(clf, X_train, y_train, X_test, y_test, train=True): if train: pred = clf.predict(X_train) clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True)) print(""Train Result:\n================================================"") print(f""Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%"") print(""_______________________________________________"") print(f""CLASSIFICATION REPORT:\n{clf_report}"") print(""_______________________________________________"") print(f""Confusion Matrix: \n {confusion_matrix(y_train, pred)}\n"") elif train==False: pred = clf.predict(X_test) clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True)) print(""Test Result:\n================================================"") print(f""Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%"") print(""_______________________________________________"") print(f""CLASSIFICATION REPORT:\n{clf_report}"") print(""_______________________________________________"") print(f""Confusion Matrix: \n {confusion_matrix(y_test, pred)}\n"") view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Now let’s split the data into training and test sets. I will split the data into 70% training and 30% testing: from sklearn.model_selection import train_test_split X = dataset.drop('target', axis=1) y = dataset.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Now let’s train the machine learning model and print the classification report of our logistic regression model: from sklearn.linear_model import LogisticRegression lr_clf = LogisticRegression(solver='liblinear') lr_clf.fit(X_train, y_train) print_score(lr_clf, X_train, y_train, X_test, y_test, train=True) print_score(lr_clf, X_train, y_train, X_test, y_test, train=False) view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Train Result: ================================================ Accuracy Score: 86.79% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.88 0.86 0.87 0.87 0.87 recall 0.82 0.90 0.87 0.86 0.87 f1-score 0.85 0.88 0.87 0.87 0.87 support 97.00 115.00 0.87 212.00 212.00 _______________________________________________ Confusion Matrix: [[ 80 17] [ 11 104]] Test Result: ================================================ Accuracy Score: 86.81% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.87 0.87 0.87 0.87 0.87 recall 0.83 0.90 0.87 0.86 0.87 f1-score 0.85 0.88 0.87 0.87 0.87 support 41.00 50.00 0.87 91.00 91.00 _______________________________________________ Confusion Matrix: [[34 7] [ 5 45]] test_score = accuracy_score(y_test, lr_clf.predict(X_test)) * 100 train_score = accuracy_score(y_train, lr_clf.predict(X_train)) * 100 results_df = pd.DataFrame(data=[[""Logistic Regression"", train_score, test_score]], columns=['Model', 'Training Accuracy %', 'Testing Accuracy %']) results_df view raw heart disease.py hosted with ❤ by GitHub View this gist on GitHub Model Training Accuracy % Testing Accuracy % Logistic Regression 86.79 86.81 As you can see the model performs very well of the test set as it is giving almost the same accuracy in the test set as in the training set. So I hope you liked this article on how to train a machine learning model for the task of heart disease prediction using machine learning. Feel free to ask your valuable questions in the comments section below.";Heart Disease Prediction using Machine Learning
2020-11-12 21:25:09;In this article, I will take you through how to create a model for the task of Earthquake Prediction using Machine Learning and the Python programming language. Predicting earthquakes is one of the great unsolved problems in the earth sciences.With the increase in the use of technology, many seismic monitoring stations have increased, so we can use machine learning and other data-driven methods to predict earthquakes.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/11/12/earthquake-prediction-model-with-machine-learning/;['keras', 'sklearn'];1.0;['CV'];['CV', 'NN', 'ML', 'ReLu', 'Classification'];['epoch', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'relu', 'activation function', 'training data', 'train', 'test data', 'neuron'];"In this article, I will take you through how to create a model for the task of Earthquake Prediction using Machine Learning and the Python programming language. Predicting earthquakes is one of the great unsolved problems in the earth sciences. With the increase in the use of technology, many seismic monitoring stations have increased, so we can use machine learning and other data-driven methods to predict earthquakes. Also, Read – Machine Learning Full Course for free. Earthquake Prediction Model with Machine Learning It is well known that if a disaster occurs in one region, it is likely to happen again. Some regions have frequent earthquakes, but this is only a comparative amount compared to other regions. So, predicting the earthquake with date and time, latitude and longitude from previous data is not a trend that follows like other things, it happens naturally. I will start this task to create a model for earthquake prediction by importing the necessary python libraries: import numpy as np import pandas as pd import matplotlib.pyplot as plt view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub Now let’s load and read the dataset. The dataset that I am using here can be easily downloaded here: data = pd.read_csv(""database.csv"") data.columns view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub Index(['Date', 'Time', 'Latitude', 'Longitude', 'Type', 'Depth', 'Depth Error', 'Depth Seismic Stations', 'Magnitude', 'Magnitude Type', 'Magnitude Error', 'Magnitude Seismic Stations', 'Azimuthal Gap', 'Horizontal Distance', 'Horizontal Error', 'Root Mean Square', 'ID', 'Source', 'Location Source', 'Magnitude Source', 'Status'], dtype='object') Now let’s see the main characteristics of earthquake data and create an object of these characteristics, namely, date, time, latitude, longitude, depth, magnitude: data = data[['Date', 'Time', 'Latitude', 'Longitude', 'Depth', 'Magnitude']] data.head() view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub dateTimeLatitudeLongitudeDepthMagnitude001/02/196513:44:1819.246145.616131.66.0101/04/196511:29:491.863127.35280.05.8201/05/196518:05:58-20.579-173.97220.06.2301/08/196518:49:43-59.076-23.55715.05.8401/09/196513:32:5011.938126.42715.05.8 Since the data is random, so we need to scale it based on the model inputs. In this, we convert the given date and time to Unix time which is in seconds and a number. This can be easily used as an entry for the network we have built: import datetime import time timestamp = [] for d, t in zip(data['Date'], data['Time']): try: ts = datetime.datetime.strptime(d+' '+t, '%m/%d/%Y %H:%M:%S') timestamp.append(time.mktime(ts.timetuple())) except ValueError: # print('ValueError') timestamp.append('ValueError') timeStamp = pd.Series(timestamp) data['Timestamp'] = timeStamp.values final_data = data.drop(['Date', 'Time'], axis=1) final_data = final_data[final_data.Timestamp != 'ValueError'] final_data.head() view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub LatitudeLongitudeDepthMagnitudeTimestamp019.246145.616131.66.0-1.57631e+0811.863127.35280.05.8-1.57466e+082-20.579-173.97220.06.2-1.57356e+083-59.076-23.55715.05.8-1.57094e+08411.938126.42715.05.8-1.57026e+08 Data Visualization Now, before we create the earthquake prediction model, let’s visualize the data on a world map that shows a clear representation of where the earthquake frequency will be more: from mpl_toolkits.basemap import Basemap m = Basemap(projection='mill',llcrnrlat=-80,urcrnrlat=80, llcrnrlon=-180,urcrnrlon=180,lat_ts=20,resolution='c') longitudes = data[""Longitude""].tolist() latitudes = data[""Latitude""].tolist() #m = Basemap(width=12000000,height=9000000,projection='lcc', #resolution=None,lat_1=80.,lat_2=55,lat_0=80,lon_0=-107.) x,y = m(longitudes,latitudes) fig = plt.figure(figsize=(12,10)) plt.title(""All affected areas"") m.plot(x, y, ""o"", markersize = 2, color = 'blue') m.drawcoastlines() m.fillcontinents(color='coral',lake_color='aqua') m.drawmapboundary() m.drawcountries() plt.show() view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub Splitting the Dataset Now, to create the earthquake prediction model, we need to divide the data into Xs and ys which respectively will be entered into the model as inputs to receive the output from the model. Here the inputs are TImestamp, Latitude and Longitude and the outputs are Magnitude and Depth. I’m going to split the xs and ys into train and test with validation. The training set contains 80% and the test set contains 20%: X = final_data[['Timestamp', 'Latitude', 'Longitude']] y = final_data[['Magnitude', 'Depth']] from sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) print(X_train.shape, X_test.shape, y_train.shape, X_test.shape) view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub (18727, 3) (4682, 3) (18727, 2) (4682, 3) Neural Network for Earthquake Prediction Now I will create a neural network to fit the data from the training set. Our neural network will consist of three dense layers each with 16, 16, 2 nodes and reread. Relu and softmax will be used as activation functions: from keras.models import Sequential from keras.layers import Dense def create_model(neurons, activation, optimizer, loss): model = Sequential() model.add(Dense(neurons, activation=activation, input_shape=(3,))) model.add(Dense(neurons, activation=activation)) model.add(Dense(2, activation='softmax')) model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy']) return model view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub Now I’m going to define the hyperparameters with two or more options to find the best fit: from keras.wrappers.scikit_learn import KerasClassifier model = KerasClassifier(build_fn=create_model, verbose=0) # neurons = [16, 64, 128, 256] neurons = [16] # batch_size = [10, 20, 50, 100] batch_size = [10] epochs = [10] # activation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear', 'exponential'] activation = ['sigmoid', 'relu'] # optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'] optimizer = ['SGD', 'Adadelta'] loss = ['squared_hinge'] param_grid = dict(neurons=neurons, batch_size=batch_size, epochs=epochs, activation=activation, optimizer=optimizer, loss=loss) view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub Now we need to find the best fit of the above model and get the mean test score and standard deviation of the best fit model: grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1) grid_result = grid.fit(X_train, y_train) print(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_['mean_test_score'] stds = grid_result.cv_results_['std_test_score'] params = grid_result.cv_results_['params'] for mean, stdev, param in zip(means, stds, params): print(""%f (%f) with: %r"" % (mean, stdev, param)) view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub Best: 0.957655 using {'activation': 'relu', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'SGD'} 0.333316 (0.471398) with: {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'SGD'} 0.000000 (0.000000) with: {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'Adadelta'} 0.957655 (0.029957) with: {'activation': 'relu', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'SGD'} 0.645111 (0.456960) with: {'activation': 'relu', 'batch_size': 10, 'epochs': 10, 'loss': 'squared_hinge', 'neurons': 16, 'optimizer': 'Adadelta'} In the step below, the best-fit parameters are used for the same model to calculate the score with the training data and the test data: model = Sequential() model.add(Dense(16, activation='relu', input_shape=(3,))) model.add(Dense(16, activation='relu')) model.add(Dense(2, activation='softmax')) model.compile(optimizer='SGD', loss='squared_hinge', metrics=['accuracy']) model.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data=(X_test, y_test)) [test_loss, test_acc] = model.evaluate(X_test, y_test) print(""Evaluation result on Test Data : Loss = {}, accuracy = {}"".format(test_loss, test_acc)) view raw earthquake.py hosted with ❤ by GitHub View this gist on GitHub Evaluation result on Test Data : Loss = 0.5038455790406056, accuracy = 0.9241777017858995 So we can see in the above output that our neural network model for earthquake prediction performs well. I hope you liked this article on how to create an earthquake prediction model with machine learning and the Python programming language. Feel free to ask your valuable questions in the comments section below.";Earthquake Prediction Model with Machine Learning
2020-11-12 00:04:30;In this article, I will walk you through the task of outlier detection in machine learning. An outlier is a terminology commonly used by analysts and data scientists because it requires special attention, otherwise, it can lead to totally wrong estimates. Simply put, outlier detection is an observation that appears far away from and diverges from an overall pattern in a sample.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/11/12/outlier-detection-with-python/;['pattern'];1.0;[];['ML', 'GRU'];['detect', 'model', 'gru', 'machine learning'];"In this article, I will walk you through the task of outlier detection in machine learning. An outlier is a terminology commonly used by analysts and data scientists because it requires special attention, otherwise, it can lead to totally wrong estimates. Simply put, outlier detection is an observation that appears far away from and diverges from an overall pattern in a sample. Also, Read – Machine Learning Full Course for free. What is Outlier? An outlier is an observation that is numerically distant from the rest of the data or, in a nutshell, is the value that is out of range. Let’s take an example to check what happens to a dataset with a dataset without outliers. Data without OutliersData with OutliersData1, 2, 3, 3, 4, 5, 41, 2, 3, 3, 4, 5, 400Mean3.14259.714Median33Standard Deviation1.345185150.057 As you can see, the dataset with outliers has a significantly different mean and standard deviation. In the first scenario, we will say that the average is 3.14. But with the outlier, the average climbs to 59.71. This would completely change the estimate. Let’s take a concrete example of an outlier. In a company of 50 employees, 45 people with a monthly salary of Rs. 6000, 5 seniors with a monthly salary of Rs. 100000 each. If you calculate the average monthly salary of the employees of the company is 14,500 rupees, which will give you a bad conclusion. But if you take the median salary, it is Rs.6000 which is more sensitive than the average. For this reason, the median is an appropriate measure for the mean. Here you can see the effect of an outlier. Now let’s have a quick look at the main causes of outliers before getting started with the task of outlier detection: Data Entry Errors: Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.Measurement Errors: It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty.Natural Outliers: When an outlier is not artificial (due to error), it is a natural outlier. Most real-world data belong to this category. Outlier Detection in Machine Learning using Hypothesis Testing Now, I will use the Python programming language for the task of outlier detection in machine learning. An outlier can be of two types: Univariate and Multivariate. Above, we have discussed the example of a univariate outlier. These outliers can be found when we look at the distribution of a single variable. Multivariate outliers are outliers in an n-dimensional space. An outlier can be of two types: univariate and multivariate. Above we have discussed the example of a univariate outlier. These outliers can be found when we look at the distribution of a single variable. Multivariate outliers are outliers in an n-dimensional space. Hypothesis testing is a common technique for detecting outliers in machine learning. Hypothesis testing is a method of testing a claim or hypothesis about a parameter in a population, using data measured in a sample. In this method, we test a hypothesis by determining the probability that a sample statistic could have been selected, if the hypothesis regarding the population parameter was true. The purpose of the hypothesis test is to determine the probability that a population parameter, such as the mean, is likely to be true. There are four steps in the hypothesis test: State the assumptions.Define the criteria for a decision.Calculate the test statistic.Make a decision. Now let’s see how to use the Python programming language to implement the hypothesis testing for the task of Outlier Detection in Machine Learning: import numpy as np import scipy.stats as stats x = np.array([12,13,14,19,21,23]) y = np.array([12,13,14,19,21,23,45]) def grubbs_test(x): n = len(x) mean_x = np.mean(x) sd_x = np.std(x) numerator = max(abs(x-mean_x)) g_calculated = numerator/sd_x print(""Grubbs Calculated Value:"",g_calculated) t_value = stats.t.ppf(1 - 0.05 / (2 * n), n - 2) g_critical = ((n - 1) * np.sqrt(np.square(t_value))) / (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value))) print(""Grubbs Critical Value:"",g_critical) if g_critical > g_calculated: print(""From grubbs_test we observe that calculated value is lesser than critical value, Accept null hypothesis and conclude that there is no outliers\n"") else: print(""From grubbs_test we observe that calculated value is greater than critical value, Reject null hypothesis and conclude that there is an outliers\n"") grubbs_test(x) grubbs_test(y) view raw outlier.py hosted with ❤ by GitHub View this gist on GitHub Grubbs Calculated Value: 1.4274928542926593 Grubbs Critical Value: 1.887145117792422 From grubbs_test we observe that calculated value is lesser than critical value, Accept null hypothesis and conclude that there is no outliers Grubbs Calculated Value: 2.2765147221587774 Grubbs Critical Value: 2.019968507680656 From grubbs_test we observe that calculated value is greater than critical value, Reject null hypothesis and conclude that there is an outliers One of the major problems with machine learning is an outlier. If you will neglect the outliers in the data, then it will result in the poor performance of your machine learning model. I hope you liked this article on the task of outlier detection in Machine Learning using hypothesis testing and the Python programming language.";Outlier Detection with Python
2020-11-14 16:36:47;Over the past decades, machine learning techniques have been widely used in intelligent health systems, particularly for breast cancer diagnosis and prognosis. In this article, I will walk you through how to create a breast cancer detection model using machine learning and the Python programming language.Breast cancer is one of the most common cancers in women globally, accounting for the majority of new cancer cases and cancer-related deaths according to global statistics, making it a major public health problem in the world. today’s society.Also, Read – Machine Learning Full Course for free.Early diagnosis of breast cancer can dramatically improve prognosis and chances of survival, as it can promote timely clinical treatment of patients. More precise classification of benign tumours can prevent patients from undergoing unnecessary treatments.;https://thecleverprogrammer.com/2020/11/14/breast-cancer-detection-with-machine-learning/;['sklearn'];1.0;[];['ML', 'Naive Bayes', 'Classification', 'Regression'];['detect', 'regression', 'predict', 'fit', 'model', 'machine learning', 'classif', 'naive bayes', 'train', 'label'];"Over the past decades, machine learning techniques have been widely used in intelligent health systems, particularly for breast cancer diagnosis and prognosis. In this article, I will walk you through how to create a breast cancer detection model using machine learning and the Python programming language. Breast cancer is one of the most common cancers in women globally, accounting for the majority of new cancer cases and cancer-related deaths according to global statistics, making it a major public health problem in the world. today’s society. Also, Read – Machine Learning Full Course for free. Early diagnosis of breast cancer can dramatically improve prognosis and chances of survival, as it can promote timely clinical treatment of patients. More precise classification of benign tumours can prevent patients from undergoing unnecessary treatments. Breast Cancer Detection using Machine Learning In this section, I will implement a Naive Bayes algorithm in Machine Learning using Python. For this task, I will use a database of breast cancer tumour information for breast cancer detection. Let’s start by importing and loading the necessary python libraries and the breast cancer dataset provided by Scikit-learn: from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import LinearRegression from sklearn.metrics import accuracy_score view raw breast cancer.py hosted with ❤ by GitHub View this gist on GitHub Now we need to create new variables for each important set of information that we find useful and assign the attributes in the dataset to those variables: data = load_breast_cancer() label_names = data[""target_names""] labels = data[""target""] feature_names = data[""feature_names""] features = data[""data""] view raw breast cancer.py hosted with ❤ by GitHub View this gist on GitHub We now have values for each set of useful information in the dataset. To better understand our dataset, let’s take a look at our data by printing our class labels, the label for the first data instance, our entity names, and the entity values for the first data instance: print(label_names) print(""Class label :"", labels[0]) print(feature_names) print(features[0], ""\n"") view raw breast cancer.py hosted with ❤ by GitHub View this gist on GitHub ['malignant' 'benign'] Class label : 0 ['mean radius' 'mean texture' 'mean perimeter' 'mean area' 'mean smoothness' 'mean compactness' 'mean concavity' 'mean concave points' 'mean symmetry' 'mean fractal dimension' 'radius error' 'texture error' 'perimeter error' 'area error' 'smoothness error' 'compactness error' 'concavity error' 'concave points error' 'symmetry error' 'fractal dimension error' 'worst radius' 'worst texture' 'worst perimeter' 'worst area' 'worst smoothness' 'worst compactness' 'worst concavity' 'worst concave points' 'worst symmetry' 'worst fractal dimension'] [1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01 4.601e-01 1.189e-01] Now that our data is loaded, we can work with our data to build our machine learning model using the Naive Bayes algorithm for the breast cancer detection task. Splitting The Dataset To evaluate the performance of a classifier, you should always test the model on invisible data. Therefore, before I create a machine learning model for breast cancer detection, I will divide your data into two parts: an 80% training set and a 20% test set: train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42)​x train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42) Using Naive Bayes for Breast Cancer Detection There are many models of machine learning, and each model has its strengths and weaknesses. For the Breast Cancer Detection Model task, I will focus on a simple algorithm that generally works well in binary classification tasks, namely the Naive Bayes classifier: gnb = GaussianNB() gnb.fit(train, train_labels) gnb = GaussianNB()gnb.fit(train, train_labels) After training the model, we can then use the trained model to make predictions on our test set, which we use the predict() function. The predict() function returns an array of predictions for each data instance in the test set. We can then print out our predictions to get a feel for what the model determined: preds = gnb.predict(test) print(preds, ""\n"") preds = gnb.predict(test)print(preds, ""\n"") [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0] Using the array of true class labels, we can assess the accuracy of our model’s predictors by comparing the two arrays (test_labels vs preds). I’ll use the accuracy_score () function provided by Scikit-Learn to determine the accuracy rate of our machine learning classifier: print(accuracy_score(test_labels, preds)) print(accuracy_score(test_labels, preds)) 0.9736842105263158 As you can see from the output above, our breast cancer detection model gives an accuracy rate of almost 97%. This means that 97% of the time the classifier is able to make the correct prediction. So this is how we can build a Breast cancer detection model using Machine Learning and the Python programming language. I hope you liked this article on how to build a breast cancer detection model with Machine Learning. Feel free to ask your valuable questions in the comments section below.";Breast Cancer Detection with Machine Learning
2020-11-14 13:25:45;Predicting the stock market is one of the most important applications of Machine Learning in finance. In this article, I will take you through a simple Data Science project on Stock Price Prediction using Machine Learning Python.At the end of this article, you will learn how to predict stock prices by using the Linear Regression model by implementing the Python programming language.Also, Read – Machine Learning Full Course for free.;https://thecleverprogrammer.com/2020/11/14/stock-price-prediction-using-machine-learning/;['sklearn'];1.0;[];['ML', 'Linear Regression', 'Regression'];['regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'train', 'label'];"Predicting the stock market is one of the most important applications of Machine Learning in finance. In this article, I will take you through a simple Data Science project on Stock Price Prediction using Machine Learning Python. At the end of this article, you will learn how to predict stock prices by using the Linear Regression model by implementing the Python programming language. Also, Read – Machine Learning Full Course for free. Stock Price Prediction Predicting the stock market has been the bane and goal of investors since its inception. Every day billions of dollars are traded on the stock exchange, and behind every dollar is an investor hoping to make a profit in one way or another. Entire companies rise and fall daily depending on market behaviour. If an investor is able to accurately predict market movements, he offers a tantalizing promise of wealth and influence. Today, so many people are making money staying at home trading in the stock market. It is a plus point for you if you use your experience in the stock market and your machine learning skills for the task of stock price prediction. Let’s see how to predict stock prices using Machine Learning and the python programming language. I will start this task by importing all the necessary python libraries that we need for this task: import numpy as np import pandas as pd from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression view raw stock price prediction.py hosted with ❤ by GitHub View this gist on GitHub Data Preparation In the above section, I started the task of stock price prediction by importing the python libraries. Now I will write a function that will prepare the dataset so that we can fit it easily in the Linear Regression model: def prepare_data(df,forecast_col,forecast_out,test_size): label = df[forecast_col].shift(-forecast_out) #creating new column called label with the last 5 rows are nan X = np.array(df[[forecast_col]]) #creating the feature array X = preprocessing.scale(X) #processing the feature array X_lately = X[-forecast_out:] #creating the column i want to use later in the predicting method X = X[:-forecast_out] # X that will contain the training and testing label.dropna(inplace=True) #dropping na values y = np.array(label) # assigning Y X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=test_size, random_state=0) #cross validation response = [X_train,X_test , Y_train, Y_test , X_lately] return response view raw stock price prediction.py hosted with ❤ by GitHub View this gist on GitHub You can easily understand the above function as I have narrated the functioning of every line step by step. Now the next thing to do is reading the data: df = pd.read_csv(""prices.csv"") df = df[df.symbol == ""GOOG""]​x df = pd.read_csv(""prices.csv"")df = df[df.symbol == ""GOOG""] Now we need to prepare three input variables as already prepared in the function created in the above section. We need to declare an input variable mentioning about which column we want to predict. The next variable we need to declare is how much far we want to predict. And the last variable that we need to declare is how much should be the size of the test set. Now let’s declare all the variables: forecast_col = 'close' forecast_out = 5 test_size = 0.2 forecast_col = 'close'forecast_out = 5test_size = 0.2 Applying Machine Learning for Stock Price Prediction Now I will split the data and fit into the linear regression model: X_train, X_test, Y_train, Y_test , X_lately =prepare_data(df,forecast_col,forecast_out,test_size); #calling the method were the cross validation and data preperation is in learner = LinearRegression() #initializing linear regression model learner.fit(X_train,Y_train) #training the linear regression model X_train, X_test, Y_train, Y_test , X_lately =prepare_data(df,forecast_col,forecast_out,test_size); #calling the method were the cross validation and data preperation is inlearner = LinearRegression() #initializing linear regression model​learner.fit(X_train,Y_train) #training the linear regression model Now let’s predict the output and have a look at the prices of the stock prices: score=learner.score(X_test,Y_test)#testing the linear regression model forecast= learner.predict(X_lately) #set that will contain the forecasted data response={}#creting json object response['test_score']=score response['forecast_set']=forecast print(response) view raw stock price prediction.py hosted with ❤ by GitHub View this gist on GitHub {‘test_score’: 0.9481024935723803, ‘forecast_set’: array([786.54352516, 788.13020371, 781.84159626, 779.65508615, 769.04187979])} So this is how we can predict the stock prices with Machine Learning. I hope you liked this article on Stock Price prediction using Python with machine learning by implementing the Linear Regression Model. Feel free to ask your valuable questions in the comments section below.";Stock Price Prediction using Machine Learning
2020-11-16 12:52:42;In Machine Learning, the Apriori algorithm is used for data mining association rules. In this article, I will take you through Market Basket Analysis using the Apriori algorithm in Machine Learning by using the Python programming language.;https://thecleverprogrammer.com/2020/11/16/apriori-algorithm-using-python/;['pattern'];1.0;[];['ML'];['label', 'machine learning'];"In Machine Learning, the Apriori algorithm is used for data mining association rules. In this article, I will take you through Market Basket Analysis using the Apriori algorithm in Machine Learning by using the Python programming language. What is Association Mining? Association mining is typically performed on transaction data from a retail marketplace or online e-commerce store. Since most transaction data is large, the a priori algorithm makes it easy to find these patterns or rules quickly. Also, Read – 100+ Machine Learning Projects solved and explained. Association rules are used to analyze retail or transactional data and are intended to identify strong rules mainly found in transactional data using measures of interest, based on the concept of strong principals. How does the Apriori Algorithm Work? The Apriori algorithm is the most popular algorithm for mining association rules. It finds the most frequent combinations in a database and identifies the rules of association between elements, based on 3 important factors: Support: the probability that X and Y meetConfidence: the conditional probability that Y knows x. In other words, how often does Y occur when X came first.Lift: the relationship between support and confidence. An increase of 2 means that the probability of buying X and Y together is twice as high as the probability of simply buying Y. Apriori uses a “bottom-up” approach, in which frequent subsets are extended one item at a time (one step is called candidate generation) and groups of candidates are tested against the data. The algorithm ends when no other successful extension is found. Now, I will take you through the task of Market Basket analysis using the Apriori Algorithm using Python and Machine Learning. Market Basket Analysis with Apriori Algorithm using Python Market basket analysis, also known as association rule learning or affinity analysis, is a data mining technique that can be used in various fields, such as marketing, bioinformatics, the field of marketing. education, nuclear science, etc. The main goal of market basket analysis in marketing is to provide the retailer with the information necessary to understand the buyer’s purchasing behaviour, which can help the retailer make incorrect decisions. There are different algorithms for performing market basket analysis. Existing algorithms operate on static data and do not capture data changes over time. But the Apriori algorithm not only leverages static data but also provides a new way to account for changes that occur in the data. I will start this task of Market Basket Analysis with Apriori Algorithm by importing the necessay Python libraries: import numpy as np # linear algebra import pandas as pd # data processing import plotly.express as px import apyori from apyori import apriori view raw apriori.py hosted with ❤ by GitHub View this gist on GitHub Now let’s load the dataset. The dataset that I am using in this task can be downloaded from here: data = pd.read_csv(""Groceries_dataset.csv"") data.head()​x data = pd.read_csv(""Groceries_dataset.csv"")data.head() Member_numberDateitemDescription0180821-07-2015tropical fruit1255205-01-2015whole milk2230019-09-2015pip fruit3118712-12-2015other vegetables4303701-02-2015whole milk Data Exploration Let’s first have a look at the top 10 most selling products: print(""Top 10 frequently sold products(Tabular Representation)"") x = data['itemDescription'].value_counts().sort_values(ascending=False)[:10] fig = px.bar(x= x.index, y= x.values) fig.update_layout(title_text= ""Top 10 frequently sold products (Graphical Representation)"", xaxis_title= ""Products"", yaxis_title=""Count"") fig.show() view raw apriori.py hosted with ❤ by GitHub View this gist on GitHub Now let’s explore the higher sales: data[""Year""] = data['Date'].str.split(""-"").str[-1] data[""Month-Year""] = data['Date'].str.split(""-"").str[1] + ""-"" + data['Date'].str.split(""-"").str[-1] fig1 = px.bar(data[""Month-Year""].value_counts(ascending=False), orientation= ""v"", color = data[""Month-Year""].value_counts(ascending=False), labels={'value':'Count', 'index':'Date','color':'Meter'}) fig1.update_layout(title_text=""Exploring higher sales by the date"") fig1.show() view raw apriori.py hosted with ❤ by GitHub View this gist on GitHub Observations: From the above visualizations we can observe that: Milk is bought the most, followed by vegetables.Most shopping takes place in August / September, while February / March is the least demanding. Implementation of Apriori Algorithm uisng Python Now, I will implement the Apriori algorithm in machine learning by using the Python programming language for the taks of market basket analysis: rules = apriori(transactions, min_support = 0.00030, min_confidence = 0.05, min_lift = 3, max_length = 2, target = ""rules"") association_results = list(rules) print(association_results[0]) view raw apriori.py hosted with ❤ by GitHub View this gist on GitHub RelationRecord(items=frozenset({'liver loaf', 'fruit/vegetable juice'}), support=0.00040098910646260775, ordered_statistics=[OrderedStatistic(items_base=frozenset({'liver loaf'}), items_add=frozenset({'fruit/vegetable juice'}), confidence=0.12, lift=3.5276227897838903)]) for item in association_results: pair = item[0] items = [x for x in pair] print(""Rule : "", items[0], "" -> "" + items[1]) print(""Support : "", str(item[1])) print(""Confidence : "",str(item[2][0][2])) print(""Lift : "", str(item[2][0][3])) print(""============================="") view raw apriori.py hosted with ❤ by GitHub View this gist on GitHub Rule : liver loaf -> fruit/vegetable juice Support : 0.00040098910646260775 Confidence : 0.12 Lift : 3.5276227897838903 ============================= Rule : ham -> pickled vegetables Support : 0.0005346521419501437 Confidence : 0.05970149253731344 Lift : 3.4895055970149254 ============================= Rule : roll products -> meat Support : 0.0003341575887188398 Confidence : 0.06097560975609757 Lift : 3.620547812620984 ============================= Rule : misc. beverages -> salt Support : 0.0003341575887188398 Confidence : 0.05617977528089888 Lift : 3.5619405827461437 ============================= Rule : spread cheese -> misc. beverages Support : 0.0003341575887188398 Confidence : 0.05 Lift : 3.170127118644068 ============================= Rule : soups -> seasonal products Support : 0.0003341575887188398 Confidence : 0.10416666666666667 Lift : 14.704205974842768 ============================= Rule : spread cheese -> sugar Support : 0.00040098910646260775 Confidence : 0.06 Lift : 3.3878490566037733 ============================= I hope you liked this article on the Apriori algorithm in Machine Learning by using the Python programming language. Feel free to ask your valuable questions in the comments section below.";Apriori Algorithm using Python
2020-11-17 12:54:09;Face detection has become a very interesting problem in image processing and computer vision. In this article, I will introduce you to a computer vision project on Face Mask Detection with Machine Learning using Python.;https://thecleverprogrammer.com/2020/11/17/face-mask-detection-with-machine-learning/;['keras', 'caffe', 'sklearn'];1.0;['NN', 'CNN'];['CV', 'NN', 'ML', 'CNN', 'ReLu', 'VGG', 'Classification'];['detect', 'relu', 'train', 'label', 'computer vision', 'image classification', 'epoch', 'fit', 'model', 'loss', 'training data', 'test data', 'recogn', 'predict', 'machine learning', 'neural network', 'classif', 'layer', 'vgg'];"Face detection has become a very interesting problem in image processing and computer vision. In this article, I will introduce you to a computer vision project on Face Mask Detection with Machine Learning using Python. Introduction to Face Mask Detection Face mask detection has a range of applications from capturing the movement of the face to facial recognition which at first requires the face to be detected with very good precision. Face detection is more relevant today as it is not only used on images, but also in video applications like real-time surveillance and face detection in videos. Also, Read – 100+ Machine Learning Projects Solved and Explained. High precision image classification is now possible with advances in convolutional networks. Pixel level information is often needed after face detection, which most face detection methods do not provide. Obtaining pixel-level detail has been a difficult part of semantic segmentation. Semantic segmentation is the process of assigning a label to each pixel in the image. Process of Face Mask Detection with Machine Learning Step 1: Extract face data for training.Step 2: Train the classifier to classify faces in mask or labels without a mask.Step 3: Detect faces while testing data using SSD face detector.Step 4: Using the trained classifier, classify the detected faces. In the third step of the above process, you have to think about what is the SSD face detector? Well, the SSD is a Single Shot Multibox Detector. This is a technique used to detect objects in images using a single deep neural network. It is used for the detection of objects in an image. Using a basic architecture of the VGG-16 architecture, the SSD can outperform other object detectors such as YOLO and Faster R-CNN in terms of speed and accuracy. Face Mask Detection with Machine Learning Now, let’s get started with the task of Face Mask Detection with Machine Learning by using the Python programming language. I will start this task by importing the necessary Python libraries that we need for this task: Download Dataset import pandas as pd import numpy as np import cv2 import json import os import matplotlib.pyplot as plt import random import seaborn as sns from keras.models import Sequential from keras import optimizers from keras import backend as K from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D, BatchNormalization from sklearn.model_selection import train_test_split from keras.preprocessing.image import ImageDataGenerator directory = ""../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/annotations"" image_directory = ""../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images"" df = pd.read_csv(""../input/face-mask-detection-dataset/train.csv"") df_test = pd.read_csv(""../input/face-mask-detection-dataset/submission.csv"") view raw face_mask_detection.py hosted with ❤ by GitHub View this gist on GitHub Creating Helper Functions I will start this task by creating two helper functions: cvNet = cv2.dnn.readNetFromCaffe('weights.caffemodel') def getJSON(filePathandName): with open(filePathandName,'r') as f: return json.load(f) def adjust_gamma(image, gamma=1.0): invGamma = 1.0 / gamma table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]) return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8)) view raw face_mask_detection.py hosted with ❤ by GitHub View this gist on GitHub The getJSON function retrieves the json file containing the bounding box data in the training dataset.The adjust_gamma function is a non-linear operation used to encode and decode luminance or tristimulus values in video or still image systems. Simply put, it is used to instil a little bit of light into the image. If gamma <1, the image will shift to the darker end of the spectrum and when gamma> 1, there will be more light in the image. Data Processing The next step is now to explore the JSON data provided for the training: jsonfiles= [] for i in os.listdir(directory): jsonfiles.append(getJSON(os.path.join(directory,i))) jsonfiles[0]​x jsonfiles= []for i in os.listdir(directory): jsonfiles.append(getJSON(os.path.join(directory,i)))jsonfiles[0] {'FileName': '2349.png', 'NumOfAnno': 4, 'Annotations': [{'isProtected': False, 'ID': 193452793312540288, 'BoundingBox': [29, 69, 285, 343], 'classname': 'face_other_covering', 'Confidence': 1, 'Attributes': {}}, {'isProtected': False, 'ID': 545570408121800384, 'BoundingBox': [303, 99, 497, 341], 'classname': 'face_other_covering', 'Confidence': 1, 'Attributes': {}}, {'isProtected': False, 'ID': 339053397051370048, 'BoundingBox': [8, 71, 287, 373], 'classname': 'hijab_niqab', 'Confidence': 1, 'Attributes': {}}, {'isProtected': False, 'ID': 100482004994698944, 'BoundingBox': [296, 99, 525, 371], 'classname': 'hijab_niqab', 'Confidence': 1, 'Attributes': {}}]} The Annotations field contains the data of all the faces present in a particular image.There are different class names, but the real class names are face_with_mask and face_no_mask. df = pd.read_csv(""train.csv"") df.head() df = pd.read_csv(""train.csv"")df.head() namex1x2y1y2classname02756.png69126294392face_with_mask12756.png50510723283face_with_mask22756.png75252264390mask_colorful32756.png521136711277mask_colorful46098.jpg36085728653face_no_mask Using the mask and the non_mask labels, the bounding box data of the json files is extracted. The faces of a particular image are extracted and stored in the data list with its tag for the learning process. data = [] img_size = 124 mask = ['face_with_mask'] non_mask = [""face_no_mask""] labels={'mask':0,'without mask':1} for i in df[""name""].unique(): f = i+"".json"" for j in getJSON(os.path.join(directory,f)).get(""Annotations""): if j[""classname""] in mask: x,y,w,h = j[""BoundingBox""] img = cv2.imread(os.path.join(image_directory,i),1) img = img[y:h,x:w] img = cv2.resize(img,(img_size,img_size)) data.append([img,labels[""mask""]]) if j[""classname""] in non_mask: x,y,w,h = j[""BoundingBox""] img = cv2.imread(os.path.join(image_directory,i),1) img = img[y:h,x:w] img = cv2.resize(img,(img_size,img_size)) data.append([img,labels[""without mask""]]) random.shuffle(data) p = [] for face in data: if(face[1] == 0): p.append(""Mask"") else: p.append(""No Mask"") sns.countplot(p) view raw face_mask_detection.py hosted with ❤ by GitHub View this gist on GitHub The visualization above tells us that the number of mask images> Number of images without a mask, so this is an unbalanced dataset. But since we’re using a pre-trained SSD model, which is trained to detect unmasked faces, this imbalance wouldn’t matter much. But let’s reshape the data before training a neural network: X = [] Y = [] for features,label in data: X.append(features) Y.append(label) X = np.array(X)/255.0 X = X.reshape(-1,124,124,3) Y = np.array(Y) view raw face_mask_detection.py hosted with ❤ by GitHub View this gist on GitHub Training Neural Network for Face Mask Detection Now the next step is to train a Neural Network for the task of Face Mask Detection with Machine Learning: model = Sequential() model.add(Conv2D(32, (3, 3), padding = ""same"", activation='relu', input_shape=(124,124,3))) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(Conv2D(128, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dropout(0.5)) model.add(Dense(50, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy']) xtrain,xval,ytrain,yval=train_test_split(X, Y,train_size=0.8,random_state=0) featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, vertical_flip=False) datagen.fit(xtrain) history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32), steps_per_epoch=xtrain.shape[0]//32, epochs=50, verbose=1, validation_data=(xval, yval)) view raw face_mask_detection.py hosted with ❤ by GitHub View this gist on GitHub Testing The Model The test dataset contains 1698 images and to evaluate the model so I took a handful of images from this dataset as there are no face tags in the dataset: test_images = ['1114.png','1504.jpg', '0072.jpg','0012.jpg','0353.jpg','1374.jpg'] gamma = 2.0 fig = plt.figure(figsize = (14,14)) rows = 3 cols = 2 axes = [] assign = {'0':'Mask','1':""No Mask""} for j,im in enumerate(test_images): image = cv2.imread(os.path.join(image_directory,im),1) image = adjust_gamma(image, gamma=gamma) (h, w) = image.shape[:2] blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0)) cvNet.setInput(blob) detections = cvNet.forward() for i in range(0, detections.shape[2]): try: box = detections[0, 0, i, 3:7] * np.array([w, h, w, h]) (startX, startY, endX, endY) = box.astype(""int"") frame = image[startY:endY, startX:endX] confidence = detections[0, 0, i, 2] if confidence > 0.2: im = cv2.resize(frame,(img_size,img_size)) im = np.array(im)/255.0 im = im.reshape(1,124,124,3) result = model.predict(im) if result>0.5: label_Y = 1 else: label_Y = 0 cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2) cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2) except:pass axes.append(fig.add_subplot(rows, cols, j+1)) plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) plt.show() view raw face_mask_detection.py hosted with ❤ by GitHub View this gist on GitHub By analyzing the output above, we can observe that the whole system works well for faces that have spatial dominance. But fails in the case of images where the faces are small and take up less space in the overall image. For best results, different image preprocessing techniques can be used, or the confidence threshold can be kept lower, or one can try different blob sizes. Hope you liked this article on face mask detection with machine learning using the Python programming language. Please feel free to ask your valuable questions in the comments section below.";Face Mask Detection with Machine Learning
2020-11-18 19:39:41;In this article, I will introduce you to a machine learning project on Restaurant Recommendation System with Python programming language. There is an extended class of applications that involve predicting user responses to a variety of options. Such a system is called a recommender system.;https://thecleverprogrammer.com/2020/11/18/restaurant-recommendation-system-with-python/;['pattern', 'sklearn', 'vocabulary', 'nltk'];1.0;['RL'];['Recommender', 'Regression', 'ML', 'RL', 'Classification'];['regression', 'predict', 'fit', 'model', 'machine learning', 'classif', 'filter', 'train', 'recommend', 'recommender'];"In this article, I will introduce you to a machine learning project on Restaurant Recommendation System with Python programming language. There is an extended class of applications that involve predicting user responses to a variety of options. Such a system is called a recommender system. How the Restaurant Recommendation System Works? The rapid growth in data collection has led to a new era of a data-driven world. Data is used to create more efficient systems and that’s where recommender systems come in. Also, Read – 100+ Machine Learning Projects Solved and Explained. Recommendation systems are a type of information filtering systems because they improve the quality of search results and provide elements that are more relevant to the search item or that are related to the search history of the user. These are active information filtering systems that personalize the information provided to a user based on their interests, relevance of the information, etc. Recommendation systems are widely used to recommend movies, items, restaurants, places to visit, items to buy, etc. There are two types of recommendation systems: Content-based filteringCollaborative filtering A Restaurant recommendation system uses content-based filtering. This method only uses information about the description and attributes of items that users have previously consumed to model user preferences. In other words, these algorithms try to recommend things similar to what a user liked in the past. The dataset I’ll be using here consists of restaurants in Bangalore, India, collected from Zomato. You can download the dataset from here. To create the Restaurant recommendation system, I will create a content-based recommendation system where when I enter the name of a restaurant, the Restaurant recommendation system will look at reviews from other restaurants, and System will recommend us to the other restaurants with similar reviews and sort them from the top-rated. Machine Learning Project on Restaurant Recommendation System with Python I will start the task of Restaurant Recommendation System by importing the necessary Python Libraries: import numpy as np import pandas as pd import seaborn as sb import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.metrics import r2_score import warnings warnings.filterwarnings('always') warnings.filterwarnings('ignore') import re from nltk.corpus import stopwords from sklearn.metrics.pairwise import linear_kernel from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer view raw Zomato.py hosted with ❤ by GitHub View this gist on GitHub Now, I will load and read the dataset: zomato_real=pd.read_csv(""zomato.csv"") zomato_real.head() # prints the first 5 rows of the dataset zomato_real=pd.read_csv(""zomato.csv"")zomato_real.head() # prints the first 5 rows of the dataset Now the next step is data cleaning and feature engineering for this step we need to do a lot of stuff with the data such as: Deleting Unnecessary ColumnsRemoving the DuplicatesRemove the NaN values from the datasetChanging the column namesData TransformationsData CleaningAdjust the column names Now, let’s perform all the above steps in our data: #Deleting Unnnecessary Columns zomato=zomato_real.drop(['url','dish_liked','phone'],axis=1) #Dropping the column ""dish_liked"", ""phone"", ""url"" and saving the new dataset as ""zomato"" #Removing the Duplicates zomato.duplicated().sum() zomato.drop_duplicates(inplace=True) #Remove the NaN values from the dataset zomato.isnull().sum() zomato.dropna(how='any',inplace=True) #Changing the column names zomato = zomato.rename(columns={'approx_cost(for two people)':'cost','listed_in(type)':'type', 'listed_in(city)':'city'}) #Some Transformations zomato['cost'] = zomato['cost'].astype(str) #Changing the cost to string zomato['cost'] = zomato['cost'].apply(lambda x: x.replace(',','.')) #Using lambda function to replace ',' from cost zomato['cost'] = zomato['cost'].astype(float) #Removing '/5' from Rates zomato = zomato.loc[zomato.rate !='NEW'] zomato = zomato.loc[zomato.rate !='-'].reset_index(drop=True) remove_slash = lambda x: x.replace('/5', '') if type(x) == np.str else x zomato.rate = zomato.rate.apply(remove_slash).str.strip().astype('float') # Adjust the column names zomato.name = zomato.name.apply(lambda x:x.title()) zomato.online_order.replace(('Yes','No'),(True, False),inplace=True) zomato.book_table.replace(('Yes','No'),(True, False),inplace=True) ## Computing Mean Rating restaurants = list(zomato['name'].unique()) zomato['Mean Rating'] = 0 for i in range(len(restaurants)): zomato['Mean Rating'][zomato['name'] == restaurants[i]] = zomato['rate'][zomato['name'] == restaurants[i]].mean() from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler(feature_range = (1,5)) zomato[['Mean Rating']] = scaler.fit_transform(zomato[['Mean Rating']]).round(2) view raw Zomato.py hosted with ❤ by GitHub View this gist on GitHub Now the next step is to perform some text preprocessing steps which include: Lower casingRemoval of PunctuationsRemoval of StopwordsRemoval of URLsSpelling correction Now let’s perform the above text preprocessing steps on the data: ## Lower Casing zomato[""reviews_list""] = zomato[""reviews_list""].str.lower() ## Removal of Puctuations import string PUNCT_TO_REMOVE = string.punctuation def remove_punctuation(text): """"""custom function to remove the punctuation"""""" return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE)) zomato[""reviews_list""] = zomato[""reviews_list""].apply(lambda text: remove_punctuation(text)) ## Removal of Stopwords from nltk.corpus import stopwords STOPWORDS = set(stopwords.words('english')) def remove_stopwords(text): """"""custom function to remove the stopwords"""""" return "" "".join([word for word in str(text).split() if word not in STOPWORDS]) zomato[""reviews_list""] = zomato[""reviews_list""].apply(lambda text: remove_stopwords(text)) ## Removal of URLS def remove_urls(text): url_pattern = re.compile(r'https?://\S+|www\.\S+') return url_pattern.sub(r'', text) zomato[""reviews_list""] = zomato[""reviews_list""].apply(lambda text: remove_urls(text)) zomato[['reviews_list', 'cuisines']].sample(5) view raw Zomato.py hosted with ❤ by GitHub View this gist on GitHub reviews_listcuisines12110rated 20 ratedn piece shit customer service wo…Ice Cream, Desserts25865rated 10 ratedn ordered chicken fried rice chi…Bengali, North Indian, Chinese555rated 40 ratedn perfect place burger coke frie…Burger, Fast Food14033rated 40 ratedn place needs introduction locat…Bakery, Cafe, Italian, Desserts37162rated 50 ratedn located city market cant miss …Healthy Food, Juices # RESTAURANT NAMES: restaurant_names = list(zomato['name'].unique()) def get_top_words(column, top_nu_of_words, nu_of_word): vec = CountVectorizer(ngram_range= nu_of_word, stop_words='english') bag_of_words = vec.fit_transform(column) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:top_nu_of_words] zomato=zomato.drop(['address','rest_type', 'type', 'menu_item', 'votes'],axis=1) import pandas # Randomly sample 60% of your dataframe df_percent = zomato.sample(frac=0.5) view raw Zomato.py hosted with ❤ by GitHub View this gist on GitHub TF-IDF Vectorization TF-IDF (Term Frequency-Inverse Document Frequency) vectors for each document. This will give you a matrix where each column represents a word in the general vocabulary (all words that appear in at least one document) and each column represents a restaurant, as before. TF-IDF is the statistical method of assessing the meaning of a word in a given document. Now, I will use the TF-IDF vectorization on the dataset: df_percent.set_index('name', inplace=True) indices = pd.Series(df_percent.index) # Creating tf-idf matrix tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english') tfidf_matrix = tfidf.fit_transform(df_percent['reviews_list']) cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix) view raw Zomato.py hosted with ❤ by GitHub View this gist on GitHub Now the last step for creating a Restaurant Recommendation System is to write a function that will recommend restaurants: def recommend(name, cosine_similarities = cosine_similarities): # Create a list to put top restaurants recommend_restaurant = [] # Find the index of the hotel entered idx = indices[indices == name].index[0] # Find the restaurants with a similar cosine-sim value and order them from bigges number score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending=False) # Extract top 30 restaurant indexes with a similar cosine-sim value top30_indexes = list(score_series.iloc[0:31].index) # Names of the top 30 restaurants for each in top30_indexes: recommend_restaurant.append(list(df_percent.index)[each]) # Creating the new data set to show similar restaurants df_new = pd.DataFrame(columns=['cuisines', 'Mean Rating', 'cost']) # Create the top 30 similar restaurants with some of their columns for each in recommend_restaurant: df_new = df_new.append(pd.DataFrame(df_percent[['cuisines','Mean Rating', 'cost']][df_percent.index == each].sample())) # Drop the same named restaurants and sort only the top 10 by the highest rating df_new = df_new.drop_duplicates(subset=['cuisines','Mean Rating', 'cost'], keep=False) df_new = df_new.sort_values(by='Mean Rating', ascending=False).head(10) print('TOP %s RESTAURANTS LIKE %s WITH SIMILAR REVIEWS: ' % (str(len(df_new)), name)) return df_new recommend('Pai Vihar') view raw Zomato.py hosted with ❤ by GitHub View this gist on GitHub cuisinesMean RatingcostCinnamonNorth Indian, Chinese, Biryani3.62550.0Prasiddhi Food CornerFast Food, North Indian, South Indian3.45200.0Shrusti CoffeeCafe, South Indian3.45150.0Shanthi SagarSouth Indian, North Indian, Chinese3.44400.0Shanthi SagarSouth Indian, North Indian, Chinese, Juices3.44250.0Mayura SagarChinese, North Indian, South Indian3.32250.0Container CoffeeSouth Indian3.11200.0 As as you can see that we got a fairly good output. So, I hope you liked this article on Machine Learning project on Restaurant Recommendation system with Python programming language. Feel free to ask your valuable questions in the comments section below.";Restaurant Recommendation System with Python
2020-11-20 00:49:49;In this article, I will take you through a very simple Machine Learning project on Hand Gesture Recognition with Python programming language. Hand gesture recognition system received great attention in the recent few years because of its manifoldness applications and the ability to interact with machine efficiently through human-computer interaction.;https://thecleverprogrammer.com/2020/11/20/hand-gesture-recognition-with-python/;['sklearn'];1.0;['CV'];['ML', 'Random Forest', 'CV', 'Classification'];['recogn', 'predict', 'fit', 'model', 'machine learning', 'random forest', 'classif', 'train'];"In this article, I will take you through a very simple Machine Learning project on Hand Gesture Recognition with Python programming language. Hand gesture recognition system received great attention in the recent few years because of its manifoldness applications and the ability to interact with machine efficiently through human-computer interaction. Hand Gesture Recognition Model The essential objective of building a hand gesture recognition model is to create a natural interaction between human and computer where the recognized gestures can be used to control a robot or transmit meaningful information. Also, Read – 100+ Machine Learning Projects Solved and Explained. The gestures can be static (posture or certain pose) which require less computational complexity or dynamic (sequence of postures) which are more complex but adapted to real-time environments. In this article, I will train a very simple model that can be easily understood by machine learning newbies. The hand gesture recognition system has been applied for different applications in different fields including; translation into sign language, virtual environments, intelligent monitoring, robot control, medical systems, etc. Machine Learning Project on Hand Gesture Recognition Model Now let’s see how to train a Machine Learning model in Hand Gesture Recognition with Python programming language. I will start with importing the necessary libraries and reading the datasets that we need for this task: import numpy as np # linear algebra import pandas as pd # data processing df0 = pd.read_csv(""emg-4/0.csv"", header=None ) df1 = pd.read_csv(""emg-4/1.csv"", header=None ) df2 = pd.read_csv(""emg-4/2.csv"", header=None ) df3 = pd.read_csv(""emg-4/3.csv"", header=None ) df = pd.concat([df0,df1,df2,df3], axis = 0) view raw hand_gesture.py hosted with ❤ by GitHub View this gist on GitHub Now I will split the data into 75% training and 25% test set: x = df.loc[:,0:63] y = df[64] from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) view raw hand_gesture.py hosted with ❤ by GitHub View this gist on GitHub Now I will rescale the data using Standard Scalar: from sklearn.preprocessing import StandardScaler sc = StandardScaler() x_train = pd.DataFrame(sc.fit_transform(x_train)) x_test = pd.DataFrame(sc.transform(x_test)) view raw hand_gesture.py hosted with ❤ by GitHub View this gist on GitHub Now I will use the Random Forest Classifier to train a Hand Gesture Recognition model with Python: from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV lr_grid = {'max_depth' : [4,8,16,32,64,128], 'criterion' : ['entropy','gini']} clf = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42) gs = GridSearchCV(estimator = clf, param_grid=lr_grid,cv = 5) gs.fit(x_train,y_train) y_pred = gs.predict(x_test) gs.best_params_ view raw hand_gesture.py hosted with ❤ by GitHub View this gist on GitHub Now let’s check the accuracy of the model using the confusion matrix and print the classification report of our machine learning model: from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix print('Classification Report: \n', classification_report(y_test,y_pred)) print('Confusion Matrix: \n', confusion_matrix(y_test,y_pred))​x from sklearn.metrics import classification_reportfrom sklearn.metrics import confusion_matrix​print('Classification Report: \n', classification_report(y_test,y_pred))print('Confusion Matrix: \n', confusion_matrix(y_test,y_pred)) Classification Report: precision recall f1-score support 0 0.94 0.97 0.95 719 1 0.96 0.92 0.94 769 2 0.91 0.95 0.93 703 3 0.89 0.86 0.87 729 accuracy 0.92 2920 macro avg 0.92 0.93 0.92 2920 weighted avg 0.92 0.92 0.92 2920 Confusion Matrix: [[698 0 7 14] [ 0 709 22 38] [ 4 7 665 27] [ 43 25 33 628]] I hope you liked this article on Hand Gesture Recognition model using the Python programming language. Feel free to ask your valuable questions in the comments section below.";Hand Gesture Recognition with Python
2020-11-20 12:00:26;In machine learning, linear regression is a statistical procedure for calculating the value of a dependent variable from an independent variable. In this article, I will introduce you to linear regression with the Python programming language.;https://thecleverprogrammer.com/2020/11/20/linear-regression-with-python/;['sklearn'];1.0;[];['ML', 'Linear Regression', 'Regression'];['regression', 'linear regression', 'predict', 'fit', 'model', 'machine learning', 'train'];In machine learning, linear regression is a statistical procedure for calculating the value of a dependent variable from an independent variable. In this article, I will introduce you to linear regression with the Python programming language. Introduction to Linear Regression in Machine Learning Linear Regression is a machine learning algorithm which uses a dependent variable to predict future outcomes based on one or more independent variables. It measures the association between two variables. Linear regression analysis is the most widely used of all machine learning algorithms. Also, Read – 100+ Machine Learning Projects Solved and Explained. Simply put, linear regression is a statistical test applied to a set of data to define and quantify the relationship between the variables considered. It is simple to use and is still considered among the most powerful algorithms. The use of the linear regression algorithm is important for the following reasons: Description: It helps to analyze the strength of the association between the result (dependent variable) and the predictor variables.Adjustment: It adjusts the effect of covariates or confounders.Predictors: It helps to estimate the important risk factors that affect the dependent variable.The extent of the prediction: It helps analyze the magnitude of the change in the independent variable of a “unit” that would affect the dependent variable.Prediction: It helps quantify new cases. Linear Regression with Python Now in this section, I will take you through how to implement Linear Regression with Python programming language. I will start this task by importing the necessary Python libraries: import matplotlib.pylab as plt import numpy as np %matplotlib inline from sklearn.linear_model import LinearRegression from sklearn import datasets view raw linear_regression.py hosted with ❤ by GitHub View this gist on GitHub Now, I will load the dataset: diabetes = datasets.load_diabetes()​x diabetes = datasets.load_diabetes() Training Linear Regression with Python To train the linear regression algorithm using the Python programming language, I will first split the dataset into 80% training and 20% test sets: from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=0) from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=0) Now let’s train the model: from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=0) # There are three steps to model something with sklearn # 1. Set up the model model = LinearRegression() # 2. Use fit model.fit(X_train, y_train) view raw linear_regression.py hosted with ❤ by GitHub View this gist on GitHub Now let’s plot our trained model by using the matplotlib library in Python: y_pred = model.predict(X_test) plt.plot(y_test, y_pred, '.') # plot a line, a perfit predict would all fall on this line x = np.linspace(0, 330, 100) y = x plt.plot(x, y) plt.show() view raw linear_regression.py hosted with ❤ by GitHub View this gist on GitHub Conclusion The Linear Regression model is used to test the relationship between two variables in the form of an equation. You can implement this model without using any library like sklearn also which you can learn from here. I hope you liked this article on Linear Regression with Python programming language. Feel free to ask your valuable questions in the comments section below.;Linear Regression with Python
2020-11-21 12:18:08;In this article, I’ll introduce you to a machine learning project on employee attrition prediction with Python programming language. Employees are considered the backbone of an organization. The success or failure of the organization depends on the employees who work for an organization. Organizations must deal with the problems when trained, skilled and experienced employees leave the organization for better opportunities.;https://thecleverprogrammer.com/2020/11/21/employee-attrition-prediction-with-python/;['sklearn', 'xgboost'];1.0;[];['ML', 'Random Forest', 'Classification', 'Regression'];['regression', 'predict', 'fit', 'model', 'loss', 'machine learning', 'random forest', 'classif', 'train', 'label'];"In this article, I’ll introduce you to a machine learning project on employee attrition prediction with Python programming language. Employees are considered the backbone of an organization. The success or failure of the organization depends on the employees who work for an organization. Organizations must deal with the problems when trained, skilled and experienced employees leave the organization for better opportunities. What is Employee Attrition Prediction? Employee attrition is downsizing in any organization where employees resign. Employees are valuable assets of any organization. It is necessary to know whether the employees are dissatisfied or whether there are other reasons for leaving their respective jobs. Also, Read – 100+ Machine Learning Projects Solved and Explained. Nowadays, for better opportunities, employees are eager to move from one organization to another. But if they quit their jobs unexpectedly, it can result in a huge loss for the organization. A new hire will consume money and time, and newly hired employees will also take time to make the respective organization profitable. Retaining skilled and hardworking employees is one of the most critical challenges many organizations face. Therefore, by improving employee satisfaction and providing a desirable working environment, we can certainly reduce this problem significantly. Machine Learning Project on Employee Attrition Prediction with Python In this section, I will take you through a Machine Learning project on predicting Employee Attrition prediction with Python programming language. I will start this task by importing the necessary Python libraries that we need for this task: Download Dataset import numpy as np # linear algebra import pandas as pd # data processing import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline # Import statements required for Plotly import plotly.offline as py py.init_notebook_mode(connected=True) import plotly.graph_objs as go import plotly.tools as tls from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import (accuracy_score, log_loss, classification_report) from imblearn.over_sampling import SMOTE import xgboost view raw Employee_attrition.py hosted with ❤ by GitHub View this gist on GitHub Now let’s read the data and do some exploratory data analysis to understand this dataset properly: attrition = pd.read_csv('Employee-Attrition.csv')​x attrition = pd.read_csv('Employee-Attrition.csv') Usually one of the first steps in data exploration is getting a rough idea of how the features are distributed among them. To do this, I’ll use the kdeplot function in the seaborn library in Python: f, axes = plt.subplots(3, 3, figsize=(10, 8), sharex=False, sharey=False) # Defining our colormap scheme s = np.linspace(0, 3, 10) cmap = sns.cubehelix_palette(start=0.0, light=1, as_cmap=True) # Generate and plot x = attrition['Age'].values y = attrition['TotalWorkingYears'].values sns.kdeplot(x, y, cmap=cmap, shade=True, cut=5, ax=axes[0,0]) axes[0,0].set( title = 'Age against Total working years') cmap = sns.cubehelix_palette(start=0.333333333333, light=1, as_cmap=True) # Generate and plot x = attrition['Age'].values y = attrition['DailyRate'].values sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,1]) axes[0,1].set( title = 'Age against Daily Rate') cmap = sns.cubehelix_palette(start=0.666666666667, light=1, as_cmap=True) # Generate and plot x = attrition['YearsInCurrentRole'].values y = attrition['Age'].values sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,2]) axes[0,2].set( title = 'Years in role against Age') cmap = sns.cubehelix_palette(start=1.0, light=1, as_cmap=True) # Generate and plot x = attrition['DailyRate'].values y = attrition['DistanceFromHome'].values sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[1,0]) axes[1,0].set( title = 'Daily Rate against DistancefromHome') cmap = sns.cubehelix_palette(start=1.333333333333, light=1, as_cmap=True) # Generate and plot x = attrition['DailyRate'].values y = attrition['JobSatisfaction'].values sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[1,1]) axes[1,1].set( title = 'Daily Rate against Job satisfaction') cmap = sns.cubehelix_palette(start=1.666666666667, light=1, as_cmap=True) # Generate and plot x = attrition['YearsAtCompany'].values y = attrition['JobSatisfaction'].values sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[1,2]) axes[1,2].set( title = 'Daily Rate against distance') cmap = sns.cubehelix_palette(start=2.0, light=1, as_cmap=True) # Generate and plot x = attrition['YearsAtCompany'].values y = attrition['DailyRate'].values sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[2,0]) axes[2,0].set( title = 'Years at company against Daily Rate') cmap = sns.cubehelix_palette(start=2.333333333333, light=1, as_cmap=True) # Generate and plot x = attrition['RelationshipSatisfaction'].values y = attrition['YearsWithCurrManager'].values sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[2,1]) axes[2,1].set( title = 'Relationship Satisfaction vs years with manager') cmap = sns.cubehelix_palette(start=2.666666666667, light=1, as_cmap=True) # Generate and plot x = attrition['WorkLifeBalance'].values y = attrition['JobSatisfaction'].values sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[2,2]) axes[2,2].set( title = 'WorklifeBalance against Satisfaction') f.tight_layout() view raw Employee_attrition.py hosted with ❤ by GitHub View this gist on GitHub Finding Correlation The next step in a data exploration is to find the correlation matrix. By plotting a correlation matrix, we get a really good look at how the features relate to each other. In this correlation plot, I will be using the Plotly library in Python to produce an interactive Pearson correlation matrix via the Heatmap function as follows: # Define a dictionary for the target mapping target_map = {'Yes':1, 'No':0} # Use the pandas apply method to numerically encode our attrition target variable attrition[""Attrition_numerical""] = attrition[""Attrition""].apply(lambda x: target_map[x]) # creating a list of only numerical values numerical = [u'Age', u'DailyRate', u'DistanceFromHome', u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction', u'HourlyRate', u'JobInvolvement', u'JobLevel', u'JobSatisfaction', u'MonthlyIncome', u'MonthlyRate', u'NumCompaniesWorked', u'PercentSalaryHike', u'PerformanceRating', u'RelationshipSatisfaction', u'StockOptionLevel', u'TotalWorkingYears', u'TrainingTimesLastYear', u'WorkLifeBalance', u'YearsAtCompany', u'YearsInCurrentRole', u'YearsSinceLastPromotion',u'YearsWithCurrManager'] data = [ go.Heatmap( z= attrition[numerical].astype(float).corr().values, # Generating the Pearson correlation x=attrition[numerical].columns.values, y=attrition[numerical].columns.values, colorscale='Viridis', reversescale = False, # text = True , opacity = 1.0 ) ] layout = go.Layout( title='Pearson Correlation of numerical features', xaxis = dict(ticks='', nticks=36), yaxis = dict(ticks='' ), width = 900, height = 700, ) fig = go.Figure(data=data, layout=layout) py.iplot(fig, filename='labelled-heatmap') view raw Employee_attrition.py hosted with ❤ by GitHub View this gist on GitHub Observations From Above Plot: From the correlation plot, we can see that a lot of our columns appear to be poorly correlated to each other. Generally, when building a predictive model, it would be better to train a model with features that are not too correlated with each other so that we don’t need to deal with redundant features. In the case where we have a large number of correlated characteristics, perhaps we could apply a technique such as principal component analysis (PCA) to reduce the characteristic space. Feature Engineering After exploring our dataset, let’s now move on to the task o feature engineering and numerically encoding the categorical values in our dataset. Feature engineering involves creating new features and relationships from the current features that we have. For this task, we’ll separate the numeric columns from the categorical columns as follows: attrition = attrition.drop(['Attrition_numerical'], axis=1) # Empty list to store columns with categorical data categorical = [] for col, value in attrition.iteritems(): if value.dtype == 'object': categorical.append(col) # Store the numerical columns in a list numerical numerical = attrition.columns.difference(categorical) view raw Employee_attrition.py hosted with ❤ by GitHub View this gist on GitHub After identifying which of our features contain categorical data, we can start to digitally encode the data. To do this, I’ll use Pandas’ get_dummies method in Python which creates dummy variables encoded from the categorical variables: attrition_cat = attrition[categorical] attrition_cat = attrition_cat.drop(['Attrition'], axis=1) # Dropping the target column attrition_cat = pd.get_dummies(attrition_cat) attrition_cat.head(3) attrition_num = attrition[numerical] attrition_final = pd.concat([attrition_num, attrition_cat], axis=1) attrition_cat = attrition[categorical]attrition_cat = attrition_cat.drop(['Attrition'], axis=1) # Dropping the target columnattrition_cat = pd.get_dummies(attrition_cat)attrition_cat.head(3)attrition_num = attrition[numerical]attrition_final = pd.concat([attrition_num, attrition_cat], axis=1) One last step we need to remember is to generate our target variable. The target, in this case, is given by the Attrition column which contains categorical variables therefore requires numeric coding. We digitally encode it by creating a dictionary with the given mapping as 1: Yes and 0: No: target_map = {'Yes':1, 'No':0} # Use the pandas apply method to numerically encode our attrition target variable target = attrition[""Attrition""].apply(lambda x: target_map[x]) target_map = {'Yes':1, 'No':0}# Use the pandas apply method to numerically encode our attrition target variabletarget = attrition[""Attrition""].apply(lambda x: target_map[x]) Machine Learning for Employee Attrition Prediction with Python Now, we need to train a Machine Learning model for predicting Employee Attrition prediction with Python. For this task, I will use the Random Forest Classification model provided by Scikit-learn. But before implementing Machine Learning for prediction of Employee Attrition prediction we need to split the data into a training set and test set: from sklearn.cross_validation import train_test_split from sklearn.cross_validation import StratifiedShuffleSplit # Split data into train and test sets as well as for validation and testing train, test, target_train, target_val = train_test_split(attrition_final, target, train_size= 0.80, random_state=0); #train, test, target_train, target_val = StratifiedShuffleSplit(attrition_final, target, random_state=0) view raw Employee_attrition.py hosted with ❤ by GitHub View this gist on GitHub Now let’s train the Random forest classification model for the task of Employee Attrition prediction using Machine Learning and Python: oversampler=SMOTE(random_state=0) smote_train, smote_target = oversampler.fit_sample(train,target_train) seed = 0 # We set our random seed to zero for reproducibility # Random Forest parameters rf_params = { 'n_jobs': -1, 'n_estimators': 1000, # 'warm_start': True, 'max_features': 0.3, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features' : 'sqrt', 'random_state' : seed, 'verbose': 0 } rf = RandomForestClassifier(**rf_params) rf.fit(smote_train, smote_target) rf_predictions = rf.predict(test) print(""Accuracy score: {}"".format(accuracy_score(target_val, rf_predictions))) print(""=""*80) print(classification_report(target_val, rf_predictions)) view raw Employee_attrition.py hosted with ❤ by GitHub View this gist on GitHub Accuracy score: 0.8537414965986394 ================================================================================ precision recall f1-score support 0 0.90 0.93 0.91 245 1 0.57 0.49 0.53 49 micro avg 0.85 0.85 0.85 294 macro avg 0.74 0.71 0.72 294 weighted avg 0.85 0.85 0.85 294 As observed, our Random Forest returns around 88% accuracy for its predictions and at first glance, this may seem like a fairly good model. Sklearn’s Random Forest classifier also contains a very handy attribute for analyzing feature importance which tells us which features in our dataset have received the most importance by the Random Forest algorithm. Let’s visualize the features taken into account by our machine learning model for employee attrition: trace = go.Scatter( y = rf.feature_importances_, x = attrition_final.columns.values, mode='markers', marker=dict( sizemode = 'diameter', sizeref = 1, size = 13, #size= rf.feature_importances_, #color = np.random.randn(500), #set color equal to a variable color = rf.feature_importances_, colorscale='Portland', showscale=True ), text = attrition_final.columns.values ) data = [trace] layout= go.Layout( autosize= True, title= 'Random Forest Feature Importance', hovermode= 'closest', xaxis= dict( ticklen= 5, showgrid=False, zeroline=False, showline=False ), yaxis=dict( title= 'Feature Importance', showgrid=False, zeroline=False, ticklen= 5, gridwidth= 2 ), showlegend= False ) fig = go.Figure(data=data, layout=layout) py.iplot(fig,filename='scatter2010') view raw Employee_attrition.py hosted with ❤ by GitHub View this gist on GitHub I hope you liked this article on Machine Learning project on Employee Attrition Prediction with Python programming language. Feel free to ask your valuable questions in the comments section below.";Employee Attrition Prediction with Python
2020-11-21 13:29:13;In Machine Learning, we train models using training data. In this article, I’ll walk you through how much training data is required for a machine learning model.;https://thecleverprogrammer.com/2020/11/21/how-much-training-data-is-required-for-machine-learning/;['pattern'];1.0;[];['ML'];['recogn', 'model', 'machine learning', 'training data', 'train'];In Machine Learning, we train models using training data. In this article, I’ll walk you through how much training data is required for a machine learning model. How Much Training Data is Required for Machine Learning? Given the difficulty of observing and collecting the response variable for data instances, you might be wondering how much training data is needed for a machine learning model to be up and running. Unfortunately, this question is so specific to the problem that it is impossible to give a universal answer or even a rule of thumb. Also, Read – 100+ Machine Learning Projects Solved and Explained. Factors Determining How Much Training Data is Required These factors determine the amount of training data required to train a machine learning model: The complexity of the problem. Does the relationship between the input features and the target variable follow a simple pattern or is it complex and nonlinear?Precision requirements. If you only need a 60% success rate for your problem, less training data is needed than if you need to achieve a 95% success rate.The dimensionality of the functional space. If only two input features are available, less training data will be needed than if there were 2000 features. A guiding principle to remember is that as the training set grows, the models will become (on average) more accurate. More training data translates to greater accuracy due to the data-driven nature of machine learning models. Since the relationship between features and target is fully learned from training data, the more you have, the better able the model is to recognize and capture more subtle patterns and relationships. The image below shows whether the existing sample of 3,333 training instances contains enough data to build an accurate machine learning model. The black line represents the average accuracy over 10 repetitions of the evaluation routine and the shaded bands represent the error bands. Conclusion We can just conclude that the machine learning model won’t improve significantly if you add more training instances. This does not mean that significant improvements could not be made by using more features. Hope you liked this article on how much training data is required for a machine learning model. Please feel free to ask your valuable questions in the comments section below.;How Much Training Data is Required for Machine Learning?
2020-11-23 14:46:03;In the machine learning workflow, we use historical data to start exploring and discovering the relationships that exist between the input features and the target. In this article, I’ll walk you through the complete process of Machine Learning.;https://thecleverprogrammer.com/2020/11/23/machine-learning-process/;['pattern'];1.0;[];['ML'];['predict', 'fit', 'model', 'machine learning', 'train'];In the machine learning workflow, we use historical data to start exploring and discovering the relationships that exist between the input features and the target. In this article, I’ll walk you through the complete process of Machine Learning. The Process of Machine Learning The goal of machine learning is to discover patterns and relationships in data and put those findings to use. This process of discovery is achieved through the use of modelling techniques that have been developed over the past 30 years in statistics, computer science and applied mathematics. These different approaches can range from simple to extremely complex, but they all share a common goal: to estimate the functional relationship between the input characteristics and the target variable. These machine learning approaches also share a common process, as depicted in the image below. First, it uses historical data to build and optimize a model which is, in turn, used to make predictions based on new data. As shown in the image above, the machine learning process usually begins with collecting historical data. Then, this data is prepared to fit into a machine learning model. Then the next step is to build the model, here we are using machine learning algorithms. The next step is to evaluate the model. Typically, this step uses the test set obtained after dividing the historical data into training and testing sets. Then the next step is model optimization, which usually means turning the hyperparameters. A common machine learning technique for tuning hyperparameters is the use of the grid search algorithm. Then the last step is to test the model on a new invisible dataset and re-evaluate the model if it is not performing well. You can learn each about each step in the workflow of Machine Learning from below: Splitting the Historical DataBuilding Machine Learning ModelModel OptimizationDeploying and Testing the model on new data. I hope you liked this article on the complete process of Machine Learning. Feel free to ask your valuable questions in the comments section below.;Machine Learning Process
2020-11-24 14:22:13;Flower recognition uses the edge and colour characteristics of flower images to classify flowers. In this article, I will introduce you to a machine learning project on flower recognition with Python.;https://thecleverprogrammer.com/2020/11/24/flower-recognition-with-python/;['keras', 'sklearn', 'tensorflow'];1.0;['NN', 'CNN'];['NN', 'ML', 'CNN', 'ReLu', 'Classification'];['recogn', 'epoch', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'filter', 'relu', 'train', 'label'];"Flower recognition uses the edge and colour characteristics of flower images to classify flowers. In this article, I will introduce you to a machine learning project on flower recognition with Python. What is Flower Recognition? There are many species of flowers in the world. Some species have many colours, such as roses. It is difficult to remember all the names of flowers and their information. Additionally, someone may be confused with similar flower species. Also, Read – 100+ Machine Learning Projects Solved and Explained. For example, white champaka and champak have similar names and petal shapes, but they have different colours and petal lengths. At present, it is almost impossible to identify any particular flower or flower species in any way other than to seek information based on personal knowledge and expert experience. The availability of such experts can be an obstacle to this search for information. Searching for such information on the Internet today is very limited to keyword research; word processor. Even then, the searcher has to come up with sufficiently useful keywords, which they cannot do, which is the crux of the matter. This article will walk you through the machine learning approach to the task of recognizing flowers with Python. Machine Learning Project on Flower Recognition with Python The dataset I am using here for the flower recognition task contains 4242 flower images. Data collection is based on Flickr data, google images, Yandex images. You can use this data set to recognize the flowers in the photo. The images are divided into five classes: chamomile, tulip, rose, sunflower, dandelion. For each class, there are approximately 800 photos. The photos are not in high resolution, approximately 320×240 pixels. Photos are not reduced to one size, they have different proportions. Now let’s import the necessary Python libraries to get started with the task of Flower Recognition with Python: Download Dataset import os import cv2 import numpy as np #Encoding and Split data into Train/Test Sets from sklearn.preprocessing import LabelEncoder from tensorflow.keras.utils import to_categorical from sklearn.model_selection import train_test_split #Tensorflow Keras CNN Model from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop #Plot Images import matplotlib.pyplot as plt folder_dir = 'dataset path' view raw flower.py hosted with ❤ by GitHub View this gist on GitHub Now the next step is to read each image in the data and create a label for each with the name of the folder: data = [] label = [] SIZE = 128 #Crop the image to 128x128 for folder in os.listdir(folder_dir): for file in os.listdir(os.path.join(folder_dir, folder)): if file.endswith(""jpg""): label.append(folder) img = cv2.imread(os.path.join(folder_dir, folder, file)) img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) im = cv2.resize(img_rgb, (SIZE,SIZE)) data.append(im) else: continue view raw flower.py hosted with ❤ by GitHub View this gist on GitHub Now let’s convert the data into numerical values: data_arr = np.array(data) label_arr = np.array(label)​x data_arr = np.array(data)label_arr = np.array(label) Now let’s use the Label encoder and normalize the data: encoder = LabelEncoder() y = encoder.fit_transform(label_arr) y = to_categorical(y,5) X = data_arr/255 encoder = LabelEncoder()y = encoder.fit_transform(label_arr)y = to_categorical(y,5)X = data_arr/255 The next step is to split the dataset into 80% training and 20% test sets: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10) Now let’s build a neural network model for the task of Flower Recognition: model = Sequential() model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu', input_shape = (SIZE,SIZE,3))) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu')) model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu')) model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dropout(rate=0.5)) model.add(Dense(5, activation = ""softmax"")) view raw flower.py hosted with ❤ by GitHub View this gist on GitHub Before compiling the model we need to create more training images to prevent overfitting: datagen = ImageDataGenerator( rotation_range=20, zoom_range = 0.20, width_shift_range=0.3, height_shift_range=0.3, horizontal_flip=True, vertical_flip=True) datagen.fit(X_train) view raw flower.py hosted with ❤ by GitHub View this gist on GitHub Now let’s compile the neural network model: model.compile(optimizer=Adam(lr=0.0001),loss='categorical_crossentropy',metrics=['accuracy']) batch_size=32 epochs=64 history = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_test,y_test), verbose = 1) view raw flower.py hosted with ❤ by GitHub View this gist on GitHub Now let’s let the model if it recognize flowers properly: categories = np.sort(os.listdir(folder_dir)) fig, ax = plt.subplots(6,6, figsize=(25, 40)) for i in range(6): for j in range(6): k = int(np.random.random_sample() * len(X_test)) if(categories[np.argmax(y_test[k])] == categories[np.argmax(model.predict(X_test)[k])]): ax[i,j].set_title(""TRUE: "" + categories[np.argmax(y_test[k])], color='green') ax[i,j].set_xlabel(""PREDICTED: "" + categories[np.argmax(model.predict(X_test)[k])], color='green') ax[i,j].imshow(np.array(X_test)[k].reshape(SIZE, SIZE, 3), cmap='gray') else: ax[i,j].set_title(""TRUE: "" + categories[np.argmax(y_test[k])], color='red') ax[i,j].set_xlabel(""PREDICTED: "" + categories[np.argmax(model.predict(X_test)[k])], color='red') ax[i,j].imshow(np.array(X_test)[k].reshape(SIZE, SIZE, 3), cmap='gray') view raw flower.py hosted with ❤ by GitHub View this gist on GitHub I hope you liked this article on Machine Learning Project on Flower Recognition with Python programming language. Feel free to ask your valuable questions in the comments section below.";Flower Recognition with Python
2020-11-25 12:10:56;In this article, I’ll walk you through a machine learning project on gender classification with the Python programming language.Gender classification is essential and critical for many applications in business fields such as human-computer interaction applications and computer-aided physiological or psychological analysis, as it contains a wide range of information regarding the difference of characteristics between man and woman.Also, Read – 100+ Machine Learning Projects Solved and Explained.;https://thecleverprogrammer.com/2020/11/25/gender-classification-with-python/;['keras', 'pattern', 'tensorflow'];1.0;[];['NN', 'AI', 'ML', 'ReLu', 'Classification'];['artificial intelligence', 'recogn', 'epoch', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'relu', 'train', 'label'];"In this article, I’ll walk you through a machine learning project on gender classification with the Python programming language. Gender classification is essential and critical for many applications in business fields such as human-computer interaction applications and computer-aided physiological or psychological analysis, as it contains a wide range of information regarding the difference of characteristics between man and woman. Also, Read – 100+ Machine Learning Projects Solved and Explained. Introduction to Gender Classification The Gender classification is gaining more and more attention, as gender contains rich and distinguished information about the social activities of men and women. Gender classification aims to recognize a person’s gender based on the characteristics that differentiate masculinity and femininity. In the field of artificial intelligence, gender classification is considered to be one of the most important applications of the pattern recognition method. Advances in gender classification research have led to many potential applications. For example, a computer system with gender recognition functions has a wide range of applications in the fields of basic and applied research, including human-computer interaction, security industry and surveillance, demographic research, business development, mobile applications and video games. Also, multiple mechanisms are proposed to improve the performance of gender recognition in terms of accuracy and efficiency. Machine Learning Project on Gender Classification with Python Now, in this section, I will take you through a Machine Learning project on Gender Classification with Python. Let’s get started with this task by importing the necessary Python libraries: Download Dataset import os from tensorflow.keras import layers from tensorflow.keras import Model from tensorflow.keras.preprocessing.image import ImageDataGenerator import tensorflow as tf view raw gender.py hosted with ❤ by GitHub View this gist on GitHub Now let’s read and import the images dataset that we are going to use to train a neural network model: train_datagen = ImageDataGenerator(rescale = 1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest') test_datagen = ImageDataGenerator( rescale = 1.0/255) train_generator = train_datagen.flow_from_directory(""gender-recognition-200k-images-celeba/Dataset/Train/"", batch_size =256 , class_mode = 'binary', target_size = (64, 64)) validation_generator = test_datagen.flow_from_directory( ""gender-recognition-200k-images-celeba/Dataset/Validation/"", batch_size = 256, class_mode = 'binary', target_size = (64, 64)) view raw gender.py hosted with ❤ by GitHub View this gist on GitHub Now we need to train and compile the neural network model for the task of Gender classification with Python: from keras.optimizers import Adam model = tf.keras.models.Sequential([ # 1st conv tf.keras.layers.Conv2D(96, (11,11),strides=(4,4), activation='relu', input_shape=(64, 64, 3)), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(2, strides=(2,2)), # 2nd conv tf.keras.layers.Conv2D(256, (11,11),strides=(1,1), activation='relu',padding=""same""), tf.keras.layers.BatchNormalization(), # 3rd conv tf.keras.layers.Conv2D(384, (3,3),strides=(1,1), activation='relu',padding=""same""), tf.keras.layers.BatchNormalization(), # 4th conv tf.keras.layers.Conv2D(384, (3,3),strides=(1,1), activation='relu',padding=""same""), tf.keras.layers.BatchNormalization(), # 5th Conv tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu',padding=""same""), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPooling2D(2, strides=(2, 2)), # To Flatten layer tf.keras.layers.Flatten(), # To FC layer 1 tf.keras.layers.Dense(4096, activation='relu'), tf.keras.layers.Dropout(0.5), #To FC layer 2 tf.keras.layers.Dense(4096, activation='relu'), tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(1, activation='sigmoid') ]) model.compile( optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'] ) hist = model.fit_generator(generator=train_generator, validation_data=validation_generator, steps_per_epoch=256, validation_steps=256, epochs=50) view raw gender.py hosted with ❤ by GitHub View this gist on GitHub Before testing this model let’s have a look at how the model performs in terms of accuracy: import matplotlib.pyplot as plt acc = hist.history['accuracy'] val_acc = hist.history['val_accuracy'] loss = hist.history['loss'] val_loss = hist.history['val_loss'] epochs = range(len(acc)) plt.plot(epochs, acc, 'r', label='Training accuracy') plt.plot(epochs, val_acc, 'b', label='Validation accuracy') plt.title('Training and validation accuracy') plt.legend(loc=0) plt.figure() plt.show() view raw gender.py hosted with ❤ by GitHub View this gist on GitHub Now let’s test our neural network model in both the cases of male and female: import numpy as np from keras.preprocessing import image # predicting images path = ""gender-recognition-200k-images-celeba/Dataset/Test/Female/160001.jpg"" img = image.load_img(path, target_size=(64, 64)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) images = np.vstack([x]) classes = model.predict(images, batch_size=1) print(classes[0]) if classes[0]>0.5: print(""is a man"") else: print( "" is a female"") plt.imshow(img) view raw gender.py hosted with ❤ by GitHub View this gist on GitHub [0.] is a female In the above output, we can see that the model performs very well. I hope you liked this article on Gender Classification with Python programming language. Feel free to ask your valuable questions in the comments section below.";Gender Classification with Python
2020-11-27 17:24:05;A chessboard is the type of game board used for the game of chess, on which pawns and chess pieces are placed. A chessboard is usually square, with an alternating pattern of squares of two colours. In this article, I’ll walk you through how to create and visualize a chessboard with the Python programming language.;https://thecleverprogrammer.com/2020/11/27/chessboard-with-python/;['pattern'];1.0;[];['ML'];['machine learning'];"A chessboard is the type of game board used for the game of chess, on which pawns and chess pieces are placed. A chessboard is usually square, with an alternating pattern of squares of two colours. In this article, I’ll walk you through how to create and visualize a chessboard with the Python programming language. Create a Chessboard with Python To create a chessboard with the Python programming language, I will use two Python libraries; Matplotlib for visualization, and NumPy for building an algorithm which will help us to create and visualize a chessboard. Let’s see how we can code to create and visualize a chessboard: Also, Read – 100+ Machine Learning Projects Solved and Explained. import matplotlib.pyplot as plt import numpy as np from matplotlib.colors import LogNorm dx, dy = 0.015, 0.05 x = np.arange(-4.0, 4.0, dx) y = np.arange(-4.0, 4.0, dy) X, Y = np.meshgrid(x, y) extent = np.min(x), np.max(x), np.min(y), np.max(y) z1 = np.add.outer(range(8), range(8)) % 2 plt.imshow(z1, cmap=""binary_r"", interpolation=""nearest"", extent=extent, alpha=1) def chess(x, y): return (1 - x / 2 + x ** 5 + y ** 6) * np.exp(-(x ** 2 + y ** 2)) z2 = chess(X, Y) plt.imshow(z2, alpha=0.7, interpolation=""bilinear"", extent=extent) plt.title(""Chess Board with Python"") plt.show() view raw chessboard.py hosted with ❤ by GitHub View this gist on GitHub So this is how we can create and visualize a chessboard with code. I hope you liked this article on how to create a chessboard by using the Matplotlib library. Feel free to ask your valuable questions in the comments section below.";Chessboard with Python
2020-11-29 13:45:18;In this article, I will introduce you to a machine learning project on Covid-19 cases prediction with Python for the next 30 days. These types of predictive models help in providing an accurate prediction of epidemics, which is essential for obtaining information on the likely spread and consequences of infectious diseases.Governments and other legislative bodies rely on these kinds of machine learning predictive models and ideas to suggest new policies and assess the effectiveness of applied policies.Also, Read – 100+ Machine Learning Projects Solved and Explained.;https://thecleverprogrammer.com/2020/11/29/covid-19-cases-prediction-with-python/;['sklearn'];1.0;[];['ML'];['predict', 'fit', 'model', 'machine learning', 'label'];"In this article, I will introduce you to a machine learning project on Covid-19 cases prediction with Python for the next 30 days. These types of predictive models help in providing an accurate prediction of epidemics, which is essential for obtaining information on the likely spread and consequences of infectious diseases. Governments and other legislative bodies rely on these kinds of machine learning predictive models and ideas to suggest new policies and assess the effectiveness of applied policies. Also, Read – 100+ Machine Learning Projects Solved and Explained. Machine Learning Project on Covid-19 Cases Prediction with Python I will start the task of Covid-19 cases prediction with Python for the next 30 days by importing the necessary Python libraries and the dataset: Download Dataset 1 Download Dataset 2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px from fbprophet import Prophet from sklearn.metrics import r2_score plt.style.use(""ggplot"") df0 = pd.read_csv(""CONVENIENT_global_confirmed_cases.csv"") df1 = pd.read_csv(""CONVENIENT_global_deaths.csv"") view raw covid19_cases.py hosted with ❤ by GitHub View this gist on GitHub Data Preparation Now the next step is data preparation, I will simply prepare new data by combining the above datasets and then we will visualize a geographical plot of the data to see what we are going to work with: world = pd.DataFrame({""Country"":[],""Cases"":[]}) world[""Country""] = df0.iloc[:,1:].columns cases = [] for i in world[""Country""]: cases.append(pd.to_numeric(df0[i][1:]).sum()) world[""Cases""]=cases country_list=list(world[""Country""].values) idx = 0 for i in country_list: sayac = 0 for j in i: if j==""."": i = i[:sayac] country_list[idx]=i elif j==""("": i = i[:sayac-1] country_list[idx]=i else: sayac += 1 idx += 1 world[""Country""]=country_list world = world.groupby(""Country"")[""Cases""].sum().reset_index() world.head() continent=pd.read_csv(""continents2.csv"") continent[""name""]=continent[""name""].str.upper() view raw covid19_cases.py hosted with ❤ by GitHub View this gist on GitHub CountryCases0Afghanistan45716.01Albania35600.02Algeria79110.03Andorra6534.04Angola14920.0 Data Visualization Now here I will prepare three visualizations. One will be a geographical visualization to visualize the worldwide spread of Covid-19. Then the next visualization will be to have a look at the daily cases of Covid-19 in the world. Then the last visualization will be to have a look at the daily death cases of Covid-19 in the world. Now let’s start data visualization by looking at the worldwide spread of Covid-19: world[""Cases Range""]=pd.cut(world[""Cases""],[-150000,50000,200000,800000,1500000,15000000],labels=[""U50K"",""50Kto200K"",""200Kto800K"",""800Kto1.5M"",""1.5M+""]) alpha =[] for i in world[""Country""].str.upper().values: if i == ""BRUNEI"": i=""BRUNEI DARUSSALAM"" elif i==""US"": i=""UNITED STATES"" if len(continent[continent[""name""]==i][""alpha-3""].values)==0: alpha.append(np.nan) else: alpha.append(continent[continent[""name""]==i][""alpha-3""].values[0]) world[""Alpha3""]=alpha fig = px.choropleth(world.dropna(), locations=""Alpha3"", color=""Cases Range"", projection=""mercator"", color_discrete_sequence=[""white"",""khaki"",""yellow"",""orange"",""red""]) fig.update_geos(fitbounds=""locations"", visible=False) fig.update_layout(margin={""r"":0,""t"":0,""l"":0,""b"":0}) fig.show() view raw covid19_cases.py hosted with ❤ by GitHub View this gist on GitHub Now let’s have a look at the daily cases all around the world: count = [] for i in range(1,len(df0)): count.append(sum(pd.to_numeric(df0.iloc[i,1:].values))) df = pd.DataFrame() df[""Date""] = df0[""Country/Region""][1:] df[""Cases""] = count df=df.set_index(""Date"") count = [] for i in range(1,len(df1)): count.append(sum(pd.to_numeric(df1.iloc[i,1:].values))) df[""Deaths""] = count df.Cases.plot(title=""Daily Covid19 Cases in World"",marker=""."",figsize=(10,5),label=""daily cases"") df.Cases.rolling(window=5).mean().plot(figsize=(10,5),label=""MA5"") plt.ylabel(""Cases"") plt.legend() plt.show() view raw covid19_cases.py hosted with ❤ by GitHub View this gist on GitHub Now let’s have a look at the daily death cases of Covid-19: df.Deaths.plot(title=""Daily Covid19 Deaths in World"",marker=""."",figsize=(10,5),label=""daily deaths"") df.Deaths.rolling(window=5).mean().plot(figsize=(10,5),label=""MA5"") plt.ylabel(""Deaths"") plt.legend() plt.show() view raw covid19_cases.py hosted with ❤ by GitHub View this gist on GitHub Covid-19 Cases Prediction with Python for Next 30 Days Now, I will use the Facebook prophet model for the task of Covid-19 cases prediction with Python for the next 30 days. Facebook prophet model uses time series method for forecasting. Let’s see how we can use the Facebook prophet model for Covid-19 cases prediction with Python for the next 30 days: class Fbprophet(object): def fit(self,data): self.data = data self.model = Prophet(weekly_seasonality=True,daily_seasonality=False,yearly_seasonality=False) self.model.fit(self.data) def forecast(self,periods,freq): self.future = self.model.make_future_dataframe(periods=periods,freq=freq) self.df_forecast = self.model.predict(self.future) def plot(self,xlabel=""Years"",ylabel=""Values""): self.model.plot(self.df_forecast,xlabel=xlabel,ylabel=ylabel,figsize=(9,4)) self.model.plot_components(self.df_forecast,figsize=(9,6)) def R2(self): return r2_score(self.data.y, self.df_forecast.yhat[:len(df)]) df_fb = pd.DataFrame({""ds"":[],""y"":[]}) df_fb[""ds""] = pd.to_datetime(df.index) df_fb[""y""] = df.iloc[:,0].values model = Fbprophet() model.fit(df_fb) model.forecast(30,""D"") model.R2() forecast = model.df_forecast[[""ds"",""yhat_lower"",""yhat_upper"",""yhat""]].tail(30).reset_index().set_index(""ds"").drop(""index"",axis=1) forecast[""yhat""].plot(marker=""."",figsize=(10,5)) plt.fill_between(x=forecast.index, y1=forecast[""yhat_lower""], y2=forecast[""yhat_upper""],color=""gray"") plt.legend([""forecast"",""Bound""],loc=""upper left"") plt.title(""Forecasting of Next 30 Days Cases"") plt.show() view raw covid19_cases.py hosted with ❤ by GitHub View this gist on GitHub I hope you liked this article on Covid-19 cases predictions for the next 30 days with Python programming language. Feel free to ask your valuable questions in the comments section below.";Covid-19 Cases Prediction with Python
2020-11-30 19:49:28;In this article, I will take you through how to write a program to correct spellings with Python programming language. For this task, I will use an NLP library in Python known as TextBlob.;https://thecleverprogrammer.com/2020/11/30/correct-spellings-with-python/;['textblob'];1.0;['NLP'];['ML', 'Classification', 'NLP', 'Sentiment Analysis'];['sentiment analysis', 'model', 'machine learning', 'classif', 'natural language processing'];"In this article, I will take you through how to write a program to correct spellings with Python programming language. For this task, I will use an NLP library in Python known as TextBlob. What is TextBlob? TextBlob is a Python library for processing text data. It provides a simple API for delving into common natural language processing tasks such as tagging part of speech, extracting nominal sentences, analyzing feelings, classifying, translating, and more. Also, Read – 100+ Machine Learning Projects Solved and Explained. It provides some very useful features for Machine Learning projects like: Noun phrase extractionPart-of-speech taggingSentiment analysisClassificationTokenizationWord and phrase frequenciesParsingn-gramsWord inflexion and lemmatizationSpelling correctionAdd new models or languages through extensionsWordNet integration You can simply install the TextBlob library in your systems by writing a pip command; pip install textblob. Correct Spellings with Python Now, I will show you how to write a Python program to correct spellings: from textblob import TextBlob words = [""Machne"", ""Learnin""] corrected_words = [] for i in words: corrected_words.append(TextBlob(i)) print(""Wrong words :"", words) print(""Corrected Words are :"") for i in corrected_words: print(i.correct(), end="" "") view raw correct spellings.py hosted with ❤ by GitHub View this gist on GitHub Wrong words : ['Machne', 'Learnin'] Corrected Words are : Machine Learning So this is how we can write a python program using the TextBlob library for correcting spellings. This feature can be used in Natural language processing projects in Machine Learning. I hope you liked this article on how to write a program for correcting spellings with Python programming language. Feel free to ask your valuable questions in the comments section below.";Correct Spellings with Python
2020-12-01 09:44:58;In this article, I will take you through a Machine Learning project on Keyword Extraction with Python programming language. In machine learning, Keyword extraction is a task of Natural Language Processing.;https://thecleverprogrammer.com/2020/12/01/keyword-extraction-with-python/;['sklearn', 'vocabulary', 'nltk'];1.0;[];['ML', 'Text Classification', 'NLP', 'Classification'];['text classification', 'fit', 'model', 'machine learning', 'classif', 'natural language processing'];"In this article, I will take you through a Machine Learning project on Keyword Extraction with Python programming language. In machine learning, Keyword extraction is a task of Natural Language Processing. What is Keyword Extraction? Keyword extraction is defined as the task of Natural language processing that automatically identifies a set of terms to describe the subject of the text. This is an important method in information retrieval (IR) systems: keywords simplify and speed up research. Keyword extraction can be used to reduce text dimensionality for further text analysis (subject modeling text classification). Also, Read – 100+ Machine Learning Projects Solved and Explained. The task of keyword extraction can be used in automatically indexing data, summarizing text, or generating tag clouds with the most representative keywords. Machine Learning Project on Keyword Extraction with Python Now, in this section, I will take you through a Machine Learning project on Keyword Extraction with Python programming language. I will start by importing the necessary libraries and the dataset: Download Dataset import numpy as np # linear algebra import pandas as pd # data processing df = pd.read_csv('papers.csv')​x import numpy as np # linear algebraimport pandas as pd # data processingdf = pd.read_csv('papers.csv') This dataset contains 7 columns: id, year, title, even_type, pdf_name, abstract and paper_text. We are mainly interested in the paper_text which includes both the title and the abstract. The next step is to preprocess our textual data. For this task, I will use the NLTK library in Python: import re from nltk.corpus import stopwords from nltk.stem.wordnet import WordNetLemmatizer stop_words = set(stopwords.words('english')) ##Creating a list of custom stopwords new_words = [""fig"",""figure"",""image"",""sample"",""using"", ""show"", ""result"", ""large"", ""also"", ""one"", ""two"", ""three"", ""four"", ""five"", ""seven"",""eight"",""nine""] stop_words = list(stop_words.union(new_words)) def pre_process(text): # lowercase text=text.lower() #remove tags text=re.sub(""&lt;/?.*?&gt;"","" &lt;&gt; "",text) # remove special characters and digits text=re.sub(""(\\d|\\W)+"","" "",text) ##Convert to list from string text = text.split() # remove stopwords text = [word for word in text if word not in stop_words] # remove words less than three letters text = [word for word in text if len(word) >= 3] # lemmatize lmtzr = WordNetLemmatizer() text = [lmtzr.lemmatize(word) for word in text] return ' '.join(text) docs = df['paper_text'].apply(lambda x:pre_process(x)) view raw keyword.py hosted with ❤ by GitHub View this gist on GitHub Using TF-IDF TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases in proportion to the number of times a word appears in the document (Text Frequency – TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency – IDF). Using the tf-idf weighting scheme, the keywords are the words with the highest TF-IDF score. For this task, I’ll first use the CountVectorizer method in Scikit-learn to create a vocabulary and generate the word count: from sklearn.feature_extraction.text import CountVectorizer #docs = docs.tolist() #create a vocabulary of words, cv=CountVectorizer(max_df=0.95, # ignore words that appear in 95% of documents max_features=10000, # the size of the vocabulary ngram_range=(1,3) # vocabulary contains single words, bigrams, trigrams ) word_count_vector=cv.fit_transform(docs) view raw keyword.py hosted with ❤ by GitHub View this gist on GitHub Now I’m going to use the TfidfTransformer in Scikit-learn to calculate the reverse frequency of documents: from sklearn.feature_extraction.text import TfidfTransformer tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) tfidf_transformer.fit(word_count_vector) view raw keyword.py hosted with ❤ by GitHub View this gist on GitHub Now, we are ready for the final step. In this step, I will create a function for the task of Keyword Extraction with Python by using the Tf-IDF vectorization: def sort_coo(coo_matrix): tuples = zip(coo_matrix.col, coo_matrix.data) return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True) def extract_topn_from_vector(feature_names, sorted_items, topn=10): """"""get the feature names and tf-idf score of top n items"""""" #use only topn items from vector sorted_items = sorted_items[:topn] score_vals = [] feature_vals = [] for idx, score in sorted_items: fname = feature_names[idx] #keep track of feature name and its corresponding score score_vals.append(round(score, 3)) feature_vals.append(feature_names[idx]) #create a tuples of feature,score #results = zip(feature_vals,score_vals) results= {} for idx in range(len(feature_vals)): results[feature_vals[idx]]=score_vals[idx] return results # get feature names feature_names=cv.get_feature_names() def get_keywords(idx, docs): #generate tf-idf for the given document tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]])) #sort the tf-idf vectors by descending order of scores sorted_items=sort_coo(tf_idf_vector.tocoo()) #extract only the top n; n here is 10 keywords=extract_topn_from_vector(feature_names,sorted_items,10) return keywords def print_results(idx,keywords, df): # now print the results print(""\n=====Title====="") print(df['title'][idx]) print(""\n=====Abstract====="") print(df['abstract'][idx]) print(""\n===Keywords==="") for k in keywords: print(k,keywords[k]) idx=941 keywords=get_keywords(idx, docs) print_results(idx,keywords, df) view raw keyword.py hosted with ❤ by GitHub View this gist on GitHub ===Keywords=== update rule 0.344 update 0.285 auxiliary 0.212 non negative matrix 0.21 negative matrix 0.209 rule 0.192 nmf 0.183 multiplicative 0.175 matrix factorization 0.163 matrix 0.163 I hope you liked this article on the Machine Learning project on Keyword Extraction with Python programming language. Feel free to ask your valuable questions in the comments section below.";Keyword Extraction with Python
2020-12-04 12:09:14;In this article, I’m going to introduce you to a data science project on online shopping intention analysis with Python. The growing popularity of online shopping has led to the emergence of new economic activities. To be successful in a highly competitive eCommerce environment, it is essential to understand customers’ online purchase intent.;https://thecleverprogrammer.com/2020/12/04/online-shopping-intention-analysis-with-python/;['sklearn'];1.0;[];['ML', 'K-Means', 'Clustering', 'Supervised Learning'];['clustering', 'k-means', 'predict', 'fit', 'supervised learning', 'machine learning', 'label'];"In this article, I’m going to introduce you to a data science project on online shopping intention analysis with Python. The growing popularity of online shopping has led to the emergence of new economic activities. To be successful in a highly competitive eCommerce environment, it is essential to understand customers’ online purchase intent. Introduction to Online Shopping Intention Analysis In recent years, e-commerce has brought huge benefits to suppliers and consumers. Defined as the use of the Internet to sell products or services to individual consumers, e-commerce has profoundly changed the way people conduct their business. Also, Read – 100+ Machine Learning Projects Solved and Explained. Indeed, it has become an important full-fledged transaction channel. A recent survey of online shopping predicted that the total amount of direct sales to customers will exceed $ 240 billion by 2007. Major technological innovations in online shopping have changed transaction channels in the information age. With the growth of online shopping, it has become important to understand the factors that influence a consumer’s intention to buy from a website rather than just browse. This emerging topic is of interest to both academics and machine learning practitioners. Online Shopping Intention Analysis with Python In this section, I will take you through a Data Science Project on Online Shopping Intention analysis with Python. I will start with this task by importing the necessary libraries and the data: Download Dataset import numpy as np # linear algebra import pandas as pd # data processing import matplotlib.pyplot as plt import seaborn as sns import plotly as py import plotly.graph_objs as go # read the dataset data = pd.read_csv('online_shoppers_intention.csv') view raw online shopping.py hosted with ❤ by GitHub View this gist on GitHub Now let’s have a look at the missing values and fill them by using the fillna method in Python pandas: missing = data.isnull().sum() print(missing)​x missing = data.isnull().sum()print(missing) Administrative 14 Administrative_Duration 14 Informational 14 Informational_Duration 14 ProductRelated 14 ProductRelated_Duration 14 BounceRates 14 ExitRates 14 PageValues 0 SpecialDay 0 Month 0 OperatingSystems 0 Browser 0 Region 0 TrafficType 0 VisitorType 0 Weekend 0 Revenue 0 dtype: int64 data.fillna(0, inplace = True) data.fillna(0, inplace = True) Now have a look at product related bounce rates of customers: x = data.iloc[:, [5, 6]].values x.shape x = data.iloc[:, [5, 6]].valuesx.shape (12330, 2) Now let’s apply the K-elbow method to determine the number of clustering groups: from sklearn.cluster import KMeans wcss = [] for i in range(1, 11): km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0, algorithm = 'full', tol = 0.001) km.fit(x) labels = km.labels_ wcss.append(km.inertia_) plt.rcParams['figure.figsize'] = (13, 7) plt.plot(range(1, 11), wcss) plt.grid() plt.tight_layout() plt.title('The Elbow Method', fontsize = 20) plt.xlabel('No. of Clusters') plt.ylabel('wcss') plt.show() view raw online shopping.py hosted with ❤ by GitHub View this gist on GitHub K Means Clustering According to the graph above, the maximum curvature is at the second index, that is, the number of optimal clustering groups for the duration of the product and the bounce rates is 2. Once the number of clusterings determined, we apply the K Means method and plot the clusters: km = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0) # get predicted cluster index for each sample: 0, 1, 2 y_means = km.fit_predict(x) plt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 50, c = 'yellow', label = 'Uninterested Customers') plt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 50, c = 'pink', label = 'Target Customers') plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid') plt.title('ProductRelated Duration vs Bounce Rate', fontsize = 20) plt.grid() plt.xlabel('ProductRelated Duration') plt.ylabel('Bounce Rates') plt.legend() plt.show() view raw online shopping.py hosted with ❤ by GitHub View this gist on GitHub Looking at this K Means grouping plot, we can say with certainty that customers who spent more time on a product-related website are very less likely to leave the website after viewing a single page. Since K-Means is not a supervised learning method, we are adopting other ways of evaluating its clustering result. The leftmost column of the confusion matrix represents the actual label (True or False revenue), and the top row represents the expected clustering groups (uninterested customers or target customers): from sklearn.preprocessing import LabelEncoder le = LabelEncoder() labels_true = le.fit_transform(data['Revenue']) # get predicted clustering result label labels_pred = y_means # print adjusted rand index, which measures the similarity of the two assignments from sklearn import metrics score = metrics.adjusted_rand_score(labels_true, labels_pred) print(""Adjusted rand index: "") print(score) # print confusion matrix #cm = metrics.plot_confusion_matrix(None, labels_true, labels_pred) #print(cm) import scikitplot as skplt plt_1 = skplt.metrics.plot_confusion_matrix(labels_true, labels_pred, normalize=False) plt_2 = skplt.metrics.plot_confusion_matrix(labels_true, labels_pred, normalize=True) view raw online shopping.py hosted with ❤ by GitHub View this gist on GitHub Observations From Above Plots: From the confusion matrix, we can see that out of 10,422 failed incomes, 9,769 are grouped into uninterested customers or 94%. However, out of 937 successful incomes, only 284 are grouped as target customers or 15%. Also, the adjusted index score is not very high. So it is clear that we have poorly bundled many successful revenue sessions as uninterested customers, which means when the high bounce rate combined with a short product-related page duration, there are still a lot of customers. targets. I hope you liked this article on Data Science Project on Online Shopping Intention Analysis with Python programming language. Feel free to ask your valuable questions in the comments section below.";Online Shopping Intention Analysis with Python
2020-12-05 12:25:55;In this article, I will introduce you to a machine learning project on sign language classification with Python. Sign language is a visual way of communicating through hand signals, gestures, facial expressions, and body language.Sign language is the primary form of communication for the deaf and hard of hearing community, but sign language can be useful for other groups of people as well. People with disabilities, including autism, apraxia of speech, cerebral palsy, and Down syndrome, may also find sign language beneficial for communication.Also, Read – 100+ Machine Learning Projects Solved and Explained.;https://thecleverprogrammer.com/2020/12/05/sign-language-classification-with-python/;['keras', 'sklearn', 'tensorflow'];1.0;[];['ML', 'Classification', 'ReLu', 'NN'];['epoch', 'predict', 'fit', 'model', 'loss', 'machine learning', 'neural network', 'classif', 'layer', 'relu', 'train', 'label'];"In this article, I will introduce you to a machine learning project on sign language classification with Python. Sign language is a visual way of communicating through hand signals, gestures, facial expressions, and body language. Sign language is the primary form of communication for the deaf and hard of hearing community, but sign language can be useful for other groups of people as well. People with disabilities, including autism, apraxia of speech, cerebral palsy, and Down syndrome, may also find sign language beneficial for communication. Also, Read – 100+ Machine Learning Projects Solved and Explained. Machine Learning Project on Sign Language Classification with Python In this section, I will introduce you to a machine learning project on the Sign Language classification with Python programming language. The dataset I’ll be using here is a montage panel of cropped images from various users and backgrounds for American Sign Language letters. This dataset was inspired by Fashion-MNIST and Sreehari’s machine learning pipeline for gestures. Now let’s start with the task of classifying sign language by importing the necessary Python libraries and the dataset: Download Dataset import numpy as np # linear algebra import pandas as pd # data processing import numpy as np import tensorflow as tf from tensorflow.keras.preprocessing.image import ImageDataGenerator from os import getcwd data_train=pd.read_csv('sign_mnist_train.csv') data_test=pd.read_csv('sign_mnist_test.csv') view raw sign language.py hosted with ❤ by GitHub View this gist on GitHub Now I will move to the task of data preparation for our machine learning model. First I will prepare the data then I will split the data into training and test sets: # In this section you will have to add another dimension to the data # So, for example, if your array is (10000, 28, 28) # You will need to make it (10000, 28, 28, 1) #training_images = np.expand_dims(training_images, axis=3) #testing_images = np.expand_dims(testing_images, axis=3) training_images = data_train.iloc[:,1:].values training_labels = data_train.iloc[:,0].values testing_images = data_test.iloc[:,1:].values testing_labels = data_test.iloc[:,0].values training_images = training_images.reshape(-1,28,28,1) testing_images = testing_images.reshape(-1,28,28,1) print(training_images.shape) print(training_labels.shape) print(testing_images.shape) print(testing_labels.shape) # Their output should be: # (27455, 28, 28, 1) # (27455,) # (7172, 28, 28, 1) # (7172,) view raw sign language.py hosted with ❤ by GitHub View this gist on GitHub Now let’s have a look at the first 10 images in the dataset: # Plotting the first 10 images fig, ax = plt.subplots(2,5) fig.set_size_inches(10, 10) k = 0 for i in range(2): for j in range(5): ax[i,j].imshow(training_images[k].reshape(28, 28) , cmap = ""gray"") k += 1 plt.tight_layout() view raw sign language.py hosted with ❤ by GitHub View this gist on GitHub Now, I will create an ImageDataGenerator and do Image Augmentation of the dataset: # Create an ImageDataGenerator and do Image Augmentation train_datagen = ImageDataGenerator( rescale=1. / 255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest' ) validation_datagen = ImageDataGenerator( rescale=1 / 255 ) # Keep These print(training_images.shape) print(testing_images.shape) # Their output should be: # (27455, 28, 28, 1) # (7172, 28, 28, 1) view raw sign language.py hosted with ❤ by GitHub View this gist on GitHub Training Neural Network Now, I will train a Neural network model for the task of Sign Language classification with Python: model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(32, (3,3), activation='relu'), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(512, activation='relu'), tf.keras.layers.Dense(26, activation='softmax') ]) # Compile Model. model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'] ) # Train the Model history = model.fit_generator(train_datagen.flow(training_images, training_labels, batch_size=32), steps_per_epoch=len(training_images) / 32, epochs=10, validation_data=validation_datagen.flow(testing_images, testing_labels, batch_size=32), validation_steps=len(testing_images) / 32) model.evaluate(testing_images, testing_labels, verbose=0) view raw sign language.py hosted with ❤ by GitHub View this gist on GitHub Now let’s have a look at the accuracy and validation score of the model: import matplotlib.pyplot as plt acc = history.history['accuracy'] val_acc = history.history['val_accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(len(acc)) plt.plot(epochs, acc, 'r', label='Training accuracy') plt.plot(epochs, val_acc, 'b', label='Validation accuracy') plt.title('Training and validation accuracy') plt.legend(loc=0) plt.figure() plt.plot(epochs, loss, 'r', label='Training Loss') plt.plot(epochs, val_loss, 'b', label='Validation Loss') plt.title('Training and validation loss') plt.legend() plt.show() view raw sign language.py hosted with ❤ by GitHub View this gist on GitHub Now let’s make a classification report and test our model: # Predictions predictions = model.predict_classes(testing_images) for i in range(len(predictions)): if(predictions[i] >= 9): predictions[i] += 1 predictions[:5] #Output #array([ 6, 8, 11, 14, 18]) # Precision, recall, f1-score for all the classes from sklearn.metrics import classification_report, confusion_matrix classes = [""Class "" + str(i) for i in range(26) if i != 9] print(classification_report(data_test['label'], predictions, target_names = classes)) # Confusion Matrix for the model predictions cm = confusion_matrix(data_test['label'],predictions) plt.figure(figsize=(12,7)) g = sns.heatmap(cm, cmap='Reds',annot=True, fmt='') view raw sign language.py hosted with ❤ by GitHub View this gist on GitHub I hope you liked this article on Machine Learning project on Sign Language Classification with Python programming language. Feel free to ask your valuable questions in the comments section below.";Sign Language Classification with Python
2020-12-06 19:05:33;Companies often receive thousands of resumes for each job posting and employ dedicated screening officers to screen qualified candidates. In this article, I will introduce you to a machine learning project on Resume Screening with Python programming language.Hiring the right talent is a challenge for all businesses. This challenge is magnified by the high volume of applicants if the business is labour-intensive, growing, and facing high attrition rates.Also, Read – 100+ Machine Learning Projects Solved and Explained.An example of such a business is that IT departments are short of growing markets. In a typical service organization, professionals with a variety of technical skills and business domain expertise are hired and assigned to projects to resolve customer issues. This task of selecting the best talent among many others is known as Resume Screening.Typically, large companies do not have enough time to open each CV, so they use machine learning algorithms for the Resume Screening task.;https://thecleverprogrammer.com/2020/12/06/resume-screening-with-python/;['sklearn', 'nltk'];1.0;['CV', 'RL'];['ML', 'CV', 'Classification', 'RL'];['predict', 'fit', 'model', 'machine learning', 'classif', 'filter', 'train', 'label'];"Companies often receive thousands of resumes for each job posting and employ dedicated screening officers to screen qualified candidates. In this article, I will introduce you to a machine learning project on Resume Screening with Python programming language. What is Resume Screening? Hiring the right talent is a challenge for all businesses. This challenge is magnified by the high volume of applicants if the business is labour-intensive, growing, and facing high attrition rates. Also, Read – 100+ Machine Learning Projects Solved and Explained. An example of such a business is that IT departments are short of growing markets. In a typical service organization, professionals with a variety of technical skills and business domain expertise are hired and assigned to projects to resolve customer issues. This task of selecting the best talent among many others is known as Resume Screening. Typically, large companies do not have enough time to open each CV, so they use machine learning algorithms for the Resume Screening task. Machine Learning Project on Resume Screening with Python In this section, I will take you through a Machine Learning project on Resume Screening with Python programming language. I will start this task by importing the necessary Python libraries and the dataset: Download Dataset import numpy as np import pandas as pd import matplotlib.pyplot as plt import warnings warnings.filterwarnings('ignore') from sklearn.naive_bayes import MultinomialNB from sklearn.multiclass import OneVsRestClassifier from sklearn import metrics from sklearn.metrics import accuracy_score from pandas.plotting import scatter_matrix from sklearn.neighbors import KNeighborsClassifier from sklearn import metrics resumeDataSet = pd.read_csv('UpdatedResumeDataSet.csv' ,encoding='utf-8') resumeDataSet['cleaned_resume'] = '' resumeDataSet.head() view raw resume screening.py hosted with ❤ by GitHub View this gist on GitHub Now let’s have a quick look at the categories of resumes present in the dataset: print (""Displaying the distinct categories of resume -"") print (resumeDataSet['Category'].unique())​x print (""Displaying the distinct categories of resume -"")print (resumeDataSet['Category'].unique()) Displaying the distinct categories of resume - ['Data Science' 'HR' 'Advocate' 'Arts' 'Web Designing' 'Mechanical Engineer' 'Sales' 'Health and fitness' 'Civil Engineer' 'Java Developer' 'Business Analyst' 'SAP Developer' 'Automation Testing' 'Electrical Engineering' 'Operations Manager' 'Python Developer' 'DevOps Engineer' 'Network Security Engineer' 'PMO' 'Database' 'Hadoop' 'ETL Developer' 'DotNet Developer' 'Blockchain' 'Testing'] Now let’s have a look at the distinct categories of resume and the number of records belonging to each category: print (""Displaying the distinct categories of resume and the number of records belonging to each category -"") print (resumeDataSet['Category'].value_counts()) print (""Displaying the distinct categories of resume and the number of records belonging to each category -"")print (resumeDataSet['Category'].value_counts()) Displaying the distinct categories of resume and the number of records belonging to each category - Java Developer 84 Testing 70 DevOps Engineer 55 Python Developer 48 Web Designing 45 HR 44 Hadoop 42 Mechanical Engineer 40 Sales 40 ETL Developer 40 Blockchain 40 Operations Manager 40 Data Science 40 Arts 36 Database 33 Electrical Engineering 30 Health and fitness 30 PMO 30 DotNet Developer 28 Business Analyst 28 Automation Testing 26 Network Security Engineer 25 SAP Developer 24 Civil Engineer 24 Advocate 20 Name: Category, dtype: int64 Now let’s visualize the number of categories in the dataset: import seaborn as sns plt.figure(figsize=(15,15)) plt.xticks(rotation=90) sns.countplot(y=""Category"", data=resumeDataSet) import seaborn as snsplt.figure(figsize=(15,15))plt.xticks(rotation=90)sns.countplot(y=""Category"", data=resumeDataSet) Now let’s visualize the distribution of categories: from matplotlib.gridspec import GridSpec targetCounts = resumeDataSet['Category'].value_counts() targetLabels = resumeDataSet['Category'].unique() # Make square figures and axes plt.figure(1, figsize=(25,25)) the_grid = GridSpec(2, 2) cmap = plt.get_cmap('coolwarm') colors = [cmap(i) for i in np.linspace(0, 1, 3)] plt.subplot(the_grid[0, 1], aspect=1, title='CATEGORY DISTRIBUTION') source_pie = plt.pie(targetCounts, labels=targetLabels, autopct='%1.1f%%', shadow=True, colors=colors) plt.show() view raw resume screening.py hosted with ❤ by GitHub View this gist on GitHub Now I will create a helper function to remove the URLs, hashtags, mentions, special letters, and punctuations: import re def cleanResume(resumeText): resumeText = re.sub('http\S+\s*', ' ', resumeText) # remove URLs resumeText = re.sub('RT|cc', ' ', resumeText) # remove RT and cc resumeText = re.sub('#\S+', '', resumeText) # remove hashtags resumeText = re.sub('@\S+', ' ', resumeText) # remove mentions resumeText = re.sub('[%s]' % re.escape(""""""!""#$%&'()*+,-./:;<=>?@[\]^_`{|}~""""""), ' ', resumeText) # remove punctuations resumeText = re.sub(r'[^\x00-\x7f]',r' ', resumeText) resumeText = re.sub('\s+', ' ', resumeText) # remove extra whitespace return resumeText resumeDataSet['cleaned_resume'] = resumeDataSet.Resume.apply(lambda x: cleanResume(x)) view raw resume screening.py hosted with ❤ by GitHub View this gist on GitHub Now as we have cleared the dataset, the next task is to have a look at the Wordcloud. A Wordcloud represents the most numbers of words larger and vice versa: import nltk from nltk.corpus import stopwords import string from wordcloud import WordCloud oneSetOfStopWords = set(stopwords.words('english')+['``',""''""]) totalWords =[] Sentences = resumeDataSet['Resume'].values cleanedSentences = """" for i in range(0,160): cleanedText = cleanResume(Sentences[i]) cleanedSentences += cleanedText requiredWords = nltk.word_tokenize(cleanedText) for word in requiredWords: if word not in oneSetOfStopWords and word not in string.punctuation: totalWords.append(word) wordfreqdist = nltk.FreqDist(totalWords) mostcommon = wordfreqdist.most_common(50) print(mostcommon) wc = WordCloud().generate(cleanedSentences) plt.figure(figsize=(15,15)) plt.imshow(wc, interpolation='bilinear') plt.axis(""off"") plt.show() view raw resume screening.py hosted with ❤ by GitHub View this gist on GitHub [('Details', 484), ('Exprience', 446), ('months', 376), ('company', 330), ('description', 310), ('1', 290), ('year', 232), ('January', 216), ('Less', 204), ('Data', 200), ('data', 192), ('Skill', 166), ('Maharashtra', 166), ('6', 164), ('Python', 156), ('Science', 154), ('I', 146), ('Education', 142), ('College', 140), ('The', 126), ('project', 126), ('like', 126), ('Project', 124), ('Learning', 116), ('India', 114), ('Machine', 112), ('University', 112), ('Web', 106), ('using', 104), ('monthsCompany', 102), ('B', 98), ('C', 98), ('SQL', 96), ('time', 92), ('learning', 90), ('Mumbai', 90), ('Pune', 90), ('Arts', 90), ('A', 84), ('application', 84), ('Engineering', 78), ('24', 76), ('various', 76), ('Software', 76), ('Responsibilities', 76), ('Nagpur', 76), ('development', 74), ('Management', 74), ('projects', 74), ('Technologies', 72)] Now I will convert these words into categorical values: from sklearn.preprocessing import LabelEncoder var_mod = ['Category'] le = LabelEncoder() for i in var_mod: resumeDataSet[i] = le.fit_transform(resumeDataSet[i]) view raw resume screening.py hosted with ❤ by GitHub View this gist on GitHub Training Machine Learning Model for Resume Screening Now the next step in the process is to train a model for the task of Resume Screening. Here I will use the one vs the rest classifier; KNeighborsClassifier. For this task, I will first split the data into training and test sets: from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from scipy.sparse import hstack requiredText = resumeDataSet['cleaned_resume'].values requiredTarget = resumeDataSet['Category'].values word_vectorizer = TfidfVectorizer( sublinear_tf=True, stop_words='english', max_features=1500) word_vectorizer.fit(requiredText) WordFeatures = word_vectorizer.transform(requiredText) print (""Feature completed ....."") X_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,random_state=0, test_size=0.2) print(X_train.shape) print(X_test.shape) view raw resume screening.py hosted with ❤ by GitHub View this gist on GitHub Now let’s train the model and print the classification report: clf = OneVsRestClassifier(KNeighborsClassifier()) clf.fit(X_train, y_train) prediction = clf.predict(X_test) print('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train))) print('Accuracy of KNeighbors Classifier on test set: {:.2f}'.format(clf.score(X_test, y_test))) print(""\n Classification report for classifier %s:\n%s\n"" % (clf, metrics.classification_report(y_test, prediction))) view raw resume screening.py hosted with ❤ by GitHub View this gist on GitHub Accuracy of KNeighbors Classifier on training set: 0.99 Accuracy of KNeighbors Classifier on test set: 0.99 Classification report for classifier OneVsRestClassifier(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform'), n_jobs=None): precision recall f1-score support 0 1.00 1.00 1.00 3 1 1.00 1.00 1.00 3 2 1.00 0.80 0.89 5 3 1.00 1.00 1.00 9 4 1.00 1.00 1.00 6 5 0.83 1.00 0.91 5 6 1.00 1.00 1.00 9 7 1.00 1.00 1.00 7 8 1.00 0.91 0.95 11 9 1.00 1.00 1.00 9 10 1.00 1.00 1.00 8 11 0.90 1.00 0.95 9 12 1.00 1.00 1.00 5 13 1.00 1.00 1.00 9 14 1.00 1.00 1.00 7 15 1.00 1.00 1.00 19 16 1.00 1.00 1.00 3 17 1.00 1.00 1.00 4 18 1.00 1.00 1.00 5 19 1.00 1.00 1.00 6 20 1.00 1.00 1.00 11 21 1.00 1.00 1.00 4 22 1.00 1.00 1.00 13 23 1.00 1.00 1.00 15 24 1.00 1.00 1.00 8 micro avg 0.99 0.99 0.99 193 macro avg 0.99 0.99 0.99 193 weighted avg 0.99 0.99 0.99 193 So this is how we can train a Machine Learning model for the task of Resume Screening. I hope you liked this article on Resume Screening with Python programming language. Feel free to ask your valuable questions in the comments section below.";Resume Screening with Python
2020-12-07 19:01:25;In Machine Learning, Sentiment analysis refers to the application of natural language processing, computational linguistics, and text analysis to identify and classify subjective opinions in source documents. In this article, I will introduce you to a machine learning project on sentiment analysis with the Python programming language.;https://thecleverprogrammer.com/2020/12/07/sentiment-analysis-with-python/;['sklearn', 'nltk'];1.0;['CV'];['CV', 'Regression', 'ML', 'Sentiment Analysis', 'NLP', 'Logistic Regression', 'Classification'];['regression', 'sentiment analysis', 'predict', 'fit', 'model', 'machine learning', 'logistic regression', 'classif', 'layer', 'natural language processing', 'train', 'label'];"In Machine Learning, Sentiment analysis refers to the application of natural language processing, computational linguistics, and text analysis to identify and classify subjective opinions in source documents. In this article, I will introduce you to a machine learning project on sentiment analysis with the Python programming language. What is Sentiment Analysis? Sentiment analysis aims to determine a writer’s attitude towards a topic or the overall contextual polarity of a document. The attitude can be his judgment or assessment, his emotional state or the intended emotional communication. Also, Read – 100+ Machine Learning Projects Solved and Explained. In sentiment analysis, the main task is to identify opinion words, which is very important. Opinion words are dominant indicators of feelings, especially adjectives, adverbs, and verbs, for example: “I love this camera. It’s amazing!” Opinion words are also known as polarity words, sentiment words, opinion lexicon, or opinion words, which can generally be divided into two types: positive words, for example, wonderful. , elegant, astonishing; and negative words, eg horrible, disgusting, poor. Machine Learning Project on Sentiment Analysis with Python Now in this section, I will take you through a Machine Learning project on sentiment analysis with Python programming language. Let’s start by importing all the necessary Python libraries and the dataset: Download Dataset import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.feature_extraction.text import CountVectorizer count=CountVectorizer() data=pd.read_csv(""Train.csv"") data.head() view raw sentiment.py hosted with ❤ by GitHub View this gist on GitHub textlabel0I grew up (b. 1965) watching and loving the Th…01When I put this movie in my DVD player, and sa…02Why do people who do not know what a particula…03Even though I have great interest in Biblical …04Im a die hard Dads Army fan and nothing will e…1 After reading the dataset which contains 40k movie reviews from IMDB, we see that there are two prominent columns. One being TEXT which contains the criticism and the other being LABEL which contains the O’s and 1’s, where 0-NEGATIVE and 1-POSITIVE. Now let’s visualize the distribution of the data: fig=plt.figure(figsize=(5,5)) colors=[""skyblue"",'pink'] pos=data[data['label']==1] neg=data[data['label']==0] ck=[pos['label'].count(),neg['label'].count()] legpie=plt.pie(ck,labels=[""Positive"",""Negative""], autopct ='%1.1f%%', shadow = True, colors = colors, startangle = 45, explode=(0, 0.1)) view raw sentiment.py hosted with ❤ by GitHub View this gist on GitHub Then we will import RE, that is, the regular expression operation, we use this library to remove html tags like ‘<a>’ or. So whenever we come across these tags, we replace them with an empty string. Then we will also modify the emojis/emoticons which can be smileys :), a sad face: (or even an upset face: /. We will change the emojis towards the end to get a clean set of text: import re def preprocessor(text): text=re.sub('<[^>]*>','',text) emojis=re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)',text) text=re.sub('[\W]+',' ',text.lower()) +\ ' '.join(emojis).replace('-','') return text data['text']=data['text'].apply(preprocessor) view raw sentiment.py hosted with ❤ by GitHub View this gist on GitHub Now, I’ll be using nltk’s PorterStemmer to simplify the data and remove unnecessary complexities in our text data: from nltk.stem.porter import PorterStemmer porter=PorterStemmer() def tokenizer(text): return text.split() def tokenizer_porter(text): return [porter.stem(word) for word in text.split()] view raw sentiment.py hosted with ❤ by GitHub View this gist on GitHub Visualizing Negative and Positive Words To visualzie the negative and postive words using a wordcloud, I will first remove the stopwords: import nltk nltk.download('stopwords') from nltk.corpus import stopwords stop=stopwords.words('english') from wordcloud import WordCloud positivedata = data[ data['label'] == 1] positivedata =positivedata['text'] negdata = data[data['label'] == 0] negdata= negdata['text'] def wordcloud_draw(data, color = 'white'): words = ' '.join(data) cleaned_word = "" "".join([word for word in words.split() if(word!='movie' and word!='film') ]) wordcloud = WordCloud(stopwords=stop, background_color=color, width=2500, height=2000 ).generate(cleaned_word) plt.figure(1,figsize=(10, 7)) plt.imshow(wordcloud) plt.axis('off') plt.show() print(""Positive words are as follows"") wordcloud_draw(positivedata,'white') print(""Negative words are as follows"") wordcloud_draw(negdata) view raw sentiment.py hosted with ❤ by GitHub View this gist on GitHub The positive words that are highlighted are love, excellent, perfect, good, beautiful, kind, excellent and The negative words that are highlighted are: horrible, wasteful, problem, stupid, horrible, bad, poor. Now I will use the TF-IDF Vertorizer to convert the raw documents into feature matrix which is very important to train a Machine Learning model: from sklearn.feature_extraction.text import TfidfVectorizer tfidf=TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None,tokenizer=tokenizer_porter,use_idf=True,norm='l2',smooth_idf=True) y=data.label.values x=tfidf.fit_transform(data.text)​x from sklearn.feature_extraction.text import TfidfVectorizer​tfidf=TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None,tokenizer=tokenizer_porter,use_idf=True,norm='l2',smooth_idf=True)y=data.label.valuesx=tfidf.fit_transform(data.text) Training Machine Learning Model for Sentiment Analysis Now to train a machine learning model I will split the data into 50 percent training and 50 percent test sets: from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.5,shuffle=False) from sklearn.model_selection import train_test_splitX_train,X_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.5,shuffle=False) Now let’s train a machine learning model for the task of sentiment analysis by using the Logistic Regression model: from sklearn.linear_model import LogisticRegressionCV clf=LogisticRegressionCV(cv=6,scoring='accuracy',random_state=0,n_jobs=-1,verbose=3,max_iter=500).fit(X_train,y_train) y_pred = clf.predict(X_test) from sklearn import metrics # Model Accuracy, how often is the classifier correct? print(""Accuracy:"",metrics.accuracy_score(y_test, y_pred)) view raw sentiment.py hosted with ❤ by GitHub View this gist on GitHub Accuracy: 0.89045 I hope you liked this article on Sentiment Analysis with Python programming language. Feel free to ask your valuable questions in the comments section below.";Sentiment Analysis with Python
