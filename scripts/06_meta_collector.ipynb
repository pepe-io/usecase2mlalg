{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import platform\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.099\n",
      "0.302\n",
      "0.387\n",
      "0.446\n",
      "0.5\n",
      "0.645\n",
      "0.749\n"
     ]
    }
   ],
   "source": [
    "# scoring function to get a score between 0...1 for integer-values, 0.5 should be at ~100\n",
    "def score(n, precision=3):\n",
    "    if isinstance(n, int) or isinstance(i, float):\n",
    "        return round(1-1/math.pow(1+n, 0.15), precision)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for n in [0,1,10,25,50,100,1000,10000]:\n",
    "    print(score(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-12 16:08:03\n"
     ]
    }
   ],
   "source": [
    "# get file modifictaion date\n",
    "# https://stackoverflow.com/questions/237079/how-to-get-file-creation-modification-date-times-in-python\n",
    "\n",
    "def creation_date(path_to_file, datetime = True):\n",
    "    \"\"\"\n",
    "    Try to get the date that a file was created, falling back to when it was\n",
    "    last modified if that isn't possible.\n",
    "    See http://stackoverflow.com/a/39501288/1709587 for explanation.\n",
    "    \"\"\"\n",
    "    if platform.system() == 'Windows':\n",
    "        timestamp = os.path.getctime(path_to_file)\n",
    "    else:\n",
    "        stat = os.stat(path_to_file)\n",
    "        try:\n",
    "            timestamp = stat.st_birthtime\n",
    "        except AttributeError:\n",
    "            # We're probably on Linux. No easy way to get creation dates here,\n",
    "            # so we'll settle for when its content was last modified.\n",
    "            timestamp = stat.st_mtime\n",
    "        \n",
    "    if datetime == True:\n",
    "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(timestamp))\n",
    "\n",
    "    return timestamp\n",
    "    \n",
    "folder_base = '../data/repositories/kaggle/competitions/c/'\n",
    "folder = '3d-object-detection-for-autonomous-vehicles/notebooks/asimandia/lyft3d-inference-kernel/'\n",
    "notebook = 'notebook_02.html'\n",
    "kernel = 'kernel.html'\n",
    "print(creation_date(folder_base+folder+notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches ['lstm', 'anomaly', 'convolutional neural network', 'neural network', 'detect', 'object detection', 'ML']\n",
      "tags ['Tag    ML\\nName: ML, dtype: object']\n",
      "['Tag    ML\\nName: ML, dtype: object', 'Tag    NLP\\nName: NLP, dtype: object']\n"
     ]
    }
   ],
   "source": [
    "# improved version (full text scan)\n",
    "# scan text for predefined terms\n",
    "\n",
    "text = 'We use LSTM for anomaly and object detection. As Convolutional Neural Networks are great for ML.'\n",
    "\n",
    "pd_ml_terms = pd.read_csv('../data/patterns/ml_terms.csv')\n",
    "ml_terms = pd_ml_terms['Term'].tolist()\n",
    "ml_slugs = pd_ml_terms['Slug'].tolist()\n",
    "ml_slugs = [x for x in ml_slugs if str(x) != 'nan']\n",
    "ml_tags = pd_ml_terms['Tag'].tolist()\n",
    "ml_tags = [x for x in ml_tags if str(x) != 'nan']\n",
    "\n",
    "#print(ml_tags)\n",
    "\n",
    "ml_libs = pd.read_csv('../data/patterns/ml_libraries.csv')\n",
    "ml_libs = ml_libs['Python Package'].tolist()\n",
    "\n",
    "def match_text(haystack, needles, toLower = False, unique = True):\n",
    "    \n",
    "    if toLower == True:\n",
    "        haystack = haystack.lower()\n",
    "        needles = [x.lower() for x in needles]\n",
    "    \n",
    "    if unique == True:\n",
    "        matches = {x for x in needles if x in haystack}\n",
    "        matches = list(matches)\n",
    "    else:\n",
    "        matches = [x for x in needles if x in haystack]\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def match_tags(haystack):\n",
    "    df = pd.read_csv('../data/patterns/ml_terms.csv')\n",
    "    tags = []\n",
    "    \n",
    "    df.set_index('Term', inplace = True)\n",
    "    for item in haystack:\n",
    "        try:\n",
    "            tag = df.loc[item].get('Tag').head(1)\n",
    "            if not 'nan' in str(tag):\n",
    "                tags.append(tag)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    df.set_index('Slug', inplace = True)\n",
    "    for item in haystack:\n",
    "        try:\n",
    "            tag = df.loc[item]\n",
    "            if not 'nan' in str(tag):\n",
    "                tags.append(str(tag))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    #if 'ANN' in tags or 'CNN' in tags or 'RNN' in tags:\n",
    "    #    tags.remove('NN')\n",
    "    \n",
    "    return list(tags)\n",
    "\n",
    "#ml_slugs, ml_terms, ml_libs, match_text(haystack, needles, toLower = False, unique = True)\n",
    "needles = {\n",
    "    'ml_slugs': ml_slugs,\n",
    "    'ml_terms': ml_terms,\n",
    "    'ml_libs': ml_libs,\n",
    "}\n",
    "needles_need_str_lower = {\n",
    "    'ml_slugs': False,\n",
    "    'ml_terms': True,\n",
    "    'ml_libs': False,\n",
    "}\n",
    "\n",
    "matches = []\n",
    "\n",
    "matches.extend(match_text(text, ml_terms, True))\n",
    "matches.extend(match_text(text, ml_slugs, False))\n",
    "print('matches', matches)\n",
    "\n",
    "tags = match_tags(matches)\n",
    "print('tags', tags)\n",
    "\n",
    "tags_test = ['ML', 'NLP', 'classif', 'convolutional neural network', 'decision tree', 'layer', 'machine learning', 'model', 'naive bayes', 'natural language processing', 'neural network', 'predict', 'regression', 'train']\n",
    "print(match_tags(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 0: 1davegalloway SchoolDistrictAnalysis\n",
      "### 1: A7med01 Deep-learning-for-Animal-Identification\n",
      "### 2: AayushG159 Plant-Leaf-Identification\n",
      "### 3: aayushmudgal Reducing-Manufacturing-Failures\n",
      "### 4: ab-bh Disease-Outbreak-Prediction\n",
      "### 5: abhiagar90 power_networks\n",
      "### 6: abjer sds\n",
      "### 7: abuchowdhury Mortgage_Bank_Loan_Analtsics\n",
      "### 8: AccelAI AI-Law-Minicourse\n",
      "### 9: aditink EMSRouting\n",
      "### 10: adrianakopf NJPublicSchools\n",
      "### 11: aeronetlab emergency-mapping\n",
      "### 12: akarazeev LegalTech\n",
      "### 13: Akesari12 LS123_Data_Prediction_Law_Spring-2019\n",
      "### 14: akpen Stockholm-0.1\n",
      "### 15: AlanConstantine KDD-Cup-2019-CAMMTR\n",
      "### 16: albahnsen ML_RiskManagement\n",
      "### 17: albertwebson Political-Vector-Projector\n",
      "### 18: albiboni AileronSimulation\n",
      "### 19: alifier Restaurant_success_model\n",
      "### 20: alistairwallace97 olympian-biotech\n",
      "### 21: altosaar food2vec\n",
      "### 22: aluo417 Financial-Engineering-Projects\n",
      "### 23: AlvaroMenduina Jupyter_Notebooks\n",
      "### 24: am-aditya Artificial-Intelligence-for-Banking\n",
      "### 25: amirrezaeian Individual-household-electric-power-consumption-Data-Set-\n",
      "### 26: amunategui Leak-At-Chemical-Factory-RL\n",
      "### 27: analystiu LICT-Project-Emergency-911-Calls\n",
      "### 28: ancilcrayton nlp_public_policy\n",
      "### 29: andrei-rizoiu hip-popularity\n",
      "### 30: andrey-lukyanov Risk-Management\n",
      "### 31: anki1909 Recruit-Restaurant-Visitor-Forecasting\n",
      "### 32: ankitkariryaa ambulanceSiteLocation\n",
      "### 33: Ankushr785 Food-amenities-demand-prediction\n",
      "### 34: anshu3769 FirmEmbeddings\n",
      "### 35: apbecker Systemic_Risk\n",
      "### 36: apoorv-goel Bank-Note-Authentication-Using-DNN-Tensorflow-Classifier-and-RandomForest\n",
      "### 37: arcadynovosyolov finance\n",
      "### 38: arcadynovosyolov reverse_stress_testing\n",
      "### 39: Architectshwet Amazon-Fine-Food-Reviews\n",
      "### 40: argha48 smarthotels\n",
      "### 41: arijitsaha FloodRisk\n",
      "### 42: arogozhnikov hep_ml\n",
      "### 43: arrahman17 Learning-Analytics-Project-\n",
      "### 44: arvindkarir retail\n",
      "### 45: aspuru-guzik-group chemical_vae\n",
      "### 46: austinbrian portfolio\n",
      "### 47: austinbrian sheriffs\n",
      "### 48: austinpetsalive distemper-outbreak\n",
      "### 49: avsolatorio world-bank-pover-t-tests-solution\n",
      "### 50: awoziji pwc_europe_data_analytics_hackathon\n",
      "### 51: awracms awra_cms_older\n",
      "### 52: Azure lstms_for_predictive_maintenance\n",
      "### 53: bahuisman NatGasModel\n",
      "### 54: bayesimpact bayeshack-transportation-ems\n",
      "### 55: benattix philly-schools\n",
      "### 56: benjaminsingleton crime-trends\n",
      "### 57: bhaveshgoyal safeLiquor\n",
      "### 58: borisbanushev stockpredictionai\n",
      "### 59: bradleyrobinson School-Performance\n",
      "### 60: Brett777 Predict-Risk\n",
      "### 61: BrianChevalier StructPy\n",
      "### 62: brightmart ai_law\n",
      "### 63: brightmart sentiment_analysis_fine_grain\n",
      "### 64: bschreck robo-chef\n",
      "### 65: budach pysster\n",
      "### 66: buddyd16 Structural-Engineering\n",
      "### 67: bukosabino financial-forecasting-challenge-gresearch\n",
      "### 68: burkesquires python_biologist\n",
      "### 69: buzz11 productionFailures\n",
      "### 70: byukan Marketing-Data-Science\n",
      "### 71: bzjin menus\n",
      "### 72: CAChemE learn\n",
      "### 73: cadrev lstm-flood-prediction\n",
      "### 74: callysto curriculum-notebooks\n",
      "### 75: carolineh101 deep-learning-architecture\n",
      "### 76: catherhuang FP3-recipe\n",
      "### 77: cavaunpeu flight-delays\n",
      "### 78: cep-kse natural_gas_formula\n",
      "### 79: CharlesHoffmanCPA charleshoffmanCPA.github.io\n",
      "### 80: chen-bowen Data_Science_in_Applied_Corporate_Finance\n",
      "### 81: chen-bowen Research_Documents_Curation_with_NLP\n",
      "### 82: chenbowen184 Computational_Finance\n",
      "### 83: chicago-justice-project article-tagging\n",
      "### 84: chrieke InstanceSegmentation_Sentinel2\n",
      "### 85: Chris-Manna charity_recommender\n",
      "### 86: chrisPiemonte crime-analysis\n",
      "### 87: CityofToronto bdit_data-sources\n",
      "### 88: cmenguy crime-analytics\n",
      "### 89: codeforboston safe-water\n",
      "### 90: crowdAI train-schedule-optimisation-challenge-starter-kit\n",
      "### 91: damontallen Construction-materials\n",
      "### 92: danielmachinelearning HotelSpamDetection\n",
      "### 93: Danila89 kaggle_mercedes\n",
      "### 94: danshorstein ficpa_article\n",
      "### 95: danshorstein python4cpas\n",
      "### 96: dariusmehri Algorithm-for-Finding-Buildings-with-Facade-Risk\n",
      "### 97: dariusmehri Geo-Spatial-Risk-Analysis-of-Inspectors\n",
      "### 98: dariusmehri Inspection-Productivity-Analysis-and-Visualization-with-Tableau\n",
      "### 99: dariusmehri Predicting-Staff-Levels-for-Front-line-Workers\n",
      "### 100: dariusmehri Predictive-Analysis-of-Building-Violations\n",
      "### 101: dariusmehri Risk-Screening-Tool-to-Predict-Accidents-at-Construction-Sites\n",
      "### 102: dariusmehri Social-Network-Analysis-to-Expose-Corruption\n",
      "### 103: dariusmehri Social-Network-Bad-Actor-Risk-Tool\n",
      "### 104: dariusmehri Topic-Modeling-and-Analysis-of-Building-Related-Injuries\n",
      "### 105: darshankaarki ml-coa-charging\n",
      "### 106: Data4Democracy crash-model\n",
      "### 107: datacamp course-resources-ml-with-experts-budgets\n",
      "### 108: datadesk california-ccscore-analysis\n",
      "### 109: datadesk california-electricity-capacity-analysis\n",
      "### 110: datadesk lapd-crime-classification-analysis\n",
      "### 111: datakind datadive-gates92y-proj3-form990\n",
      "### 112: DataScienceForGood TaxationInequality\n",
      "### 113: davidawad lobe\n",
      "### 114: davidjwiner political_affiliation_prediction\n",
      "### 115: davidmasse US-supreme-court-prediction\n",
      "### 116: davidsontheath bias_corrected_estimators\n",
      "### 117: davidsteinar structural-uncertainty\n",
      "### 118: dchannah fraudhacker\n",
      "### 119: dchannah materials_mining\n",
      "### 120: deadskull7 Agricultural-Price-Prediction-and-Visualization-on-Android-App\n",
      "### 121: deepchem deepchem\n",
      "### 122: denadai2 real-estate-neighborhood-prediction\n",
      "### 123: DFS-UCU UkrainianAgriculture\n",
      "### 124: DimaStoyanov Ambulance-Dispatching\n",
      "### 125: DistrictDataLabs city-dash\n",
      "### 126: divyam3897 agriculture\n",
      "### 127: DivyaMadhu School-Budget-Prediction\n",
      "### 128: dkirkby MachineLearningStatistics\n",
      "### 129: DLColumbia DL_forFinance\n",
      "### 130: dmodjeska barnet_transactions\n",
      "### 131: DocVaughan MCHE485---Mechanical-Vibrations\n",
      "### 132: ds-modules EEP-147\n",
      "### 133: dssg triage\n",
      "### 134: DTUWindEnergy Python4WindEnergy\n",
      "### 135: duncangh FSA\n",
      "### 136: ebrahimraeyat Civil\n",
      "### 137: edmundooo more-money-more-problems\n",
      "### 138: ehsanasgari Deep-Proteomics\n",
      "### 139: EliadProject Hotels-Data-Science\n",
      "### 140: eloyekunle student_intervention\n",
      "### 141: EricHe98 Financial-Statements-Text-Analysis\n",
      "### 142: erickjtorres AI_LegalDoc_Hackathon\n",
      "### 143: ernestyalumni CompPhys\n",
      "### 144: eswar3 Zillow-prediction-models\n",
      "### 145: everAspiring RegressionAnalysis\n",
      "### 146: fangshulin Vulnerability-Analysis-for-Transportation-Networks\n",
      "### 147: farkhondehm Social-Assistance\n",
      "### 148: farwacheema DA-electricity-price-forecasting\n",
      "### 149: Featuretools predict-household-poverty\n",
      "### 150: Featuretools predict-loan-repayment\n",
      "### 151: felzek Crime-Review-Data-Analysis\n",
      "### 152: finnqiao cohort_online_retail\n",
      "### 153: firmai financial-machine-learning\n",
      "### 154: firmai interactive-corporate-report\n",
      "### 155: FPAL-Stanford-University FloATPy\n",
      "### 156: fpli-mbr LA-Parking-Tickets\n",
      "### 157: FUSED-Wind FUSED-Wake\n",
      "### 158: fvisconti gammas_machine_learning\n",
      "### 159: gablg1 ORGAN\n",
      "### 160: gabrielilharco snap-n-eat\n",
      "### 161: gauravmunjal13 Agriculture\n",
      "### 162: geoscixyz em-apps\n",
      "### 163: GiggleLiu marburg\n",
      "### 164: girishkuniyal Predict-housing-prices-in-Portland\n",
      "### 165: GirrajMaheshwari Legal-Analytics-project---Court-misclassification\n",
      "### 166: GirrajMaheshwari Web-scrapping-\n",
      "### 167: GitiHubi deepAI\n",
      "### 168: Global-Witness overseas-companies-land-ownership\n",
      "### 169: google deepvariant\n",
      "### 170: govwiki rating_history\n",
      "### 171: greenelab adage\n",
      "### 172: greenelab SPRINT_gan\n",
      "### 173: GretelDePaepe FindingDonuts\n",
      "### 174: gschivley carbon-index\n",
      "### 175: gumballhead mpr\n",
      "### 176: Gunnstein fatpack\n",
      "### 177: gzsuyu Data-Analysis-NYC-Restaurant-Inspection-Data\n",
      "### 178: hackingmaterials atomate\n",
      "### 179: hackoregon 2019-transportation-data-science\n",
      "### 180: hallba WritingSimulators\n",
      "### 181: hamaadshah market_risk_gan_keras\n",
      "### 182: han-yan-ds Kaggle-Bosch\n",
      "### 183: hardmaru estool\n",
      "### 184: Hasan330 Order-Cancellation-Prediction-Model\n",
      "### 185: hathix youtube-transcriber\n",
      "### 186: hbutsuak95 Quality-Optimization-of-Steel\n",
      "### 187: healthgradient sec-doc-info-extraction\n",
      "### 188: healthgradient sec_employee_information_extraction\n",
      "### 189: hep-lbdl adversarial-jets\n",
      "### 190: hep-lbdl CaloGAN\n",
      "### 191: higgsfield interaction_network_pytorch\n",
      "### 192: HIPS neural-fingerprint\n",
      "### 193: HitarthiShah Radiation-Data-Analysis\n",
      "### 194: hkacmaz Bankin_Recovery\n",
      "### 195: hockeyjudson Legal-Entity-Detection\n",
      "### 196: HowardNTUST Marketing-Data-Science-Application\n",
      "### 197: hussius deeplearning-biology\n",
      "### 198: hvantil ElectricityDemandForecasting\n",
      "### 199: hyattsaleh15 RealStateRecommender\n",
      "### 200: IBM iot-predictive-analytics\n",
      "### 201: IBM-Cloud-DevFest-2018 Data-Science-for-Banking\n",
      "### 202: IBM-DSE CyberShop-Analytics\n",
      "### 203: ikatsov algorithmic-examples\n",
      "### 204: imhgchoi Corr_Prediction_ARIMA_LSTM_Hybrid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 205: ishank011 gs-quantify-bond-prediction\n",
      "### 206: iurisegtovich PyTherm-applied-thermodynamics\n",
      "### 207: ivan-bilan Painting_Forensics\n",
      "### 208: jamesypeng Smarter-Emergency-Dispatch\n",
      "### 209: janzaib-masood Educational-Data-Mining\n",
      "### 210: Jean-njoroge coal-exploratory\n",
      "### 211: jellespijker pymech\n",
      "### 212: jerryxyx EquineTrading\n",
      "### 213: jfzhang95 LSTM-water-table-depth-prediction\n",
      "### 214: jhconning Dev-II\n",
      "### 215: jinsonfernandez NLP_School-Budget-Project\n",
      "### 216: jjakimoto finance_ml\n",
      "### 217: jlperla ECON407_2018\n",
      "### 218: joelowj Machine-Learning-and-Reinforcement-Learning-in-Finance\n",
      "### 219: johnfwhitesell CensusPull\n",
      "### 220: johnpfay USWaterAccounting\n",
      "### 221: JonathanREB Budget_SchoolsAnalysis\n",
      "### 222: JorgeDeLosSantos nusa\n",
      "### 223: jorgehas smart-defect-inspection\n",
      "### 224: josephofiowa dc-michelin-challenge\n",
      "### 225: jrieke lstm-biology\n",
      "### 226: JSchelldorfer ActuarialDataScience\n",
      "### 227: jstac econometrics\n",
      "### 228: jstac quantecon_nyu_2016\n",
      "### 229: jubins DeepLearning-Food-Image-Recognition-And-Calorie-Estimation\n",
      "### 230: JulianMar11 SentimentPoliticalCompass\n",
      "### 231: k2inno-tech jupyter-genomics\n",
      "### 232: kaiareyes ambulance\n",
      "### 233: kaitai stress-testing-with-jupyter\n",
      "### 234: Karagul BoE_stress_test\n",
      "### 235: kaumaron Data_Science\n",
      "### 236: Kevin-McIsaac Nightlife\n",
      "### 237: khanhnamle1994 fashion-recommendation\n",
      "### 238: kkirchhoff01 DebateAnalysis\n",
      "### 239: klin90 missinglink\n",
      "### 240: kmunger YT_descriptive\n",
      "### 241: kralmachine WholesaleCustomerAnalysis\n",
      "### 242: krpiyush5 Amazon-Fine-Food-Review\n",
      "### 243: kshitizkhanal7 Aerogami\n",
      "### 244: KSpiliop Fraud_Detection\n",
      "### 245: LaranIkal ProductAnomaliesDetection\n",
      "### 246: LauraChen 02-Metis-Web-Scraping\n",
      "### 247: Law-AI summarization\n",
      "### 248: LeadingIndiaAI Forest-Fire-Detection-through-UAV-imagery-using-CNNs\n",
      "### 249: Lemma1 DPFE\n",
      "### 250: leportella federal-road-accidents\n",
      "### 251: LexPredict lexpredict-contraxsuite\n",
      "### 252: LexPredict openedgar\n",
      "### 253: limberc tianchi-IMQF\n",
      "### 254: llSourcell AI_Supply_Chain\n",
      "### 255: llSourcell mapo\n",
      "### 256: lnsongxf Applied_Computational_Economics_and_Finance\n",
      "### 257: Lolonon Restaurant-Analytical-Solution\n",
      "### 258: LongOnly Quantitative-Notebooks\n",
      "### 259: longtng frauddetectionproject\n",
      "### 260: luqmanhakim research-on-sp-wholesale\n",
      "### 261: m-hoff maintsim\n",
      "### 262: manuvarkey Gestimator\n",
      "### 263: marcotav hotels\n",
      "### 264: MarcusOlivecrona REINVENT\n",
      "### 265: materialsproject emmet\n",
      "### 266: materialsproject pymatgen\n",
      "### 267: materialsvirtuallab megnet\n",
      "### 268: MAydogdu ConsumerFinancialProtectionBureau\n",
      "### 269: MAydogdu TextualAnalysis\n",
      "### 270: mbravidor PyDSout\n",
      "### 271: mdeff pygsp_tutorial_graphsip\n",
      "### 272: Meena-Mani SECOM_class_imbalance\n",
      "### 273: Mehranov UnderstandingAndPredictingPropertyMaintenanceFines\n",
      "### 274: MengchuanFu Suspecious-Apps-Detection\n",
      "### 275: mesgarpour T-CARER\n",
      "### 276: mfsatya PlantSeedlings-Classification\n",
      "### 277: Michaels72 AML-Due-Diligence\n",
      "### 278: mick-zhang Quality-Control-for-Banking-using-LDA-and-LDA-Mallet\n",
      "### 279: microsoft EconML\n",
      "### 280: mikhailklassen Mining-the-Social-Web-3rd-Edition\n",
      "### 281: mila-iqia gene-graph-conv\n",
      "### 282: mingzhangyang Mybiotools\n",
      "### 283: mitmedialab Evolutron\n",
      "### 284: MiyainNYC Financial-Modeling\n",
      "### 285: mohan-mj Manufacturing-Line-I4.0\n",
      "### 286: Montclair-State-University-Info368 Assignment-6\n",
      "### 287: morenobcn capstone_hotels_arcpy\n",
      "### 288: morenobcn hotels_vs_airbnb_proof_of_concept\n",
      "### 289: mratsim Apartment-Interest-Prediction\n",
      "### 290: mratsim McKinsey-SmartCities-Traffic-Prediction\n",
      "### 291: mroberge hydrofunctions\n",
      "### 292: mschermann forensic_accounting\n",
      "### 293: muntisa Deep-Politics\n",
      "### 294: Murgio Food-Recipe-CNN\n",
      "### 295: Myau5x anti-recommender\n",
      "### 296: nd1 DC_RestaurantViolationForecasting\n",
      "### 297: nealmcb pr_voting_methods\n",
      "### 298: neokt car-damage-detective\n",
      "### 299: Niels-Peter XBRL-AI\n",
      "### 300: nishanthgampa Time-Series-Analysis-on-Transportation-Data\n",
      "### 301: NomaanAhmed BigData_AnimalSpeciesAnalysis\n",
      "### 302: nonsignificantp ambulance-response-time\n",
      "### 303: npatta01 web-deep-learning-classifier\n",
      "### 304: nprapps school-choice\n",
      "### 305: nus-usp room-allocation\n",
      "### 306: nvodoor RBA\n",
      "### 307: nymarya school-budgets-for-education\n",
      "### 308: OGGM oggm-edu\n",
      "### 309: okfn-brasil perfil-politico\n",
      "### 310: Open-Power-System-Data renewable_power_plants\n",
      "### 311: openalea eartrack\n",
      "### 312: OpenChemE CHBE356\n",
      "### 313: openeemeter eemeter\n",
      "### 314: Paresh3189 Bankruptcy-Prediction-Growth-Modelling\n",
      "### 315: ParticipaPY politic-bots\n",
      "### 316: paultopia concrete_NLP_tutorial\n",
      "### 317: pawelmorawiecki traffic_jam_Nairobi\n",
      "### 318: pedrohserrano graph-analytics-nederlands\n",
      "### 319: pgromano Political-Identity-Analysis\n",
      "### 320: philiplbean facebook_political_ads\n",
      "### 321: philippschmalen Project_tsds\n",
      "### 322: philxchen Clustering-Canadian-regulations\n",
      "### 323: pipette Electricity-load-disaggregation\n",
      "### 324: Polichinel Master_Thesis\n",
      "### 325: prakhardogra921 Clustering-Analysis-on-customers-of-a-wholesale-distributor\n",
      "### 326: pratishthakapoor RetailReplenishement\n",
      "### 327: PrincetonUniversity gerrymandertests\n",
      "### 328: propublica congress-api-docs\n",
      "### 329: ProximaDas nlp-govt-regulations\n",
      "### 330: psi4 psi4numpy\n",
      "### 331: pymedphys pymedphys\n",
      "### 332: PyPSA WHOBS\n",
      "### 333: pzivich Python-for-Epidemiologists\n",
      "### 334: pzivich zEpid\n",
      "### 335: QuantEcon columbia_mini_course\n",
      "### 336: rajaswa Disaster-Management-\n",
      "### 337: rameshcalamur fin-stmt-anom\n",
      "### 338: RandomFractals ChicagoCrimes\n",
      "### 339: rawillis98 alpaca\n",
      "### 340: raymond180 FINRA_TRACE\n",
      "### 341: rdbraatz data-driven-prediction-of-battery-cycle-life-before-capacity-degradation\n",
      "### 342: RealRadOne Gyani-The-Loan-Eligibility-Predictor\n",
      "### 343: richardddli state_electricity_rates\n",
      "### 344: ricket-sjtu bioinformatics\n",
      "### 345: ritchie46 anaStruct\n",
      "### 346: rjhere Prescription-compliance-prediction\n",
      "### 347: robmarkcole Useful-python-for-medical-physics\n",
      "### 348: rochiecuevas shared_accommodations\n",
      "### 349: RohithYogi Student-Performance-Prediction\n",
      "### 350: roshetty Supporting-Emergency-Room-Decision-Making-with-Relevant-Scientific-Literature\n",
      "### 351: rphaneendra Menu-Similarity\n",
      "### 352: rshea3 alpha-insurance\n",
      "### 353: ryanschaub The-U.S.-Mexican-Border-Wall-and-Staffing-A-Statistical-Approach-\n",
      "### 354: sa7mon venmo-data\n",
      "### 355: sacul-git hierarpy\n",
      "### 356: SaiBiswas Bank-Grievance-Compliance-Management\n",
      "### 357: saisrivatsan deep-opt-auctions\n",
      "### 358: samarthiith DE_CoalPhaseOut\n",
      "### 359: Samimust predictive-maintenance\n",
      "### 360: sanatasy Restaurant_Risk\n",
      "### 361: sarachmax MarketCrashes_Prediction\n",
      "### 362: SarahMestiri online-retail-case\n",
      "### 363: Sardhendu PropertyClassification\n",
      "### 364: scngo SD-ambulance-allocation\n",
      "### 365: sdasadia Oil-Natural-Gas-Price-Prediction\n",
      "### 366: SeanMcOwen FinanceAndPython.com-BasicFinance\n",
      "### 367: SeanMcOwen FinanceAndPython.com-ClusteringIndustries\n",
      "### 368: SeanMcOwen FinanceAndPython.com-Derivatives\n",
      "### 369: SeanMcOwen FinanceAndPython.com-Investments\n",
      "### 370: sekhansen mpc_minutes_demo\n",
      "### 371: Semionn JB-wholesale-distribution-analysis\n",
      "### 372: Senkichi The_Catastrophe_Coefficient\n",
      "### 373: sentinel-hub water-observatory-backend\n",
      "### 374: sharmaroshan Education-Process-Mining\n",
      "### 375: sharmaroshan Insurance-Claim-Prediction\n",
      "### 376: sharmaroshan Online-Retail-Transactions-of-UK\n",
      "### 377: sharmaroshan SECOM-Detecting-Defected-Items\n",
      "### 378: sharmaroshan Students-Performance-Analytics\n",
      "### 379: sharmaroshan World-Food-Production\n",
      "### 380: shayanray GlassBox\n",
      "### 381: shivangchopra11 InfyHack\n",
      "### 382: Shomona Bank-Failure-Prediction\n",
      "### 383: Shreyas3108 house-price-prediction\n",
      "### 384: siddhantmaharana text-analysis-on-FINRA-docs\n",
      "### 385: SiddheshAcharekar Liveright\n",
      "### 386: sierraporta asphalt_binder\n",
      "### 387: sky-t hack-or-emergency-response\n",
      "### 388: slegroux claimdenial\n",
      "### 389: staceb charities_in_the_united_states\n",
      "### 390: starfoe Eye-bnb\n",
      "### 391: stratospark food-101-keras\n",
      "### 392: surajmall Agriculture-Assistant\n",
      "### 393: sushant2811 SchoolBudgetData\n",
      "### 394: t-davidson hate-speech-and-offensive-language\n",
      "### 395: talmo leap\n",
      "### 396: tanoybhattacharya 911-Data-Analysis\n",
      "### 397: tecoevo agriculture\n",
      "### 398: TiesdeKok Python_NLP_Tutorial\n",
      "### 399: TiesdeKok UW_Python_Camp\n",
      "### 400: tilde-lab awesome-materials-informatics\n",
      "### 401: timsainb AVGN\n",
      "### 402: tina31726 Crime-Prediction\n",
      "### 403: toningega Data_Generator\n",
      "### 404: tradeasystems tradeasystems_connector\n",
      "### 405: tradingeconomics tradingeconomics\n",
      "### 406: trentwoodbury ManufacturingAuctionRegression\n",
      "### 407: tslindner Effects-of-Cannabis-Legalization\n",
      "### 408: tstreamDOTh Instacart-Market-Basket-Analysis\n",
      "### 409: tullyvelte SchoolPerformanceDataAnalysis\n",
      "### 410: txytju air-quality-prediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 411: ual rental-listings\n",
      "### 412: uci-cbcl D-GEX\n",
      "### 413: un-modelling Electricity_Consumption_Surveys\n",
      "### 414: usnistgov modelmeth\n",
      "### 415: vibrationtoolbox vibration_toolbox\n",
      "### 416: vicelab slaer\n",
      "### 417: victorpena1 Natural-Gas-Demand-Prediction\n",
      "### 418: vikram-bhati PAASBAAN-crime-prediction\n",
      "### 419: Vipul115 Statistical-Time-Series-Analysis-on-Agricultural-Commodity-Prices\n",
      "### 420: viritaromero Plant-diseases-classifier\n",
      "### 421: vsub21 systemic-risk-dashboard\n",
      "### 422: vtyeh pandas-challenge\n",
      "### 423: waldronlab AppStatBio\n",
      "### 424: wassname pipe-segmentation\n",
      "### 425: whs2k GPO-AI\n",
      "### 426: whugue school-closure\n",
      "### 427: widdowquinn Teaching-EMBL-Plant-Path-Genomics\n",
      "### 428: williamadams1 natural-gas-consumption-forecasting\n",
      "### 429: worldbank ML-classification-algorithms-poverty\n",
      "### 430: xiaofei6677 TourismFlickrMiner\n",
      "### 431: xinychen transdim\n",
      "### 432: yajnab pySteel\n",
      "### 433: yiaktan Secondhand_Concert_Tickets\n",
      "### 434: YungChunLu UCI-Power-Plant\n",
      "### 435: zhentaoshi econ5170\n",
      "### 436: zischwartz workerfatalities\n",
      "### parsed 437 git-projects  in 8.332 seconds ###\n"
     ]
    }
   ],
   "source": [
    "# bundle collected data and store in single json-file\n",
    "# suitable for database building\n",
    "dir_base = '../data/repositories/git/'\n",
    "json_files = {\n",
    "    'additional_info': '.meta/additional_info.json',\n",
    "    # 'git_analyzer': '.meta/git_analyzer_results.json',\n",
    "    'git_analyzer': '.meta/git_analyzer_results_v02.json',\n",
    "    'github_api': '.meta/github_api_meta.json',\n",
    "    'github_meta': '.meta/github_meta.json',\n",
    "}\n",
    "date_ref_file = '.meta/log_clone.txt'\n",
    "\n",
    "meta_collection = '.meta/summary.json'\n",
    "\n",
    "authors = os.listdir(dir_base)\n",
    "\n",
    "p = 0\n",
    "\n",
    "start = 0\n",
    "end = 1000\n",
    "\n",
    "runtime_start = time.time()\n",
    "\n",
    "i = start\n",
    "for author in authors:\n",
    "    if os.path.isdir(os.path.join(dir_base, author)):\n",
    "        \n",
    "        projects = os.listdir(os.path.join(dir_base, author))\n",
    "        #print(author, projects)\n",
    "        for project in projects:\n",
    "            #print(project)\n",
    "            if p >= start:\n",
    "                path = os.path.join(dir_base, author, project)\n",
    "                if os.path.isdir(path):\n",
    "                    print('### ' + str(i) + ': ' + author, project.strip())\n",
    "                    raw_json = {}\n",
    "                    \n",
    "                    # load json files\n",
    "                    for key in json_files:\n",
    "                        #print(key, json_files[key])\n",
    "                        json_fp = os.path.join(path, json_files[key])\n",
    "                        if os.path.isfile(json_fp):\n",
    "                            with open(json_fp, 'r') as fp:\n",
    "                                data = fp.read()\n",
    "                                raw_json[key] = json.loads(data)\n",
    "                        else:\n",
    "                            raw_json[key] = {}\n",
    "\n",
    "                    # readme\n",
    "                    readme_fp = os.path.join(path, 'README.md')\n",
    "                    if os.path.isfile(readme_fp):\n",
    "                        with open(readme_fp, 'r', encoding='utf-8', errors=\"ignore\") as fp:\n",
    "                            readme = fp.read()\n",
    "                    else:\n",
    "                        readme = ''\n",
    "                        \n",
    "                    # combine ml_libs and keywords\n",
    "                    ml_libs = []\n",
    "                    ml_terms = []\n",
    "                    ml_tags = []\n",
    "                    \n",
    "                    for file in raw_json['git_analyzer']:\n",
    "                        #print(file)\n",
    "                        #print(file['meta']['ml_libs'])\n",
    "                        if file['meta'].get('ml_libs'):\n",
    "                            ml_libs.extend(file['meta'].get('ml_libs'))\n",
    "                        if file['meta'].get('keywords'):\n",
    "                            ml_terms.extend(file['meta'].get('keywords'))\n",
    "                        if file['meta'].get('ml_tags'):\n",
    "                            ml_tags.extend(file['meta'].get('ml_tags'))\n",
    "                    \n",
    "                    # make values unique\n",
    "                    ml_libs = np.unique(ml_libs).tolist()\n",
    "                    ml_terms = np.unique(ml_terms).tolist()\n",
    "                    ml_tags = np.unique(ml_tags).tolist()\n",
    "                    #ml_tags = match_tags(ml_terms)\n",
    "                    \n",
    "                    # parse created_at\n",
    "                    created_at = raw_json['github_api'].get('created_at')\n",
    "                    if created_at != None:\n",
    "                        created_at = created_at.replace('T',' ').replace('Z','')\n",
    "                    \n",
    "                    # get pushed_at\n",
    "                    pushed_at = raw_json['github_api'].get('pushed_at')\n",
    "                    if pushed_at == None:\n",
    "                        pushed_at = raw_json['github_meta'].get('last_commit')\n",
    "                    if pushed_at == None:\n",
    "                        pushed_at = ''\n",
    "                    \n",
    "                    # 2018-07-25T04:01:33Z\n",
    "                    pushed_at = pushed_at.replace('T',' ').replace('Z','')\n",
    "                    #date_time_str = pushed_at.split('Z')\n",
    "                    #date_time_str = date_time_str[0].strip()\n",
    "                    #date_time_obj = datetime.datetime.strptime(date_time_str, '%Y-%m-%dT%H:%M:%S')\n",
    "                    #pushed_at = date_time_obj\n",
    "                    #print(pushed_at)\n",
    "                    \n",
    "                    # get scraped_at\n",
    "                    ref_fp = os.path.join(dir_base, author, project)\n",
    "                    ref_fp += '/' + date_ref_file\n",
    "                    if os.path.isfile(ref_fp):\n",
    "                        scraped_at = creation_date(ref_fp)\n",
    "                    else:\n",
    "                        scraped_at = creation_date(path)\n",
    "                    \n",
    "                    # get primary language\n",
    "                    language_primary = raw_json['github_api'].get('language')\n",
    "                    if language_primary == None:\n",
    "                        language_primary = raw_json['github_meta'].get('languages')\n",
    "                        if language_primary != None and len(language_primary) > 0:\n",
    "                            language_primary = language_primary[0].split(':')[0]\n",
    "                        else:\n",
    "                            language_primary = ''\n",
    "                    #print(language_primary)\n",
    "                    \n",
    "                    # parse description\n",
    "                    description = raw_json['github_meta'].get('about')\n",
    "                    if description != None:\n",
    "                        description = description.replace(';', '.')\n",
    "                        description = description.replace('\\n', ' ').replace('\\r', '')\n",
    "                        description = description.replace('<br/>', '').replace('<br>', '')\n",
    "                        \n",
    "                    # parse readme\n",
    "                    if readme != None:\n",
    "                        readme = readme.replace(';', '.')\n",
    "                        readme = readme.replace('\\n', ' ').replace('\\r', '')\n",
    "                        #readme = readme.replace('<br/>', '').replace('<br>', '')\n",
    "                    \n",
    "                    # build ml score\n",
    "                    ml_detected = 0; #len(ml_libs) > 0 or len(keywords) > 0\n",
    "                    if len(ml_terms) > 0:\n",
    "                        ml_detected += 0.2\n",
    "                    if len(ml_tags) > 0:\n",
    "                        ml_detected += 0.3\n",
    "                    if len(ml_libs) > 0:\n",
    "                        ml_detected += 0.5\n",
    "                        \n",
    "                    # parse license\n",
    "                    license = raw_json['github_meta'].get('license')\n",
    "                    if license != None:\n",
    "                        if 'Nan' in license:\n",
    "                            license = ''\n",
    "                        if 'View license' in license:\n",
    "                            license = ''\n",
    "                        if '+ ' in license:\n",
    "                            license = ''\n",
    "                        if 'License' in license:\n",
    "                            license = license.replace('License','').strip()\n",
    "                    \n",
    "                    # build final json\n",
    "                    final_json = {\n",
    "                        'author': author,\n",
    "                        'title': project,\n",
    "                        'link': raw_json['github_meta'].get('url'),\n",
    "                        'ml_detected': ml_detected,\n",
    "                        'description': description,\n",
    "                        'license': license,\n",
    "                        'language_primary': language_primary,\n",
    "                        'languages': raw_json['github_meta'].get('languages'),\n",
    "                        'created_at': created_at,\n",
    "                        'pushed_at': str(pushed_at),\n",
    "                        'scraped_at': str(scraped_at),\n",
    "                        'stars': raw_json['github_api'].get('stars'),\n",
    "                        'stars_score': score(raw_json['github_api'].get('stars')),\n",
    "                        'contributors': raw_json['github_meta'].get('contributors'),\n",
    "                        'ml_libs': ml_libs,\n",
    "                        'ml_tags': ml_tags,\n",
    "                        'ml_terms': ml_terms,\n",
    "                        'readme': readme,\n",
    "                        'industry': raw_json['additional_info'].get('industry'),\n",
    "                        'type': raw_json['additional_info'].get('type'),\n",
    "                        'name': raw_json['additional_info'].get('name'),\n",
    "                        'description2': raw_json['additional_info'].get('description'),\n",
    "                        #'raw': raw_json,\n",
    "                    }\n",
    "                    #print(final_json)\n",
    "                    \n",
    "                    # store final json\n",
    "                    with open(os.path.join(path, meta_collection), 'w') as fp:\n",
    "                        fp.write(json.dumps(final_json))\n",
    "                    \n",
    "                # count parsed projects\n",
    "                i += 1\n",
    "\n",
    "            # count projects\n",
    "            p += 1\n",
    "        \n",
    "    # break loop\n",
    "    if i>=end and True:\n",
    "        break\n",
    "    \n",
    "runtime_end = time.time()\n",
    "print('### parsed {0} git-projects  in {1} seconds ###'.format(i-start, round(runtime_end - runtime_start, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### parsed 437 git-projects  in 7.017 seconds ###\n",
      "          author contributors           created_at  \\\n",
      "0  1davegalloway            0  2018-07-19 02:41:45   \n",
      "1        A7med01            0  2018-11-04 12:14:31   \n",
      "2     AayushG159            2  2018-04-13 14:07:44   \n",
      "3   aayushmudgal            0  2016-12-21 21:11:52   \n",
      "4          ab-bh            3                 None   \n",
      "\n",
      "                                         description  \\\n",
      "0  This analysis investigates the data of test sc...   \n",
      "1  Animal Identification with Deep Convolutional ...   \n",
      "2  Identification of plants through plant leaves ...   \n",
      "3  Reducing Manufacturing Failures - A Kaggle Cha...   \n",
      "4  Innovaccer Hackercamp '17 Project for predicti...   \n",
      "\n",
      "                                        description2  \\\n",
      "0                   School budget vs school results.   \n",
      "1           Deep learning for animal identification.   \n",
      "2  Identification of plants through plant leaves ...   \n",
      "3                   Reducing manufacturing failures.   \n",
      "4  Machine Learning implementation based on multi...   \n",
      "\n",
      "                           industry  language_primary  \\\n",
      "0              Education & Research  Jupyter Notebook   \n",
      "1  Biotechnological & Life Sciences  Jupyter Notebook   \n",
      "2  Biotechnological & Life Sciences  Jupyter Notebook   \n",
      "3                     Manufacturing  Jupyter Notebook   \n",
      "4                Emergency & Relief  Jupyter Notebook   \n",
      "\n",
      "                                          languages     license  \\\n",
      "0           [Jupyter Notebook: 96.8%, Python: 3.2%]               \n",
      "1                        [Jupyter Notebook: 100.0%]               \n",
      "2                        [Jupyter Notebook: 100.0%]         MIT   \n",
      "3  [Jupyter Notebook: 97.1%, Python: 2.1%, R: 0.8%]  Apache-2.0   \n",
      "4                        [Jupyter Notebook: 100.0%]               \n",
      "\n",
      "                                                link  ...  \\\n",
      "0  https://github.com/1davegalloway/SchoolDistric...  ...   \n",
      "1  https://github.com/A7med01/Deep-learning-for-A...  ...   \n",
      "2  https://github.com/AayushG159/Plant-Leaf-Ident...  ...   \n",
      "3  https://github.com/aayushmudgal/Reducing-Manuf...  ...   \n",
      "4  https://github.com/ab-bh/Disease-Outbreak-Pred...  ...   \n",
      "\n",
      "                                             ml_tags  \\\n",
      "0                                                 []   \n",
      "1                               [Classification, DL]   \n",
      "2                       [AI, CV, Classification, ML]   \n",
      "3  [CV, Classification, Gradient Boosting, Naive ...   \n",
      "4                   [Classification, ML, Regression]   \n",
      "\n",
      "                                            ml_terms  \\\n",
      "0                                                 []   \n",
      "1  [DL, classif, computer vision, deep learning, ...   \n",
      "2     [CV, classif, filter, model, recommend, train]   \n",
      "3  [classif, gradient boosting, naive bayes, rand...   \n",
      "4                                          [predict]   \n",
      "\n",
      "                          name            pushed_at  \\\n",
      "0                     PyCity 2  2018-07-25 04:01:33   \n",
      "1        Animal Identification  2020-08-13 15:12:16   \n",
      "2          Leaf Identification  2020-08-03 13:25:47   \n",
      "3       Manufacturing Failures  2016-12-23 03:08:34   \n",
      "4  Predicting Disease Outbreak  2017-12-20 16:38:14   \n",
      "\n",
      "                                              readme           scraped_at  \\\n",
      "0  # SchoolDistrictAnalysis This analysis investi...  2020-10-31 00:31:50   \n",
      "1  # **Identify the Animal challenge**   <img src...  2020-10-31 00:20:36   \n",
      "2  # Plant Leaf Identification  Identification of...  2020-10-30 23:55:15   \n",
      "3  # Reducing-Manufacturing-Failures ## Reducing ...  2020-10-31 00:44:24   \n",
      "4  # Disease-Outbreak-Prediction Innovaccer Hacke...  2020-10-31 00:35:37   \n",
      "\n",
      "  stars stars_score                                    title  \\\n",
      "0     0       0.000                   SchoolDistrictAnalysis   \n",
      "1    11       0.311  Deep-learning-for-Animal-Identification   \n",
      "2    44       0.435                Plant-Leaf-Identification   \n",
      "3     2       0.152          Reducing-Manufacturing-Failures   \n",
      "4  None       0.000              Disease-Outbreak-Prediction   \n",
      "\n",
      "                  type  \n",
      "0               School  \n",
      "1        Life-sciences  \n",
      "2        Life-sciences  \n",
      "3              Failure  \n",
      "4  Disaster Management  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# build single csv from bundled data for visualising purpose\n",
    "dir_base = '../data/repositories/git/'\n",
    "meta_collection = '.meta/summary.json'\n",
    "output_csv = '../data/database/db_04_analyzed_v02.csv'\n",
    "\n",
    "authors = os.listdir(dir_base)\n",
    "\n",
    "p = 0\n",
    "\n",
    "start = 0\n",
    "end = 1000\n",
    "\n",
    "runtime_start = time.time()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "i = start\n",
    "for author in authors:\n",
    "    if os.path.isdir(os.path.join(dir_base, author)):\n",
    "        \n",
    "        projects = os.listdir(os.path.join(dir_base, author))\n",
    "        #print(author, projects)\n",
    "        for project in projects:\n",
    "            #print(project)\n",
    "            if p >= start:\n",
    "                path = os.path.join(dir_base, author, project)\n",
    "                if os.path.isdir(path):\n",
    "                    #print('### ' + str(i) + ': ' + author, project)\n",
    "                    # store final json\n",
    "                    with open(os.path.join(path, meta_collection), 'r') as fp:\n",
    "                        data = fp.read()\n",
    "                        data = json.loads(data)\n",
    "                        df = df.append(data, ignore_index=True)\n",
    "                    \n",
    "                # count parsed projects\n",
    "                i += 1\n",
    "\n",
    "            # count projects\n",
    "            p += 1\n",
    "        \n",
    "    # break loop\n",
    "    if i>=end and True:\n",
    "        break\n",
    "    \n",
    "runtime_end = time.time()\n",
    "print('### parsed {0} git-projects  in {1} seconds ###'.format(i-start, round(runtime_end - runtime_start, 3)))\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates(['link'])\n",
    "\n",
    "# drop rows without link\n",
    "df = df.dropna(axis=0, subset=['link'])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# drop columns\n",
    "df.drop(columns=['readme'], inplace=True)\n",
    "\n",
    "df.to_csv(output_csv, sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
