{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook converts the CSV to ES mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some long text\n",
    "# source: https://www.kaggle.com/c/stanford-covid-vaccine\n",
    "text1 = '''\n",
    "Winning the fight against the COVID-19 pandemic will require an effective vaccine that can be equitably and widely distributed. Building upon decades of research has allowed scientists to accelerate the search for a vaccine against COVID-19, but every day that goes by without a vaccine has enormous costs for the world nonetheless. We need new, fresh ideas from all corners of the world. Could online gaming and crowdsourcing help solve a worldwide pandemic? Pairing scientific and crowdsourced intelligence could help computational biochemists make measurable progress.\n",
    "mRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19, but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA). Conventional vaccines (like your seasonal flu shots) are packaged in disposable syringes and shipped under refrigeration around the world, but that is not currently possible for mRNA vaccines.\n",
    "Researchers have observed that RNA molecules have the tendency to spontaneously degrade. This is a serious limitation--a single cut can render the mRNA vaccine useless. Currently, little is known on the details of where in the backbone of a given RNA is most prone to being affected. Without this knowledge, current mRNA vaccines against COVID-19 must be prepared and shipped under intense refrigeration, and are unlikely to reach more than a tiny fraction of human beings on the planet unless they can be stabilized.\n",
    "The Eterna community, led by Professor Rhiju Das, a computational biochemist at Stanford’s School of Medicine, brings together scientists and gamers to solve puzzles and invent medicine. Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles. The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules. The Eterna community has previously unlocked new scientific principles, made new diagnostics against deadly diseases, and engaged the world’s most potent intellectual resources for the betterment of the public. The Eterna community has advanced biotechnology through its contribution in over 20 publications, including advances in RNA biotechnology.\n",
    "In this competition, we are looking to leverage the data science expertise of the Kaggle community to develop models and design rules for RNA degradation. Your model will predict likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position. We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines. These final test sequences are currently being synthesized and experimentally characterized at Stanford University in parallel to your modeling efforts -- Nature will score your models!\n",
    "Improving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve. Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19. The problem we are trying to solve has eluded academic labs, industry R&D groups, and supercomputers, and so we are turning to you. To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic. \n",
    "'''\n",
    "\n",
    "# and a short one\n",
    "text2 = 'The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 564\n",
      "words: 9\n",
      "words: 0\n"
     ]
    }
   ],
   "source": [
    "# function to count words\n",
    "def word_count(text):\n",
    "    if isinstance(text, str):\n",
    "        s = text.split(' ')\n",
    "        return len(s)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print('words:', word_count(text1))\n",
    "print('words:', word_count(text2))\n",
    "print('words:', word_count(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: 20\n",
      "sentences: 1\n",
      "sentences: 0\n"
     ]
    }
   ],
   "source": [
    "# function to count sentences\n",
    "def sentence_count(text):\n",
    "    if isinstance(text, str):\n",
    "        s = text.split('. ')\n",
    "        return len(s)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print('sentences:', sentence_count(text1))\n",
    "print('sentences:', sentence_count(text2))\n",
    "print('sentences:', sentence_count(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 139\n",
      "Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles.\n",
      "The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules.\n",
      "We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines.\n",
      "Improving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve.\n",
      "Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19.\n",
      "To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.\n"
     ]
    }
   ],
   "source": [
    "# text summarization 100% -> n%\n",
    "def nltk_ratio(text, ratio=0.25):\n",
    "    return summarize(text, ratio=ratio)\n",
    "\n",
    "sum_nltk_ratio = nltk_ratio(text1, ratio=0.25)\n",
    "print('words:', word_count(sum_nltk_ratio))\n",
    "print(sum_nltk_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 98\n",
      "Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles.\n",
      "We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines.\n",
      "Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19.\n",
      "To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.\n"
     ]
    }
   ],
   "source": [
    "# text summarization 100% -> n words\n",
    "def nltk_count(text, word_count=100):\n",
    "    return summarize(text, word_count=word_count)\n",
    "\n",
    "sum_nltk_count = nltk_count(text1, word_count=100)\n",
    "print('words:', word_count(sum_nltk_count))\n",
    "print(sum_nltk_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptive summarization\n",
    "# https://www.machinelearningplus.com/nlp/text-summarization-approaches-nlp-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART\n",
    "# Importing the model\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Loading the model and tokenizer for bart-large-cnn\\ntokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\\nmodel=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\\n#\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Loading the model and tokenizer for bart-large-cnn\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Encoding the inputs and passing them to model.generate()\\ndef bart(text):\\n    inputs = tokenizer.batch_encode_plus([text],return_tensors='pt')\\n    summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\\n\\n    # Decoding and printing the summary\\n    bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\n    \\n    return bart_summary\\n\\n# long text\\nstart = time.time()\\nsum_bart_l = bart(text1)\\nend = time.time()\\n\\nprint('### long text ###')\\nprint('runtime:', end-start)\\nprint('words:', word_count(sum_bart_l))\\nprint('sentences:', sentence_count(sum_bart_l))\\nprint(sum_bart_l)\\nprint('')\\n\\n# short text\\nprint('### short text ###')\\nstart = time.time()\\nsum_bart_s = bart(text2)\\nend = time.time()\\n\\nprint('runtime:', end-start)\\nprint('words:', word_count(sum_bart_s))\\nprint('sentences:', sentence_count(sum_bart_s))\\nprint(sum_bart_s)\\n#\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Encoding the inputs and passing them to model.generate()\n",
    "def bart(text):\n",
    "    inputs = tokenizer.batch_encode_plus([text],return_tensors='pt')\n",
    "    summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
    "\n",
    "    # Decoding and printing the summary\n",
    "    bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return bart_summary\n",
    "\n",
    "# long text\n",
    "start = time.time()\n",
    "sum_bart_l = bart(text1)\n",
    "end = time.time()\n",
    "\n",
    "print('### long text ###')\n",
    "print('runtime:', end-start)\n",
    "print('words:', word_count(sum_bart_l))\n",
    "print('sentences:', sentence_count(sum_bart_l))\n",
    "print(sum_bart_l)\n",
    "print('')\n",
    "\n",
    "# short text\n",
    "print('### short text ###')\n",
    "start = time.time()\n",
    "sum_bart_s = bart(text2)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime:', end-start)\n",
    "print('words:', word_count(sum_bart_s))\n",
    "print('sentences:', sentence_count(sum_bart_s))\n",
    "print(sum_bart_s)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# T5\n",
    "# https://towardsdatascience.com/summarize-reddit-comments-using-t5-bart-gpt-2-xlnet-models-a3e78a5ab944\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# long text\\nstart = time.time()\\nsum_t5_l = t5(text1)\\nend = time.time()\\n\\nprint('### long text ###')\\nprint('runtime:', end-start)\\nprint('words:', word_count(sum_t5_l))\\nprint('sentences:', sentence_count(sum_t5_l))\\nprint(sum_t5_l)\\nprint('')\\n\\n# short text\\nprint('### short text ###')\\nstart = time.time()\\nsum_t5_s = t5(text2)\\nend = time.time()\\n\\nprint('runtime:', end-start)\\nprint('words:', word_count(sum_t5_s))\\nprint('sentences:', sentence_count(sum_t5_s))\\nprint(sum_t5_s)\\n#\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def t5(text):\n",
    "    Preprocessed_text = \"summarize: \" + text\n",
    "    tokens_input = tokenizer.encode(Preprocessed_text,return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(tokens_input, min_length=100, max_length=180, length_penalty=4.0)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "'''\n",
    "# long text\n",
    "start = time.time()\n",
    "sum_t5_l = t5(text1)\n",
    "end = time.time()\n",
    "\n",
    "print('### long text ###')\n",
    "print('runtime:', end-start)\n",
    "print('words:', word_count(sum_t5_l))\n",
    "print('sentences:', sentence_count(sum_t5_l))\n",
    "print(sum_t5_l)\n",
    "print('')\n",
    "\n",
    "# short text\n",
    "print('### short text ###')\n",
    "start = time.time()\n",
    "sum_t5_s = t5(text2)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime:', end-start)\n",
    "print('words:', word_count(sum_t5_s))\n",
    "print('sentences:', sentence_count(sum_t5_s))\n",
    "print(sum_t5_s)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# industry categories\n",
    "\n",
    "# https://www.census.gov/programs-surveys/aces/information/iccl.html\n",
    "cat_sic = ['Agriculture','Forestry','Fishing','Mining','Construction','Manufacturing','Transportation','Communications','Electric','Gas','Sanitary','Wholesale Trade','Retail Trade','Finance','Insurance','Real Estate','Services','Public Administration']\n",
    "# https://www.marketing91.com/19-types-of-business-industries/\n",
    "cat_19 = ['Aerospace','Transport','Computer','Telecommunication','Agriculture','Construction','Education','Pharmaceutical','Food','Health care','Hospitality','Entertainment','News Media','Energy','Manufacturing','Music','Mining','Worldwide web','Electronics']\n",
    "# https://simplicable.com/new/industries\n",
    "cat_simple = ['Advertising','Agriculture','Communication','Construction','Creative','Education','Entertainment','Fashion','Finance','Health care','Information Technology','Manufacturing','Media','Retail','Research','Robotics','Space']\n",
    "\n",
    "cat = ['Accommodation & Food','Accounting','Agriculture','Banking & Insurance','Biotechnological & Life Sciences','Construction & Engineering','Economics','Education & Research','Emergency & Relief','Finance','Government and Public Works','Healthcare','Justice, Law and Regulations','Manufacturing','Media & Publishing','Miscellaneous','Physics','Real Estate, Rental & Leasing','Utilities','Wholesale & Retail']\n",
    "subcat = ['Failure','Food','Fraud','General','Genomics','Insurance and Risk','Judicial Applied','Life-sciences','Machine Learning','Maintenance','Management and Operations','Marketing','Material Science','Physical','Policy and Regulatory','Politics','Preventative and Reactive','Quality','Real Estate','Rental & Leasing','Restaurant','Retail','School','Sequencing','Social Policies','Student','Textual Analysis','Tools','Tourism','Trading & Investment','Transportation','Valuation','Water & Pollution','Wholesale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# zero shot classification\n",
    "# https://towardsdatascience.com/zero-shot-text-classification-with-hugging-face-7f533ba83cd6\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# test classifictaion with nltk (200 words)\\n\\ns = nltk_count(text1, word_count=100)\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_sic = classifier(s, cat_sic, multi_class=True)\\nend = time.time()\\n\\nprint('runtime sic:', end-start)\\n#print(res_sic)\\nprint(res_sic['labels'][0:3])\\nprint(res_sic['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_c19 = classifier(s, cat_19, multi_class=True)\\nend = time.time()\\n\\nprint('runtime c19:', end-start)\\n#print(res_c19)\\nprint(res_c19['labels'][0:3])\\nprint(res_c19['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_simple = classifier(s, cat_simple, multi_class=True)\\nend = time.time()\\n\\nprint('runtime simple:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat)\\nres_simple = classifier(s, cat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime category:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), subcat)\\nres_simple = classifier(s, subcat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime subcategory:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n#\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test classifictaion with nltk (200 words)\n",
    "\n",
    "s = nltk_count(text1, word_count=100)\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_sic = classifier(s, cat_sic, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime sic:', end-start)\n",
    "#print(res_sic)\n",
    "print(res_sic['labels'][0:3])\n",
    "print(res_sic['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_c19 = classifier(s, cat_19, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime c19:', end-start)\n",
    "#print(res_c19)\n",
    "print(res_c19['labels'][0:3])\n",
    "print(res_c19['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_simple = classifier(s, cat_simple, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime simple:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat)\n",
    "res_simple = classifier(s, cat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime category:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), subcat)\n",
    "res_simple = classifier(s, subcat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime subcategory:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# test classifictaion with t5\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_sic = classifier(sum_t5_l, cat_sic, multi_class=True)\\nend = time.time()\\n\\nprint('runtime sic:', end-start)\\n#print(res_sic)\\nprint(res_sic['labels'][0:3])\\nprint(res_sic['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_c19 = classifier(sum_t5_l, cat_19, multi_class=True)\\nend = time.time()\\n\\nprint('runtime c19:', end-start)\\n#print(res_c19)\\nprint(res_c19['labels'][0:3])\\nprint(res_c19['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_simple = classifier(sum_t5_l, cat_simple, multi_class=True)\\nend = time.time()\\n\\nprint('runtime simple:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat)\\nres_simple = classifier(sum_t5_l, cat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime category:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), subcat)\\nres_simple = classifier(sum_t5_l, subcat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime subcategory:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n#\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test classifictaion with t5\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_sic = classifier(sum_t5_l, cat_sic, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime sic:', end-start)\n",
    "#print(res_sic)\n",
    "print(res_sic['labels'][0:3])\n",
    "print(res_sic['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_c19 = classifier(sum_t5_l, cat_19, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime c19:', end-start)\n",
    "#print(res_c19)\n",
    "print(res_c19['labels'][0:3])\n",
    "print(res_c19['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_simple = classifier(sum_t5_l, cat_simple, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime simple:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat)\n",
    "res_simple = classifier(sum_t5_l, cat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime category:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), subcat)\n",
    "res_simple = classifier(sum_t5_l, subcat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime subcategory:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# test classifictaion with bart\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_sic = classifier(sum_bart_l, cat_sic, multi_class=True)\\nend = time.time()\\n\\nprint('runtime sic:', end-start)\\n#print(res_sic)\\nprint(res_sic['labels'][0:3])\\nprint(res_sic['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_c19 = classifier(sum_bart_l, cat_19, multi_class=True)\\nend = time.time()\\n\\nprint('runtime c19:', end-start)\\n#print(res_c19)\\nprint(res_c19['labels'][0:3])\\nprint(res_c19['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat_19)\\nres_simple = classifier(sum_bart_l, cat_simple, multi_class=True)\\nend = time.time()\\n\\nprint('runtime simple:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), cat)\\nres_simple = classifier(sum_bart_l, cat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime category:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n\\n\\nstart = time.time()\\n#res = classifier(nltk_count(text1, word_count=200), subcat)\\nres_simple = classifier(sum_bart_l, subcat, multi_class=True)\\nend = time.time()\\n\\nprint('runtime subcategory:', end-start)\\n#print(res_simple)\\nprint(res_simple['labels'][0:3])\\nprint(res_simple['scores'][0:3])\\n#\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test classifictaion with bart\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_sic = classifier(sum_bart_l, cat_sic, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime sic:', end-start)\n",
    "#print(res_sic)\n",
    "print(res_sic['labels'][0:3])\n",
    "print(res_sic['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_c19 = classifier(sum_bart_l, cat_19, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime c19:', end-start)\n",
    "#print(res_c19)\n",
    "print(res_c19['labels'][0:3])\n",
    "print(res_c19['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat_19)\n",
    "res_simple = classifier(sum_bart_l, cat_simple, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime simple:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), cat)\n",
    "res_simple = classifier(sum_bart_l, cat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime category:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#res = classifier(nltk_count(text1, word_count=200), subcat)\n",
    "res_simple = classifier(sum_bart_l, subcat, multi_class=True)\n",
    "end = time.time()\n",
    "\n",
    "print('runtime subcategory:', end-start)\n",
    "#print(res_simple)\n",
    "print(res_simple['labels'][0:3])\n",
    "print(res_simple['scores'][0:3])\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Biotechnological & Life Sciences': 0.8398016095161438, 'Healthcare': 0.7811025977134705, 'Education & Research': 0.7176370620727539, 'Utilities': 0.6823796629905701}\n",
      "{'category': None, 'score': None}\n",
      "{'Biotechnological & Life Sciences': 0.8398016095161438, 'Healthcare': 0.7811025977134705, 'Education & Research': 0.7176370620727539, 'Utilities': 0.6823796629905701, 'runtime': 16.704}\n",
      "{'category': None, 'score': None, 'runtime': 16.704}\n"
     ]
    }
   ],
   "source": [
    "# category function\n",
    "def categorize(text, categories, first=True, treshold=0, runtime=False):\n",
    "    start = time.time()\n",
    "    res = classifier(text, categories, multi_class=True)\n",
    "    #print(res)\n",
    "    end = time.time()\n",
    "    dur = round(end-start, 3)\n",
    "    if first == True:\n",
    "        ret = {\n",
    "            'category': res['labels'][0],\n",
    "            'score': res['scores'][0],\n",
    "        } if res['scores'][0] >= treshold else {\n",
    "            'category': None,\n",
    "            'score': None,\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        ret = dict(zip(res['labels'], res['scores']))\n",
    "        ret = {key: val for key, val in filter(lambda sub: sub[1] >= treshold, ret.items())}\n",
    "        \n",
    "    if runtime == True:\n",
    "        ret['runtime'] = dur\n",
    "    return ret\n",
    "        \n",
    "\n",
    "print(categorize(nltk_count(text1), cat, first=False, treshold=0.5))\n",
    "print(categorize(nltk_count(text1), cat, first=True, treshold=0.9))\n",
    "print(categorize(nltk_count(text1), cat, first=False, treshold=0.5, runtime=True))\n",
    "print(categorize(nltk_count(text1), cat, first=True, treshold=0.9, runtime=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# measure error treshold\\ncsv_in = '../data/database/db_04_analyzed_v02.csv'\\ncsv_out = '../data/database/categorizer.csv'\\ndf = pd.read_csv(csv_in, sep=';')\\nprint(df.shape)\\n\\ndf_out = []\\nquit = 0\\nmatch_c = sim_c = match_sc = sim_sc = 0\\n\\nstart = time.time()\\nfor index, row in df.iterrows():\\n    print('###')\\n    print(index, row['link'])\\n    c = row['industry']\\n    sc = row['type']\\n    d = row['description']\\n    item = {\\n        'link': row['link'],\\n        'category': c,\\n        'subcategory': sc,\\n    }\\n    \\n    print('category:', c)\\n    try:\\n        c_guess = categorize(d, cat, first=False, treshold=0.25)\\n    except:\\n        c_guess = {}\\n    print('guess:', c_guess)\\n    item['category_guess'] = json.dumps(c_guess)\\n    item['category_match'] = False\\n    item['category_similar'] = False\\n    if len(c_guess) > 0:\\n        keys = list(c_guess.keys())\\n        if c == keys[0]:\\n            print('MATCH')\\n            item['category_match'] = True\\n            match_c += 1\\n        elif c in keys:\\n            print('SIMILAR')\\n            item['category_similar'] = True\\n            sim_c += 1\\n    \\n    print('subcategory:', sc)\\n    try:\\n        sc_guess = categorize(d, subcat, first=False, treshold=0.25)\\n    except:\\n        sc_guess = {}\\n    print('guess:', sc_guess)\\n    item['subcategory_guess'] = json.dumps(sc_guess)\\n    item['subcategory_match'] = False\\n    item['subcategory_similar'] = False\\n    if len(sc_guess) > 0:\\n        keys = list(sc_guess.keys())\\n        if sc == keys[0]:\\n            print('MATCH')\\n            item['subcategory_match'] = True\\n            match_sc += 1\\n        elif sc in keys:\\n            print('SIMILAR')\\n            item['subcategory_similar'] = True\\n            sim_sc += 1\\n    \\n    df_out.append(item)\\n    \\n    if quit != 0 and index+1 >= quit:\\n        break\\nend = time.time()\\n\\ndf_out = pd.DataFrame(df_out)\\ndf_out.to_csv(csv_out, sep=';', index=False)\\nprint('done in', round(end-start, 3), 'sec')\\nprint(index+1, match_c, sim_c, match_sc, sim_sc)\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# measure error treshold\n",
    "csv_in = '../data/database/db_04_analyzed_v02.csv'\n",
    "csv_out = '../data/database/categorizer.csv'\n",
    "df = pd.read_csv(csv_in, sep=';')\n",
    "print(df.shape)\n",
    "\n",
    "df_out = []\n",
    "quit = 0\n",
    "match_c = sim_c = match_sc = sim_sc = 0\n",
    "\n",
    "start = time.time()\n",
    "for index, row in df.iterrows():\n",
    "    print('###')\n",
    "    print(index, row['link'])\n",
    "    c = row['industry']\n",
    "    sc = row['type']\n",
    "    d = row['description']\n",
    "    item = {\n",
    "        'link': row['link'],\n",
    "        'category': c,\n",
    "        'subcategory': sc,\n",
    "    }\n",
    "    \n",
    "    print('category:', c)\n",
    "    try:\n",
    "        c_guess = categorize(d, cat, first=False, treshold=0.25)\n",
    "    except:\n",
    "        c_guess = {}\n",
    "    print('guess:', c_guess)\n",
    "    item['category_guess'] = json.dumps(c_guess)\n",
    "    item['category_match'] = False\n",
    "    item['category_similar'] = False\n",
    "    if len(c_guess) > 0:\n",
    "        keys = list(c_guess.keys())\n",
    "        if c == keys[0]:\n",
    "            print('MATCH')\n",
    "            item['category_match'] = True\n",
    "            match_c += 1\n",
    "        elif c in keys:\n",
    "            print('SIMILAR')\n",
    "            item['category_similar'] = True\n",
    "            sim_c += 1\n",
    "    \n",
    "    print('subcategory:', sc)\n",
    "    try:\n",
    "        sc_guess = categorize(d, subcat, first=False, treshold=0.25)\n",
    "    except:\n",
    "        sc_guess = {}\n",
    "    print('guess:', sc_guess)\n",
    "    item['subcategory_guess'] = json.dumps(sc_guess)\n",
    "    item['subcategory_match'] = False\n",
    "    item['subcategory_similar'] = False\n",
    "    if len(sc_guess) > 0:\n",
    "        keys = list(sc_guess.keys())\n",
    "        if sc == keys[0]:\n",
    "            print('MATCH')\n",
    "            item['subcategory_match'] = True\n",
    "            match_sc += 1\n",
    "        elif sc in keys:\n",
    "            print('SIMILAR')\n",
    "            item['subcategory_similar'] = True\n",
    "            sim_sc += 1\n",
    "    \n",
    "    df_out.append(item)\n",
    "    \n",
    "    if quit != 0 and index+1 >= quit:\n",
    "        break\n",
    "end = time.time()\n",
    "\n",
    "df_out = pd.DataFrame(df_out)\n",
    "df_out.to_csv(csv_out, sep=';', index=False)\n",
    "print('done in', round(end-start, 3), 'sec')\n",
    "print(index+1, match_c, sim_c, match_sc, sim_sc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 'sw', 'language': 'swahili', 'probability': '0.9999971210408874'}\n",
      "de\n",
      "en\n",
      "en\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# language detection\n",
    "# https://towardsdatascience.com/how-to-detect-and-translate-languages-for-nlp-project-dfd52af0c3b5\n",
    "from langdetect import detect, detect_langs, DetectorFactory\n",
    "\n",
    "language_codes = {'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu', 'fil': 'Filipino', 'he': 'Hebrew'}\n",
    "\n",
    "def lingo(text, simple=True):\n",
    "    DetectorFactory.seed = 0\n",
    "    try:\n",
    "        if simple == True:\n",
    "            return detect(text) #language_codes[detect(text)]\n",
    "        else:\n",
    "            l = str(detect_langs(text)[0]).split(':')\n",
    "            l = {\n",
    "                'code': l[0],\n",
    "                'language': language_codes[ l[0] ],\n",
    "                'probability': l[1],\n",
    "            }\n",
    "            return l\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "sentence = \"Tanzania ni nchi inayoongoza kwa utalii barani afrika\"\n",
    "sentence2 = \"Heute schneit es.\"\n",
    "\n",
    "print(lingo(sentence, simple=False))\n",
    "print(lingo(sentence2))\n",
    "print(lingo(text1))\n",
    "print(lingo(text2))\n",
    "print(lingo(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to rebuild list from string\n",
    "# that happens when it is stored in CSV without json-encode the data\n",
    "def str_to_list(s):\n",
    "    s = s.replace(\"'\", \"\").replace(' ,', ',').replace(\n",
    "        '[', '').replace(']', '').split(',')\n",
    "    s = [i.replace('\"','').strip() for i in s if i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create folder create_folder\n",
    "def create_folder(path):\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path))\n",
    "            print(path + ' created')\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic store data to file function\n",
    "def store_data(data, file, mode='w', toJson=False):\n",
    "    if toJson:\n",
    "        data = json.dumps(data)\n",
    "    with open(file, mode, encoding='utf-8') as fp:\n",
    "        result = fp.write(data)\n",
    "        return result\n",
    "    \n",
    "# generic load data from file function\n",
    "def load_data(file, fromJson=False):\n",
    "    if os.path.isfile(file):\n",
    "        with open(file, 'r', encoding='utf-8', errors=\"ignore\") as fp:\n",
    "            data = fp.read()\n",
    "            if fromJson:\n",
    "                data = json.loads(data)\n",
    "            return data\n",
    "    else:\n",
    "        return 'file not found'\n",
    "\n",
    "# test text\n",
    "#print(store_data('Hello', '../data/repositories/mlart/test.txt'))\n",
    "#print(load_data('../data/repositories/mlart/test.txt'))\n",
    "\n",
    "# test json\n",
    "#print(store_data({'msg':'Hello World'}, '../data/repositories/mlart/test.json', toJson=True))\n",
    "#print(load_data('../data/repositories/mlart/test.json', fromJson=True))\n",
    "\n",
    "#store_data(result[0]['html'], '../data/repositories/kaggle/notebook.html')\n",
    "#store_data(result[0]['iframe'], '../data/repositories/kaggle/kernel.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "def clean_text(text):\n",
    "    # Ref: https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1#gistcomment-3208085\n",
    "    # Ref: https://en.wikipedia.org/wiki/Unicode_block\n",
    "    EMOJI_PATTERN = re.compile(\n",
    "        \"([\"\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"])\"\n",
    "    )\n",
    "    text = re.sub(EMOJI_PATTERN, '', text)\n",
    "    \n",
    "    # additional cleanup\n",
    "    text = text.replace('•','').replace('\\n',' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RNN']\n"
     ]
    }
   ],
   "source": [
    "tag_filter = {\n",
    "    # '3D',\n",
    "    '3D Ken Burns Effect': 'Ken Burns 3D',\n",
    "    # '3D Photo Inpainting',\n",
    "    'AI': None,\n",
    "    # 'ANN',\n",
    "    # 'Ableton Live',\n",
    "    'Activations': None,\n",
    "    'Aeriolod': None,\n",
    "    # 'AlexNet',\n",
    "    'Analytics  Competition': None,\n",
    "    # 'Animating Landscape',\n",
    "    # 'Anomaly Detection',\n",
    "    'ArtBreeder': 'Artbreeder',\n",
    "    # 'Artbreeder',\n",
    "    # 'AttnGAN',\n",
    "    # 'AutoML',\n",
    "    # 'B3D',\n",
    "    # 'BASNet',\n",
    "    # 'Background Removal',\n",
    "    # 'Bayesian',\n",
    "    # 'BigGAN',\n",
    "    # 'BodyPix',\n",
    "    # 'Boltzmann Machine',\n",
    "    # 'CAD',\n",
    "    # 'CMA-ES',\n",
    "    # 'CMT',\n",
    "    # 'CNN',\n",
    "    # 'CPPN',\n",
    "    # 'CV',\n",
    "    'Camera': None,\n",
    "    # 'Chatbot',\n",
    "    # 'Classification',\n",
    "    'Classifier': 'Classification',\n",
    "    # 'Clustering',\n",
    "    # 'Colorization',\n",
    "    'Contentless': None,\n",
    "    # 'Corpus-based synthesis',\n",
    "    # 'CycleGAN',\n",
    "    # 'DCGAN',\n",
    "    # 'DDSP',\n",
    "    # 'DL',\n",
    "    # 'DLIB',\n",
    "    # 'DeOldify',\n",
    "    # 'Decision Tree',\n",
    "    'Deep Fakes': 'DeepFake',\n",
    "    # 'Deep Painterly Harmonization',\n",
    "    # 'DeepDream',\n",
    "    # 'DeepFake',\n",
    "    # 'DeepFlow',\n",
    "    # 'DenseCap',\n",
    "    'Depth Map': None,\n",
    "    # 'Detectron',\n",
    "    'Detectron2': 'Detectron',\n",
    "    'Device': None,\n",
    "    'Discriminator': None,\n",
    "    'Document Summarization': 'Summarization',\n",
    "    # 'ESR-GAN',\n",
    "    # 'Edge Detection',\n",
    "    # 'Expression Detection',\n",
    "    'Face Alignment': 'Face Detection',\n",
    "    # 'Face Detection',\n",
    "    # 'Face Recognition',\n",
    "    'Face Tracking': 'Face Detection',\n",
    "    'Face markers': 'Face Detection',\n",
    "    'Face recognition': 'Face Recognition',\n",
    "    'Facial Detection': 'Face Detection',\n",
    "    'Facial Recognition': 'Face Recognition',\n",
    "    # 'FastPhotoStyle',\n",
    "    # 'Feature Mixing',\n",
    "    'Feature vectors': None,\n",
    "    'Featured Code Competition': None,\n",
    "    'Featured Simulation Competition': None,\n",
    "    'Featured prediction Competition': None,\n",
    "    # 'Federated Learning',\n",
    "    # 'Few Shot Animation',\n",
    "    'First Order Motion': None,\n",
    "    # 'GAN',\n",
    "    # 'GBM',\n",
    "    # 'GPT',\n",
    "    'GPT-2': 'GPT',\n",
    "    # 'GRU',\n",
    "    'Game of Life': None,\n",
    "    # 'GauGAN',\n",
    "    # 'Gaussian Mixture Model',\n",
    "    # 'Genetic Algorithm',\n",
    "    'Getting Started prediction Competition': None,\n",
    "    # 'Gradient Ascent',\n",
    "    # 'Gradient Boosting',\n",
    "    # 'Gradient Smoothing',\n",
    "    # 'Grannma Magnet',\n",
    "    'Hardware': None,\n",
    "    'Height map': None,\n",
    "    'Heightfield': None,\n",
    "    'Houdini': None,\n",
    "    # 'Image Captioning',\n",
    "    # 'Image Segmentation',\n",
    "    # 'ImageJ',\n",
    "    # 'ImageNet',\n",
    "    # 'Inception',\n",
    "    # 'Inpainting',\n",
    "    'Interpolation': None,\n",
    "    # 'IoT',\n",
    "    'K-Means': 'K-means',\n",
    "    # 'K-means',\n",
    "    # 'KNN',\n",
    "    # 'Ken Burns 3D',\n",
    "    # 'Kolmogorov complexity',\n",
    "    # 'LSTM',\n",
    "    # 'Laplacian Pyramid',\n",
    "    'Lenticular': 'Lenticular Printing',\n",
    "    'Lenticular Print': 'Lenticular Printing',\n",
    "    # 'Linear Regression',\n",
    "    # 'Logistic Regression',\n",
    "    'ML': None,\n",
    "    # 'Machine Translation',\n",
    "    'Machine translation': 'Machine Translation',\n",
    "    'Magenta': None,\n",
    "    # 'Markov Chain',\n",
    "    'Markov Chains': 'Markov Chain',\n",
    "    'Memory Mosaic': None,\n",
    "    'Microphone': None,\n",
    "    'Mixture Density Networks': 'MDN',\n",
    "    'Multi-Domain Multi-Modality I2I translation': 'Image2Image',\n",
    "    'Multi-Style Transfer': 'Style Transfer',\n",
    "    # 'Music Transformer',\n",
    "    # 'N-gram',\n",
    "    # 'NER',\n",
    "    # 'NLG',\n",
    "    # 'NLP',\n",
    "    # 'NLU',\n",
    "    'NN': None,\n",
    "    # 'NNS',\n",
    "    # 'NSynth',\n",
    "    'NSynth Super': 'NSynth',\n",
    "    # 'Naive Bayes',\n",
    "    'Nerual CA': 'Neural Cellular Automata',\n",
    "    # 'Neural Cellular Automata',\n",
    "    # 'Object Detection',\n",
    "    # 'Occams razor',\n",
    "    'Open Pose': 'OpenPose',\n",
    "    # 'OpenCV',\n",
    "    # 'OpenPose',\n",
    "    # 'Optical Flow',\n",
    "    'Optical flow': 'Optical Flow',\n",
    "    'Perlin Noise': None,\n",
    "    # 'Photogrammetry',\n",
    "    'Photoshop': None,\n",
    "    'Pix2Pix': 'Image2Image',\n",
    "    'Pix2pix': 'Image2Image',\n",
    "    'Pixel2style2pixel': 'Image2Image',\n",
    "    'Playground Code Competition': None,\n",
    "    'Playground prediction Competition': None,\n",
    "    # 'Point Cloud',\n",
    "    # 'PoseNet',\n",
    "    # 'ProGAN',\n",
    "    'Progressively Grown GAN': 'ProGAN',\n",
    "    # 'Projective Non-negative Matrix Factorization',\n",
    "    # 'Quantum Computer',\n",
    "    # 'QuickDraw',\n",
    "    # 'RL',\n",
    "    # 'RNN',\n",
    "    # 'Random Forest',\n",
    "    # 'Raymarching',\n",
    "    # 'ReLu',\n",
    "    # 'Recommender',\n",
    "    'Recruitment prediction Competition': None,\n",
    "    'Rectifier': 'ReLu',\n",
    "    # 'Regression',\n",
    "    'Reinforcement Learning': 'RL',\n",
    "    # 'ResNet',\n",
    "    'Research Code Competition': None,\n",
    "    'Research prediction Competition': None,\n",
    "    'Resnet': 'ResNet',\n",
    "    # 'SIFT',\n",
    "    # 'SNGAN',\n",
    "    # 'SOM',\n",
    "    # 'Self-attention',\n",
    "    'Semantic search': 'Semantic Search',\n",
    "    # 'Sentiment Analysis',\n",
    "    # 'Simplex Volume Maximization',\n",
    "    # 'SinGAN',\n",
    "    # 'SketchRNN',\n",
    "    # 'Sparse Transformer',\n",
    "    # 'Speech Recognition',\n",
    "    # 'Speech to text',\n",
    "    # 'Style Transfer',\n",
    "    # 'StyleGAN',\n",
    "    'StyleGAN2': 'StyleGAN',\n",
    "    'StyleTransfer': 'Style Transfer',\n",
    "    # 'Super Slo Mo',\n",
    "    # 'Super-Resolution',\n",
    "    'Super-resolution': 'Super-Resolution',\n",
    "    'Superresolution': 'Super-Resolution',\n",
    "    # 'Supervised Learning',\n",
    "    'Support Vector Machines': 'SVM',\n",
    "    'TensorFlow.js': None,\n",
    "    'Tensorflow.js': None,\n",
    "    'Text Classification': 'Classification',\n",
    "    'Text To Speech': 'Text To Speech',\n",
    "    'Text classification': 'Classification',\n",
    "    'Text to Animation of Virtual Characters': 'Text to Animation',\n",
    "    # 'Texture synthesis',\n",
    "    # 'Transformer',\n",
    "    # 'Translation',\n",
    "    # 'U-Net',\n",
    "    'U-net': 'U-Net',\n",
    "    # 'UMAP',\n",
    "    'Unsupervised learning': 'Unsupervised Learning',\n",
    "    # 'VAE',\n",
    "    # 'VGG',\n",
    "    'VQ-VAE': 'VAE',\n",
    "    # 'VR',\n",
    "    'Video StyleTransfer': 'Style Transfer',\n",
    "    # 'Voice Detection',\n",
    "    'Voice detection': 'Voice Detection',\n",
    "    # 'Watson Beat',\n",
    "    # 'Wav2Lip',\n",
    "    # 'WaveGAN',\n",
    "    # 'Wavenet',\n",
    "    'Weights': None,\n",
    "    # 'Word2Vec',\n",
    "    'advanced': None,\n",
    "    'animals': None,\n",
    "    'arts and entertainment': 'Arts and Entertainment',\n",
    "    'astronomy': 'Astronomy',\n",
    "    'audio data': None,\n",
    "    'automobiles and vehicles': 'Automotive',\n",
    "    'banking': 'Banking',\n",
    "    'basketball': 'Sports',\n",
    "    'bayesian statistics': None,\n",
    "    'beginner': None,\n",
    "    'bigquery': 'BigQuery',\n",
    "    'binary classification': 'Classification',\n",
    "    'biology': 'Biology',\n",
    "    # 'cDCGAN',\n",
    "    'california': None,\n",
    "    'categorical data': None,\n",
    "    'china': None,\n",
    "    'classification': 'Classification',\n",
    "    'clustering': 'CLustering',\n",
    "    'cnn': 'CNN',\n",
    "    'computer science': None,\n",
    "    'computer vision': 'CV',\n",
    "    'covid19': None,\n",
    "    'cuml-UMAP': 'UMAP',\n",
    "    'dailychallenge': None,\n",
    "    'data analytics': None,\n",
    "    'data cleaning': None,\n",
    "    'data visualization': None,\n",
    "    'decision tree': 'Decision Tree',\n",
    "    'deep learning': 'DL',\n",
    "    'deepflow': 'DeepFlow',\n",
    "    'dimensionality reduction': 'Dimensionality Reduction',\n",
    "    'diseases': 'Diseases',\n",
    "    'e-commerce services': 'E-Commerce',\n",
    "    'earth and nature': 'Earth and Nature',\n",
    "    'employment': 'Employment',\n",
    "    'ensembling': 'Ensembling',\n",
    "    'environment': 'Environment',\n",
    "    'exploratory data analysis': 'Exploratory Data Analysis',\n",
    "    'feature engineering': 'Feature Engineering',\n",
    "    'finance': 'Finance',\n",
    "    'forestry': 'Forestry',\n",
    "    'games': 'Games',\n",
    "    'gan': 'GAN',\n",
    "    'genetics': 'Genetics',\n",
    "    'geospatial analysis': 'Geospatial Analysis',\n",
    "    'gpu': None,\n",
    "    'gradient boosting': 'Gradient Boosting',\n",
    "    'health': 'Healthcare',\n",
    "    'healthcare': 'Healthcare',\n",
    "    'image data': None,\n",
    "    'india': None,\n",
    "    'intermediate': None,\n",
    "    'jobs and career': None,\n",
    "    'k-means': 'K-means',\n",
    "    'keras': 'Keras',\n",
    "    'languages': None,\n",
    "    'learn': None,\n",
    "    'lightgbm': 'Gradient Boosting',\n",
    "    'linear regression': 'Linear Regression',\n",
    "    'linguistics': None,\n",
    "    'logistic regression': 'Logistic Regression',\n",
    "    'lstm': 'LSTM',\n",
    "    'medicine': 'Healthcare',\n",
    "    'microcontroller': None,\n",
    "    'model comparison': 'Model Comparison',\n",
    "    'model explainability': 'Model Explainability',\n",
    "    'multiclass classification': 'Classification',\n",
    "    'multilabel classification': 'Classification',\n",
    "    'naive bayes': 'Naive Bayes',\n",
    "    'neural networks': None,\n",
    "    'nlp': 'NLP',\n",
    "    'ofxSelfOrganizingMap': 'Self-organizing map',\n",
    "    'openFrameworks': None,\n",
    "    'optimization': None,\n",
    "    'outlier analysis': 'Outlier Analysis',\n",
    "    'pca': None,\n",
    "    'physics': 'Physics',\n",
    "    'pix2code': 'Pix2Code',\n",
    "    'pix2pix': 'Image2Image',\n",
    "    'plants': 'Plants',\n",
    "    'pollution': 'Pollution',\n",
    "    'puzzles': None,\n",
    "    'python': None,\n",
    "    'pytorch': None,\n",
    "    # 'rGMIR',\n",
    "    'random forest': 'Random Forest',\n",
    "    'recommender systems': 'Recommender',\n",
    "    'regression': 'Regression',\n",
    "    'reinforcement learning': 'RL',\n",
    "    'research': None,\n",
    "    'rnn': 'RNN',\n",
    "    'robotics': 'Robotics',\n",
    "    'sampling': None,\n",
    "    'signal processing': None,\n",
    "    'simulations': None,\n",
    "    'spaCy': 'NLP',\n",
    "    'sports': 'Sports',\n",
    "    'survey analysis': None,\n",
    "    'svm': 'SVM',\n",
    "    # 't-SNE',\n",
    "    'tabular data': None,\n",
    "    'tensorflow': None,\n",
    "    'text data': None,\n",
    "    'text mining': None,\n",
    "    'time series analysis': 'Time Series Analysis',\n",
    "    'tpu': None,\n",
    "    'transfer learning': 'Transfer Learning',\n",
    "    'utility script': None,\n",
    "    'video games': 'Games',\n",
    "    'xgboost': 'Xgboost',\n",
    "}\n",
    "\n",
    "def tag_equalizer(tags):\n",
    "    tags = [tag_filter.get(x, x) for x in tags]\n",
    "    tags = list(filter(None, tags))\n",
    "    return tags\n",
    "\n",
    "print(tag_equalizer(['tpu', 'rnn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper to convert CSV to the mapping of Elasticsearch index\n",
    "def mapper(row, style, extra={}):\n",
    "    '''\n",
    "    mapper to adopt csv to db-schema\n",
    "\n",
    "    \"title\"\n",
    "    \"summarization\"\n",
    "    \"words\"\n",
    "    \"sum_words\"\n",
    "    \"link\"\n",
    "    \"source\"\n",
    "    \"category\"\n",
    "    \"category_score\"\n",
    "    \"subcategory\"\n",
    "    \"subcategory_score\"\n",
    "    \"tags\"\n",
    "    \"kind\"\n",
    "    \"ml_libs\"\n",
    "    \"host\"\n",
    "    \"license\"\n",
    "    \"programming_language\"\n",
    "    \"ml_score\"\n",
    "    \"learn_score\"\n",
    "    \"explore_score\"\n",
    "    \"compete_score\"\n",
    "    \"engagement_score\"\n",
    "    \"date_project\"\n",
    "    \"date_scraped\"\n",
    "    '''\n",
    "\n",
    "    # kaggle competition mapping\n",
    "    if style == 'kaggle_competition':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            'description': row['subtitle'] + row['description'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': list(set(str_to_list(row['tags']) + str_to_list(row['type']))),\n",
    "            'kind': ['Project', '(Competition)', '(Dataset)'],\n",
    "            # 'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'www.kaggle.com',\n",
    "            # 'license': row['license'],\n",
    "            # 'programming_language': row['type'],\n",
    "            # 'ml_score': 0,\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "            'engagement_score': row['teams_score'],\n",
    "            'date_project': datetime.strptime(row['date_closed'], \"%Y-%m-%d %H:%M:%S\") if 'date_closed' in row else '',\n",
    "            # 'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            # 'ml_terms': row['ml_terms'],\n",
    "            # 'score_raw': json.dumps({'views': row['views'], 'votes': row['votes'], 'score_private': row['score_private'], 'score_public': row['score_public']}),\n",
    "        }\n",
    "    \n",
    "    # kaggle dataset mapping\n",
    "    if style == 'kaggle_dataset':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            'description': row['subtitle'] + row['description'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': list(set(str_to_list(row['tags']) + str_to_list(row['type']))),\n",
    "            'kind': ['Project', '(Dataset)'],\n",
    "            # 'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'www.kaggle.com',\n",
    "            # 'license': row['license'],\n",
    "            # 'programming_language': row['type'],\n",
    "            # 'ml_score': 0,\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "            'engagement_score': row['teams_score'],\n",
    "            'date_project': datetime.strptime(row['date_closed'], \"%Y-%m-%d %H:%M:%S\") if 'date_closed' in row else '',\n",
    "            # 'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            # 'ml_terms': row['ml_terms'],\n",
    "            # 'score_raw': json.dumps({'views': row['views'], 'votes': row['votes'], 'score_private': row['score_private'], 'score_public': row['score_public']}),\n",
    "        }\n",
    "    \n",
    "    # kaggle notebook mapping\n",
    "    if style == 'kaggle_notebook':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            'description': row['description'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': list(set(str_to_list(row['tags']) + str_to_list(row['tags']))),\n",
    "            'kind': ['Project', '(Notebook)'],\n",
    "            'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'www.kaggle.com',\n",
    "            'license': row['license'],\n",
    "            'programming_language': row['type'],\n",
    "            'ml_score': row['ml_detected'],\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "            'engagement_score': row['score_views'] if 'score_views' in row else None,\n",
    "            'date_project': datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S\") if row['date'] != '' else None,\n",
    "            'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\") if row['scraped_at'] != '' else None,\n",
    "            # 'ml_terms': row['ml_terms'],\n",
    "            # 'score_raw': json.dumps({'views': row['views'], 'votes': row['votes'], 'score_private': row['score_private'], 'score_public': row['score_public']}),\n",
    "        }\n",
    "\n",
    "    # github mapping\n",
    "    if style == 'github':\n",
    "        title = row['name'] if row['name'] != '' else row['title']\n",
    "        title = title.replace('-',' ').replace('_',' ').strip()\n",
    "        cat_score = 1 if row['industry'] != '' else 0\n",
    "        subcat_score = 1 if row['type'] != '' else 0\n",
    "        #tags = row['ml_tags'] if len(row['ml_tags']) > 0 else ''\n",
    "        ret = {\n",
    "            'title': title,\n",
    "            'description': row['description2'],\n",
    "            'link': row['link'],\n",
    "            'category': row['industry'],\n",
    "            'category_score': cat_score,\n",
    "            'subcategory': row['type'],\n",
    "            'subcategory_score': subcat_score,\n",
    "            'tags': str_to_list(row['ml_tags']),\n",
    "            'kind': 'Project',\n",
    "            'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'www.github.com',\n",
    "            'license': row['license'],\n",
    "            'programming_language': row['language_primary'],\n",
    "            'ml_score': row['ml_detected'],\n",
    "            'engagement_score': row['stars_score'],\n",
    "            'date_project': datetime.strptime(row['pushed_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            # 'ml_terms': row['keywords'],\n",
    "            # 'score_raw': json.dumps({'stars': row['stars'], 'contributors': row['contributors']}),\n",
    "        }\n",
    "\n",
    "    # mlart mapping\n",
    "    if style == 'mlart':\n",
    "        title = row['Title'] if row['Title'] != '' else row['title']\n",
    "        cat_score = 1 if row['Theme'] != '' else 0\n",
    "        subcat_score = 1 if row['Medium'] != '' else 0\n",
    "        ret = {\n",
    "            'title': title,\n",
    "            'description': row['subtitle'],\n",
    "            'link': row['url'],\n",
    "            'category': 'Miscellaneous',\n",
    "            'category_score': cat_score,\n",
    "            'subcategory': 'Art',\n",
    "            'subcategory_score': subcat_score,\n",
    "            'tags': str_to_list(row['Theme']) + str_to_list(row['Medium']) + str_to_list(row['Technology']),\n",
    "            'kind': 'Showcase',\n",
    "            # 'ml_libs': [],\n",
    "            'host': 'mlart.co',\n",
    "            # 'license': '',\n",
    "            # 'programming_language': '',\n",
    "            # 'ml_score': row['ml_detected'],\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 1,\n",
    "            'compete_score': 0,\n",
    "            # 'engagement_score': 0,\n",
    "            'date_project': datetime.strptime(row['Date'], \"%Y-%m-%d\"),\n",
    "            'date_scraped': datetime.strptime(row['scraped_at'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            # 'score_raw': json.dumps({'days_since_featured': row['Days Since Featured']}),\n",
    "        }\n",
    "\n",
    "    # thecleverprogrammer\n",
    "    if style == 'tcp':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            'description': row['description'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': str_to_list(row['ml_tags']),\n",
    "            'kind': 'Project',\n",
    "            'ml_libs': str_to_list(row['ml_libs']),\n",
    "            'host': 'thecleverprogrammer.com',\n",
    "            # 'license': '',\n",
    "            'programming_language': 'Python',\n",
    "            'ml_score': row['ml_score'],\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "            # 'engagement_score': 0,\n",
    "            'date_project': datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S\"),\n",
    "            'date_scraped': datetime.strptime('2020-12-20', \"%Y-%m-%d\"),\n",
    "            # 'score_raw': json.dumps({'days_since_featured': row['Days Since Featured']}),\n",
    "        }\n",
    "    \n",
    "    # zalando / bcgdv / medium\n",
    "    if style == 'manual':\n",
    "        ret = {\n",
    "            'title': row['title'],\n",
    "            # 'description': row['description'] if row['description'] != '' else row['text'],\n",
    "            'link': row['link'],\n",
    "            # 'category': '',\n",
    "            # 'category_score': 0,\n",
    "            # 'subcategory': '',\n",
    "            # 'subcategory_score': 0,\n",
    "            'tags': str_to_list(row['tags']),\n",
    "            # 'kind': 'Project',\n",
    "            # 'ml_libs': str_to_list(row['ml_libs']),\n",
    "            # 'host': 'thecleverprogrammer.com',\n",
    "            # 'license': '',\n",
    "            # 'programming_language': 'Python',\n",
    "            # 'ml_score': row['ml_score'],\n",
    "            # 'engagement_score': 0,\n",
    "            # 'date_project': datetime.strptime(row['date'], \"%d.%m.%Y\"),\n",
    "            # 'date_scraped': datetime.strptime(row['date_scraped'], \"%d.%m.%Y\"),\n",
    "            # 'score_raw': json.dumps({'days_since_featured': row['Days Since Featured']}),\n",
    "        }\n",
    "        if ret['title'] == '' and 'company' in row:\n",
    "            ret['title'] = row['company']\n",
    "            \n",
    "        if 'description' in row and row['description'] != '':\n",
    "            ret['description'] = row['description']\n",
    "        else:\n",
    "            ret['description'] = row['text']\n",
    "            \n",
    "        if 'source' in row:\n",
    "            ret['source'] = row['source']\n",
    "            \n",
    "        if 'category' in row:\n",
    "            ret['category'] = row['category']\n",
    "            if 'category_score' in row:\n",
    "                ret['category_score'] = row['category_score']\n",
    "            elif ret['category'] != '':\n",
    "                ret['category_score'] = 1\n",
    "        else:\n",
    "            ret['category'] = ''\n",
    "            \n",
    "        if 'subcategory' in row:\n",
    "            ret['subcategory']= row['subcategory']\n",
    "            if 'subcategory_score' in row:\n",
    "                ret['subcategory_score'] = row['subcategory_score']\n",
    "            elif ret['subcategory'] != '':\n",
    "                ret['subcategory_score'] = 1\n",
    "        else:\n",
    "            ret['subcategory'] = ''\n",
    "            \n",
    "        if 'date' in row and row['date'] != '':\n",
    "            try:\n",
    "                ret['date_project'] = datetime.strptime(row['date'], \"%d.%m.%Y\")\n",
    "            except:\n",
    "                try:\n",
    "                    ret['date_project'] = datetime.strptime(row['date'], \"%Y\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "        if 'date_scraped' in row:\n",
    "            row['date_scraped'] = datetime.strptime(row['date_scraped'], \"%d.%m.%Y\")\n",
    "        \n",
    "        if 'ml_score' in row:\n",
    "            ret['ml_score'] = row['ml_score']\n",
    "            \n",
    "        if 'learn_score' in row:\n",
    "            ret['learn_score'] = row['learn_score']\n",
    "            \n",
    "        if 'explore_score' in row:\n",
    "            ret['explore_score'] = row['explore_score']\n",
    "            \n",
    "        if 'compete_score' in row:\n",
    "            ret['compete_score'] = row['compete_score']\n",
    "        \n",
    "    attach = {**extra}\n",
    "    if 'tags' in attach:\n",
    "        ret['tags'].extend(attach['tags'])\n",
    "        attach.pop('tags')\n",
    "    ret.update(attach)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test gpu usage\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to transform data row-wise\n",
    "def transform_loop(csv_in, csv_format, subfolder, quit=0, overwrite=False, inplace=True, printItem=False, extra={}):\n",
    "    \n",
    "    with open(csv_in, encoding='utf-8') as csvfile:\n",
    "        \n",
    "        # let's store converted csv to temp-folder for analysis\n",
    "        csv_out = '../data/database/csv/'\n",
    "        json_out = '../data/database/json/'\n",
    "        json_out_item = '../data/database/json/'+subfolder\n",
    "        create_folder(json_out_item)\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        # readCSV = csv.reader(csvfile, delimiter=';')\n",
    "        readCSV = csv.DictReader(csvfile, delimiter=';')\n",
    "        # next(readCSV, None)  # skip the headers\n",
    "        \n",
    "        i = j = 0\n",
    "        out = []\n",
    "        \n",
    "        for row in readCSV:\n",
    "            row = mapper(row, csv_format, extra=extra)\n",
    "            if printItem == True:\n",
    "                print(json.dumps(row, indent=3, sort_keys=True, default=str))\n",
    "            \n",
    "            # check if file already exists\n",
    "            link = row['link']\n",
    "            md5 = hashlib.md5(link.encode(\"utf-8\")).hexdigest()\n",
    "            \n",
    "            json_fp = json_out_item + md5 + '.json'\n",
    "            \n",
    "            old = {}\n",
    "            if os.path.isfile(json_fp) and overwrite == True or not os.path.isfile(json_fp):\n",
    "                if os.path.isfile(json_fp):\n",
    "                    old = load_data(json_fp, fromJson=True)\n",
    "                \n",
    "                print(i, row['link'])\n",
    "                item_start = time.time()\n",
    "\n",
    "                # clean title & description\n",
    "                row['title'] = clean_text(row['title'])\n",
    "                text = row['description'] = clean_text(row['description'])\n",
    "                words = row['words'] = word_count(text)\n",
    "                sentences = row['sentences'] = sentence_count(text)\n",
    "\n",
    "                # create summarization\n",
    "                if words > 200 and sentences > 1 and (not 'sum_nltk' in old or not 'sum_t5' in old):\n",
    "                    print('summarize')\n",
    "                    \n",
    "                    # nltk\n",
    "                    if not 'sum_nltk' in old:\n",
    "                        start = time.time()\n",
    "                        row['sum_nltk'] = nltk_count(text, word_count=200)\n",
    "                        end = time.time()\n",
    "                        dur = round(end-start,3)\n",
    "\n",
    "                        row['sum_nltk_words'] = word_count(row['sum_nltk'])\n",
    "                        row['sum_nltk_runtime'] = dur\n",
    "                        print('done (nltk)', dur, 'sec')\n",
    "                    \n",
    "                    # t5\n",
    "                    if not 'sum_t5' in old:\n",
    "                        start = time.time()\n",
    "                        row['sum_t5'] = t5(text)\n",
    "                        end = time.time()\n",
    "                        dur = round(end-start,3)\n",
    "\n",
    "                        row['sum_t5_words'] = word_count(row['sum_t5'])\n",
    "                        row['sum_t5_runtime'] = dur\n",
    "                        print('done (t5)', dur, 'sec')\n",
    "                \n",
    "                # detect language\n",
    "                if not 'language_code' in old:\n",
    "                    s = row['description'] if 'description' in row and row['description'] != '' else row['title']\n",
    "                    lang = lingo(s, simple=False)\n",
    "                    if lang != None:\n",
    "                        row['language_code'] = lang['code']\n",
    "                        row['language'] = lang['language']\n",
    "                        row['language_score'] = lang['probability']\n",
    "                    else:\n",
    "                        row['language_code'] = None\n",
    "                        row['language'] = None\n",
    "                        row['language_score'] = None\n",
    "\n",
    "                # equalizer\n",
    "                if 'programming_language' in row and row['programming_language'] == 'Python notebook':\n",
    "                    row['programming_language'] = 'Jupyter Notebook'\n",
    "                    \n",
    "                if 'license' in row:\n",
    "                    if row['license'] == 'Apache 2.0':\n",
    "                        row['license'] = 'Apache-2.0'\n",
    "                    if row['license'] == 'Learn more about GitHub Sponsors':\n",
    "                        row['license'] = None\n",
    "                    if row['license'] == 'Unlicense':\n",
    "                        row['license'] = None\n",
    "                        \n",
    "                row['tags'] = tag_equalizer(row['tags'])\n",
    "                \n",
    "\n",
    "                # convert datetime to string\n",
    "                if 'date_project' in row:\n",
    "                    row['date_project'] = str(row['date_project'])\n",
    "                if 'date_scraped' in row:\n",
    "                    row['date_scraped'] = str(row['date_scraped'])\n",
    "                    \n",
    "                # runtime\n",
    "                item_end = time.time()\n",
    "                item_dur = round(item_end-item_start, 3)\n",
    "                row['runtime'] = item_dur\n",
    "\n",
    "                #df = df.append(row, ignore_index=True)\n",
    "\n",
    "                # json encode\n",
    "                #out.append(row)\n",
    "                \n",
    "                if overwrite == True and inplace==True:\n",
    "                    row = {**old, **row}\n",
    "                    drop = ['score']\n",
    "                    for key in drop:\n",
    "                        if key in row:\n",
    "                            row.pop(key)\n",
    "                    # restore category, subcategory and runtime\n",
    "                    if row['category'] == '' and 'category' in old:\n",
    "                        row['category'] = old['category']\n",
    "                    if row['category_score'] == '' and 'category_score' in old:\n",
    "                        row['category_score'] = old['category_score']\n",
    "                    if row['subcategory'] == '' and 'subcategory' in old:\n",
    "                        row['subcategory'] = old['subcategory']\n",
    "                    if row['subcategory_score'] == '' and 'subcategory_score' in old:\n",
    "                        row['subcategory_score'] = old['subcategory_score']\n",
    "                    if row['runtime'] == '' and 'runtime' in old:\n",
    "                        row['runtime'] = old['runtime']\n",
    "                            \n",
    "                #print(row)\n",
    "                #sys.exit()\n",
    "                \n",
    "                if row != old:\n",
    "                    store_data(row, json_fp, toJson=True)\n",
    "                    print('stored:', json_fp)\n",
    "                j += 1\n",
    "\n",
    "            #print(i, row['link'])\n",
    "            i += 1\n",
    "\n",
    "            # keep count of # rows processed\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "\n",
    "            if quit != 0 and i >= quit:\n",
    "                break\n",
    "\n",
    "        # store parsed csv\n",
    "        #fp = csv_in.split('/')[-1]\n",
    "        #df.to_csv(csv_out + fp, sep=';', index=False)\n",
    "        #path = json_out + fp\n",
    "        #path = path.replace('.csv', '.json')\n",
    "        #store_data(out, path, toJson=True)\n",
    "        \n",
    "        print('DONE parsed', i, 'items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "za_bl\n",
      "0 https://github.com/flairNLP/flair\n",
      "stored: ../data/database/json/za/0d4e98dc5312c9b4ec96665d5af364b0.json\n",
      "1 https://engineering.zalando.com/posts/2017/03/deep-learning-in-production-for-predicting-consumer-behavior.html\n",
      "stored: ../data/database/json/za/5cbb244aa92009613710b223c31b5f3a.json\n",
      "2 https://engineering.zalando.com/posts/2018/09/shop-look-deep-learning.html\n",
      "stored: ../data/database/json/za/fbc34098cc6faa78a18deb9141f2e930.json\n",
      "DONE parsed 3 items\n",
      "za_jo\n",
      "0 https://jobs.zalando.com/en/jobs/2419780-senior-applied-scientist-w-m-d-advice-and-inspiration/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/0641bcf216a7a29247bd3fda748a903a.json\n",
      "1 https://jobs.zalando.com/en/jobs/2523598--senior-research-scientist-builder-platform-and-ai/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/4aceb49e319eb060f76bf601951cfc34.json\n",
      "2 https://jobs.zalando.com/en/jobs/2261169-senior-python-backend-engineer-competitive-analytics-engineering/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/39f7a137f3e8f283935379a0192e9c8a.json\n",
      "3 https://jobs.zalando.com/en/jobs/2243058-senior-data-scientist-customer-analytics/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/497e4e053425e98b97f4eb6d4ed17bee.json\n",
      "4 https://jobs.zalando.com/en/jobs/2326918-sr-product-manager-forecasting-machine-learning-w-m-d/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/c7a126b74aa9891c4ed3abff9f960345.json\n",
      "5 https://jobs.zalando.com/en/jobs/2426949-senior-softwareentwickler-fulfillment-core-m-w-d/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/b2ce716f37d261f80cc7f877e0d4a5aa.json\n",
      "6 https://jobs.zalando.com/en/jobs/2381594-applied-scientist-research-engineer-fulfillment-planning-w-m-d/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/7683f2329b436cb8235d83110c4b1e74.json\n",
      "7 https://jobs.zalando.com/en/jobs/2332284-senior-research-engineer-nlp-search-w-m-d/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/3ecb8a90ba195d5dd065ffb2867ac7d7.json\n",
      "8 https://jobs.zalando.com/en/jobs/2323481-senior-data-scientist-pricing-forecasting-w-m-d/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/af255667af454be2159035bcb740adf2.json\n",
      "9 https://jobs.zalando.com/en/jobs/2450295-vp-profile-and-personalization-digital-experience/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/7a811fdff5ec7b6a2ab8af5a923bf183.json\n",
      "10 https://jobs.zalando.com/en/jobs/2549206-senior-backend-engineer-purchase-risk-management-machine-learning/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/139229760e226ccb336e414a96e8812d.json\n",
      "11 https://jobs.zalando.com/en/jobs/2352248-applied-scientist-machine-learning-dl/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/251a1a0638db465cb256562a6ec90c05.json\n",
      "12 https://jobs.zalando.com/en/jobs/2255961-senior-or-principal-applied-scientist-machine-learning-w-m-d/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/9f5d69ddab19284d87646a7d7acb9875.json\n",
      "13 https://jobs.zalando.com/en/jobs/2385949-applied-scientist-payments-w-m-d/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/8593961178468efdc9ac90edfb24fdd5.json\n",
      "14 https://jobs.zalando.com/en/jobs/2319248-senior-java-engineer-machine-learning-zalando-payments/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/188bda8c611be8e6bb143d058805afec.json\n",
      "15 https://jobs.zalando.com/en/jobs/2177109-sr-product-manager-sales-supply-technology-w-m-d/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/309da34b8a426d50378453b20404a27b.json\n",
      "16 https://jobs.zalando.com/en/jobs/2368858-engineering-manager-style-advice-team-data-engineering/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/1047cca10499683e6e2b25cf964c862d.json\n",
      "17 https://jobs.zalando.com/en/jobs/2401416-applied-scientist-supply-planning-and-analytics-w-m-d/?gh_src=gk03hq\n",
      "stored: ../data/database/json/za/3b124d8907b79fdbd172e749aa4f48e4.json\n",
      "DONE parsed 18 items\n",
      "za_pr\n",
      "0 https://research.zalando.com/welcome/mission/research-projects/adversarial-learning/\n",
      "stored: ../data/database/json/za/3accc8fbcbf4d69aa97f0acdb11cdc70.json\n",
      "1 https://research.zalando.com/welcome/mission/research-projects/determinantal-point-processes/\n",
      "stored: ../data/database/json/za/2d52ff1fefc6561bd032a1258501ef7e.json\n",
      "2 https://research.zalando.com/welcome/mission/research-projects/improving-fashion-item-encoding-and-retrieval/\n",
      "stored: ../data/database/json/za/f170fb7da88d5d8d636859ea78c6dce0.json\n",
      "3 https://research.zalando.com/welcome/mission/research-projects/swapping-of-fashion-on-people-images-with-generative-modelling/\n",
      "stored: ../data/database/json/za/d515093a9b81889cf069ea7e9373ca88.json\n",
      "4 https://research.zalando.com/welcome/mission/research-projects/fashion-renderer/\n",
      "stored: ../data/database/json/za/afef5ffc288f9e6e33fbe66106208205.json\n",
      "5 https://research.zalando.com/welcome/mission/research-projects/forecasting-the-customers-preference/\n",
      "stored: ../data/database/json/za/a5319701aa2214a40c5f6d19a54eba41.json\n",
      "6 https://research.zalando.com/welcome/mission/research-projects/generative-fashion-design/\n",
      "stored: ../data/database/json/za/ddf211190c0222098003f68accb71726.json\n",
      "7 https://research.zalando.com/welcome/mission/research-projects/syntax-aware-language-modeling-using-recurrent-neural-networks-salms/\n",
      "stored: ../data/database/json/za/4d39881e9c1905a02fd1fbf386ab78da.json\n",
      "8 https://research.zalando.com/welcome/mission/research-projects/personalized-size-recommendation/\n",
      "stored: ../data/database/json/za/11093393e67b12300aecc44c40b862cb.json\n",
      "9 https://research.zalando.com/welcome/mission/research-projects/robust-reinforcement-learning/\n",
      "stored: ../data/database/json/za/118fc8b78ced53f01ae35aab7e9ccf51.json\n",
      "10 https://research.zalando.com/welcome/mission/research-projects/sample-efficient-reinforcement-learning/\n",
      "stored: ../data/database/json/za/0edb5dba756562ec79549c98fbd4c294.json\n",
      "DONE parsed 11 items\n",
      "za_pu\n",
      "0 https://arxiv.org/abs/2010.15778\n",
      "stored: ../data/database/json/za/562c844cf4c55555359285a96320ce3a.json\n",
      "1 https://research.zalando.com/welcome/mission/publications/\n",
      "stored: ../data/database/json/za/46f0950641aec1ee02e68121212d1f02.json\n",
      "2 https://research.zalando.com/welcome/mission/publications/\n",
      "stored: ../data/database/json/za/46f0950641aec1ee02e68121212d1f02.json\n",
      "3 https://research.zalando.com/welcome/mission/publications/\n",
      "stored: ../data/database/json/za/46f0950641aec1ee02e68121212d1f02.json\n",
      "4 https://research.zalando.com/welcome/mission/publications/\n",
      "stored: ../data/database/json/za/46f0950641aec1ee02e68121212d1f02.json\n",
      "5 https://research.zalando.com/welcome/mission/publications/\n",
      "stored: ../data/database/json/za/46f0950641aec1ee02e68121212d1f02.json\n",
      "6 https://epubs.siam.org/doi/pdf/10.1137/1.9781611976236.7\n",
      "stored: ../data/database/json/za/398ad9714a8c7ce06ebce88f214bcc20.json\n",
      "7 https://arxiv.org/abs/1905.11784\n",
      "stored: ../data/database/json/za/c0a92981922068f89fe55a9423438b4d.json\n",
      "8 https://arxiv.org/abs/1910.07236\n",
      "stored: ../data/database/json/za/b87d88994c4e5107d0f0374aee6f852c.json\n",
      "9 https://arxiv.org/abs/1908.08847\n",
      "10 https://link.springer.com/article/10.1007/s00332-019-09574-z\n",
      "stored: ../data/database/json/za/1d281345eaab559c064b1d62b65f6ce0.json\n",
      "11 https://arxiv.org/abs/1907.09844\n",
      "stored: ../data/database/json/za/32895656c025666a8106d9cb42a9f25b.json\n",
      "12 https://arxiv.org/abs/1906.09400\n",
      "stored: ../data/database/json/za/d04448ff52b7de4fbb463bf8af51f0d1.json\n",
      "13 https://arxiv.org/abs/1902.03657\n",
      "stored: ../data/database/json/za/022b16eb66844bbf9a7e37728df816a9.json\n",
      "14 https://arxiv.org/abs/1811.09236\n",
      "stored: ../data/database/json/za/f6b2f92385cecdd3030b85e7685af19c.json\n",
      "15 https://rguigoures.github.io/pdf/hierarchical-bayesian-model_final.pdf\n",
      "stored: ../data/database/json/za/45638e845ec88dd1669ca9eac60cba19.json\n",
      "16 https://arxiv.org/abs/1806.07819\n",
      "17 https://arxiv.org/abs/1802.04591\n",
      "stored: ../data/database/json/za/27b779b58c427a1da10e43688bb8ca1b.json\n",
      "18 https://arxiv.org/abs/1803.03665\n",
      "stored: ../data/database/json/za/9980b0b7e9047840f9d6a43f2613ba26.json\n",
      "19 https://link.springer.com/chapter/10.1007/978-3-030-05499-1_1\n",
      "stored: ../data/database/json/za/73c6d6a947adb4cb56d127de0b981169.json\n",
      "20 http://www.scitepress.org/PublicationsDetail.aspx?ID=hT0FJ9RWeJs=&t=1\n",
      "stored: ../data/database/json/za/9f1d11e7a6d8b3c07a86eab35749feb5.json\n",
      "21 https://lld-workshop.github.io/papers/LLD_2017_paper_6.pdf\n",
      "22 https://arxiv.org/abs/1712.01141\n",
      "stored: ../data/database/json/za/8c01c73f6c4d63a4b5f1350afef72d39.json\n",
      "23 https://arxiv.org/abs/1712.00269\n",
      "stored: ../data/database/json/za/68801ed51833a5631610c6ac60f4eeb2.json\n",
      "24 https://arxiv.org/abs/1709.04695\n",
      "stored: ../data/database/json/za/fbaf6c8c3aad6e4212ccaa3289897559.json\n",
      "25 http://www.aclweb.org/anthology/D17-1205\n",
      "stored: ../data/database/json/za/c71e4050807be39abde8fc69eb95b24e.json\n",
      "26 https://arxiv.org/abs/1708.07747\n",
      "stored: ../data/database/json/za/adfb22cd24b85c636d33ca2a5d30d048.json\n",
      "27 https://research.zalando.com/welcome/mission/publications/\n",
      "stored: ../data/database/json/za/46f0950641aec1ee02e68121212d1f02.json\n",
      "28 http://proceedings.mlr.press/v70/bergmann17a.html\n",
      "stored: ../data/database/json/za/6122446f4ce5eda82269b116031a68f9.json\n",
      "29 https://arxiv.org/abs/1611.08207\n",
      "stored: ../data/database/json/za/d380d9359fa9b3bedc62479a181f5db9.json\n",
      "30 https://arxiv.org/abs/1609.02489\n",
      "stored: ../data/database/json/za/bc195ba0fe8ed602aa3e4a6e0c7f01a6.json\n",
      "DONE parsed 31 items\n"
     ]
    }
   ],
   "source": [
    "# run the loop\n",
    "\n",
    "#transform = ['ka_c', 'ka_cn', 'ka_d', 'ka_dn', 'ma', 'gh', 'tcp', 'bc']\n",
    "transform = ['ka_c', 'ka_cn', 'ma', 'gh', 'tcp', 'bc', 'me_ft', 'bcg_fo', 'bcg_ha',\n",
    "             'me_ft', 'bcg_fo', 'bcg_ha', 'za_bl', 'za_jo', 'za_pr', 'za_pu']\n",
    "transform = ['za_bl', 'za_jo', 'za_pr', 'za_pu']\n",
    "\n",
    "datasets = {\n",
    "    # kaggle competitions\n",
    "    'ka_c': {\n",
    "        'csv_in': '../data/database/kaggle_competitions_correlated_01.csv',\n",
    "        'csv_format': 'kaggle_competition',\n",
    "    },\n",
    "    # kaggle competitions notebooks\n",
    "    'ka_cn': {\n",
    "        'csv_in': '../data/database/kaggle_competitions_01_original.csv',\n",
    "        'csv_format': 'kaggle_notebook',\n",
    "    },\n",
    "    # kaggle datasets\n",
    "    'ka_d': {\n",
    "        'csv_in': '../data/database/kaggle_datasets_correlated_01.csv',\n",
    "        'csv_format': 'kaggle_dataset',\n",
    "    },\n",
    "    # kaggle datasets notebooks\n",
    "    'ka_dn': {\n",
    "        'csv_in': '../data/database/kaggle_datasets_01_original.csv',\n",
    "        'csv_format': 'kaggle_notebook',\n",
    "    },\n",
    "    # mlart\n",
    "    'ma': {\n",
    "        'csv_in': '../data/database/mlart_01_original.csv',\n",
    "        'csv_format':'mlart',\n",
    "        'extra': {\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 1,\n",
    "            'compete_score': 0,\n",
    "        },\n",
    "    },\n",
    "    # github\n",
    "    'gh': {\n",
    "        'csv_in': '../data/database/db_04_analyzed_v02.csv',\n",
    "        'csv_format': 'github',\n",
    "        'extra': {\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0.25,\n",
    "        },\n",
    "    },\n",
    "    # thecleverprogrammer\n",
    "    'tcp': {\n",
    "        'csv_in': '../data/database/thecleverprogrammer_01_original.csv',\n",
    "        'csv_format': 'tcp',\n",
    "        'extra': {\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "        },\n",
    "    },\n",
    "    # blobcity\n",
    "    'bc': {\n",
    "        'csv_in': '../data/database/blobcity_02_analyzed.csv',\n",
    "        'csv_format': 'github',\n",
    "        'extra': {\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0,\n",
    "        },\n",
    "    },\n",
    "    # medium_fintech\n",
    "    'me_ft': {\n",
    "        'csv_in': '../data/database/medium_fintech_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'medium.com',\n",
    "            'kind': 'Article',\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 1,\n",
    "        },\n",
    "        'out': 'me'\n",
    "    },\n",
    "    # bcgdv founded\n",
    "    'bcg_fo': {\n",
    "        'csv_in': '../data/database/bcgdv_founded_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'bcgdv.com',\n",
    "            'kind': 'Article',\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 1,\n",
    "        },\n",
    "        'out': 'bcg'\n",
    "    },\n",
    "    # bcgdv hackaton\n",
    "    'bcg_ha': {\n",
    "        'csv_in': '../data/database/bcgdv_hackaton_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'bcgdv.com',\n",
    "            'kind': ['Article', 'Project'],\n",
    "            'learn_score': 1,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 1,\n",
    "        },\n",
    "        'out': 'bcg'\n",
    "    },\n",
    "    # zalando blog\n",
    "    'za_bl': {\n",
    "        'csv_in': '../data/database/zalando_blog_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'zalando.com',\n",
    "            'kind': 'Article'\n",
    "        },\n",
    "        'out': 'za'\n",
    "    },\n",
    "    # zalando jobs\n",
    "    'za_jo': {\n",
    "        'csv_in': '../data/database/zalando_jobs_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'zalando.com',\n",
    "            'kind': 'Article',\n",
    "            'tags': ['Fashion'],\n",
    "            'learn_score': 0,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 1,\n",
    "        },\n",
    "        'out': 'za'\n",
    "    },\n",
    "    # zalando research projects\n",
    "    'za_pr': {\n",
    "        'csv_in': '../data/database/zalando_projects_01.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'zalando.com',\n",
    "            'kind': 'Article',\n",
    "        },\n",
    "        'out': 'za'\n",
    "    },\n",
    "    # zalando research publications\n",
    "    'za_pu': {\n",
    "        'csv_in': '../data/database/zalando_publications_04.csv',\n",
    "        'csv_format': 'manual',\n",
    "        'extra': {\n",
    "            'host': 'zalando.com',\n",
    "            'kind': 'Article',\n",
    "            'date_scraped': datetime.strptime('17.01.2021', \"%d.%m.%Y\"),\n",
    "            'tags': ['Fashion'],\n",
    "            'learn_score': 0.5,\n",
    "            'explore_score': 0,\n",
    "            'compete_score': 0.75,\n",
    "        },\n",
    "        'out': 'za'\n",
    "    },\n",
    "}\n",
    "\n",
    "    \n",
    "for key in transform:\n",
    "    print(key)\n",
    "    item = datasets[key]\n",
    "    extra = item['extra'] if 'extra' in item else {}\n",
    "    out = key+'/' if not 'out' in item else item['out']+'/'\n",
    "    printItem=False\n",
    "    transform_loop(item['csv_in'], item['csv_format'], out, overwrite=True, extra=extra, printItem=printItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot categorization is computational intense\n",
    "# so let's keep it out from the loop and process it seperatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat)\n",
    "print(subcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "\n",
    "folder = '../data/database/json/'\n",
    "subfolder = os.listdir(folder)\n",
    "#print(subfolder)\n",
    "\n",
    "#transform = ['ka_c', 'ka_cn', 'ka_d', 'ka_dn', 'ma', 'gh', 'tcp', 'bc']\n",
    "transform = ['ka_c', 'ka_cn', 'ma', 'gh', 'tcp', 'bc']\n",
    "#transform = ['ma']\n",
    "\n",
    "recreate_category = False\n",
    "save = True\n",
    "categorzie_t5 = False\n",
    "categorize_nltk = True\n",
    "categorize_fallback = True\n",
    "\n",
    "quit = 0\n",
    "i = j = 0\n",
    "for item in subfolder:\n",
    "    print('folder', item)\n",
    "    fp = os.path.join(folder, item)\n",
    "    if os.path.isdir(fp) and item in transform:\n",
    "        print('###')\n",
    "        print(item)\n",
    "        files = os.listdir(fp)\n",
    "        print('files in folder:', len(files))\n",
    "        for file in files:\n",
    "            row = load_data(os.path.join(folder, item, file), fromJson=True)\n",
    "            #print(row)\n",
    "            \n",
    "            print('row:', i, 'item:', j, 'link:', row['link'], 'file:', file)\n",
    "            \n",
    "            # zero shot categorization\n",
    "            if not 'category' in row or row.get('category') == '' or recreate_category == True:\n",
    "                print('categorize')\n",
    "                start = time.time()\n",
    "                j += 1\n",
    "\n",
    "                # create category and subcategory from t5\n",
    "                if 'sum_t5' in row and row['sum_t5'] != '' and categorzie_t5 == True:\n",
    "                    s = row['sum_t5']\n",
    "                    res = categorize(s, cat)\n",
    "                    #row['t5_category_raw'] = res\n",
    "                    c = row['t5_category'] = res['category']\n",
    "                    c_score = row['t5_category_score'] = res['score']\n",
    "                    row['t5_category_runtime'] = res['runtime']\n",
    "                    print('t5 category', res['runtime'], 'sec')\n",
    "\n",
    "                    res = categorize(s, subcat)\n",
    "                    #row['t5_subcategory_raw'] = res\n",
    "                    sc = row['t5_subcategory'] = res['category']\n",
    "                    sc_score = row['t5_subcategory_score'] = res['score']\n",
    "                    row['t5_subcategory_runtime'] = res['runtime']\n",
    "                    print('t5 subcategory', res['runtime'], 'sec')\n",
    "                else:\n",
    "                    print('t5 skipped')\n",
    "\n",
    "                # create category and subcategory from nltk\n",
    "                if 'sum_nltk' in row and row['sum_nltk'] != '' and categorize_nltk == True:\n",
    "                    s = row['sum_nltk']\n",
    "                    res = categorize(s, cat)\n",
    "                    #print(res)\n",
    "                    #row['nltk_category_raw'] = res\n",
    "                    c = row['nltk_category'] = res['category']\n",
    "                    c_score = row['nltk_category_score'] = res['score']\n",
    "                    row['nltk_category_runtime'] = res['runtime']\n",
    "                    print('nltk category', res['runtime'], 'sec')\n",
    "\n",
    "                    res = categorize(s, subcat)\n",
    "                    #print(res)\n",
    "                    #row['nltk_subcategory_raw'] = res\n",
    "                    sc = row['nltk_subcategory'] = res['category']\n",
    "                    sc_score = row['nltk_subcategory_score'] = res['score']\n",
    "                    row['nltk_subcategory_runtime'] = res['runtime']\n",
    "                    print('nltk subcategory', res['runtime'], 'sec')\n",
    "                else:\n",
    "                    print('nltk skipped')\n",
    "\n",
    "                # create category and subcategory from title or description if not already done\n",
    "                if categorize_fallback == True and not 't5_category' in row and not 'nltk_category' in row:\n",
    "                    if len(row['description']) > 0:\n",
    "                        s = row['description']\n",
    "                        res = categorize(s, cat)\n",
    "                        #row['description_category_raw'] = res\n",
    "                        c = row['description_category'] = res['category']\n",
    "                        c_score = row['description_category_score'] = res['score']\n",
    "                        row['description_category_runtime'] = res['runtime']\n",
    "                        print('description category', res['runtime'], 'sec')\n",
    "\n",
    "                        res = categorize(s, subcat)\n",
    "                        #row['description_subcategory_raw'] = res\n",
    "                        sc = row['description_subcategory'] = res['category']\n",
    "                        sc_score = row['description_subcategory_score'] = res['score']\n",
    "                        row['description_subcategory_runtime'] = res['runtime']\n",
    "                        print('description subcategory', res['runtime'], 'sec')\n",
    "                    else:\n",
    "                        s = row['title']\n",
    "                        if s != '':\n",
    "                            res = categorize(s, cat)\n",
    "                            #row['title_category_raw'] = res\n",
    "                            c = row['title_category'] = res['category']\n",
    "                            c_score = row['title_category_score'] = res['score']\n",
    "                            row['title_category_runtime'] = res['runtime']\n",
    "                            print('title category', res['runtime'], 'sec')\n",
    "\n",
    "                            res = categorize(s, subcat)\n",
    "                            #row['title_subcategory_raw'] = res\n",
    "                            sc = row['title_subcategory'] = res['category']\n",
    "                            sc_score = row['title_subcategory_score'] = res['score']\n",
    "                            row['title_subcategory_runtime'] = res['runtime']\n",
    "                            print('title subcategory', res['runtime'], 'sec')\n",
    "                        else:\n",
    "                            print('nothing found to categorize')\n",
    "                            c = sc = ''\n",
    "                            c_score = sc_score = 0\n",
    "                            j -= 1\n",
    "\n",
    "                row['category'] = c\n",
    "                row['category_score'] = c_score\n",
    "                row['subcategory'] = sc\n",
    "                row['subcategory_score'] = sc_score\n",
    "\n",
    "                end = time.time()\n",
    "                dur = round(end-start, 3)\n",
    "                row['runtime_cat'] = dur\n",
    "                \n",
    "                fp = os.path.join(folder, item, file)\n",
    "                if save == True:\n",
    "                    store_data(row, fp, toJson=True)\n",
    "                else:\n",
    "                    print('NOT SAVED')\n",
    "                    print(row)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            \n",
    "            if quit!= 0 and i >= quit:\n",
    "                break\n",
    "    if quit!= 0 and i >= quit:\n",
    "                break\n",
    "            \n",
    "print('DONE parsed', i, 'items')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
